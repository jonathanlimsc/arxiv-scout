{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2eb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import cohere\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582db428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJ_DIR = Path.cwd().parent\n",
    "DOTENV_PATH = PROJ_DIR / '.env'\n",
    "dotenv.load_dotenv(DOTENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "94c50e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_arxiv_articles_df(num_articles=10):\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=cat:cs.AI+OR+cat:cs.CV+OR+cat:cs.LG+OR+cat:cs.MA+OR+cat:cs.NE+OR+cat:cs.RO+OR+cat:cs.SD&start=0&max_results={num_articles}\"\n",
    "\n",
    "    payload={}\n",
    "    headers = {}\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    parsed_xml = xmltodict.parse(response.text)\n",
    "    \n",
    "    articles_dict = {'link': [],\n",
    "                     'updated_ts': [],\n",
    "                     'published_ts': [],\n",
    "                     'title': [],\n",
    "                     'summary': [],\n",
    "                     'author': [],\n",
    "                     'category': [],\n",
    "                    }\n",
    "    \n",
    "    for article in parsed_xml['feed']['entry']:\n",
    "        try:\n",
    "            articles_dict['link'].append(article['id'])\n",
    "            articles_dict['updated_ts'].append(article['updated'])\n",
    "            articles_dict['published_ts'].append(article['published'])\n",
    "            articles_dict['title'].append(article['title'])\n",
    "            articles_dict['summary'].append(article['summary'])\n",
    "            articles_dict['author'].append(\", \".join([author['name'] for author in article['author']]) if isinstance(article['author'], list) else article['author']['name'])\n",
    "            articles_dict['category'].append(article['arxiv:primary_category']['@term'])\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            continue\n",
    "\n",
    "    articles_df = pd.DataFrame.from_dict(articles_dict)\n",
    "    \n",
    "    return articles_df, parsed_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "131c2072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 157 ms, sys: 10.8 ms, total: 168 ms\n",
      "Wall time: 6.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles_df, parsed_xml = retrieve_arxiv_articles_df(num_articles=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "30092baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feed': {'@xmlns': 'http://www.w3.org/2005/Atom',\n",
       "  'link': {'@href': 'http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%20OR%20cat%3Acs.CV%20OR%20cat%3Acs.LG%20OR%20cat%3Acs.MA%20OR%20cat%3Acs.NE%20OR%20cat%3Acs.RO%20OR%20cat%3Acs.SD%26id_list%3D%26start%3D0%26max_results%3D1000',\n",
       "   '@rel': 'self',\n",
       "   '@type': 'application/atom+xml'},\n",
       "  'title': {'@type': 'html',\n",
       "   '#text': 'ArXiv Query: search_query=cat:cs.AI OR cat:cs.CV OR cat:cs.LG OR cat:cs.MA OR cat:cs.NE OR cat:cs.RO OR cat:cs.SD&id_list=&start=0&max_results=1000'},\n",
       "  'id': 'http://arxiv.org/api/YrhbSbUjZDECGfakQABax52OiAA',\n",
       "  'updated': '2023-01-29T00:00:00-05:00',\n",
       "  'opensearch:totalResults': {'@xmlns:opensearch': 'http://a9.com/-/spec/opensearch/1.1/',\n",
       "   '#text': '246955'},\n",
       "  'opensearch:startIndex': {'@xmlns:opensearch': 'http://a9.com/-/spec/opensearch/1.1/',\n",
       "   '#text': '0'},\n",
       "  'opensearch:itemsPerPage': {'@xmlns:opensearch': 'http://a9.com/-/spec/opensearch/1.1/',\n",
       "   '#text': '1000'},\n",
       "  'entry': [{'id': 'http://arxiv.org/abs/1709.06620v1',\n",
       "    'updated': '2017-09-19T19:26:20Z',\n",
       "    'published': '2017-09-19T19:26:20Z',\n",
       "    'title': 'Learning of Coordination Policies for Robotic Swarms',\n",
       "    'summary': 'Inspired by biological swarms, robotic swarms are envisioned to solve\\nreal-world problems that are difficult for individual agents. Biological swarms\\ncan achieve collective intelligence based on local interactions and simple\\nrules; however, designing effective distributed policies for large-scale\\nrobotic swarms to achieve a global objective can be challenging. Although it is\\noften possible to design an optimal centralized strategy for smaller numbers of\\nagents, those methods can fail as the number of agents increases. Motivated by\\nthe growing success of machine learning, we develop a deep learning approach\\nthat learns distributed coordination policies from centralized policies. In\\ncontrast to traditional distributed control approaches, which are usually based\\non human-designed policies for relatively simple tasks, this learning-based\\napproach can be adapted to more difficult tasks. We demonstrate the efficacy of\\nour proposed approach on two different tasks, the well-known rendezvous problem\\nand a more difficult particle assignment problem. For the latter, no known\\ndistributed policy exists. From extensive simulations, it is shown that the\\nperformance of the learned coordination policies is comparable to the\\ncentralized policies, surpassing state-of-the-art distributed policies.\\nThereby, our proposed approach provides a promising alternative for real-world\\ncoordination problems that would be otherwise computationally expensive to\\nsolve or intangible to explore.',\n",
       "    'author': [{'name': 'Qiyang Li'},\n",
       "     {'name': 'Xintong Du'},\n",
       "     {'name': 'Yizhou Huang'},\n",
       "     {'name': 'Quinlan Sykora'},\n",
       "     {'name': 'Angela P. Schoellig'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 11 figures, submitted to 2018 IEEE International Conference\\n  on Robotics and Automation'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.06620v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.06620v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.05605v2',\n",
       "    'updated': '2020-11-20T18:19:32Z',\n",
       "    'published': '2020-11-11T07:35:21Z',\n",
       "    'title': 'Decentralized Motion Planning for Multi-Robot Navigation using Deep\\n  Reinforcement Learning',\n",
       "    'summary': 'This work presents a decentralized motion planning framework for addressing\\nthe task of multi-robot navigation using deep reinforcement learning. A custom\\nsimulator was developed in order to experimentally investigate the navigation\\nproblem of 4 cooperative non-holonomic robots sharing limited state information\\nwith each other in 3 different settings. The notion of decentralized motion\\nplanning with common and shared policy learning was adopted, which allowed\\nrobust training and testing of this approach in a stochastic environment since\\nthe agents were mutually independent and exhibited asynchronous motion\\nbehavior. The task was further aggravated by providing the agents with a sparse\\nobservation space and requiring them to generate continuous action commands so\\nas to efficiently, yet safely navigate to their respective goal locations,\\nwhile avoiding collisions with other dynamic peers and static obstacles at all\\ntimes. The experimental results are reported in terms of quantitative measures\\nand qualitative remarks for both training and deployment phases.',\n",
       "    'author': [{'name': 'Sivanathan Kandhasamy'},\n",
       "     {'name': 'Vinayagam Babu Kuppusamy'},\n",
       "     {'name': 'Tanmay Vilas Samak'},\n",
       "     {'name': 'Chinmay Vilas Samak'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICISS49785.2020.9316033'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICISS49785.2020.9316033',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2011.05605v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.05605v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at IEEE International Conference on Intelligent Sustainable\\n  Systems (ICISS) 2020'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '3rd International Conference on Intelligent Sustainable Systems\\n  (ICISS), Thoothukudi, India, pp. 709-716, 2020'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.14745v2',\n",
       "    'updated': '2022-12-29T08:48:05Z',\n",
       "    'published': '2022-09-29T13:02:58Z',\n",
       "    'title': 'A Multiagent Framework for the Asynchronous and Collaborative Extension\\n  of Multitask ML Systems',\n",
       "    'summary': 'The traditional ML development methodology does not enable a large number of\\ncontributors, each with distinct objectives, to work collectively on the\\ncreation and extension of a shared intelligent system. Enabling such a\\ncollaborative methodology can accelerate the rate of innovation, increase ML\\ntechnologies accessibility and enable the emergence of novel capabilities. We\\nbelieve that this novel methodology for ML development can be demonstrated\\nthrough a modularized representation of ML models and the definition of novel\\nabstractions allowing to implement and execute diverse methods for the\\nasynchronous use and extension of modular intelligent systems. We present a\\nmultiagent framework for the collaborative and asynchronous extension of\\ndynamic large-scale multitask systems.',\n",
       "    'author': {'name': 'Andrea Gesmundo'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: text overlap with arXiv:2209.07326'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.14745v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.14745v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.02608v1',\n",
       "    'updated': '2020-11-05T01:47:23Z',\n",
       "    'published': '2020-11-05T01:47:23Z',\n",
       "    'title': 'Learning a Decentralized Multi-arm Motion Planner',\n",
       "    'summary': 'We present a closed-loop multi-arm motion planner that is scalable and\\nflexible with team size. Traditional multi-arm robot systems have relied on\\ncentralized motion planners, whose runtimes often scale exponentially with team\\nsize, and thus, fail to handle dynamic environments with open-loop control. In\\nthis paper, we tackle this problem with multi-agent reinforcement learning,\\nwhere a decentralized policy is trained to control one robot arm in the\\nmulti-arm system to reach its target end-effector pose given observations of\\nits workspace state and target end-effector pose. The policy is trained using\\nSoft Actor-Critic with expert demonstrations from a sampling-based motion\\nplanning algorithm (i.e., BiRRT). By leveraging classical planning algorithms,\\nwe can improve the learning efficiency of the reinforcement learning algorithm\\nwhile retaining the fast inference time of neural networks. The resulting\\npolicy scales sub-linearly and can be deployed on multi-arm systems with\\nvariable team sizes. Thanks to the closed-loop and decentralized formulation,\\nour approach generalizes to 5-10 multi-arm systems and dynamic moving targets\\n(>90% success rate for a 10-arm system), despite being trained on only 1-4 arm\\nplanning tasks with static targets. Code and data links can be found at\\nhttps://multiarm.cs.columbia.edu.',\n",
       "    'author': [{'name': 'Huy Ha'},\n",
       "     {'name': 'Jingxi Xu'},\n",
       "     {'name': 'Shuran Song'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.02608v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.02608v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.05894v1',\n",
       "    'updated': '2020-12-10T18:55:51Z',\n",
       "    'published': '2020-12-10T18:55:51Z',\n",
       "    'title': 'AutoSelect: Automatic and Dynamic Detection Selection for 3D\\n  Multi-Object Tracking',\n",
       "    'summary': '3D multi-object tracking is an important component in robotic perception\\nsystems such as self-driving vehicles. Recent work follows a\\ntracking-by-detection pipeline, which aims to match past tracklets with\\ndetections in the current frame. To avoid matching with false positive\\ndetections, prior work filters out detections with low confidence scores via a\\nthreshold. However, finding a proper threshold is non-trivial, which requires\\nextensive manual search via ablation study. Also, this threshold is sensitive\\nto many factors such as target object category so we need to re-search the\\nthreshold if these factors change. To ease this process, we propose to\\nautomatically select high-quality detections and remove the efforts needed for\\nmanual threshold search. Also, prior work often uses a single threshold per\\ndata sequence, which is sub-optimal in particular frames or for certain\\nobjects. Instead, we dynamically search threshold per frame or per object to\\nfurther boost performance. Through experiments on KITTI and nuScenes, our\\nmethod can filter out $45.7\\\\%$ false positives while maintaining the recall,\\nachieving new S.O.T.A. performance and removing the need for manually threshold\\ntuning.',\n",
       "    'author': [{'name': 'Xinshuo Weng'}, {'name': 'Kris Kitani'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.05894v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.05894v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.12710v1',\n",
       "    'updated': '2021-03-23T17:31:14Z',\n",
       "    'published': '2021-03-23T17:31:14Z',\n",
       "    'title': 'Spatial Intention Maps for Multi-Agent Mobile Manipulation',\n",
       "    'summary': \"The ability to communicate intention enables decentralized multi-agent robots\\nto collaborate while performing physical tasks. In this work, we present\\nspatial intention maps, a new intention representation for multi-agent\\nvision-based deep reinforcement learning that improves coordination between\\ndecentralized mobile manipulators. In this representation, each agent's\\nintention is provided to other agents, and rendered into an overhead 2D map\\naligned with visual observations. This synergizes with the recently proposed\\nspatial action maps framework, in which state and action representations are\\nspatially aligned, providing inductive biases that encourage emergent\\ncooperative behaviors requiring spatial coordination, such as passing objects\\nto each other or avoiding collisions. Experiments across a variety of\\nmulti-agent environments, including heterogeneous robot teams with different\\nabilities (lifting, pushing, or throwing), show that incorporating spatial\\nintention maps improves performance for different mobile manipulation tasks\\nwhile significantly enhancing cooperative behaviors.\",\n",
       "    'author': [{'name': 'Jimmy Wu'},\n",
       "     {'name': 'Xingyuan Sun'},\n",
       "     {'name': 'Andy Zeng'},\n",
       "     {'name': 'Shuran Song'},\n",
       "     {'name': 'Szymon Rusinkiewicz'},\n",
       "     {'name': 'Thomas Funkhouser'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICRA48506.2021.9561359'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICRA48506.2021.9561359',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2103.12710v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.12710v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear at IEEE International Conference on Robotics and Automation\\n  (ICRA), 2021. Project page: https://spatial-intention-maps.cs.princeton.edu/'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.14023v3',\n",
       "    'updated': '2021-10-07T05:04:39Z',\n",
       "    'published': '2021-03-25T17:59:01Z',\n",
       "    'title': 'AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent\\n  Forecasting',\n",
       "    'summary': \"Predicting accurate future trajectories of multiple agents is essential for\\nautonomous systems, but is challenging due to the complex agent interaction and\\nthe uncertainty in each agent's future behavior. Forecasting multi-agent\\ntrajectories requires modeling two key dimensions: (1) time dimension, where we\\nmodel the influence of past agent states over future states; (2) social\\ndimension, where we model how the state of each agent affects others. Most\\nprior methods model these two dimensions separately, e.g., first using a\\ntemporal model to summarize features over time for each agent independently and\\nthen modeling the interaction of the summarized features with a social model.\\nThis approach is suboptimal since independent feature encoding over either the\\ntime or social dimension can result in a loss of information. Instead, we would\\nprefer a method that allows an agent's state at one time to directly affect\\nanother agent's state at a future time. To this end, we propose a new\\nTransformer, AgentFormer, that jointly models the time and social dimensions.\\nThe model leverages a sequence representation of multi-agent trajectories by\\nflattening trajectory features across time and agents. Since standard attention\\noperations disregard the agent identity of each element in the sequence,\\nAgentFormer uses a novel agent-aware attention mechanism that preserves agent\\nidentities by attending to elements of the same agent differently than elements\\nof other agents. Based on AgentFormer, we propose a stochastic multi-agent\\ntrajectory prediction model that can attend to features of any agent at any\\nprevious timestep when inferring an agent's future position. The latent intent\\nof all agents is also jointly modeled, allowing the stochasticity in one\\nagent's behavior to affect other agents. Our method substantially improves the\\nstate of the art on well-established pedestrian and autonomous driving\\ndatasets.\",\n",
       "    'author': [{'name': 'Ye Yuan'},\n",
       "     {'name': 'Xinshuo Weng'},\n",
       "     {'name': 'Yanglan Ou'},\n",
       "     {'name': 'Kris Kitani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICCV 2021. Code: https://github.com/Khrylx/AgentFormer. Project page:\\n  https://www.ye-yuan.com/agentformer'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.14023v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.14023v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.05145v1',\n",
       "    'updated': '2021-05-11T16:02:32Z',\n",
       "    'published': '2021-05-11T16:02:32Z',\n",
       "    'title': 'Visual Perspective Taking for Opponent Behavior Modeling',\n",
       "    'summary': \"In order to engage in complex social interaction, humans learn at a young age\\nto infer what others see and cannot see from a different point-of-view, and\\nlearn to predict others' plans and behaviors. These abilities have been mostly\\nlacking in robots, sometimes making them appear awkward and socially inept.\\nHere we propose an end-to-end long-term visual prediction framework for robots\\nto begin to acquire both these critical cognitive skills, known as Visual\\nPerspective Taking (VPT) and Theory of Behavior (TOB). We demonstrate our\\napproach in the context of visual hide-and-seek - a game that represents a\\ncognitive milestone in human development. Unlike traditional visual predictive\\nmodel that generates new frames from immediate past frames, our agent can\\ndirectly predict to multiple future timestamps (25s), extrapolating by 175%\\nbeyond the training horizon. We suggest that visual behavior modeling and\\nperspective taking skills will play a critical role in the ability of physical\\nrobots to fully integrate into real-world multi-agent activities. Our website\\nis at http://www.cs.columbia.edu/~bchen/vpttob/.\",\n",
       "    'author': [{'name': 'Boyuan Chen'},\n",
       "     {'name': 'Yuhang Hu'},\n",
       "     {'name': 'Robert Kwiatkowski'},\n",
       "     {'name': 'Shuran Song'},\n",
       "     {'name': 'Hod Lipson'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2021. Website: http://www.cs.columbia.edu/~bchen/vpttob/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.05145v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.05145v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.08236v3',\n",
       "    'updated': '2021-09-17T16:38:31Z',\n",
       "    'published': '2021-08-18T16:57:03Z',\n",
       "    'title': 'LOKI: Long Term and Key Intentions for Trajectory Prediction',\n",
       "    'summary': \"Recent advances in trajectory prediction have shown that explicit reasoning\\nabout agents' intent is important to accurately forecast their motion. However,\\nthe current research activities are not directly applicable to intelligent and\\nsafety critical systems. This is mainly because very few public datasets are\\navailable, and they only consider pedestrian-specific intents for a short\\ntemporal horizon from a restricted egocentric view. To this end, we propose\\nLOKI (LOng term and Key Intentions), a novel large-scale dataset that is\\ndesigned to tackle joint trajectory and intention prediction for heterogeneous\\ntraffic agents (pedestrians and vehicles) in an autonomous driving setting. The\\nLOKI dataset is created to discover several factors that may affect intention,\\nincluding i) agent's own will, ii) social interactions, iii) environmental\\nconstraints, and iv) contextual information. We also propose a model that\\njointly performs trajectory and intention prediction, showing that recurrently\\nreasoning about intention can assist with trajectory prediction. We show our\\nmethod outperforms state-of-the-art trajectory prediction methods by upto\\n$27\\\\%$ and also provide a baseline for frame-wise intention estimation.\",\n",
       "    'author': [{'name': 'Harshayu Girase'},\n",
       "     {'name': 'Haiming Gang'},\n",
       "     {'name': 'Srikanth Malla'},\n",
       "     {'name': 'Jiachen Li'},\n",
       "     {'name': 'Akira Kanehara'},\n",
       "     {'name': 'Karttikeya Mangalam'},\n",
       "     {'name': 'Chiho Choi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICCV 2021 (The dataset is available at https://usa.honda-ri.com/loki)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.08236v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.08236v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.02173v3',\n",
       "    'updated': '2022-03-02T20:37:01Z',\n",
       "    'published': '2021-09-05T21:56:54Z',\n",
       "    'title': 'Multi-Agent Variational Occlusion Inference Using People as Sensors',\n",
       "    'summary': 'Autonomous vehicles must reason about spatial occlusions in urban\\nenvironments to ensure safety without being overly cautious. Prior work\\nexplored occlusion inference from observed social behaviors of road agents,\\nhence treating people as sensors. Inferring occupancy from agent behaviors is\\nan inherently multimodal problem; a driver may behave similarly for different\\noccupancy patterns ahead of them (e.g., a driver may move at constant speed in\\ntraffic or on an open road). Past work, however, does not account for this\\nmultimodality, thus neglecting to model this source of aleatoric uncertainty in\\nthe relationship between driver behaviors and their environment. We propose an\\nocclusion inference method that characterizes observed behaviors of human\\nagents as sensor measurements, and fuses them with those from a standard sensor\\nsuite. To capture the aleatoric uncertainty, we train a conditional variational\\nautoencoder with a discrete latent space to learn a multimodal mapping from\\nobserved driver trajectories to an occupancy grid representation of the view\\nahead of the driver. Our method handles multi-agent scenarios, combining\\nmeasurements from multiple observed drivers using evidential theory to solve\\nthe sensor fusion problem. Our approach is validated on a cluttered, real-world\\nintersection, outperforming baselines and demonstrating real-time capable\\nperformance. Our code is available at\\nhttps://github.com/sisl/MultiAgentVariationalOcclusionInference .',\n",
       "    'author': [{'name': 'Masha Itkina'},\n",
       "     {'name': 'Ye-Ji Mun'},\n",
       "     {'name': 'Katherine Driggs-Campbell'},\n",
       "     {'name': 'Mykel J. Kochenderfer'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 9 figures, International Conference on Robotics and\\n  Automation (ICRA) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.02173v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.02173v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.10', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.07882v1',\n",
       "    'updated': '2019-10-15T01:27:09Z',\n",
       "    'published': '2019-10-15T01:27:09Z',\n",
       "    'title': 'Visual Hide and Seek',\n",
       "    'summary': 'We train embodied agents to play Visual Hide and Seek where a prey must\\nnavigate in a simulated environment in order to avoid capture from a predator.\\nWe place a variety of obstacles in the environment for the prey to hide behind,\\nand we only give the agents partial observations of their environment using an\\negocentric perspective. Although we train the model to play this game from\\nscratch, experiments and visualizations suggest that the agent learns to\\npredict its own visibility in the environment. Furthermore, we quantitatively\\nanalyze how agent weaknesses, such as slower speed, effect the learned policy.\\nOur results suggest that, although agent weaknesses make the learning problem\\nmore challenging, they also cause more useful features to be learned. Our\\nproject website is available at: http://www.cs.columbia.edu/\\n~bchen/visualhideseek/.',\n",
       "    'author': [{'name': 'Boyuan Chen'},\n",
       "     {'name': 'Shuran Song'},\n",
       "     {'name': 'Hod Lipson'},\n",
       "     {'name': 'Carl Vondrick'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages, 8 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1910.07882v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.07882v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.08376v3',\n",
       "    'updated': '2020-11-07T02:48:22Z',\n",
       "    'published': '2020-03-18T17:54:28Z',\n",
       "    'title': 'Inverting the Pose Forecasting Pipeline with SPF2: Sequential Pointcloud\\n  Forecasting for Sequential Pose Forecasting',\n",
       "    'summary': \"Many autonomous systems forecast aspects of the future in order to aid\\ndecision-making. For example, self-driving vehicles and robotic manipulation\\nsystems often forecast future object poses by first detecting and tracking\\nobjects. However, this detect-then-forecast pipeline is expensive to scale, as\\npose forecasting algorithms typically require labeled sequences of object\\nposes, which are costly to obtain in 3D space. Can we scale performance without\\nrequiring additional labels? We hypothesize yes, and propose inverting the\\ndetect-then-forecast pipeline. Instead of detecting, tracking and then\\nforecasting the objects, we propose to first forecast 3D sensor data (e.g.,\\npoint clouds with $100$k points) and then detect/track objects on the predicted\\npoint cloud sequences to obtain future poses, i.e., a forecast-then-detect\\npipeline. This inversion makes it less expensive to scale pose forecasting, as\\nthe sensor data forecasting task requires no labels. Part of this work's focus\\nis on the challenging first step -- Sequential Pointcloud Forecasting (SPF),\\nfor which we also propose an effective approach, SPFNet. To compare our\\nforecast-then-detect pipeline relative to the detect-then-forecast pipeline, we\\npropose an evaluation procedure and two metrics. Through experiments on a\\nrobotic manipulation dataset and two driving datasets, we show that SPFNet is\\neffective for the SPF task, our forecast-then-detect pipeline outperforms the\\ndetect-then-forecast approaches to which we compared, and that pose forecasting\\nperformance improves with the addition of unlabeled data.\",\n",
       "    'author': [{'name': 'Xinshuo Weng'},\n",
       "     {'name': 'Jianren Wang'},\n",
       "     {'name': 'Sergey Levine'},\n",
       "     {'name': 'Kris Kitani'},\n",
       "     {'name': 'Nicholas Rhinehart'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in Conference on Robot Learning (CoRL), 2020. Project\\n  webpage: http://www.xinshuoweng.com/projects/SPF2/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.08376v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.08376v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.12760v1',\n",
       "    'updated': '2020-08-28T17:35:22Z',\n",
       "    'published': '2020-08-28T17:35:22Z',\n",
       "    'title': 'AllenAct: A Framework for Embodied AI Research',\n",
       "    'summary': 'The domain of Embodied AI, in which agents learn to complete tasks through\\ninteraction with their environment from egocentric observations, has\\nexperienced substantial growth with the advent of deep reinforcement learning\\nand increased interest from the computer vision, NLP, and robotics communities.\\nThis growth has been facilitated by the creation of a large number of simulated\\nenvironments (such as AI2-THOR, Habitat and CARLA), tasks (like point\\nnavigation, instruction following, and embodied question answering), and\\nassociated leaderboards. While this diversity has been beneficial and organic,\\nit has also fragmented the community: a huge amount of effort is required to do\\nsomething as simple as taking a model trained in one environment and testing it\\nin another. This discourages good science. We introduce AllenAct, a modular and\\nflexible learning framework designed with a focus on the unique requirements of\\nEmbodied AI research. AllenAct provides first-class support for a growing\\ncollection of embodied environments, tasks and algorithms, provides\\nreproductions of state-of-the-art models and includes extensive documentation,\\ntutorials, start-up code, and pre-trained models. We hope that our framework\\nmakes Embodied AI more accessible and encourages new researchers to join this\\nexciting area. The framework can be accessed at: https://allenact.org/',\n",
       "    'author': [{'name': 'Luca Weihs'},\n",
       "     {'name': 'Jordi Salvador'},\n",
       "     {'name': 'Klemen Kotar'},\n",
       "     {'name': 'Unnat Jain'},\n",
       "     {'name': 'Kuo-Hao Zeng'},\n",
       "     {'name': 'Roozbeh Mottaghi'},\n",
       "     {'name': 'Aniruddha Kembhavi'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.12760v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.12760v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.09117v1',\n",
       "    'updated': '2021-02-18T02:25:35Z',\n",
       "    'published': '2021-02-18T02:25:35Z',\n",
       "    'title': 'Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction\\n  and Tracking',\n",
       "    'summary': 'An effective understanding of the environment and accurate trajectory\\nprediction of surrounding dynamic obstacles are indispensable for intelligent\\nmobile systems (e.g. autonomous vehicles and social robots) to achieve safe and\\nhigh-quality planning when they navigate in highly interactive and crowded\\nscenarios. Due to the existence of frequent interactions and uncertainty in the\\nscene evolution, it is desired for the prediction system to enable relational\\nreasoning on different entities and provide a distribution of future\\ntrajectories for each agent. In this paper, we propose a generic generative\\nneural system (called STG-DAT) for multi-agent trajectory prediction involving\\nheterogeneous agents. The system takes a step forward to explicit interaction\\nmodeling by incorporating relational inductive biases with a dynamic graph\\nrepresentation and leverages both trajectory and scene context information. We\\nalso employ an efficient kinematic constraint layer applied to vehicle\\ntrajectory prediction. The constraint not only ensures physical feasibility but\\nalso enhances model performance. Moreover, the proposed prediction model can be\\neasily adopted by multi-target tracking frameworks. The tracking accuracy\\nproves to be improved by empirical results. The proposed system is evaluated on\\nthree public benchmark datasets for trajectory prediction, where the agents\\ncover pedestrians, cyclists and on-road vehicles. The experimental results\\ndemonstrate that our model achieves better performance than various baseline\\napproaches in terms of prediction and tracking accuracy.',\n",
       "    'author': [{'name': 'Jiachen Li'},\n",
       "     {'name': 'Hengbo Ma'},\n",
       "     {'name': 'Zhihao Zhang'},\n",
       "     {'name': 'Jinning Li'},\n",
       "     {'name': 'Masayoshi Tomizuka'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This work has been submitted to the IEEE for possible publication.\\n  Copyright may be transferred without notice, after which this version may no\\n  longer be accessible'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.09117v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.09117v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.00563v3',\n",
       "    'updated': '2022-02-11T04:59:43Z',\n",
       "    'published': '2021-02-19T18:53:26Z',\n",
       "    'title': 'Latent Variable Sequential Set Transformers For Joint Multi-Agent Motion\\n  Prediction',\n",
       "    'summary': 'Robust multi-agent trajectory prediction is essential for the safe control of\\nrobotic systems. A major challenge is to efficiently learn a representation\\nthat approximates the true joint distribution of contextual, social, and\\ntemporal information to enable planning. We propose Latent Variable Sequential\\nSet Transformers which are encoder-decoder architectures that generate\\nscene-consistent multi-agent trajectories. We refer to these architectures as\\n\"AutoBots\". The encoder is a stack of interleaved temporal and social\\nmulti-head self-attention (MHSA) modules which alternately perform equivariant\\nprocessing across the temporal and social dimensions. The decoder employs\\nlearnable seed parameters in combination with temporal and social MHSA modules\\nallowing it to perform inference over the entire future scene in a single\\nforward pass efficiently. AutoBots can produce either the trajectory of one\\nego-agent or a distribution over the future trajectories for all agents in the\\nscene. For the single-agent prediction case, our model achieves top results on\\nthe global nuScenes vehicle motion prediction leaderboard, and produces strong\\nresults on the Argoverse vehicle prediction challenge. In the multi-agent\\nsetting, we evaluate on the synthetic partition of TrajNet++ dataset to\\nshowcase the model\\'s socially-consistent predictions. We also demonstrate our\\nmodel on general sequences of sets and provide illustrative experiments\\nmodelling the sequential structure of the multiple strokes that make up symbols\\nin the Omniglot data. A distinguishing feature of AutoBots is that all models\\nare trainable on a single desktop GPU (1080 Ti) in under 48h.',\n",
       "    'author': [{'name': 'Roger Girgis'},\n",
       "     {'name': 'Florian Golemo'},\n",
       "     {'name': 'Felipe Codevilla'},\n",
       "     {'name': 'Martin Weiss'},\n",
       "     {'name': \"Jim Aldon D'Souza\"},\n",
       "     {'name': 'Samira Ebrahimi Kahou'},\n",
       "     {'name': 'Felix Heide'},\n",
       "     {'name': 'Christopher Pal'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '26 pages, 17 figures, 8 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.00563v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.00563v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.01316v1',\n",
       "    'updated': '2021-08-03T06:30:30Z',\n",
       "    'published': '2021-08-03T06:30:30Z',\n",
       "    'title': 'RAIN: Reinforced Hybrid Attention Inference Network for Motion\\n  Forecasting',\n",
       "    'summary': 'Motion forecasting plays a significant role in various domains (e.g.,\\nautonomous driving, human-robot interaction), which aims to predict future\\nmotion sequences given a set of historical observations. However, the observed\\nelements may be of different levels of importance. Some information may be\\nirrelevant or even distracting to the forecasting in certain situations. To\\naddress this issue, we propose a generic motion forecasting framework (named\\nRAIN) with dynamic key information selection and ranking based on a hybrid\\nattention mechanism. The general framework is instantiated to handle\\nmulti-agent trajectory prediction and human motion forecasting tasks,\\nrespectively. In the former task, the model learns to recognize the relations\\nbetween agents with a graph representation and to determine their relative\\nsignificance. In the latter task, the model learns to capture the temporal\\nproximity and dependency in long-term human motions. We also propose an\\neffective double-stage training pipeline with an alternating training strategy\\nto optimize the parameters in different modules of the framework. We validate\\nthe framework on both synthetic simulations and motion forecasting benchmarks\\nin different domains, demonstrating that our method not only achieves\\nstate-of-the-art forecasting performance, but also provides interpretable and\\nreasonable hybrid attention weights.',\n",
       "    'author': [{'name': 'Jiachen Li'},\n",
       "     {'name': 'Fan Yang'},\n",
       "     {'name': 'Hengbo Ma'},\n",
       "     {'name': 'Srikanth Malla'},\n",
       "     {'name': 'Masayoshi Tomizuka'},\n",
       "     {'name': 'Chiho Choi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICCV 2021 (Project website:\\n  https://jiachenli94.github.io/publications/RAIN/)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.01316v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.01316v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.10143v1',\n",
       "    'updated': '2021-12-19T13:20:45Z',\n",
       "    'published': '2021-12-19T13:20:45Z',\n",
       "    'title': 'RoboAssembly: Learning Generalizable Furniture Assembly Policy in a\\n  Novel Multi-robot Contact-rich Simulation Environment',\n",
       "    'summary': 'Part assembly is a typical but challenging task in robotics, where robots\\nassemble a set of individual parts into a complete shape. In this paper, we\\ndevelop a robotic assembly simulation environment for furniture assembly. We\\nformulate the part assembly task as a concrete reinforcement learning problem\\nand propose a pipeline for robots to learn to assemble a diverse set of chairs.\\nExperiments show that when testing with unseen chairs, our approach achieves a\\nsuccess rate of 74.5% under the object-centric setting and 50.0% under the full\\nsetting. We adopt an RRT-Connect algorithm as the baseline, which only achieves\\na success rate of 18.8% after a significantly longer computation time.\\nSupplemental materials and videos are available on our project webpage.',\n",
       "    'author': [{'name': 'Mingxin Yu'},\n",
       "     {'name': 'Lin Shao'},\n",
       "     {'name': 'Zhehuan Chen'},\n",
       "     {'name': 'Tianhao Wu'},\n",
       "     {'name': 'Qingnan Fan'},\n",
       "     {'name': 'Kaichun Mo'},\n",
       "     {'name': 'Hao Dong'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to IEEE International Conference on Robotics and Automation\\n  (ICRA) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.10143v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.10143v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.05470v1',\n",
       "    'updated': '2022-08-10T17:57:10Z',\n",
       "    'published': '2022-08-10T17:57:10Z',\n",
       "    'title': 'EvolveHypergraph: Group-Aware Dynamic Relational Reasoning for\\n  Trajectory Prediction',\n",
       "    'summary': 'While the modeling of pair-wise relations has been widely studied in\\nmulti-agent interacting systems, its ability to capture higher-level and\\nlarger-scale group-wise activities is limited. In this paper, we propose a\\ngroup-aware relational reasoning approach (named EvolveHypergraph) with\\nexplicit inference of the underlying dynamically evolving relational\\nstructures, and we demonstrate its effectiveness for multi-agent trajectory\\nprediction. In addition to the edges between a pair of nodes (i.e., agents), we\\npropose to infer hyperedges that adaptively connect multiple nodes to enable\\ngroup-aware relational reasoning in an unsupervised manner without fixing the\\nnumber of hyperedges. The proposed approach infers the dynamically evolving\\nrelation graphs and hypergraphs over time to capture the evolution of\\nrelations, which are used by the trajectory predictor to obtain future states.\\nMoreover, we propose to regularize the smoothness of the relation evolution and\\nthe sparsity of the inferred graphs or hypergraphs, which effectively improves\\ntraining stability and enhances the explainability of inferred relations. The\\nproposed approach is validated on both synthetic crowd simulations and multiple\\nreal-world benchmark datasets. Our approach infers explainable, reasonable\\ngroup-aware relations and achieves state-of-the-art performance in long-term\\nprediction.',\n",
       "    'author': [{'name': 'Jiachen Li'},\n",
       "     {'name': 'Chuanbo Hua'},\n",
       "     {'name': 'Jinkyoo Park'},\n",
       "     {'name': 'Hengbo Ma'},\n",
       "     {'name': 'Victoria Dax'},\n",
       "     {'name': 'Mykel J. Kochenderfer'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.05470v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.05470v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.13626v1',\n",
       "    'updated': '2022-08-26T02:21:31Z',\n",
       "    'published': '2022-08-26T02:21:31Z',\n",
       "    'title': 'CH-MARL: A Multimodal Benchmark for Cooperative, Heterogeneous\\n  Multi-Agent Reinforcement Learning',\n",
       "    'summary': 'We propose a multimodal (vision-and-language) benchmark for cooperative and\\nheterogeneous multi-agent learning. We introduce a benchmark multimodal dataset\\nwith tasks involving collaboration between multiple simulated heterogeneous\\nrobots in a rich multi-room home environment. We provide an integrated learning\\nframework, multimodal implementations of state-of-the-art multi-agent\\nreinforcement learning techniques, and a consistent evaluation protocol. Our\\nexperiments investigate the impact of different modalities on multi-agent\\nlearning performance. We also introduce a simple message passing method between\\nagents. The results suggest that multimodality introduces unique challenges for\\ncooperative multi-agent learning and there is significant room for advancing\\nmulti-agent reinforcement learning methods in such settings.',\n",
       "    'author': [{'name': 'Vasu Sharma'},\n",
       "     {'name': 'Prasoon Goyal'},\n",
       "     {'name': 'Kaixiang Lin'},\n",
       "     {'name': 'Govind Thattai'},\n",
       "     {'name': 'Qiaozi Gao'},\n",
       "     {'name': 'Gaurav S. Sukhatme'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.13626v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.13626v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.02545v2',\n",
       "    'updated': '2022-11-08T21:31:55Z',\n",
       "    'published': '2022-11-04T16:10:50Z',\n",
       "    'title': 'GoRela: Go Relative for Viewpoint-Invariant Motion Forecasting',\n",
       "    'summary': \"The task of motion forecasting is critical for self-driving vehicles (SDVs)\\nto be able to plan a safe maneuver. Towards this goal, modern approaches reason\\nabout the map, the agents' past trajectories and their interactions in order to\\nproduce accurate forecasts. The predominant approach has been to encode the map\\nand other agents in the reference frame of each target agent. However, this\\napproach is computationally expensive for multi-agent prediction as inference\\nneeds to be run for each agent. To tackle the scaling challenge, the solution\\nthus far has been to encode all agents and the map in a shared coordinate frame\\n(e.g., the SDV frame). However, this is sample inefficient and vulnerable to\\ndomain shift (e.g., when the SDV visits uncommon states). In contrast, in this\\npaper, we propose an efficient shared encoding for all agents and the map\\nwithout sacrificing accuracy or generalization. Towards this goal, we leverage\\npair-wise relative positional encodings to represent geometric relationships\\nbetween the agents and the map elements in a heterogeneous spatial graph. This\\nparameterization allows us to be invariant to scene viewpoint, and save online\\ncomputation by re-using map embeddings computed offline. Our decoder is also\\nviewpoint agnostic, predicting agent goals on the lane graph to enable diverse\\nand context-aware multimodal prediction. We demonstrate the effectiveness of\\nour approach on the urban Argoverse 2 benchmark as well as a novel highway\\ndataset.\",\n",
       "    'author': [{'name': 'Alexander Cui'},\n",
       "     {'name': 'Sergio Casas'},\n",
       "     {'name': 'Kelvin Wong'},\n",
       "     {'name': 'Simon Suo'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.02545v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.02545v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.05281v2',\n",
       "    'updated': '2020-11-15T21:47:37Z',\n",
       "    'published': '2020-10-27T20:41:30Z',\n",
       "    'title': 'A Genetic Algorithm Based Approach for Satellite Autonomy',\n",
       "    'summary': 'Autonomous spacecraft maneuver planning using an evolutionary algorithmic\\napproach is investigated. Simulated spacecraft were placed into four different\\ninitial orbits. Each was allowed a string of thirty delta-v impulse maneuvers\\nin six cartesian directions, the positive and negative x, y and z directions.\\nThe goal of the spacecraft maneuver string was to, starting from some non-polar\\nstarting orbit, place the spacecraft into a polar, low eccentricity orbit. A\\ngenetic algorithm was implemented, using a mating, fitness, mutation and\\ncrossover scheme for impulse strings. The genetic algorithm was successfully\\nable to produce this result for all the starting orbits. Performance and future\\nwork is also discussed.',\n",
       "    'author': [{'name': 'Sidhdharth Sikka'}, {'name': 'Harshvardhan Sikka'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.13140/RG.2.2.17934.18247/2'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.13140/RG.2.2.17934.18247/2',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2011.05281v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.05281v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1501.01457v1',\n",
       "    'updated': '2015-01-07T12:11:27Z',\n",
       "    'published': '2015-01-07T12:11:27Z',\n",
       "    'title': 'Comparison of Selection Methods in On-line Distributed Evolutionary\\n  Robotics',\n",
       "    'summary': 'In this paper, we study the impact of selection methods in the context of\\non-line on-board distributed evolutionary algorithms. We propose a variant of\\nthe mEDEA algorithm in which we add a selection operator, and we apply it in a\\ntaskdriven scenario. We evaluate four selection methods that induce different\\nintensity of selection pressure in a multi-robot navigation with obstacle\\navoidance task and a collective foraging task. Experiments show that a small\\nintensity of selection pressure is sufficient to rapidly obtain good\\nperformances on the tasks at hand. We introduce different measures to compare\\nthe selection methods, and show that the higher the selection pressure, the\\nbetter the performances obtained, especially for the more challenging food\\nforaging task.',\n",
       "    'author': [{'name': 'Iñaki Fernández Pérez',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'INRIA Nancy - Grand Est / LORIA'}},\n",
       "     {'name': 'Amine Boumaza',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'INRIA Nancy - Grand Est / LORIA'}},\n",
       "     {'name': 'François Charpillet',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'INRIA Nancy - Grand Est / LORIA'}}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.7551/978-0-262-32621-6-ch046'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.7551/978-0-262-32621-6-ch046',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1501.01457v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1501.01457v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ALIFE 14, Jul 2014, New York, United States. Artificial Life 14 in\\n  Complex Adaptive Systems, MIT Press, Artificial Life 14'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.08992v2',\n",
       "    'updated': '2018-02-09T14:56:37Z',\n",
       "    'published': '2017-09-26T13:08:27Z',\n",
       "    'title': 'Embodied Evolution in Collective Robotics: A Review',\n",
       "    'summary': 'This paper provides an overview of evolutionary robotics techniques applied\\nto on-line distributed evolution for robot collectives -- namely, embodied\\nevolution. It provides a definition of embodied evolution as well as a thorough\\ndescription of the underlying concepts and mechanisms. The paper also presents\\na comprehensive summary of research published in the field since its inception\\n(1999-2017), providing various perspectives to identify the major trends. In\\nparticular, we identify a shift from considering embodied evolution as a\\nparallel search method within small robot collectives (fewer than 10 robots) to\\nembodied evolution as an on-line distributed learning method for designing\\ncollective behaviours in swarm-like collectives. The paper concludes with a\\ndiscussion of applications and open questions, providing a milestone for past\\nand an inspiration for future research.',\n",
       "    'author': [{'name': 'Nicolas Bredeche'},\n",
       "     {'name': 'Evert Haasdijk'},\n",
       "     {'name': 'Abraham Prieto'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.3389/frobt.2018.00012'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.3389/frobt.2018.00012',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1709.08992v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.08992v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '23 pages, 1 figure, 1 table'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '(2018) Embodied Evolution in Collective Robotics: A Review. Front.\\n  Robot. AI 5:12'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.11444v1',\n",
       "    'updated': '2020-12-21T15:54:37Z',\n",
       "    'published': '2020-12-21T15:54:37Z',\n",
       "    'title': 'Rapidly adapting robot swarms with Swarm Map-based Bayesian Optimisation',\n",
       "    'summary': 'Rapid performance recovery from unforeseen environmental perturbations\\nremains a grand challenge in swarm robotics. To solve this challenge, we\\ninvestigate a behaviour adaptation approach, where one searches an archive of\\ncontrollers for potential recovery solutions. To apply behaviour adaptation in\\nswarm robotic systems, we propose two algorithms: (i) Swarm Map-based\\nOptimisation (SMBO), which selects and evaluates one controller at a time, for\\na homogeneous swarm, in a centralised fashion; and (ii) Swarm Map-based\\nOptimisation Decentralised (SMBO-Dec), which performs an asynchronous\\nbatch-based Bayesian optimisation to simultaneously explore different\\ncontrollers for groups of robots in the swarm. We set up foraging experiments\\nwith a variety of disturbances: injected faults to proximity sensors, ground\\nsensors, and the actuators of individual robots, with 100 unique combinations\\nfor each type. We also investigate disturbances in the operating environment of\\nthe swarm, where the swarm has to adapt to drastic changes in the number of\\nresources available in the environment, and to one of the robots behaving\\ndisruptively towards the rest of the swarm, with 30 unique conditions for each\\nsuch perturbation. The viability of SMBO and SMBO-Dec is demonstrated,\\ncomparing favourably to variants of random search and gradient descent, and\\nvarious ablations, and improving performance up to 80% compared to the\\nperformance at the time of fault injection within at most 30 evaluations.',\n",
       "    'author': [{'name': 'David M. Bossens'}, {'name': 'Danesh Tarapore'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.11444v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.11444v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.12196v1',\n",
       "    'updated': '2021-05-25T20:17:10Z',\n",
       "    'published': '2021-05-25T20:17:10Z',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football',\n",
       "    'summary': 'Intelligent behaviour in the physical world exhibits structure at multiple\\nspatial and temporal scales. Although movements are ultimately executed at the\\nlevel of instantaneous muscle tensions or joint torques, they must be selected\\nto serve goals defined on much longer timescales, and in terms of relations\\nthat extend far beyond the body itself, ultimately involving coordination with\\nother agents. Recent research in artificial intelligence has shown the promise\\nof learning-based approaches to the respective problems of complex movement,\\nlonger-term planning and multi-agent coordination. However, there is limited\\nresearch aimed at their integration. We study this problem by training teams of\\nphysically simulated humanoid avatars to play football in a realistic virtual\\nenvironment. We develop a method that combines imitation learning, single- and\\nmulti-agent reinforcement learning and population-based training, and makes use\\nof transferable representations of behaviour for decision making at different\\nlevels of abstraction. In a sequence of stages, players first learn to control\\na fully articulated body to perform realistic, human-like movements such as\\nrunning and turning; they then acquire mid-level football skills such as\\ndribbling and shooting; finally, they develop awareness of others and play as a\\nteam, bridging the gap between low-level motor control at a timescale of\\nmilliseconds, and coordinated goal-directed behaviour as a team at the\\ntimescale of tens of seconds. We investigate the emergence of behaviours at\\ndifferent levels of abstraction, as well as the representations that underlie\\nthese behaviours using several analysis techniques, including statistics from\\nreal-world sports analytics. Our work constitutes a complete demonstration of\\nintegrated decision-making at multiple scales in a physically embodied\\nmulti-agent setting. See project video at https://youtu.be/KHMwq9pv7mg.',\n",
       "    'author': [{'name': 'Siqi Liu'},\n",
       "     {'name': 'Guy Lever'},\n",
       "     {'name': 'Zhe Wang'},\n",
       "     {'name': 'Josh Merel'},\n",
       "     {'name': 'S. M. Ali Eslami'},\n",
       "     {'name': 'Daniel Hennes'},\n",
       "     {'name': 'Wojciech M. Czarnecki'},\n",
       "     {'name': 'Yuval Tassa'},\n",
       "     {'name': 'Shayegan Omidshafiei'},\n",
       "     {'name': 'Abbas Abdolmaleki'},\n",
       "     {'name': 'Noah Y. Siegel'},\n",
       "     {'name': 'Leonard Hasenclever'},\n",
       "     {'name': 'Luke Marris'},\n",
       "     {'name': 'Saran Tunyasuvunakool'},\n",
       "     {'name': 'H. Francis Song'},\n",
       "     {'name': 'Markus Wulfmeier'},\n",
       "     {'name': 'Paul Muller'},\n",
       "     {'name': 'Tuomas Haarnoja'},\n",
       "     {'name': 'Brendan D. Tracey'},\n",
       "     {'name': 'Karl Tuyls'},\n",
       "     {'name': 'Thore Graepel'},\n",
       "     {'name': 'Nicolas Heess'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.12196v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.12196v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.13456v1',\n",
       "    'updated': '2022-02-27T21:22:14Z',\n",
       "    'published': '2022-02-27T21:22:14Z',\n",
       "    'title': 'PheroCom: Decentralised and asynchronous swarm robotics coordination\\n  based on virtual pheromone and vibroacoustic communication',\n",
       "    'summary': \"Representation and control of the dynamics of stigmergic substances used by\\nbio-inspired approaches is a challenge when applied to robotics. In order to\\novercome this challenge, this work proposes a model to coordinate swarms of\\nrobots based on the virtualisation and control of these substances in a local\\nscope. The model presents a new pheromone modelling, which enables the\\ndecentralisation and asynchronicity of navigation decisions. Each robot\\nmaintains an independent virtual pheromone map, which is continuously updated\\nwith the robot's deposits and pheromone evaporation. Moreover, the individual\\npheromone map is also updated by aggregating information from other robots that\\nare exploring nearby areas. Thus, individual and independent maps replace the\\nneed of a centralising agent that controls and distributes the pheromone\\ninformation, which is not always practicable. Pheromone information propagation\\nis inspired by ants' vibroacoustic communication, which, in turn, is\\ncharacterised as an indirect communication through a type of gossip protocol.\\nThe proposed model was evaluated through an agent simulation software,\\nimplemented by the authors, and in the Webots platform. Experiments were\\ncarried out to validate the model in different environments, with different\\nshapes and sizes, as well as varying the number of robots. The analysis of the\\nresults has shown that the model was able to perform the coordination of the\\nswarm, and the robots have exhibited an expressive performance executing the\\nsurveillance task.\",\n",
       "    'author': [{'name': 'Claudiney R. Tinoco',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Federal University of Uberlândia, Uberlândia/MG, Brazil'}},\n",
       "     {'name': 'Gina M. B. Oliveira',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Federal University of Uberlândia, Uberlândia/MG, Brazil'}}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '26 pages, 15 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.13456v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.13456v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.8; I.2.1; I.2.11',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.04867v1',\n",
       "    'updated': '2021-08-10T18:37:54Z',\n",
       "    'published': '2021-08-10T18:37:54Z',\n",
       "    'title': 'AuraSense: Robot Collision Avoidance by Full Surface Proximity Detection',\n",
       "    'summary': 'Perceiving obstacles and avoiding collisions is fundamental to the safe\\noperation of a robot system, particularly when the robot must operate in highly\\ndynamic human environments. Proximity detection using on-robot sensors can be\\nused to avoid or mitigate impending collisions. However, existing proximity\\nsensing methods are orientation and placement dependent, resulting in blind\\nspots even with large numbers of sensors. In this paper, we introduce the\\nphenomenon of the Leaky Surface Wave (LSW), a novel sensing modality, and\\npresent AuraSense, a proximity detection system using the LSW. AuraSense is the\\nfirst system to realize no-dead-spot proximity sensing for robot arms. It\\nrequires only a single pair of piezoelectric transducers, and can easily be\\napplied to off-the-shelf robots with minimal modifications. We further\\nintroduce a set of signal processing techniques and a lightweight neural\\nnetwork to address the unique challenges in using the LSW for proximity\\nsensing. Finally, we demonstrate a prototype system consisting of a single\\npiezoelectric element pair on a robot manipulator, which validates our design.\\nWe conducted several micro benchmark experiments and performed more than 2000\\non-robot proximity detection trials with various potential robot arm materials,\\ncolliding objects, approach patterns, and robot movement patterns. AuraSense\\nachieves 100% and 95.3% true positive proximity detection rates when the arm\\napproaches static and mobile obstacles respectively, with a true negative rate\\nover 99%, showing the real-world viability of this system.',\n",
       "    'author': [{'name': 'Xiaoran Fan'},\n",
       "     {'name': 'Riley Simmons-Edler'},\n",
       "     {'name': 'Daewon Lee'},\n",
       "     {'name': 'Larry Jackel'},\n",
       "     {'name': 'Richard Howard'},\n",
       "     {'name': 'Daniel Lee'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IROS 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.04867v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.04867v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.09622v3',\n",
       "    'updated': '2021-02-11T18:36:45Z',\n",
       "    'published': '2020-08-21T18:00:33Z',\n",
       "    'title': 'Learning to Set Waypoints for Audio-Visual Navigation',\n",
       "    'summary': 'In audio-visual navigation, an agent intelligently travels through a complex,\\nunmapped 3D environment using both sights and sounds to find a sound source\\n(e.g., a phone ringing in another room). Existing models learn to act at a\\nfixed granularity of agent motion and rely on simple recurrent aggregations of\\nthe audio observations. We introduce a reinforcement learning approach to\\naudio-visual navigation with two key novel elements: 1) waypoints that are\\ndynamically set and learned end-to-end within the navigation policy, and 2) an\\nacoustic memory that provides a structured, spatially grounded record of what\\nthe agent has heard as it moves. Both new ideas capitalize on the synergy of\\naudio and visual data for revealing the geometry of an unmapped space. We\\ndemonstrate our approach on two challenging datasets of real-world 3D scenes,\\nReplica and Matterport3D. Our model improves the state of the art by a\\nsubstantial margin, and our experiments reveal that learning the links between\\nsights, sounds, and space is essential for audio-visual navigation. Project:\\nhttp://vision.cs.utexas.edu/projects/audio_visual_waypoints.',\n",
       "    'author': [{'name': 'Changan Chen'},\n",
       "     {'name': 'Sagnik Majumder'},\n",
       "     {'name': 'Ziad Al-Halah'},\n",
       "     {'name': 'Ruohan Gao'},\n",
       "     {'name': 'Santhosh Kumar Ramakrishnan'},\n",
       "     {'name': 'Kristen Grauman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICLR 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.09622v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.09622v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1602.00991v2',\n",
       "    'updated': '2016-03-08T22:09:05Z',\n",
       "    'published': '2016-02-02T16:10:16Z',\n",
       "    'title': 'Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks',\n",
       "    'summary': 'This paper presents to the best of our knowledge the first end-to-end object\\ntracking approach which directly maps from raw sensor input to object tracks in\\nsensor space without requiring any feature engineering or system identification\\nin the form of plant or sensor models. Specifically, our system accepts a\\nstream of raw sensor data at one end and, in real-time, produces an estimate of\\nthe entire environment state at the output including even occluded objects. We\\nachieve this by framing the problem as a deep learning task and exploit\\nsequence models in the form of recurrent neural networks to learn a mapping\\nfrom sensor measurements to object tracks. In particular, we propose a learning\\nmethod based on a form of input dropout which allows learning in an\\nunsupervised manner, only based on raw, occluded sensor data without access to\\nground-truth annotations. We demonstrate our approach using a synthetic dataset\\ndesigned to mimic the task of tracking objects in 2D laser data -- as commonly\\nencountered in robotics applications -- and show that it learns to track many\\ndynamic objects despite occlusions and the presence of sensor noise.',\n",
       "    'author': [{'name': 'Peter Ondruska'}, {'name': 'Ingmar Posner'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in The Thirtieth AAAI Conference on Artificial Intelligence\\n  (AAAI-16), Video: https://youtu.be/cdeWCpfUGWc, Code:\\n  http://mrg.robots.ox.ac.uk/mrg_people/peter-ondruska/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1602.00991v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1602.00991v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.6; I.2.9; I.2.10',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1707.03374v2',\n",
       "    'updated': '2018-06-18T21:00:13Z',\n",
       "    'published': '2017-07-11T17:23:53Z',\n",
       "    'title': 'Imitation from Observation: Learning to Imitate Behaviors from Raw Video\\n  via Context Translation',\n",
       "    'summary': 'Imitation learning is an effective approach for autonomous systems to acquire\\ncontrol policies when an explicit reward function is unavailable, using\\nsupervision provided as demonstrations from an expert, typically a human\\noperator. However, standard imitation learning methods assume that the agent\\nreceives examples of observation-action tuples that could be provided, for\\ninstance, to a supervised learning algorithm. This stands in contrast to how\\nhumans and animals imitate: we observe another person performing some behavior\\nand then figure out which actions will realize that behavior, compensating for\\nchanges in viewpoint, surroundings, object positions and types, and other\\nfactors. We term this kind of imitation learning \"imitation-from-observation,\"\\nand propose an imitation learning method based on video prediction with context\\ntranslation and deep reinforcement learning. This lifts the assumption in\\nimitation learning that the demonstration should consist of observations in the\\nsame environment configuration, and enables a variety of interesting\\napplications, including learning robotic skills that involve tool use simply by\\nobserving videos of human tool use. Our experimental results show the\\neffectiveness of our approach in learning a wide range of real-world robotic\\ntasks modeled after common household chores from videos of a human\\ndemonstrator, including sweeping, ladling almonds, pushing objects as well as a\\nnumber of tasks in simulation.',\n",
       "    'author': [{'name': 'YuXuan Liu'},\n",
       "     {'name': 'Abhishek Gupta'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at ICRA 2018, Brisbane. YuXuan Liu and Abhishek Gupta had\\n  equal contribution'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1707.03374v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1707.03374v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.04142v1',\n",
       "    'updated': '2019-10-09T17:37:52Z',\n",
       "    'published': '2019-10-09T17:37:52Z',\n",
       "    'title': 'Imagined Value Gradients: Model-Based Policy Optimization with\\n  Transferable Latent Dynamics Models',\n",
       "    'summary': 'Humans are masters at quickly learning many complex tasks, relying on an\\napproximate understanding of the dynamics of their environments. In much the\\nsame way, we would like our learning agents to quickly adapt to new tasks. In\\nthis paper, we explore how model-based Reinforcement Learning (RL) can\\nfacilitate transfer to new tasks. We develop an algorithm that learns an\\naction-conditional, predictive model of expected future observations, rewards\\nand values from which a policy can be derived by following the gradient of the\\nestimated value along imagined trajectories. We show how robust policy\\noptimization can be achieved in robot manipulation tasks even with approximate\\nmodels that are learned directly from vision and proprioception. We evaluate\\nthe efficacy of our approach in a transfer learning scenario, re-using\\npreviously learned models on tasks with different reward structures and visual\\ndistractors, and show a significant improvement in learning speed compared to\\nstrong off-policy baselines. Videos with results can be found at\\nhttps://sites.google.com/view/ivg-corl19',\n",
       "    'author': [{'name': 'Arunkumar Byravan'},\n",
       "     {'name': 'Jost Tobias Springenberg'},\n",
       "     {'name': 'Abbas Abdolmaleki'},\n",
       "     {'name': 'Roland Hafner'},\n",
       "     {'name': 'Michael Neunert'},\n",
       "     {'name': 'Thomas Lampe'},\n",
       "     {'name': 'Noah Siegel'},\n",
       "     {'name': 'Nicolas Heess'},\n",
       "     {'name': 'Martin Riedmiller'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear at the 3rd annual Conference on Robot Learning, Osaka,\\n  Japan (CoRL 2019). 24 pages including appendix (main paper - 8 pages)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1910.04142v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.04142v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.02886v3',\n",
       "    'updated': '2021-04-05T17:59:16Z',\n",
       "    'published': '2021-02-04T20:58:37Z',\n",
       "    'title': 'Ivy: Templated Deep Learning for Inter-Framework Portability',\n",
       "    'summary': 'We introduce Ivy, a templated Deep Learning (DL) framework which abstracts\\nexisting DL frameworks. Ivy unifies the core functions of these frameworks to\\nexhibit consistent call signatures, syntax and input-output behaviour. New\\nhigh-level framework-agnostic functions and classes, which are usable alongside\\nframework-specific code, can then be implemented as compositions of the unified\\nlow-level Ivy functions. Ivy currently supports TensorFlow, PyTorch, MXNet, Jax\\nand NumPy. We also release four pure-Ivy libraries for mechanics, 3D vision,\\nrobotics, and differentiable environments. Through our evaluations, we show\\nthat Ivy can significantly reduce lines of code with a runtime overhead of less\\nthan 1% in most cases. We welcome developers to join the Ivy community by\\nwriting their own functions, layers and libraries in Ivy, maximizing their\\naudience and helping to accelerate DL research through inter-framework\\ncodebases. More information can be found at https://ivy-dl.org.',\n",
       "    'author': [{'name': 'Daniel Lenton'},\n",
       "     {'name': 'Fabio Pardo'},\n",
       "     {'name': 'Fabian Falck'},\n",
       "     {'name': 'Stephen James'},\n",
       "     {'name': 'Ronald Clark'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Code at https://github.com/ivy-dl/ivy'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.02886v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.02886v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.07764v2',\n",
       "    'updated': '2021-02-17T18:56:39Z',\n",
       "    'published': '2021-02-15T18:59:07Z',\n",
       "    'title': 'End-to-End Egospheric Spatial Memory',\n",
       "    'summary': \"Spatial memory, or the ability to remember and recall specific locations and\\nobjects, is central to autonomous agents' ability to carry out tasks in real\\nenvironments. However, most existing artificial memory modules are not very\\nadept at storing spatial information. We propose a parameter-free module,\\nEgospheric Spatial Memory (ESM), which encodes the memory in an ego-sphere\\naround the agent, enabling expressive 3D representations. ESM can be trained\\nend-to-end via either imitation or reinforcement learning, and improves both\\ntraining efficiency and final performance against other memory baselines on\\nboth drone and manipulator visuomotor control tasks. The explicit egocentric\\ngeometry also enables us to seamlessly combine the learned controller with\\nother non-learned modalities, such as local obstacle avoidance. We further show\\napplications to semantic segmentation on the ScanNet dataset, where ESM\\nnaturally combines image-level and map-level inference modalities. Through our\\nbroad set of experiments, we show that ESM provides a general computation graph\\nfor embodied spatial reasoning, and the module forms a bridge between real-time\\nmapping systems and differentiable memory architectures. Implementation at:\\nhttps://github.com/ivy-dl/memory.\",\n",
       "    'author': [{'name': 'Daniel Lenton'},\n",
       "     {'name': 'Stephen James'},\n",
       "     {'name': 'Ronald Clark'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Conference paper at ICLR 2021. Implementation:\\n  https://github.com/ivy-dl/memory Project page: https://djl11.github.io/ESM/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.07764v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.07764v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.15007v2',\n",
       "    'updated': '2022-11-02T17:59:47Z',\n",
       "    'published': '2022-09-29T17:59:55Z',\n",
       "    'title': 'Understanding Collapse in Non-Contrastive Siamese Representation\\n  Learning',\n",
       "    'summary': 'Contrastive methods have led a recent surge in the performance of\\nself-supervised representation learning (SSL). Recent methods like BYOL or\\nSimSiam purportedly distill these contrastive methods down to their essence,\\nremoving bells and whistles, including the negative examples, that do not\\ncontribute to downstream performance. These \"non-contrastive\" methods work\\nsurprisingly well without using negatives even though the global minimum lies\\nat trivial collapse. We empirically analyze these non-contrastive methods and\\nfind that SimSiam is extraordinarily sensitive to dataset and model size. In\\nparticular, SimSiam representations undergo partial dimensional collapse if the\\nmodel is too small relative to the dataset size. We propose a metric to measure\\nthe degree of this collapse and show that it can be used to forecast the\\ndownstream task performance without any fine-tuning or labels. We further\\nanalyze architectural design choices and their effect on the downstream\\nperformance. Finally, we demonstrate that shifting to a continual learning\\nsetting acts as a regularizer and prevents collapse, and a hybrid between\\ncontinual and multi-epoch training can improve linear probe accuracy by as many\\nas 18 percentage points using ResNet-18 on ImageNet. Our project page is at\\nhttps://alexanderli.com/noncontrastive-ssl/.',\n",
       "    'author': [{'name': 'Alexander C. Li'},\n",
       "     {'name': 'Alexei A. Efros'},\n",
       "     {'name': 'Deepak Pathak'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at ECCV 2022. Project page at\\n  https://alexanderli.com/noncontrastive-ssl/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.15007v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.15007v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.04303v1',\n",
       "    'updated': '2022-10-09T16:56:45Z',\n",
       "    'published': '2022-10-09T16:56:45Z',\n",
       "    'title': 'Are All Vision Models Created Equal? A Study of the Open-Loop to\\n  Closed-Loop Causality Gap',\n",
       "    'summary': 'There is an ever-growing zoo of modern neural network models that can\\nefficiently learn end-to-end control from visual observations. These advanced\\ndeep models, ranging from convolutional to patch-based networks, have been\\nextensively tested on offline image classification and regression tasks. In\\nthis paper, we study these vision architectures with respect to the open-loop\\nto closed-loop causality gap, i.e., offline training followed by an online\\nclosed-loop deployment. This causality gap typically emerges in robotics\\napplications such as autonomous driving, where a network is trained to imitate\\nthe control commands of a human. In this setting, two situations arise: 1)\\nClosed-loop testing in-distribution, where the test environment shares\\nproperties with those of offline training data. 2) Closed-loop testing under\\ndistribution shifts and out-of-distribution. Contrary to recently reported\\nresults, we show that under proper training guidelines, all vision models\\nperform indistinguishably well on in-distribution deployment, resolving the\\ncausality gap. In situation 2, We observe that the causality gap disrupts\\nperformance regardless of the choice of the model architecture. Our results\\nimply that the causality gap can be solved in situation one with our proposed\\ntraining guideline with any modern network architecture, whereas achieving\\nout-of-distribution generalization (situation two) requires further\\ninvestigations, for instance, on data diversity rather than the model\\narchitecture.',\n",
       "    'author': [{'name': 'Mathias Lechner'},\n",
       "     {'name': 'Ramin Hasani'},\n",
       "     {'name': 'Alexander Amini'},\n",
       "     {'name': 'Tsun-Hsuan Wang'},\n",
       "     {'name': 'Thomas A. Henzinger'},\n",
       "     {'name': 'Daniela Rus'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.04303v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.04303v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1301.4862v1',\n",
       "    'updated': '2013-01-21T13:26:07Z',\n",
       "    'published': '2013-01-21T13:26:07Z',\n",
       "    'title': 'Active Learning of Inverse Models with Intrinsically Motivated Goal\\n  Exploration in Robots',\n",
       "    'summary': 'We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive\\nCuriosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal\\nexploration mechanism which allows active learning of inverse models in\\nhigh-dimensional redundant robots. This allows a robot to efficiently and\\nactively learn distributions of parameterized motor skills/policies that solve\\na corresponding distribution of parameterized tasks/goals. The architecture\\nmakes the robot sample actively novel parameterized tasks in the task space,\\nbased on a measure of competence progress, each of which triggers low-level\\ngoal-directed learning of the motor policy pa- rameters that allow to solve it.\\nFor both learning and generalization, the system leverages regression\\ntechniques which allow to infer the motor policy parameters corresponding to a\\ngiven novel parameterized task, and based on the previously learnt\\ncorrespondences between policy and task parameters. We present experiments with\\nhigh-dimensional continuous sensorimotor spaces in three different robotic\\nsetups: 1) learning the inverse kinematics in a highly-redundant robotic arm,\\n2) learning omnidirectional locomotion with motor primitives in a quadruped\\nrobot, 3) an arm learning to control a fishing rod with a flexible wire. We\\nshow that 1) exploration in the task space can be a lot faster than exploration\\nin the actuator space for learning inverse models in redundant robots; 2)\\nselecting goals maximizing competence progress creates developmental\\ntrajectories driving the robot to progressively focus on tasks of increasing\\ncomplexity and is statistically significantly more efficient than selecting\\ntasks randomly, as well as more efficient than different standard active motor\\nbabbling methods; 3) this architecture allows the robot to actively discover\\nwhich parts of its task space it can learn to reach and which part it cannot.',\n",
       "    'author': [{'name': 'Adrien Baranes'}, {'name': 'Pierre-Yves Oudeyer'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.robot.2012.05.008'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.robot.2012.05.008',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1301.4862v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1301.4862v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Baranes, A., Oudeyer, P-Y. (2013) Active Learning of Inverse\\n  Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and\\n  Autonomous Systems, 61(1), pp. 49-73'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1604.05091v2',\n",
       "    'updated': '2016-04-19T14:09:26Z',\n",
       "    'published': '2016-04-18T11:15:56Z',\n",
       "    'title': 'End-to-End Tracking and Semantic Segmentation Using Recurrent Neural\\n  Networks',\n",
       "    'summary': \"In this work we present a novel end-to-end framework for tracking and\\nclassifying a robot's surroundings in complex, dynamic and only partially\\nobservable real-world environments. The approach deploys a recurrent neural\\nnetwork to filter an input stream of raw laser measurements in order to\\ndirectly infer object locations, along with their identity in both visible and\\noccluded areas. To achieve this we first train the network using unsupervised\\nDeep Tracking, a recently proposed theoretical framework for end-to-end space\\noccupancy prediction. We show that by learning to track on a large amount of\\nunsupervised data, the network creates a rich internal representation of its\\nenvironment which we in turn exploit through the principle of inductive\\ntransfer of knowledge to perform the task of it's semantic classification. As a\\nresult, we show that only a small amount of labelled data suffices to steer the\\nnetwork towards mastering this additional task. Furthermore we propose a novel\\nrecurrent neural network architecture specifically tailored to tracking and\\nsemantic classification in real-world robotics applications. We demonstrate the\\ntracking and classification performance of the method on real-world data\\ncollected at a busy road junction. Our evaluation shows that the proposed\\nend-to-end framework compares favourably to a state-of-the-art, model-free\\ntracking solution and that it outperforms a conventional one-shot training\\nscheme for semantic classification.\",\n",
       "    'author': [{'name': 'Peter Ondruska'},\n",
       "     {'name': 'Julie Dequaire'},\n",
       "     {'name': 'Dominic Zeng Wang'},\n",
       "     {'name': 'Ingmar Posner'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1604.05091v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1604.05091v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1609.06666v2',\n",
       "    'updated': '2017-03-05T15:29:45Z',\n",
       "    'published': '2016-09-21T18:32:11Z',\n",
       "    'title': 'Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient\\n  Convolutional Neural Networks',\n",
       "    'summary': 'This paper proposes a computationally efficient approach to detecting objects\\nnatively in 3D point clouds using convolutional neural networks (CNNs). In\\nparticular, this is achieved by leveraging a feature-centric voting scheme to\\nimplement novel convolutional layers which explicitly exploit the sparsity\\nencountered in the input. To this end, we examine the trade-off between\\naccuracy and speed for different architectures and additionally propose to use\\nan L1 penalty on the filter activations to further encourage sparsity in the\\nintermediate representations. To the best of our knowledge, this is the first\\nwork to propose sparse convolutional layers and L1 regularisation for efficient\\nlarge-scale processing of 3D data. We demonstrate the efficacy of our approach\\non the KITTI object detection benchmark and show that Vote3Deep models with as\\nfew as three layers outperform the previous state of the art in both laser and\\nlaser-vision based approaches by margins of up to 40% while remaining highly\\ncompetitive in terms of processing time.',\n",
       "    'author': [{'name': 'Martin Engelcke'},\n",
       "     {'name': 'Dushyant Rao'},\n",
       "     {'name': 'Dominic Zeng Wang'},\n",
       "     {'name': 'Chi Hay Tong'},\n",
       "     {'name': 'Ingmar Posner'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To be published at the IEEE International Conference on Robotics and\\n  Automation 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1609.06666v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1609.06666v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1804.08328v1',\n",
       "    'updated': '2018-04-23T10:46:28Z',\n",
       "    'published': '2018-04-23T10:46:28Z',\n",
       "    'title': 'Taskonomy: Disentangling Task Transfer Learning',\n",
       "    'summary': 'Do visual tasks have a relationship, or are they unrelated? For instance,\\ncould having surface normals simplify estimating the depth of an image?\\nIntuition answers these questions positively, implying existence of a structure\\namong visual tasks. Knowing this structure has notable values; it is the\\nconcept underlying transfer learning and provides a principled way for\\nidentifying redundancies across tasks, e.g., to seamlessly reuse supervision\\namong related tasks or solve many tasks in one system without piling up the\\ncomplexity.\\n  We proposes a fully computational approach for modeling the structure of\\nspace of visual tasks. This is done via finding (first and higher-order)\\ntransfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D,\\nand semantic tasks in a latent space. The product is a computational taxonomic\\nmap for task transfer learning. We study the consequences of this structure,\\ne.g. nontrivial emerged relationships, and exploit them to reduce the demand\\nfor labeled data. For example, we show that the total number of labeled\\ndatapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3\\n(compared to training independently) while keeping the performance nearly the\\nsame. We provide a set of tools for computing and probing this taxonomical\\nstructure including a solver that users can employ to devise efficient\\nsupervision policies for their use cases.',\n",
       "    'author': [{'name': 'Amir Zamir'},\n",
       "     {'name': 'Alexander Sax'},\n",
       "     {'name': 'William Shen'},\n",
       "     {'name': 'Leonidas Guibas'},\n",
       "     {'name': 'Jitendra Malik'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2018 (Oral). See project website and live demos at\\n  http://taskonomy.vision/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1804.08328v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1804.08328v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.11714v2',\n",
       "    'updated': '2019-03-12T23:17:17Z',\n",
       "    'published': '2018-10-27T21:26:42Z',\n",
       "    'title': 'The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints',\n",
       "    'summary': 'A robot can now grasp an object more effectively than ever before, but once\\nit has the object what happens next? We show that a mild relaxation of the task\\nand workspace constraints implicit in existing object grasping datasets can\\ncause neural network based grasping algorithms to fail on even a simple block\\nstacking task when executed under more realistic circumstances.\\n  To address this, we introduce the JHU CoSTAR Block Stacking Dataset (BSD),\\nwhere a robot interacts with 5.1 cm colored blocks to complete an\\norder-fulfillment style block stacking task. It contains dynamic scenes and\\nreal time-series data in a less constrained environment than comparable\\ndatasets. There are nearly 12,000 stacking attempts and over 2 million frames\\nof real data. We discuss the ways in which this dataset provides a valuable\\nresource for a broad range of other topics of investigation.\\n  We find that hand-designed neural networks that work on prior datasets do not\\ngeneralize to this task. Thus, to establish a baseline for this dataset, we\\ndemonstrate an automated search of neural network based models using a novel\\nmultiple-input HyperTree MetaModel, and find a final model which makes\\nreasonable 3D pose predictions for grasping and stacking on our dataset.\\n  The CoSTAR BSD, code, and instructions are available at\\nhttps://sites.google.com/site/costardataset.',\n",
       "    'author': [{'name': 'Andrew Hundt'},\n",
       "     {'name': 'Varun Jain'},\n",
       "     {'name': 'Chia-Hung Lin'},\n",
       "     {'name': 'Chris Paxton'},\n",
       "     {'name': 'Gregory D. Hager'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This is a major revision refocusing the topic towards the JHU CoSTAR\\n  Block Stacking Dataset, workspace constraints, and a comparison of HyperTrees\\n  with hand-designed algorithms. 12 pages, 10 figures, and 3 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.11714v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.11714v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1812.11971v3',\n",
       "    'updated': '2019-04-22T07:12:34Z',\n",
       "    'published': '2018-12-31T18:59:25Z',\n",
       "    'title': 'Mid-Level Visual Representations Improve Generalization and Sample\\n  Efficiency for Learning Visuomotor Policies',\n",
       "    'summary': 'How much does having visual priors about the world (e.g. the fact that the\\nworld is 3D) assist in learning to perform downstream motor tasks (e.g.\\ndelivering a package)? We study this question by integrating a generic\\nperceptual skill set (e.g. a distance estimator, an edge detector, etc.) within\\na reinforcement learning framework--see Figure 1. This skill set (hereafter\\nmid-level perception) provides the policy with a more processed state of the\\nworld compared to raw images.\\n  We find that using a mid-level perception confers significant advantages over\\ntraining end-to-end from scratch (i.e. not leveraging priors) in\\nnavigation-oriented tasks. Agents are able to generalize to situations where\\nthe from-scratch approach fails and training becomes significantly more sample\\nefficient. However, we show that realizing these gains requires careful\\nselection of the mid-level perceptual skills. Therefore, we refine our findings\\ninto an efficient max-coverage feature set that can be adopted in lieu of raw\\nimages. We perform our study in completely separate buildings for training and\\ntesting and compare against visually blind baseline policies and\\nstate-of-the-art feature learning methods.',\n",
       "    'author': [{'name': 'Alexander Sax'},\n",
       "     {'name': 'Bradley Emi'},\n",
       "     {'name': 'Amir R. Zamir'},\n",
       "     {'name': 'Leonidas Guibas'},\n",
       "     {'name': 'Silvio Savarese'},\n",
       "     {'name': 'Jitendra Malik'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'See project website, demos, and code at http://perceptual.actor'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1812.11971v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1812.11971v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2002.12336v1',\n",
       "    'updated': '2020-02-27T18:54:42Z',\n",
       "    'published': '2020-02-27T18:54:42Z',\n",
       "    'title': 'Hallucinative Topological Memory for Zero-Shot Visual Planning',\n",
       "    'summary': 'In visual planning (VP), an agent learns to plan goal-directed behavior from\\nobservations of a dynamical system obtained offline, e.g., images obtained from\\nself-supervised robot interaction. Most previous works on VP approached the\\nproblem by planning in a learned latent space, resulting in low-quality visual\\nplans, and difficult training algorithms. Here, instead, we propose a simple VP\\nmethod that plans directly in image space and displays competitive performance.\\nWe build on the semi-parametric topological memory (SPTM) method: image samples\\nare treated as nodes in a graph, the graph connectivity is learned from image\\nsequence data, and planning can be performed using conventional graph search\\nmethods. We propose two modifications on SPTM. First, we train an energy-based\\ngraph connectivity function using contrastive predictive coding that admits\\nstable training. Second, to allow zero-shot planning in new domains, we learn a\\nconditional VAE model that generates images given a context of the domain, and\\nuse these hallucinated samples for building the connectivity graph and\\nplanning. We show that this simple approach significantly outperform the\\nstate-of-the-art VP methods, in terms of both plan interpretability and success\\nrate when using the plan to guide a trajectory-following controller.\\nInterestingly, our method can pick up non-trivial visual properties of objects,\\nsuch as their geometry, and account for it in the plans.',\n",
       "    'author': [{'name': 'Kara Liu'},\n",
       "     {'name': 'Thanard Kurutach'},\n",
       "     {'name': 'Christine Tung'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Aviv Tamar'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2002.12336v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2002.12336v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.03659v3',\n",
       "    'updated': '2022-04-09T16:56:11Z',\n",
       "    'published': '2021-10-07T17:51:05Z',\n",
       "    'title': 'Transform2Act: Learning a Transform-and-Control Policy for Efficient\\n  Agent Design',\n",
       "    'summary': \"An agent's functionality is largely determined by its design, i.e., skeletal\\nstructure and joint attributes (e.g., length, size, strength). However, finding\\nthe optimal agent design for a given function is extremely challenging since\\nthe problem is inherently combinatorial and the design space is prohibitively\\nlarge. Additionally, it can be costly to evaluate each candidate design which\\nrequires solving for its optimal controller. To tackle these problems, our key\\nidea is to incorporate the design procedure of an agent into its\\ndecision-making process. Specifically, we learn a conditional policy that, in\\nan episode, first applies a sequence of transform actions to modify an agent's\\nskeletal structure and joint attributes, and then applies control actions under\\nthe new design. To handle a variable number of joints across designs, we use a\\ngraph-based policy where each graph node represents a joint and uses message\\npassing with its neighbors to output joint-specific actions. Using policy\\ngradient methods, our approach enables joint optimization of agent design and\\ncontrol as well as experience sharing across different designs, which improves\\nsample efficiency substantially. Experiments show that our approach,\\nTransform2Act, outperforms prior methods significantly in terms of convergence\\nspeed and final performance. Notably, Transform2Act can automatically discover\\nplausible designs similar to giraffes, squids, and spiders. Code and videos are\\navailable at https://sites.google.com/view/transform2act.\",\n",
       "    'author': [{'name': 'Ye Yuan'},\n",
       "     {'name': 'Yuda Song'},\n",
       "     {'name': 'Zhengyi Luo'},\n",
       "     {'name': 'Wen Sun'},\n",
       "     {'name': 'Kris Kitani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICLR 2022 (Oral). Project page:\\n  https://sites.google.com/view/transform2act. Code:\\n  https://github.com/Khrylx/Transform2Act'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.03659v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.03659v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.03257v1',\n",
       "    'updated': '2021-12-06T18:59:52Z',\n",
       "    'published': '2021-12-06T18:59:52Z',\n",
       "    'title': 'Functional Regularization for Reinforcement Learning via Learned Fourier\\n  Features',\n",
       "    'summary': \"We propose a simple architecture for deep reinforcement learning by embedding\\ninputs into a learned Fourier basis and show that it improves the sample\\nefficiency of both state-based and image-based RL. We perform infinite-width\\nanalysis of our architecture using the Neural Tangent Kernel and theoretically\\nshow that tuning the initial variance of the Fourier basis is equivalent to\\nfunctional regularization of the learned deep network. That is, these learned\\nFourier features allow for adjusting the degree to which networks underfit or\\noverfit different frequencies in the training data, and hence provide a\\ncontrolled mechanism to improve the stability and performance of RL\\noptimization. Empirically, this allows us to prioritize learning low-frequency\\nfunctions and speed up learning by reducing networks' susceptibility to noise\\nin the optimization process, such as during Bellman updates. Experiments on\\nstandard state-based and image-based RL benchmarks show clear benefits of our\\narchitecture over the baselines. Website at\\nhttps://alexanderli.com/learned-fourier-features\",\n",
       "    'author': [{'name': 'Alexander C. Li'}, {'name': 'Deepak Pathak'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at NeurIPS 2021. Website at\\n  https://alexanderli.com/learned-fourier-features'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.03257v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.03257v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.10387v3',\n",
       "    'updated': '2020-01-24T17:23:47Z',\n",
       "    'published': '2019-09-23T14:28:56Z',\n",
       "    'title': 'An Adversarial Approach to Private Flocking in Mobile Robot Teams',\n",
       "    'summary': \"Privacy is an important facet of defence against adversaries. In this letter,\\nwe introduce the problem of private flocking. We consider a team of mobile\\nrobots flocking in the presence of an adversary, who is able to observe all\\nrobots' trajectories, and who is interested in identifying the leader. We\\npresent a method that generates private flocking controllers that hide the\\nidentity of the leader robot. Our approach towards privacy leverages a\\ndata-driven adversarial co-optimization scheme. We design a mechanism that\\noptimizes flocking control parameters, such that leader inference is hindered.\\nAs the flocking performance improves, we successively train an adversarial\\ndiscriminator that tries to infer the identity of the leader robot. To evaluate\\nthe performance of our co-optimization scheme, we investigate different classes\\nof reference trajectories. Although it is reasonable to assume that there is an\\ninherent trade-off between flocking performance and privacy, our results\\ndemonstrate that we are able to achieve high flocking performance and\\nsimultaneously reduce the risk of revealing the leader.\",\n",
       "    'author': [{'name': 'Hehui Zheng',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'University of Cambridge'}},\n",
       "     {'name': 'Jacopo Panerati',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Polytechnique Montreal'}},\n",
       "     {'name': 'Giovanni Beltrame',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Polytechnique Montreal'}},\n",
       "     {'name': 'Amanda Prorok',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'University of Cambridge'}}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2020.2967331'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2020.2967331',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1909.10387v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.10387v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages, 13 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters (2020)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.6; I.2.8; I.2.9',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/0812.0743v2',\n",
       "    'updated': '2009-10-10T09:10:36Z',\n",
       "    'published': '2008-12-03T15:46:03Z',\n",
       "    'title': 'A Novel Clustering Algorithm Based on Quantum Games',\n",
       "    'summary': \"Enormous successes have been made by quantum algorithms during the last\\ndecade. In this paper, we combine the quantum game with the problem of data\\nclustering, and then develop a quantum-game-based clustering algorithm, in\\nwhich data points in a dataset are considered as players who can make decisions\\nand implement quantum strategies in quantum games. After each round of a\\nquantum game, each player's expected payoff is calculated. Later, he uses a\\nlink-removing-and-rewiring (LRR) function to change his neighbors and adjust\\nthe strength of links connecting to them in order to maximize his payoff.\\nFurther, algorithms are discussed and analyzed in two cases of strategies, two\\npayoff matrixes and two LRR functions. Consequently, the simulation results\\nhave demonstrated that data points in datasets are clustered reasonably and\\nefficiently, and the clustering algorithms have fast rates of convergence.\\nMoreover, the comparison with other algorithms also provides an indication of\\nthe effectiveness of the proposed approach.\",\n",
       "    'author': [{'name': 'Qiang Li'},\n",
       "     {'name': 'Yan He'},\n",
       "     {'name': 'Jing-ping Jiang'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1088/1751-8113/42/44/445303'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1088/1751-8113/42/44/445303',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/0812.0743v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/0812.0743v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '19 pages, 5 figures, 5 tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2009 J. Phys. A: Math. Theor. 42 445303'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'quant-ph', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/cs/0105025v1',\n",
       "    'updated': '2001-05-15T19:07:28Z',\n",
       "    'published': '2001-05-15T19:07:28Z',\n",
       "    'title': 'Market-Based Reinforcement Learning in Partially Observable Worlds',\n",
       "    'summary': 'Unlike traditional reinforcement learning (RL), market-based RL is in\\nprinciple applicable to worlds described by partially observable Markov\\nDecision Processes (POMDPs), where an agent needs to learn short-term memories\\nof relevant previous events in order to execute optimal actions. Most previous\\nwork, however, has focused on reactive settings (MDPs) instead of POMDPs. Here\\nwe reimplement a recent approach to market-based RL and for the first time\\nevaluate it in a toy POMDP setting.',\n",
       "    'author': [{'name': 'Ivo Kwee'},\n",
       "     {'name': 'Marcus Hutter'},\n",
       "     {'name': 'Juergen Schmidhuber'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 LaTeX pages, 2 postscript figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Lecture Notes in Computer Science (LNCS 2130), Proceeding of the\\n  International Conference on Artificial Neural Networks ICANN (2001) 865-873'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/cs/0105025v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/cs/0105025v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1511.04143v4',\n",
       "    'updated': '2016-02-16T16:30:34Z',\n",
       "    'published': '2015-11-13T02:34:33Z',\n",
       "    'title': 'Deep Reinforcement Learning in Parameterized Action Space',\n",
       "    'summary': 'Recent work has shown that deep neural networks are capable of approximating\\nboth value functions and policies in reinforcement learning domains featuring\\ncontinuous state and action spaces. However, to the best of our knowledge no\\nprevious work has succeeded at using deep neural networks in structured\\n(parameterized) continuous action spaces. To fill this gap, this paper focuses\\non learning within the domain of simulated RoboCup soccer, which features a\\nsmall set of discrete action types, each of which is parameterized with\\ncontinuous variables. The best learned agent can score goals more reliably than\\nthe 2012 RoboCup champion agent. As such, this paper represents a successful\\nextension of deep reinforcement learning to the class of parameterized action\\nspace MDPs.',\n",
       "    'author': [{'name': 'Matthew Hausknecht'}, {'name': 'Peter Stone'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1511.04143v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1511.04143v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1811.08225v1',\n",
       "    'updated': '2018-11-20T13:00:51Z',\n",
       "    'published': '2018-11-20T13:00:51Z',\n",
       "    'title': 'Self Organizing Classifiers: First Steps in Structured Evolutionary\\n  Machine Learning',\n",
       "    'summary': \"Learning classifier systems (LCSs) are evolutionary machine learning\\nalgorithms, flexible enough to be applied to reinforcement, supervised and\\nunsupervised learning problems with good performance. Recently, self organizing\\nclassifiers were proposed which are similar to LCSs but have the advantage that\\nin its structured population no balance between niching and fitness pressure is\\nnecessary. However, more tests and analysis are required to verify its\\nbenefits. Here, a variation of the first algorithm is proposed which uses a\\nparameterless self organizing map (SOM). This algorithm is applied in\\nchallenging problems such as big, noisy as well as dynamically changing\\ncontinuous input-action mazes (growing and compressing mazes are included) with\\ngood performance. Moreover, a genetic operator is proposed which utilizes the\\ntopological information of the SOM's population structure, improving the\\nresults. Thus, the first steps in structured evolutionary machine learning are\\nshown, nonetheless, the problems faced are more difficult than the state-of-art\\ncontinuous input-action multi-step ones.\",\n",
       "    'author': [{'name': 'Danilo Vasconcellos Vargas'},\n",
       "     {'name': 'Hirotaka Takano'},\n",
       "     {'name': 'Junichi Murata'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Evolutionary Intelligence 6 (2), 57-72 (2013)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1811.08225v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1811.08225v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1811.08226v1',\n",
       "    'updated': '2018-11-20T13:01:29Z',\n",
       "    'published': '2018-11-20T13:01:29Z',\n",
       "    'title': 'Self Organizing Classifiers and Niched Fitness',\n",
       "    'summary': 'Learning classifier systems are adaptive learning systems which have been\\nwidely applied in a multitude of application domains. However, there are still\\nsome generalization problems unsolved. The hurdle is that fitness and niching\\npressures are difficult to balance. Here, a new algorithm called Self\\nOrganizing Classifiers is proposed which faces this problem from a different\\nperspective. Instead of balancing the pressures, both pressures are separated\\nand no balance is necessary. In fact, the proposed algorithm possesses a\\ndynamical population structure that self-organizes itself to better project the\\ninput space into a map. The niched fitness concept is defined along with its\\ndynamical population structure, both are indispensable for the understanding of\\nthe proposed method. Promising results are shown on two continuous multi-step\\nproblems. One of which is yet more challenging than previous problems of this\\nclass in the literature.',\n",
       "    'author': [{'name': 'Danilo Vasconcellos Vargas'},\n",
       "     {'name': 'Hirotaka Takano'},\n",
       "     {'name': 'Junichi Murata'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: text overlap with arXiv:1811.08225'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the 15th annual conference on Genetic and\\n  evolutionary computation (GECCO 2013)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1811.08226v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1811.08226v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.10545v5',\n",
       "    'updated': '2020-04-28T03:29:36Z',\n",
       "    'published': '2019-03-25T18:39:04Z',\n",
       "    'title': \"Winning Isn't Everything: Enhancing Game Development with Intelligent\\n  Agents\",\n",
       "    'summary': 'Recently, there have been several high-profile achievements of agents\\nlearning to play games against humans and beat them. In this paper, we study\\nthe problem of training intelligent agents in service of game development.\\nUnlike the agents built to \"beat the game\", our agents aim to produce\\nhuman-like behavior to help with game evaluation and balancing. We discuss two\\nfundamental metrics based on which we measure the human-likeness of agents,\\nnamely skill and style, which are multi-faceted concepts with practical\\nimplications outlined in this paper. We report four case studies in which the\\nstyle and skill requirements inform the choice of algorithms and metrics used\\nto train agents; ranging from A* search to state-of-the-art deep reinforcement\\nlearning. We, further, show that the learning potential of state-of-the-art\\ndeep RL models does not seamlessly transfer from the benchmark environments to\\ntarget ones without heavily tuning their hyperparameters, leading to linear\\nscaling of the engineering efforts and computational cost with the number of\\ntarget domains.',\n",
       "    'author': [{'name': 'Yunqi Zhao'},\n",
       "     {'name': 'Igor Borovikov'},\n",
       "     {'name': 'Fernando de Mesentier Silva'},\n",
       "     {'name': 'Ahmad Beirami'},\n",
       "     {'name': 'Jason Rupert'},\n",
       "     {'name': 'Caedmon Somers'},\n",
       "     {'name': 'Jesse Harder'},\n",
       "     {'name': 'John Kolen'},\n",
       "     {'name': 'Jervis Pinto'},\n",
       "     {'name': 'Reza Pourabolghasem'},\n",
       "     {'name': 'James Pestrak'},\n",
       "     {'name': 'Harold Chaput'},\n",
       "     {'name': 'Mohsen Sardari'},\n",
       "     {'name': 'Long Lin'},\n",
       "     {'name': 'Sundeep Narravula'},\n",
       "     {'name': 'Navid Aghdaie'},\n",
       "     {'name': 'Kazi Zaman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE Trans. Games'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.10545v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.10545v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.02010v1',\n",
       "    'updated': '2019-05-26T10:45:58Z',\n",
       "    'published': '2019-05-26T10:45:58Z',\n",
       "    'title': 'A Hybrid Algorithm for Metaheuristic Optimization',\n",
       "    'summary': 'We propose a novel, flexible algorithm for combining together\\nmetaheuristicoptimizers for non-convex optimization problems. Our approach\\ntreatsthe constituent optimizers as a team of complex agents that\\ncommunicateinformation amongst each other at various intervals during the\\nsimulationprocess. The information produced by each individual agent can be\\ncombinedin various ways via higher-level operators. In our experiments on\\nkeybenchmark functions, we investigate how the performance of our\\nalgorithmvaries with respect to several of its key modifiable properties.\\nFinally,we apply our proposed algorithm to classification problems involving\\ntheoptimization of support-vector machine classifiers.',\n",
       "    'author': [{'name': 'Sujit Pramod Khanna'},\n",
       "     {'name': 'Alexander Ororbia II'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages, 5 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.02010v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.02010v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.06212v1',\n",
       "    'updated': '2020-03-13T11:56:14Z',\n",
       "    'published': '2020-03-13T11:56:14Z',\n",
       "    'title': 'Accelerating and Improving AlphaZero Using Population Based Training',\n",
       "    'summary': 'AlphaZero has been very successful in many games. Unfortunately, it still\\nconsumes a huge amount of computing resources, the majority of which is spent\\nin self-play. Hyperparameter tuning exacerbates the training cost since each\\nhyperparameter configuration requires its own time to train one run, during\\nwhich it will generate its own self-play records. As a result, multiple runs\\nare usually needed for different hyperparameter configurations. This paper\\nproposes using population based training (PBT) to help tune hyperparameters\\ndynamically and improve strength during training time. Another significant\\nadvantage is that this method requires a single run only, while incurring a\\nsmall additional time cost, since the time for generating self-play records\\nremains unchanged though the time for optimization is increased following the\\nAlphaZero training algorithm. In our experiments for 9x9 Go, the PBT method is\\nable to achieve a higher win rate for 9x9 Go than the baselines, each with its\\nown hyperparameter configuration and trained individually. For 19x19 Go, with\\nPBT, we are able to obtain improvements in playing strength. Specifically, the\\nPBT agent can obtain up to 74% win rate against ELF OpenGo, an open-source\\nstate-of-the-art AlphaZero program using a neural network of a comparable\\ncapacity. This is compared to a saturated non-PBT agent, which achieves a win\\nrate of 47% against ELF OpenGo under the same circumstances.',\n",
       "    'author': [{'name': 'Ti-Rong Wu'},\n",
       "     {'name': 'Ting-Han Wei'},\n",
       "     {'name': 'I-Chen Wu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'accepted by AAAI2020 as oral presentation. In this version,\\n  supplementary materials are added'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.06212v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.06212v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.14616v1',\n",
       "    'updated': '2020-09-26T11:58:16Z',\n",
       "    'published': '2020-09-26T11:58:16Z',\n",
       "    'title': 'Lineage Evolution Reinforcement Learning',\n",
       "    'summary': 'We propose a general agent population learning system, and on this basis, we\\npropose lineage evolution reinforcement learning algorithm. Lineage evolution\\nreinforcement learning is a kind of derivative algorithm which accords with the\\ngeneral agent population learning system. We take the agents in DQN and its\\nrelated variants as the basic agents in the population, and add the selection,\\nmutation and crossover modules in the genetic algorithm to the reinforcement\\nlearning algorithm. In the process of agent evolution, we refer to the\\ncharacteristics of natural genetic behavior, add lineage factor to ensure the\\nretention of potential performance of agent, and comprehensively consider the\\ncurrent performance and lineage value when evaluating the performance of agent.\\nWithout changing the parameters of the original reinforcement learning\\nalgorithm, lineage evolution reinforcement learning can optimize different\\nreinforcement learning algorithms. Our experiments show that the idea of\\nevolution with lineage improves the performance of original reinforcement\\nlearning algorithm in some games in Atari 2600.',\n",
       "    'author': [{'name': 'Zeyu Zhang'}, {'name': 'Guisheng Yin'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.14616v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.14616v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.08858v1',\n",
       "    'updated': '2020-12-16T10:59:22Z',\n",
       "    'published': '2020-12-16T10:59:22Z',\n",
       "    'title': 'Lévy walks derived from a Bayesian decision-making model in\\n  non-stationary environments',\n",
       "    'summary': \"L\\\\'evy walks are found in the migratory behaviour patterns of various\\norganisms, and the reason for this phenomenon has been much discussed. We use\\nsimulations to demonstrate that learning causes the changes in confidence level\\nduring decision-making in non-stationary environments, and results in\\nL\\\\'evy-walk-like patterns. One inference algorithm involving confidence is\\nBayesian inference. We propose an algorithm that introduces the effects of\\nlearning and forgetting into Bayesian inference, and simulate an imitation game\\nin which two decision-making agents incorporating the algorithm estimate each\\nother's internal models from their opponent's observational data. For\\nforgetting without learning, agent confidence levels remained low due to a lack\\nof information on the counterpart and Brownian walks occurred for a wide range\\nof forgetting rates. Conversely, when learning was introduced, high confidence\\nlevels occasionally occurred even at high forgetting rates, and Brownian walks\\nuniversally became L\\\\'evy walks through a mixture of high- and low-confidence\\nstates.\",\n",
       "    'author': [{'name': 'Shuji Shinohara'},\n",
       "     {'name': 'Nobuhito Manome'},\n",
       "     {'name': 'Yoshihiro Nakajima'},\n",
       "     {'name': 'Yukio Pegio Gunji'},\n",
       "     {'name': 'Toru Moriyama'},\n",
       "     {'name': 'Hiroshi Okamoto'},\n",
       "     {'name': 'Shunji Mitsuyoshi'},\n",
       "     {'name': 'Ung-il Chung'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.08858v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.08858v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.01636v1',\n",
       "    'updated': '2021-03-02T10:48:29Z',\n",
       "    'published': '2021-03-02T10:48:29Z',\n",
       "    'title': 'Sparse Training Theory for Scalable and Efficient Agents',\n",
       "    'summary': 'A fundamental task for artificial intelligence is learning. Deep Neural\\nNetworks have proven to cope perfectly with all learning paradigms, i.e.\\nsupervised, unsupervised, and reinforcement learning. Nevertheless, traditional\\ndeep learning approaches make use of cloud computing facilities and do not\\nscale well to autonomous agents with low computational resources. Even in the\\ncloud, they suffer from computational and memory limitations, and they cannot\\nbe used to model adequately large physical worlds for agents which assume\\nnetworks with billions of neurons. These issues are addressed in the last few\\nyears by the emerging topic of sparse training, which trains sparse networks\\nfrom scratch. This paper discusses sparse training state-of-the-art, its\\nchallenges and limitations while introducing a couple of new theoretical\\nresearch directions which has the potential of alleviating sparse training\\nlimitations to push deep learning scalability well beyond its current\\nboundaries. Nevertheless, the theoretical advancements impact in complex\\nmulti-agents settings is discussed from a real-world perspective, using the\\nsmart grid case study.',\n",
       "    'author': [{'name': 'Decebal Constantin Mocanu'},\n",
       "     {'name': 'Elena Mocanu'},\n",
       "     {'name': 'Tiago Pinto'},\n",
       "     {'name': 'Selima Curci'},\n",
       "     {'name': 'Phuong H. Nguyen'},\n",
       "     {'name': 'Madeleine Gibescu'},\n",
       "     {'name': 'Damien Ernst'},\n",
       "     {'name': 'Zita A. Vale'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '20th International Conference on Autonomous Agents and Multiagent\\n  Systems (AAMAS 2021)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.01636v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.01636v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.03359v1',\n",
       "    'updated': '2021-02-27T07:29:43Z',\n",
       "    'published': '2021-02-27T07:29:43Z',\n",
       "    'title': 'Cognitive Homeostatic Agents',\n",
       "    'summary': \"Human brain has been used as an inspiration for building autonomous agents,\\nbut it is not obvious what level of computational description of the brain one\\nshould use. This has led to overly opinionated symbolic approaches and overly\\nunstructured connectionist approaches. We propose that using homeostasis as the\\ncomputational description provides a good compromise. Similar to how\\nphysiological homeostasis is the regulation of certain homeostatic variables,\\ncognition can be interpreted as the regulation of certain 'cognitive\\nhomeostatic variables'. We present an outline of a Cognitive Homeostatic Agent,\\nbuilt as a hierarchy of physiological and cognitive homeostatic subsystems and\\ndescribe structures and processes to guide future exploration. We expect this\\nto be a fruitful line of investigation towards building sophisticated\\nartificial agents that can act flexibly in complex environments, and produce\\nbehaviors indicating planning, thinking and feelings.\",\n",
       "    'author': {'name': 'Amol Kelkar'},\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.5555/3461017.3461021'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.5555/3461017.3461021',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2103.03359v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.03359v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at AAMAS2021 Blue Sky Ideas Track'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In Proc. of the 20th International Conference on Autonomous Agents\\n  and Multiagent Systems (AAMAS 2021), Online, May 3-7, 2021, IFAAMAS, 5 pages'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.15691v2',\n",
       "    'updated': '2022-10-12T18:19:12Z',\n",
       "    'published': '2021-06-29T19:53:15Z',\n",
       "    'title': 'Deep Multiagent Reinforcement Learning: Challenges and Directions',\n",
       "    'summary': \"This paper surveys the field of deep multiagent reinforcement learning. The\\ncombination of deep neural networks with reinforcement learning has gained\\nincreased traction in recent years and is slowly shifting the focus from\\nsingle-agent to multiagent environments. Dealing with multiple agents is\\ninherently more complex as (a) the future rewards depend on multiple players'\\njoint actions and (b) the computational complexity increases. We present the\\nmost common multiagent problem representations and their main challenges, and\\nidentify five research areas that address one or more of these challenges:\\ncentralised training and decentralised execution, opponent modelling,\\ncommunication, efficient coordination, and reward shaping. We find that many\\ncomputational studies rely on unrealistic assumptions or are not generalisable\\nto other settings; they struggle to overcome the curse of dimensionality or\\nnonstationarity. Approaches from psychology and sociology capture promising\\nrelevant behaviours, such as communication and coordination, to help agents\\nachieve better performance in multiagent settings. We suggest that, for\\nmultiagent reinforcement learning to be successful, future research should\\naddress these challenges with an interdisciplinary approach to open up new\\npossibilities in multiagent reinforcement learning.\",\n",
       "    'author': [{'name': 'Annie Wong'},\n",
       "     {'name': 'Thomas Bäck'},\n",
       "     {'name': 'Anna V. Kononova'},\n",
       "     {'name': 'Aske Plaat'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '41 pages, 6 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.15691v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.15691v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'A.1; I.2.6; I.2.8; J.4',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.06855v2',\n",
       "    'updated': '2022-05-03T23:12:08Z',\n",
       "    'published': '2022-03-14T04:53:26Z',\n",
       "    'title': 'DIAS: A Domain-Independent Alife-Based Problem-Solving System',\n",
       "    'summary': 'A domain-independent problem-solving system based on principles of Artificial\\nLife is introduced. In this system, DIAS, the input and output dimensions of\\nthe domain are laid out in a spatial medium. A population of actors, each\\nseeing only part of this medium, solves problems collectively in it. The\\nprocess is independent of the domain and can be implemented through different\\nkinds of actors. Through a set of experiments on various problem domains, DIAS\\nis shown able to solve problems with different dimensionality and complexity,\\nto require no hyperparameter tuning for new problems, and to exhibit lifelong\\nlearning, i.e. adapt rapidly to run-time changes in the problem domain, and do\\nit better than a standard non-collective approach. DIAS therefore demonstrates\\na role for Alife in building scalable, general, and adaptive problem-solving\\nsystems.',\n",
       "    'author': [{'name': 'Babak Hodjat'},\n",
       "     {'name': 'Hormoz Shahrzad'},\n",
       "     {'name': 'Risto Miikkulainen'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 6 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the 2022 Conference on Artificial Life'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.06855v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.06855v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.04957v1',\n",
       "    'updated': '2022-08-09T16:16:28Z',\n",
       "    'published': '2022-08-09T16:16:28Z',\n",
       "    'title': 'Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution',\n",
       "    'summary': 'Generating agents that can achieve Zero-Shot Coordination (ZSC) with unseen\\npartners is a new challenge in cooperative Multi-Agent Reinforcement Learning\\n(MARL). Recently, some studies have made progress in ZSC by exposing the agents\\nto diverse partners during the training process. They usually involve self-play\\nwhen training the partners, implicitly assuming that the tasks are homogeneous.\\nHowever, many real-world tasks are heterogeneous, and hence previous methods\\nmay fail. In this paper, we study the heterogeneous ZSC problem for the first\\ntime and propose a general method based on coevolution, which coevolves two\\npopulations of agents and partners through three sub-processes: pairing,\\nupdating and selection. Experimental results on a collaborative cooking task\\nshow the necessity of considering the heterogeneous setting and illustrate that\\nour proposed method is a promising solution for heterogeneous cooperative MARL.',\n",
       "    'author': [{'name': 'Ke Xue'},\n",
       "     {'name': 'Yutong Wang'},\n",
       "     {'name': 'Lei Yuan'},\n",
       "     {'name': 'Cong Guan'},\n",
       "     {'name': 'Chao Qian'},\n",
       "     {'name': 'Yang Yu'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.04957v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.04957v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.06835v1',\n",
       "    'updated': '2022-10-13T08:39:32Z',\n",
       "    'published': '2022-10-13T08:39:32Z',\n",
       "    'title': 'Multi-agent Dynamic Algorithm Configuration',\n",
       "    'summary': 'Automated algorithm configuration relieves users from tedious,\\ntrial-and-error tuning tasks. A popular algorithm configuration tuning paradigm\\nis dynamic algorithm configuration (DAC), in which an agent learns dynamic\\nconfiguration policies across instances by reinforcement learning (RL).\\nHowever, in many complex algorithms, there may exist different types of\\nconfiguration hyperparameters, and such heterogeneity may bring difficulties\\nfor classic DAC which uses a single-agent RL policy. In this paper, we aim to\\naddress this issue and propose multi-agent DAC (MA-DAC), with one agent working\\nfor one type of configuration hyperparameter. MA-DAC formulates the dynamic\\nconfiguration of a complex algorithm with multiple types of hyperparameters as\\na contextual multi-agent Markov decision process and solves it by a cooperative\\nmulti-agent RL (MARL) algorithm. To instantiate, we apply MA-DAC to a\\nwell-known optimization algorithm for multi-objective optimization problems.\\nExperimental results show the effectiveness of MA-DAC in not only achieving\\nsuperior performance compared with other configuration tuning approaches based\\non heuristic rules, multi-armed bandits, and single-agent RL, but also being\\ncapable of generalizing to different problem classes. Furthermore, we release\\nthe environments in this paper as a benchmark for testing MARL algorithms, with\\nthe hope of facilitating the application of MARL.',\n",
       "    'author': [{'name': 'Ke Xue'},\n",
       "     {'name': 'Jiacheng Xu'},\n",
       "     {'name': 'Lei Yuan'},\n",
       "     {'name': 'Miqing Li'},\n",
       "     {'name': 'Chao Qian'},\n",
       "     {'name': 'Zongzhang Zhang'},\n",
       "     {'name': 'Yang Yu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2022 Accept'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.06835v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.06835v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1804.01144v1',\n",
       "    'updated': '2018-04-03T19:44:09Z',\n",
       "    'published': '2018-04-03T19:44:09Z',\n",
       "    'title': 'Self-Organization and Artificial Life: A Review',\n",
       "    'summary': 'Self-organization has been an important concept within a number of\\ndisciplines, which Artificial Life (ALife) also has heavily utilized since its\\ninception. The term and its implications, however, are often confusing or\\nmisinterpreted. In this work, we provide a mini-review of self-organization and\\nits relationship with ALife, aiming at initiating discussions on this important\\ntopic with the interested audience. We first articulate some fundamental\\naspects of self-organization, outline its usage, and review its applications to\\nALife within its soft, hard, and wet domains. We also provide perspectives for\\nfurther research.',\n",
       "    'author': [{'name': 'Carlos Gershenson'},\n",
       "     {'name': 'Vito Trianni'},\n",
       "     {'name': 'Justin Werfel'},\n",
       "     {'name': 'Hiroki Sayama'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, submitted to ALife 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1804.01144v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1804.01144v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'nlin.AO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'nlin.AO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.10110v1',\n",
       "    'updated': '2021-06-18T13:05:25Z',\n",
       "    'published': '2021-06-18T13:05:25Z',\n",
       "    'title': 'Towards Distraction-Robust Active Visual Tracking',\n",
       "    'summary': \"In active visual tracking, it is notoriously difficult when distracting\\nobjects appear, as distractors often mislead the tracker by occluding the\\ntarget or bringing a confusing appearance. To address this issue, we propose a\\nmixed cooperative-competitive multi-agent game, where a target and multiple\\ndistractors form a collaborative team to play against a tracker and make it\\nfail to follow. Through learning in our game, diverse distracting behaviors of\\nthe distractors naturally emerge, thereby exposing the tracker's weakness,\\nwhich helps enhance the distraction-robustness of the tracker. For effective\\nlearning, we then present a bunch of practical methods, including a reward\\nfunction for distractors, a cross-modal teacher-student learning strategy, and\\na recurrent attention mechanism for the tracker. The experimental results show\\nthat our tracker performs desired distraction-robust active visual tracking and\\ncan be well generalized to unseen environments. We also show that the\\nmulti-agent game can be used to adversarially test the robustness of trackers.\",\n",
       "    'author': [{'name': 'Fangwei Zhong'},\n",
       "     {'name': 'Peng Sun'},\n",
       "     {'name': 'Wenhan Luo'},\n",
       "     {'name': 'Tingyun Yan'},\n",
       "     {'name': 'Yizhou Wang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in ICML2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.10110v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.10110v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2007.13729v1',\n",
       "    'updated': '2020-07-27T17:59:08Z',\n",
       "    'published': '2020-07-27T17:59:08Z',\n",
       "    'title': 'Noisy Agents: Self-supervised Exploration by Predicting Auditory Events',\n",
       "    'summary': \"Humans integrate multiple sensory modalities (e.g. visual and audio) to build\\na causal understanding of the physical world. In this work, we propose a novel\\ntype of intrinsic motivation for Reinforcement Learning (RL) that encourages\\nthe agent to understand the causal effect of its actions through auditory event\\nprediction. First, we allow the agent to collect a small amount of acoustic\\ndata and use K-means to discover underlying auditory event clusters. We then\\ntrain a neural network to predict the auditory events and use the prediction\\nerrors as intrinsic rewards to guide RL exploration. Experimental results on\\nAtari games show that our new intrinsic motivation significantly outperforms\\nseveral state-of-the-art baselines. We further visualize our noisy agents'\\nbehavior in a physics environment and demonstrate that our newly designed\\nintrinsic reward leads to the emergence of physical interaction behaviors (e.g.\\ncontact with objects).\",\n",
       "    'author': [{'name': 'Chuang Gan'},\n",
       "     {'name': 'Xiaoyu Chen'},\n",
       "     {'name': 'Phillip Isola'},\n",
       "     {'name': 'Antonio Torralba'},\n",
       "     {'name': 'Joshua B. Tenenbaum'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page: http://noisy-agent.csail.mit.edu'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2007.13729v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2007.13729v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.09341v1',\n",
       "    'updated': '2018-07-24T20:46:05Z',\n",
       "    'published': '2018-07-24T20:46:05Z',\n",
       "    'title': 'Learning Plannable Representations with Causal InfoGAN',\n",
       "    'summary': \"In recent years, deep generative models have been shown to 'imagine'\\nconvincing high-dimensional observations such as images, audio, and even video,\\nlearning directly from raw data. In this work, we ask how to imagine\\ngoal-directed visual plans -- a plausible sequence of observations that\\ntransition a dynamical system from its current configuration to a desired goal\\nstate, which can later be used as a reference trajectory for control. We focus\\non systems with high-dimensional observations, such as images, and propose an\\napproach that naturally combines representation learning and planning. Our\\nframework learns a generative model of sequential observations, where the\\ngenerative process is induced by a transition in a low-dimensional planning\\nmodel, and an additional noise. By maximizing the mutual information between\\nthe generated observations and the transition in the planning model, we obtain\\na low-dimensional representation that best explains the causal nature of the\\ndata. We structure the planning model to be compatible with efficient planning\\nalgorithms, and we propose several such models based on either discrete or\\ncontinuous states. Finally, to generate a visual plan, we project the current\\nand goal observations onto their respective states in the planning model, plan\\na trajectory, and then use the generative model to transform the trajectory to\\na sequence of observations. We demonstrate our method on imagining plausible\\nvisual plans of rope manipulation.\",\n",
       "    'author': [{'name': 'Thanard Kurutach'},\n",
       "     {'name': 'Aviv Tamar'},\n",
       "     {'name': 'Ge Yang'},\n",
       "     {'name': 'Stuart Russell'},\n",
       "     {'name': 'Pieter Abbeel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML / IJCAI / AAMAS 2018 Workshop on Planning and Learning (PAL-18)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.09341v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.09341v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2005.05960v2',\n",
       "    'updated': '2020-06-30T23:05:50Z',\n",
       "    'published': '2020-05-12T17:59:45Z',\n",
       "    'title': 'Planning to Explore via Self-Supervised World Models',\n",
       "    'summary': 'Reinforcement learning allows solving complex tasks, however, the learning\\ntends to be task-specific and the sample efficiency remains a challenge. We\\npresent Plan2Explore, a self-supervised reinforcement learning agent that\\ntackles both these challenges through a new approach to self-supervised\\nexploration and fast adaptation to new tasks, which need not be known during\\nexploration. During exploration, unlike prior methods which retrospectively\\ncompute the novelty of observations after the agent has already reached them,\\nour agent acts efficiently by leveraging planning to seek out expected future\\nnovelty. After exploration, the agent quickly adapts to multiple downstream\\ntasks in a zero or a few-shot manner. We evaluate on challenging control tasks\\nfrom high-dimensional image inputs. Without any training supervision or\\ntask-specific interaction, Plan2Explore outperforms prior self-supervised\\nexploration methods, and in fact, almost matches the performances oracle which\\nhas access to rewards. Videos and code at\\nhttps://ramanans1.github.io/plan2explore/',\n",
       "    'author': [{'name': 'Ramanan Sekar'},\n",
       "     {'name': 'Oleh Rybkin'},\n",
       "     {'name': 'Kostas Daniilidis'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Danijar Hafner'},\n",
       "     {'name': 'Deepak Pathak'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at ICML 2020. Videos and code at\\n  https://ramanans1.github.io/plan2explore/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2005.05960v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2005.05960v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1902.05546v2',\n",
       "    'updated': '2019-11-21T21:35:27Z',\n",
       "    'published': '2019-02-14T18:59:05Z',\n",
       "    'title': 'Learning to Control Self-Assembling Morphologies: A Study of\\n  Generalization via Modularity',\n",
       "    'summary': \"Contemporary sensorimotor learning approaches typically start with an\\nexisting complex agent (e.g., a robotic arm), which they learn to control. In\\ncontrast, this paper investigates a modular co-evolution strategy: a collection\\nof primitive agents learns to dynamically self-assemble into composite bodies\\nwhile also learning to coordinate their behavior to control these bodies. Each\\nprimitive agent consists of a limb with a motor attached at one end. Limbs may\\nchoose to link up to form collectives. When a limb initiates a link-up action,\\nand there is another limb nearby, the latter is magnetically connected to the\\n'parent' limb's motor. This forms a new single agent, which may further link\\nwith other agents. In this way, complex morphologies can emerge, controlled by\\na policy whose architecture is in explicit correspondence with the morphology.\\nWe evaluate the performance of these dynamic and modular agents in simulated\\nenvironments. We demonstrate better generalization to test-time changes both in\\nthe environment, as well as in the structure of the agent, compared to static\\nand monolithic baselines. Project video and code are available at\\nhttps://pathak22.github.io/modular-assemblies/\",\n",
       "    'author': [{'name': 'Deepak Pathak'},\n",
       "     {'name': 'Chris Lu'},\n",
       "     {'name': 'Trevor Darrell'},\n",
       "     {'name': 'Phillip Isola'},\n",
       "     {'name': 'Alexei A. Efros'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2019 (Spotlight). Videos at\\n  https://pathak22.github.io/modular-assemblies/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1902.05546v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1902.05546v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.15824v1',\n",
       "    'updated': '2022-11-28T23:20:47Z',\n",
       "    'published': '2022-11-28T23:20:47Z',\n",
       "    'title': 'CLAS: Coordinating Multi-Robot Manipulation with Central Latent Action\\n  Spaces',\n",
       "    'summary': 'Multi-robot manipulation tasks involve various control entities that can be\\nseparated into dynamically independent parts. A typical example of such\\nreal-world tasks is dual-arm manipulation. Learning to naively solve such tasks\\nwith reinforcement learning is often unfeasible due to the sample complexity\\nand exploration requirements growing with the dimensionality of the action and\\nstate spaces. Instead, we would like to handle such environments as multi-agent\\nsystems and have several agents control parts of the whole. However,\\ndecentralizing the generation of actions requires coordination across agents\\nthrough a channel limited to information central to the task. This paper\\nproposes an approach to coordinating multi-robot manipulation through learned\\nlatent action spaces that are shared across different agents. We validate our\\nmethod in simulated multi-robot manipulation tasks and demonstrate improvement\\nover previous baselines in terms of sample efficiency and learning performance.',\n",
       "    'author': [{'name': 'Elie Aljalbout'},\n",
       "     {'name': 'Maximilian Karl'},\n",
       "     {'name': 'Patrick van der Smagt'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.15824v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.15824v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.6; I.2.8; I.2.9',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1302.0723v2',\n",
       "    'updated': '2013-02-05T05:50:14Z',\n",
       "    'published': '2013-02-04T15:34:12Z',\n",
       "    'title': 'Multi-Robot Informative Path Planning for Active Sensing of\\n  Environmental Phenomena: A Tale of Two Algorithms',\n",
       "    'summary': 'A key problem of robotic environmental sensing and monitoring is that of\\nactive sensing: How can a team of robots plan the most informative observation\\npaths to minimize the uncertainty in modeling and predicting an environmental\\nphenomenon? This paper presents two principled approaches to efficient\\ninformation-theoretic path planning based on entropy and mutual information\\ncriteria for in situ active sensing of an important broad class of\\nwidely-occurring environmental phenomena called anisotropic fields. Our\\nproposed algorithms are novel in addressing a trade-off between active sensing\\nperformance and time efficiency. An important practical consequence is that our\\nalgorithms can exploit the spatial correlation structure of Gaussian\\nprocess-based anisotropic fields to improve time efficiency while preserving\\nnear-optimal active sensing performance. We analyze the time complexity of our\\nalgorithms and prove analytically that they scale better than state-of-the-art\\nalgorithms with increasing planning horizon length. We provide theoretical\\nguarantees on the active sensing performance of our algorithms for a class of\\nexploration tasks called transect sampling, which, in particular, can be\\nimproved with longer planning time and/or lower spatial correlation along the\\ntransect. Empirical evaluation on real-world anisotropic field data shows that\\nour algorithms can perform better or at least as well as the state-of-the-art\\nalgorithms while often incurring a few orders of magnitude less computational\\ntime, even when the field conditions are less favorable.',\n",
       "    'author': [{'name': 'Nannan Cao'},\n",
       "     {'name': 'Kian Hsiang Low'},\n",
       "     {'name': 'John M. Dolan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12th International Conference on Autonomous Agents and Multiagent\\n  Systems (AAMAS 2013), Extended version with proofs, 15 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1302.0723v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1302.0723v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1305.6129v1',\n",
       "    'updated': '2013-05-27T07:28:05Z',\n",
       "    'published': '2013-05-27T07:28:05Z',\n",
       "    'title': 'Information-Theoretic Approach to Efficient Adaptive Path Planning for\\n  Mobile Robotic Environmental Sensing',\n",
       "    'summary': 'Recent research in robot exploration and mapping has focused on sampling\\nenvironmental hotspot fields. This exploration task is formalized by Low,\\nDolan, and Khosla (2008) in a sequential decision-theoretic planning under\\nuncertainty framework called MASP. The time complexity of solving MASP\\napproximately depends on the map resolution, which limits its use in\\nlarge-scale, high-resolution exploration and mapping. To alleviate this\\ncomputational difficulty, this paper presents an information-theoretic approach\\nto MASP (iMASP) for efficient adaptive path planning; by reformulating the\\ncost-minimizing iMASP as a reward-maximizing problem, its time complexity\\nbecomes independent of map resolution and is less sensitive to increasing robot\\nteam size as demonstrated both theoretically and empirically. Using the\\nreward-maximizing dual, we derive a novel adaptive variant of maximum entropy\\nsampling, thus improving the induced exploration policy performance. It also\\nallows us to establish theoretical bounds quantifying the performance advantage\\nof optimal adaptive over non-adaptive policies and the performance quality of\\napproximately optimal vs. optimal adaptive policies. We show analytically and\\nempirically the superior performance of iMASP-based policies for sampling the\\nlog-Gaussian process to that of policies for the widely-used Gaussian process\\nin mapping the hotspot field. Lastly, we provide sufficient conditions that,\\nwhen met, guarantee adaptivity has no benefit under an assumed environment\\nmodel.',\n",
       "    'author': [{'name': 'Kian Hsiang Low'},\n",
       "     {'name': 'John M. Dolan'},\n",
       "     {'name': 'Pradeep Khosla'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '19th International Conference on Automated Planning and Scheduling\\n  (ICAPS 2009), Extended version with proofs, 11 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1305.6129v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1305.6129v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1703.02702v1',\n",
       "    'updated': '2017-03-08T04:58:51Z',\n",
       "    'published': '2017-03-08T04:58:51Z',\n",
       "    'title': 'Robust Adversarial Reinforcement Learning',\n",
       "    'summary': 'Deep neural networks coupled with fast simulation and improved computation\\nhave led to recent successes in the field of reinforcement learning (RL).\\nHowever, most current RL-based approaches fail to generalize since: (a) the gap\\nbetween simulation and real world is so large that policy-learning approaches\\nfail to transfer; (b) even if policy learning is done in real world, the data\\nscarcity leads to failed generalization from training to test scenarios (e.g.,\\ndue to different friction or object masses). Inspired from H-infinity control\\nmethods, we note that both modeling errors and differences in training and test\\nscenarios can be viewed as extra forces/disturbances in the system. This paper\\nproposes the idea of robust adversarial reinforcement learning (RARL), where we\\ntrain an agent to operate in the presence of a destabilizing adversary that\\napplies disturbance forces to the system. The jointly trained adversary is\\nreinforced -- that is, it learns an optimal destabilization policy. We\\nformulate the policy learning as a zero-sum, minimax objective function.\\nExtensive experiments in multiple environments (InvertedPendulum, HalfCheetah,\\nSwimmer, Hopper and Walker2d) conclusively demonstrate that our method (a)\\nimproves training stability; (b) is robust to differences in training/test\\nconditions; and c) outperform the baseline even in the absence of the\\nadversary.',\n",
       "    'author': [{'name': 'Lerrel Pinto'},\n",
       "     {'name': 'James Davidson'},\n",
       "     {'name': 'Rahul Sukthankar'},\n",
       "     {'name': 'Abhinav Gupta'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1703.02702v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1703.02702v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.10082v3',\n",
       "    'updated': '2018-05-20T08:36:24Z',\n",
       "    'published': '2017-09-28T17:44:09Z',\n",
       "    'title': 'Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep\\n  Reinforcement Learning',\n",
       "    'summary': \"Developing a safe and efficient collision avoidance policy for multiple\\nrobots is challenging in the decentralized scenarios where each robot generate\\nits paths without observing other robots' states and intents. While other\\ndistributed multi-robot collision avoidance systems exist, they often require\\nextracting agent-level features to plan a local collision-free action, which\\ncan be computationally prohibitive and not robust. More importantly, in\\npractice the performance of these methods are much lower than their centralized\\ncounterparts.\\n  We present a decentralized sensor-level collision avoidance policy for\\nmulti-robot systems, which directly maps raw sensor measurements to an agent's\\nsteering commands in terms of movement velocity. As a first step toward\\nreducing the performance gap between decentralized and centralized methods, we\\npresent a multi-scenario multi-stage training framework to find an optimal\\npolicy which is trained over a large number of robots on rich, complex\\nenvironments simultaneously using a policy gradient based reinforcement\\nlearning algorithm. We validate the learned sensor-level collision avoidance\\npolicy in a variety of simulated scenarios with thorough performance\\nevaluations and show that the final learned policy is able to find time\\nefficient, collision-free paths for a large-scale robot system. We also\\ndemonstrate that the learned policy can be well generalized to new scenarios\\nthat do not appear in the entire training period, including navigating a\\nheterogeneous group of robots and a large-scale scenario with 100 robots.\\nVideos are available at https://sites.google.com/view/drlmaca\",\n",
       "    'author': [{'name': 'Pinxin Long'},\n",
       "     {'name': 'Tingxiang Fan'},\n",
       "     {'name': 'Xinyi Liao'},\n",
       "     {'name': 'Wenxi Liu'},\n",
       "     {'name': 'Hao Zhang'},\n",
       "     {'name': 'Jia Pan'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.10082v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.10082v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.00584v1',\n",
       "    'updated': '2019-11-01T20:38:37Z',\n",
       "    'published': '2019-11-01T20:38:37Z',\n",
       "    'title': 'A Perceived Environment Design using a Multi-Modal Variational\\n  Autoencoder for learning Active-Sensing',\n",
       "    'summary': 'This contribution comprises the interplay between a multi-modal variational\\nautoencoder and an environment to a perceived environment, on which an agent\\ncan act. Furthermore, we conclude our work with a comparison to\\ncuriosity-driven learning.',\n",
       "    'author': [{'name': 'Timo Korthals'},\n",
       "     {'name': 'Malte Schilling'},\n",
       "     {'name': 'Jürgen Leitner'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Extended Abstract for the IROS 2019 Workshop on Deep Probabilistic\\n  Generative Models for Cognitive Architecture in Robotics'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.00584v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.00584v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.06095v2',\n",
       "    'updated': '2020-07-14T13:04:20Z',\n",
       "    'published': '2019-12-12T17:48:14Z',\n",
       "    'title': 'Graph Neural Networks for Decentralized Multi-Robot Path Planning',\n",
       "    'summary': \"Effective communication is key to successful, decentralized, multi-robot path\\nplanning. Yet, it is far from obvious what information is crucial to the task\\nat hand, and how and when it must be shared among robots. To side-step these\\nissues and move beyond hand-crafted heuristics, we propose a combined model\\nthat automatically synthesizes local communication and decision-making policies\\nfor robots navigating in constrained workspaces. Our architecture is composed\\nof a convolutional neural network (CNN) that extracts adequate features from\\nlocal observations, and a graph neural network (GNN) that communicates these\\nfeatures among robots. We train the model to imitate an expert algorithm, and\\nuse the resulting model online in decentralized planning involving only local\\ncommunication and local observations. We evaluate our method in simulations {by\\nnavigating teams of robots to their destinations in 2D} cluttered workspaces.\\nWe measure the success rates and sum of costs over the planned paths. The\\nresults show a performance close to that of our expert algorithm, demonstrating\\nthe validity of our approach. In particular, we show our model's capability to\\ngeneralize to previously unseen cases (involving larger environments and larger\\nrobot teams).\",\n",
       "    'author': [{'name': 'Qingbiao Li'},\n",
       "     {'name': 'Fernando Gama'},\n",
       "     {'name': 'Alejandro Ribeiro'},\n",
       "     {'name': 'Amanda Prorok'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This paper has been accepted in the IEEE/RSJ International Conference\\n  on Intelligent Robots and Systems (IROS) 2020. For the simulation demo, see\\n  this https URL \"https://youtu.be/AGDk2RozpMQ\"'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1912.06095v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.06095v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2001.00786v3',\n",
       "    'updated': '2021-04-28T09:22:53Z',\n",
       "    'published': '2020-01-03T11:16:41Z',\n",
       "    'title': 'Intelligent Roundabout Insertion using Deep Reinforcement Learning',\n",
       "    'summary': 'An important topic in the autonomous driving research is the development of\\nmaneuver planning systems. Vehicles have to interact and negotiate with each\\nother so that optimal choices, in terms of time and safety, are taken. For this\\npurpose, we present a maneuver planning module able to negotiate the entering\\nin busy roundabouts. The proposed module is based on a neural network trained\\nto predict when and how entering the roundabout throughout the whole duration\\nof the maneuver. Our model is trained with a novel implementation of A3C, which\\nwe will call Delayed A3C (D-A3C), in a synthetic environment where vehicles\\nmove in a realistic manner with interaction capabilities. In addition, the\\nsystem is trained such that agents feature a unique tunable behavior, emulating\\nreal world scenarios where drivers have their own driving styles. Similarly,\\nthe maneuver can be performed using different aggressiveness levels, which is\\nparticularly useful to manage busy scenarios where conservative rule-based\\npolicies would result in undefined waits.',\n",
       "    'author': [{'name': 'Alessandro Paolo Capasso'},\n",
       "     {'name': 'Giulio Bacchiani'},\n",
       "     {'name': 'Daniele Molinari'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.5220/0008915003780385'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.5220/0008915003780385',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2001.00786v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2001.00786v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of ICAART 2020, ISBN: 978-989-758-395-7'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.03168v1',\n",
       "    'updated': '2020-03-06T12:57:25Z',\n",
       "    'published': '2020-03-06T12:57:25Z',\n",
       "    'title': 'Lane-Merging Using Policy-based Reinforcement Learning and\\n  Post-Optimization',\n",
       "    'summary': 'Many current behavior generation methods struggle to handle real-world\\ntraffic situations as they do not scale well with complexity. However,\\nbehaviors can be learned off-line using data-driven approaches. Especially,\\nreinforcement learning is promising as it implicitly learns how to behave\\nutilizing collected experiences. In this work, we combine policy-based\\nreinforcement learning with local optimization to foster and synthesize the\\nbest of the two methodologies. The policy-based reinforcement learning\\nalgorithm provides an initial solution and guiding reference for the\\npost-optimization. Therefore, the optimizer only has to compute a single\\nhomotopy class, e.g.\\\\ drive behind or in front of the other vehicle. By storing\\nthe state-history during reinforcement learning, it can be used for constraint\\nchecking and the optimizer can account for interactions. The post-optimization\\nadditionally acts as a safety-layer and the novel method, thus, can be applied\\nin safety-critical applications. We evaluate the proposed method using\\nlane-change scenarios with a varying number of vehicles.',\n",
       "    'author': [{'name': 'Patrick Hart'},\n",
       "     {'name': 'Leonard Rychly'},\n",
       "     {'name': 'Alois Knol'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ITSC.2019.8917002'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ITSC.2019.8917002',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2003.03168v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.03168v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2007.15724v1',\n",
       "    'updated': '2020-07-30T20:14:42Z',\n",
       "    'published': '2020-07-30T20:14:42Z',\n",
       "    'title': 'MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement\\n  Learning in Mixed Dynamic Environments',\n",
       "    'summary': \"Multi-agent navigation in dynamic environments is of great industrial value\\nwhen deploying a large scale fleet of robot to real-world applications. This\\npaper proposes a decentralized partially observable multi-agent path planning\\nwith evolutionary reinforcement learning (MAPPER) method to learn an effective\\nlocal planning policy in mixed dynamic environments. Reinforcement\\nlearning-based methods usually suffer performance degradation on long-horizon\\ntasks with goal-conditioned sparse rewards, so we decompose the long-range\\nnavigation task into many easier sub-tasks under the guidance of a global\\nplanner, which increases agents' performance in large environments. Moreover,\\nmost existing multi-agent planning approaches assume either perfect information\\nof the surrounding environment or homogeneity of nearby dynamic agents, which\\nmay not hold in practice. Our approach models dynamic obstacles' behavior with\\nan image-based representation and trains a policy in mixed dynamic environments\\nwithout homogeneity assumption. To ensure multi-agent training stability and\\nperformance, we propose an evolutionary training approach that can be easily\\nscaled to large and complex environments. Experiments show that MAPPER is able\\nto achieve higher success rates and more stable performance when exposed to a\\nlarge number of non-cooperative dynamic obstacles compared with traditional\\nreaction-based planner LRA* and the state-of-the-art learning-based method.\",\n",
       "    'author': [{'name': 'Zuxin Liu'},\n",
       "     {'name': 'Baiming Chen'},\n",
       "     {'name': 'Hongyi Zhou'},\n",
       "     {'name': 'Guru Koushik'},\n",
       "     {'name': 'Martial Hebert'},\n",
       "     {'name': 'Ding Zhao'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, accepted at the 2020 IEEE/RSJ International Conference on\\n  Intelligent Robots and Systems (IROS 2020)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2007.15724v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2007.15724v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.04452v1',\n",
       "    'updated': '2020-08-10T23:09:05Z',\n",
       "    'published': '2020-08-10T23:09:05Z',\n",
       "    'title': 'Multi-Agent Safe Planning with Gaussian Processes',\n",
       "    'summary': \"Multi-agent safe systems have become an increasingly important area of study\\nas we can now easily have multiple AI-powered systems operating together. In\\nsuch settings, we need to ensure the safety of not only each individual agent,\\nbut also the overall system. In this paper, we introduce a novel multi-agent\\nsafe learning algorithm that enables decentralized safe navigation when there\\nare multiple different agents in the environment. This algorithm makes mild\\nassumptions about other agents and is trained in a decentralized fashion, i.e.\\nwith very little prior knowledge about other agents' policies. Experiments show\\nour algorithm performs well with the robots running other algorithms when\\noptimizing various objectives.\",\n",
       "    'author': [{'name': 'Zheqing Zhu'},\n",
       "     {'name': 'Erdem Bıyık'},\n",
       "     {'name': 'Dorsa Sadigh'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/IROS45743.2020.9341169'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/IROS45743.2020.9341169',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2008.04452v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.04452v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 5 figures. Published at IEEE/RSJ International Conference on\\n  Intelligent Robots and Systems (IROS) 2020'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.08807v1',\n",
       "    'updated': '2020-09-13T02:03:25Z',\n",
       "    'published': '2020-09-13T02:03:25Z',\n",
       "    'title': 'Monte Carlo Tree Search Based Tactical Maneuvering',\n",
       "    'summary': 'In this paper we explore the application of simultaneous move Monte Carlo\\nTree Search (MCTS) based online framework for tactical maneuvering between two\\nunmanned aircrafts. Compared to other techniques, MCTS enables efficient search\\nover long horizons and uses self-play to select best maneuver in the current\\nstate while accounting for the opponent aircraft tactics. We explore different\\nalgorithmic choices in MCTS and demonstrate the framework numerically in a\\nsimulated 2D tactical maneuvering application.',\n",
       "    'author': [{'name': 'Kunal Srivastava'}, {'name': 'Amit Surana'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2009.08807v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.08807v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.03894v1',\n",
       "    'updated': '2020-11-08T02:56:42Z',\n",
       "    'published': '2020-11-08T02:56:42Z',\n",
       "    'title': 'Multimodal Trajectory Prediction via Topological Invariance for\\n  Navigation at Uncontrolled Intersections',\n",
       "    'summary': \"We focus on decentralized navigation among multiple non-communicating\\nrational agents at \\\\emph{uncontrolled} intersections, i.e., street\\nintersections without traffic signs or signals. Avoiding collisions in such\\ndomains relies on the ability of agents to predict each others' intentions\\nreliably, and react quickly. Multiagent trajectory prediction is NP-hard\\nwhereas the sample complexity of existing data-driven approaches limits their\\napplicability. Our key insight is that the geometric structure of the\\nintersection and the incentive of agents to move efficiently and avoid\\ncollisions (rationality) reduces the space of likely behaviors, effectively\\nrelaxing the problem of trajectory prediction. In this paper, we collapse the\\nspace of multiagent trajectories at an intersection into a set of modes\\nrepresenting different classes of multiagent behavior, formalized using a\\nnotion of topological invariance. Based on this formalism, we design Multiple\\nTopologies Prediction (MTP), a data-driven trajectory-prediction mechanism that\\nreconstructs trajectory representations of high-likelihood modes in multiagent\\nintersection scenes. We show that MTP outperforms a state-of-the-art multimodal\\ntrajectory prediction baseline (MFP) in terms of prediction accuracy by 78.24%\\non a challenging simulated dataset. Finally, we show that MTP enables our\\noptimization-based planner, MTPnav, to achieve collision-free and\\ntime-efficient navigation across a variety of challenging intersection\\nscenarios on the CARLA simulator.\",\n",
       "    'author': [{'name': 'Junha Roh'},\n",
       "     {'name': 'Christoforos Mavrogiannis'},\n",
       "     {'name': 'Rishabh Madan'},\n",
       "     {'name': 'Dieter Fox'},\n",
       "     {'name': 'Siddhartha S. Srinivasa'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Preprint of a paper with the same title, accepted to the Conference\\n  on Robot Learning 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.03894v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.03894v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.00499v1',\n",
       "    'updated': '2021-05-02T16:01:34Z',\n",
       "    'published': '2021-05-02T16:01:34Z',\n",
       "    'title': 'Curious Exploration and Return-based Memory Restoration for Deep\\n  Reinforcement Learning',\n",
       "    'summary': 'Reward engineering and designing an incentive reward function are non-trivial\\ntasks to train agents in complex environments. Furthermore, an inaccurate\\nreward function may lead to a biased behaviour which is far from an efficient\\nand optimised behaviour. In this paper, we focus on training a single agent to\\nscore goals with binary success/failure reward function in Half Field Offense\\ndomain. As the major advantage of this research, the agent has no presumption\\nabout the environment which means it only follows the original formulation of\\nreinforcement learning agents. The main challenge of using such a reward\\nfunction is the high sparsity of positive reward signals. To address this\\nproblem, we use a simple prediction-based exploration strategy (called Curious\\nExploration) along with a Return-based Memory Restoration (RMR) technique which\\ntends to remember more valuable memories. The proposed method can be utilized\\nto train agents in environments with fairly complex state and action spaces.\\nOur experimental results show that many recent solutions including our baseline\\nmethod fail to learn and perform in complex soccer domain. However, the\\nproposed method can converge easily to the nearly optimal behaviour. The video\\npresenting the performance of our trained agent is available at\\nhttp://bit.ly/HFO_Binary_Reward.',\n",
       "    'author': [{'name': 'Saeed Tafazzol'},\n",
       "     {'name': 'Erfan Fathi'},\n",
       "     {'name': 'Mahdi Rezaei'},\n",
       "     {'name': 'Ehsan Asali'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.00499v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.00499v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.08601v3',\n",
       "    'updated': '2022-09-14T15:04:06Z',\n",
       "    'published': '2021-05-18T15:32:07Z',\n",
       "    'title': 'Graph Neural Networks for Decentralized Multi-Robot Submodular Action\\n  Selection',\n",
       "    'summary': \"The problem of decentralized multi-robot target tracking asks for jointly\\nselecting actions, e.g., motion primitives, for the robots to maximize target\\ntracking performance with local communications. One major challenge for\\npractical implementations is to make target tracking approaches scalable for\\nlarge-scale problem instances. In this work, we propose a general-purpose\\nlearning architecture toward collaborative target tracking at scale, with\\ndecentralized communications. Particularly, our learning architecture leverages\\na graph neural network (GNN) to capture local interactions of the robots and\\nlearns decentralized decision-making for the robots. We train the learning\\nmodel by imitating an expert solution and implement the resulting model for\\ndecentralized action selection involving local observations and communications\\nonly. We demonstrate the performance of our GNN-based learning approach in a\\nscenario of active target tracking with large networks of robots. The\\nsimulation results show our approach nearly matches the tracking performance of\\nthe expert algorithm, and yet runs several orders faster with up to 100 robots.\\nMoreover, it slightly outperforms a decentralized greedy algorithm but runs\\nfaster (especially with more than 20 robots). The results also exhibit our\\napproach's generalization capability in previously unseen scenarios, e.g.,\\nlarger environments and larger networks of robots.\",\n",
       "    'author': [{'name': 'Lifeng Zhou'},\n",
       "     {'name': 'Vishnu D. Sharma'},\n",
       "     {'name': 'Qingbiao Li'},\n",
       "     {'name': 'Amanda Prorok'},\n",
       "     {'name': 'Alejandro Ribeiro'},\n",
       "     {'name': 'Pratap Tokekar'},\n",
       "     {'name': 'Vijay Kumar'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.08601v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.08601v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.09012v1',\n",
       "    'updated': '2021-12-16T16:47:00Z',\n",
       "    'published': '2021-12-16T16:47:00Z',\n",
       "    'title': 'Centralizing State-Values in Dueling Networks for Multi-Robot\\n  Reinforcement Learning Mapless Navigation',\n",
       "    'summary': 'We study the problem of multi-robot mapless navigation in the popular\\nCentralized Training and Decentralized Execution (CTDE) paradigm. This problem\\nis challenging when each robot considers its path without explicitly sharing\\nobservations with other robots and can lead to non-stationary issues in Deep\\nReinforcement Learning (DRL). The typical CTDE algorithm factorizes the joint\\naction-value function into individual ones, to favor cooperation and achieve\\ndecentralized execution. Such factorization involves constraints (e.g.,\\nmonotonicity) that limit the emergence of novel behaviors in an individual as\\neach agent is trained starting from a joint action-value. In contrast, we\\npropose a novel architecture for CTDE that uses a centralized state-value\\nnetwork to compute a joint state-value, which is used to inject global state\\ninformation in the value-based updates of the agents. Consequently, each model\\ncomputes its gradient update for the weights, considering the overall state of\\nthe environment. Our idea follows the insights of Dueling Networks as a\\nseparate estimation of the joint state-value has both the advantage of\\nimproving sample efficiency, while providing each robot information whether the\\nglobal state is (or is not) valuable. Experiments in a robotic navigation task\\nwith 2 4, and 8 robots, confirm the superior performance of our approach over\\nprior CTDE methods (e.g., VDN, QMIX).',\n",
       "    'author': [{'name': 'Enrico Marchesini'},\n",
       "     {'name': 'Alessandro Farinelli'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 5 figures, 1 table. Accepted at IROS 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.09012v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.09012v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.08484v4',\n",
       "    'updated': '2022-06-24T20:24:35Z',\n",
       "    'published': '2022-01-20T22:54:32Z',\n",
       "    'title': 'Iterated Reasoning with Mutual Information in Cooperative and Byzantine\\n  Decentralized Teaming',\n",
       "    'summary': \"Information sharing is key in building team cognition and enables\\ncoordination and cooperation. High-performing human teams also benefit from\\nacting strategically with hierarchical levels of iterated communication and\\nrationalizability, meaning a human agent can reason about the actions of their\\nteammates in their decision-making. Yet, the majority of prior work in\\nMulti-Agent Reinforcement Learning (MARL) does not support iterated\\nrationalizability and only encourage inter-agent communication, resulting in a\\nsuboptimal equilibrium cooperation strategy. In this work, we show that\\nreformulating an agent's policy to be conditional on the policies of its\\nneighboring teammates inherently maximizes Mutual Information (MI) lower-bound\\nwhen optimizing under Policy Gradient (PG). Building on the idea of\\ndecision-making under bounded rationality and cognitive hierarchy theory, we\\nshow that our modified PG approach not only maximizes local agent rewards but\\nalso implicitly reasons about MI between agents without the need for any\\nexplicit ad-hoc regularization terms. Our approach, InfoPG, outperforms\\nbaselines in learning emergent collaborative behaviors and sets the\\nstate-of-the-art in decentralized cooperative MARL tasks. Our experiments\\nvalidate the utility of InfoPG by achieving higher sample efficiency and\\nsignificantly larger cumulative reward in several complex cooperative\\nmulti-agent domains.\",\n",
       "    'author': [{'name': 'Sachin Konan'},\n",
       "     {'name': 'Esmaeil Seraj'},\n",
       "     {'name': 'Matthew Gombolay'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The first two authors contributed equally to this work (Published in\\n  ICLR 2022)'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Learning Representations 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.08484v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.08484v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.11994v2',\n",
       "    'updated': '2022-01-31T08:01:14Z',\n",
       "    'published': '2022-01-28T09:12:01Z',\n",
       "    'title': 'FCMNet: Full Communication Memory Net for Team-Level Cooperation in\\n  Multi-Agent Systems',\n",
       "    'summary': \"Decentralized cooperation in partially-observable multi-agent systems\\nrequires effective communications among agents. To support this effort, this\\nwork focuses on the class of problems where global communications are available\\nbut may be unreliable, thus precluding differentiable communication learning\\nmethods. We introduce FCMNet, a reinforcement learning based approach that\\nallows agents to simultaneously learn a) an effective multi-hop communications\\nprotocol and b) a common, decentralized policy that enables team-level\\ndecision-making. Specifically, our proposed method utilizes the hidden states\\nof multiple directional recurrent neural networks as communication messages\\namong agents. Using a simple multi-hop topology, we endow each agent with the\\nability to receive information sequentially encoded by every other agent at\\neach time step, leading to improved global cooperation. We demonstrate FCMNet\\non a challenging set of StarCraft II micromanagement tasks with shared rewards,\\nas well as a collaborative multi-agent pathfinding task with individual\\nrewards. There, our comparison results show that FCMNet outperforms\\nstate-of-the-art communication-based reinforcement learning methods in all\\nStarCraft II micromanagement tasks, and value decomposition methods in certain\\ntasks. We further investigate the robustness of FCMNet under realistic\\ncommunication disturbances, such as random message loss or binarized messages\\n(i.e., non-differentiable communication channels), to showcase FMCNet's\\npotential applicability to robotic tasks under a variety of real-world\\nconditions.\",\n",
       "    'author': [{'name': 'Yutong Wang'}, {'name': 'Guillaume Sartoretti'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in the International Conference on Autonomous Agents and\\n  Multiagent Systems (AAMAS 2022)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.11994v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.11994v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.15925v2',\n",
       "    'updated': '2023-01-19T19:49:47Z',\n",
       "    'published': '2022-03-29T22:02:28Z',\n",
       "    'title': 'Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional\\n  Reasoning Approach',\n",
       "    'summary': 'Multi-agent policy gradient methods have demonstrated success in games and\\nrobotics but are often limited to problems with low-level action space.\\nHowever, when agents take higher-level, temporally-extended actions (i.e.\\noptions), when and how to derive a centralized control policy, its gradient as\\nwell as sampling options for all agents while not interrupting current option\\nexecutions, becomes a challenge. This is mostly because agents may choose and\\nterminate their options \\\\textit{asynchronously}. In this work, we propose a\\nconditional reasoning approach to address this problem, and empirically\\nvalidate its effectiveness on representative option-based multi-agent\\ncooperative tasks.',\n",
       "    'author': [{'name': 'Xubo Lyu'},\n",
       "     {'name': 'Amin Banitalebi-Dehkordi'},\n",
       "     {'name': 'Mo Chen'},\n",
       "     {'name': 'Yong Zhang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICRA2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.15925v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.15925v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.10759v1',\n",
       "    'updated': '2022-04-22T15:26:25Z',\n",
       "    'published': '2022-04-22T15:26:25Z',\n",
       "    'title': 'The Boltzmann Policy Distribution: Accounting for Systematic\\n  Suboptimality in Human Models',\n",
       "    'summary': 'Models of human behavior for prediction and collaboration tend to fall into\\ntwo categories: ones that learn from large amounts of data via imitation\\nlearning, and ones that assume human behavior to be noisily-optimal for some\\nreward function. The former are very useful, but only when it is possible to\\ngather a lot of human data in the target environment and distribution. The\\nadvantage of the latter type, which includes Boltzmann rationality, is the\\nability to make accurate predictions in new environments without extensive data\\nwhen humans are actually close to optimal. However, these models fail when\\nhumans exhibit systematic suboptimality, i.e. when their deviations from\\noptimal behavior are not independent, but instead consistent over time. Our key\\ninsight is that systematic suboptimality can be modeled by predicting policies,\\nwhich couple action choices over time, instead of trajectories. We introduce\\nthe Boltzmann policy distribution (BPD), which serves as a prior over human\\npolicies and adapts via Bayesian inference to capture systematic deviations by\\nobserving human actions during a single episode. The BPD is difficult to\\ncompute and represent because policies lie in a high-dimensional continuous\\nspace, but we leverage tools from generative and sequence models to enable\\nefficient sampling and inference. We show that the BPD enables prediction of\\nhuman behavior and human-AI collaboration equally as well as imitation\\nlearning-based human models while using far less data.',\n",
       "    'author': [{'name': 'Cassidy Laidlaw'}, {'name': 'Anca Dragan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at ICLR 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.10759v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.10759v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.08686v2',\n",
       "    'updated': '2022-10-11T09:08:26Z',\n",
       "    'published': '2022-06-17T11:09:06Z',\n",
       "    'title': 'Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement\\n  Learning',\n",
       "    'summary': 'Achieving human-level dexterity is an important open problem in robotics.\\nHowever, tasks of dexterous hand manipulation, even at the baby level, are\\nchallenging to solve through reinforcement learning (RL). The difficulty lies\\nin the high degrees of freedom and the required cooperation among heterogeneous\\nagents (e.g., joints of fingers). In this study, we propose the Bimanual\\nDexterous Hands Benchmark (Bi-DexHands), a simulator that involves two\\ndexterous hands with tens of bimanual manipulation tasks and thousands of\\ntarget objects. Specifically, tasks in Bi-DexHands are designed to match\\ndifferent levels of human motor skills according to cognitive science\\nliterature. We built Bi-DexHands in the Issac Gym; this enables highly\\nefficient RL training, reaching 30,000+ FPS by only one single NVIDIA RTX 3090.\\nWe provide a comprehensive benchmark for popular RL algorithms under different\\nsettings; this includes Single-agent/Multi-agent RL, Offline RL, Multi-task RL,\\nand Meta RL. Our results show that the PPO type of on-policy algorithms can\\nmaster simple manipulation tasks that are equivalent up to 48-month human\\nbabies (e.g., catching a flying object, opening a bottle), while multi-agent RL\\ncan further help to master manipulations that require skilled bimanual\\ncooperation (e.g., lifting a pot, stacking blocks). Despite the success on each\\nsingle task, when it comes to acquiring multiple manipulation skills, existing\\nRL algorithms fail to work in most of the multi-task and the few-shot learning\\nsettings, which calls for more substantial development from the RL community.\\nOur project is open sourced at https://github.com/PKU-MARL/DexterousHands.',\n",
       "    'author': [{'name': 'Yuanpei Chen'},\n",
       "     {'name': 'Tianhao Wu'},\n",
       "     {'name': 'Shengjie Wang'},\n",
       "     {'name': 'Xidong Feng'},\n",
       "     {'name': 'Jiechuang Jiang'},\n",
       "     {'name': 'Stephen Marcus McAleer'},\n",
       "     {'name': 'Yiran Geng'},\n",
       "     {'name': 'Hao Dong'},\n",
       "     {'name': 'Zongqing Lu'},\n",
       "     {'name': 'Song-Chun Zhu'},\n",
       "     {'name': 'Yaodong Yang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '38 pages, 8 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.08686v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.08686v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.09889v2',\n",
       "    'updated': '2022-11-28T15:57:10Z',\n",
       "    'published': '2022-06-20T16:51:44Z',\n",
       "    'title': 'Nocturne: a scalable driving benchmark for bringing multi-agent learning\\n  one step closer to the real world',\n",
       "    'summary': 'We introduce \\\\textit{Nocturne}, a new 2D driving simulator for investigating\\nmulti-agent coordination under partial observability. The focus of Nocturne is\\nto enable research into inference and theory of mind in real-world multi-agent\\nsettings without the computational overhead of computer vision and feature\\nextraction from images. Agents in this simulator only observe an obstructed\\nview of the scene, mimicking human visual sensing constraints. Unlike existing\\nbenchmarks that are bottlenecked by rendering human-like observations directly\\nusing a camera input, Nocturne uses efficient intersection methods to compute a\\nvectorized set of visible features in a C++ back-end, allowing the simulator to\\nrun at $2000+$ steps-per-second. Using open-source trajectory and map data, we\\nconstruct a simulator to load and replay arbitrary trajectories and scenes from\\nreal-world driving data. Using this environment, we benchmark\\nreinforcement-learning and imitation-learning agents and demonstrate that the\\nagents are quite far from human-level coordination ability and deviate\\nsignificantly from the expert trajectories.',\n",
       "    'author': [{'name': 'Eugene Vinitsky'},\n",
       "     {'name': 'Nathan Lichtlé'},\n",
       "     {'name': 'Xiaomeng Yang'},\n",
       "     {'name': 'Brandon Amos'},\n",
       "     {'name': 'Jakob Foerster'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.09889v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.09889v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.08347v1',\n",
       "    'updated': '2022-09-17T15:10:49Z',\n",
       "    'published': '2022-09-17T15:10:49Z',\n",
       "    'title': 'Sub-optimal Policy Aided Multi-Agent Reinforcement Learning for Flocking\\n  Control',\n",
       "    'summary': 'Flocking control is a challenging problem, where multiple agents, such as\\ndrones or vehicles, need to reach a target position while maintaining the flock\\nand avoiding collisions with obstacles and collisions among agents in the\\nenvironment. Multi-agent reinforcement learning has achieved promising\\nperformance in flocking control. However, methods based on traditional\\nreinforcement learning require a considerable number of interactions between\\nagents and the environment. This paper proposes a sub-optimal policy aided\\nmulti-agent reinforcement learning algorithm (SPA-MARL) to boost sample\\nefficiency. SPA-MARL directly leverages a prior policy that can be manually\\ndesigned or solved with a non-learning method to aid agents in learning, where\\nthe performance of the policy can be sub-optimal. SPA-MARL recognizes the\\ndifference in performance between the sub-optimal policy and itself, and then\\nimitates the sub-optimal policy if the sub-optimal policy is better. We\\nleverage SPA-MARL to solve the flocking control problem. A traditional control\\nmethod based on artificial potential fields is used to generate a sub-optimal\\npolicy. Experiments demonstrate that SPA-MARL can speed up the training process\\nand outperform both the MARL baseline and the used sub-optimal policy.',\n",
       "    'author': [{'name': 'Yunbo Qiu'},\n",
       "     {'name': 'Yue Jin'},\n",
       "     {'name': 'Jian Wang'},\n",
       "     {'name': 'Xudong Zhang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by IEEE International Conference on Systems, Man, and\\n  Cybernetics (SMC) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.08347v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.08347v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.10113v2',\n",
       "    'updated': '2022-10-11T01:48:28Z',\n",
       "    'published': '2022-09-20T16:36:23Z',\n",
       "    'title': 'Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning',\n",
       "    'summary': 'Synchronizing decisions across multiple agents in realistic settings is\\nproblematic since it requires agents to wait for other agents to terminate and\\ncommunicate about termination reliably. Ideally, agents should learn and\\nexecute asynchronously instead. Such asynchronous methods also allow temporally\\nextended actions that can take different amounts of time based on the situation\\nand action executed. Unfortunately, current policy gradient methods are not\\napplicable in asynchronous settings, as they assume that agents synchronously\\nreason about action selection at every time step. To allow asynchronous\\nlearning and decision-making, we formulate a set of asynchronous multi-agent\\nactor-critic methods that allow agents to directly optimize asynchronous\\npolicies in three standard training paradigms: decentralized learning,\\ncentralized learning, and centralized training for decentralized execution.\\nEmpirical results (in simulation and hardware) in a variety of realistic\\ndomains demonstrate the superiority of our approaches in large multi-agent\\nproblems and validate the effectiveness of our algorithms for learning\\nhigh-quality and asynchronous solutions.',\n",
       "    'author': [{'name': 'Yuchen Xiao'},\n",
       "     {'name': 'Weihao Tan'},\n",
       "     {'name': 'Christopher Amato'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: substantial text overlap with arXiv:2209.10003'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.10113v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.10113v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.14983v1',\n",
       "    'updated': '2022-11-28T01:11:11Z',\n",
       "    'published': '2022-11-28T01:11:11Z',\n",
       "    'title': 'Multiagent Reinforcement Learning for Autonomous Routing and Pickup\\n  Problem with Adaptation to Variable Demand',\n",
       "    'summary': 'We derive a learning framework to generate routing/pickup policies for a\\nfleet of vehicles tasked with servicing stochastically appearing requests on a\\ncity map. We focus on policies that 1) give rise to coordination amongst the\\nvehicles, thereby reducing wait times for servicing requests, 2) are\\nnon-myopic, considering a-priori unknown potential future requests, and 3) can\\nadapt to changes in the underlying demand distribution. Specifically, we are\\ninterested in adapting to fluctuations of actual demand conditions in urban\\nenvironments, such as on-peak vs. off-peak hours. We achieve this through a\\ncombination of (i) online play, a lookahead optimization method that improves\\nthe performance of rollout methods via an approximate policy iteration step,\\nand (ii) an offline approximation scheme that allows for adapting to changes in\\nthe underlying demand model. In particular, we achieve adaptivity of our\\nlearned policy to different demand distributions by quantifying a region of\\nvalidity using the q-valid radius of a Wasserstein Ambiguity Set. We propose a\\nmechanism for switching the originally trained offline approximation when the\\ncurrent demand is outside the original validity region. In this case, we\\npropose to use an offline architecture, trained on a historical demand model\\nthat is closer to the current demand in terms of Wasserstein distance. We learn\\nrouting and pickup policies over real taxicab requests in downtown San\\nFrancisco with high variability between on-peak and off-peak hours,\\ndemonstrating the ability of our method to adapt to real fluctuation in demand\\ndistributions. Our numerical results demonstrate that our method outperforms\\nrollout-based reinforcement learning, as well as several benchmarks based on\\nclassical methods from the field of operations research.',\n",
       "    'author': [{'name': 'Daniel Garces'},\n",
       "     {'name': 'Sushmita Bhattacharya'},\n",
       "     {'name': 'Stephanie Gil'},\n",
       "     {'name': 'Dimitri Bertsekas'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 6 figures, 3 tables, submitted to ICRA'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.14983v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.14983v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.15414v1',\n",
       "    'updated': '2022-11-14T13:25:22Z',\n",
       "    'published': '2022-11-14T13:25:22Z',\n",
       "    'title': 'Dynamic Collaborative Multi-Agent Reinforcement Learning Communication\\n  for Autonomous Drone Reforestation',\n",
       "    'summary': 'We approach autonomous drone-based reforestation with a collaborative\\nmulti-agent reinforcement learning (MARL) setup. Agents can communicate as part\\nof a dynamically changing network. We explore collaboration and communication\\non the back of a high-impact problem. Forests are the main resource to control\\nrising CO2 conditions. Unfortunately, the global forest volume is decreasing at\\nan unprecedented rate. Many areas are too large and hard to traverse to plant\\nnew trees. To efficiently cover as much area as possible, here we propose a\\nGraph Neural Network (GNN) based communication mechanism that enables\\ncollaboration. Agents can share location information on areas needing\\nreforestation, which increases viewed area and planted tree count. We compare\\nour proposed communication mechanism with a multi-agent baseline without the\\nability to communicate. Results show how communication enables collaboration\\nand increases collective performance, planting precision and the risk-taking\\npropensity of individual agents.',\n",
       "    'author': {'name': 'Philipp Dominic Siedler'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Deep Reinforcement Learning Workshop at the 36th Conference on Neural\\n  Information Processing Systems (NeurIPS 2022)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.15414v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.15414v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.08230v3',\n",
       "    'updated': '2023-01-25T02:31:02Z',\n",
       "    'published': '2022-12-16T01:38:35Z',\n",
       "    'title': 'An Energy-aware, Fault-tolerant, and Robust Deep Reinforcement Learning\\n  based approach for Multi-agent Patrolling Problems',\n",
       "    'summary': 'Autonomous vehicles are suited for continuous area patrolling problems.\\nHowever, finding an optimal patrolling strategy can be challenging for many\\nreasons. Firstly, patrolling environments are often complex and can include\\nunknown environmental factors. Secondly, autonomous vehicles can have failures\\nor hardware constraints, such as limited battery life. Importantly, patrolling\\nlarge areas often requires multiple agents that need to collectively coordinate\\ntheir actions. In this work, we consider these limitations and propose an\\napproach based on model-free, deep multi-agent reinforcement learning. In this\\napproach, the agents are trained to automatically recharge themselves when\\nrequired, to support continuous collective patrolling. A distributed\\nhomogeneous multi-agent architecture is proposed, where all patrolling agents\\nexecute identical policies locally based on their local observations and shared\\ninformation. This architecture provides a fault-tolerant and robust patrolling\\nsystem that can tolerate agent failures and allow supplementary agents to be\\nadded to replace failed agents or to increase the overall patrol performance.\\nThe solution is validated through simulation experiments from multiple\\nperspectives, including the overall patrol performance, the efficiency of\\nbattery recharging strategies, and the overall fault tolerance and robustness.',\n",
       "    'author': [{'name': 'Chenhao Tong'},\n",
       "     {'name': 'Aaron Harwood'},\n",
       "     {'name': 'Maria A. Rodriguez'},\n",
       "     {'name': 'Richard O. Sinnott'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.08230v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.08230v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.01250v1',\n",
       "    'updated': '2022-12-12T00:01:27Z',\n",
       "    'published': '2022-12-12T00:01:27Z',\n",
       "    'title': 'Decentralized cooperative perception for autonomous vehicles: Learning\\n  to value the unknown',\n",
       "    'summary': \"Recently, we have been witnesses of accidents involving autonomous vehicles\\nand their lack of sufficient information. One way to tackle this issue is to\\nbenefit from the perception of different view points, namely cooperative\\nperception. We propose here a decentralized collaboration, i.e. peer-to-peer,\\nin which the agents are active in their quest for full perception by asking for\\nspecific areas in their surroundings on which they would like to know more.\\nUltimately, we want to optimize a trade-off between the maximization of\\nknowledge about moving objects and the minimization of the total volume of\\ninformation received from others, to limit communication costs and message\\nprocessing time. For this, we propose a way to learn a communication policy\\nthat reverses the usual communication paradigm by only requesting from other\\nvehicles what is unknown to the ego-vehicle, instead of filtering on the sender\\nside. We tested three different generative models to be taken as base for a\\nDeep Reinforcement Learning (DRL) algorithm, and compared them to a\\nbroadcasting policy and a policy randomly selecting areas. In particular, we\\npropose Locally Predictable VAE (LP-VAE), which appears to be producing better\\nbelief states for predictions than state-of-the-art models, both as a\\nstandalone model and in the context of DRL. Experiments were conducted in the\\ndriving simulator CARLA. Our best models reached on average a gain of 25% of\\nthe total complementary information, while only requesting about 5% of the\\nego-vehicle's perceptual field. This trade-off is adjustable through the\\ninterpretable hyperparameters of our reward function.\",\n",
       "    'author': [{'name': 'Maxime Chaveroche'},\n",
       "     {'name': 'Franck Davoine'},\n",
       "     {'name': 'Véronique Cherfaoui'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Standalone version of the last chapter of Maxime Chaveroche's\\n  doctoral thesis, published in October 2021\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.01250v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.01250v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1101.5632v1',\n",
       "    'updated': '2011-01-28T21:27:31Z',\n",
       "    'published': '2011-01-28T21:27:31Z',\n",
       "    'title': 'Active Markov Information-Theoretic Path Planning for Robotic\\n  Environmental Sensing',\n",
       "    'summary': 'Recent research in multi-robot exploration and mapping has focused on\\nsampling environmental fields, which are typically modeled using the Gaussian\\nprocess (GP). Existing information-theoretic exploration strategies for\\nlearning GP-based environmental field maps adopt the non-Markovian problem\\nstructure and consequently scale poorly with the length of history of\\nobservations. Hence, it becomes computationally impractical to use these\\nstrategies for in situ, real-time active sampling. To ease this computational\\nburden, this paper presents a Markov-based approach to efficient\\ninformation-theoretic path planning for active sampling of GP-based fields. We\\nanalyze the time complexity of solving the Markov-based path planning problem,\\nand demonstrate analytically that it scales better than that of deriving the\\nnon-Markovian strategies with increasing length of planning horizon. For a\\nclass of exploration tasks called the transect sampling task, we provide\\ntheoretical guarantees on the active sampling performance of our Markov-based\\npolicy, from which ideal environmental field conditions and sampling task\\nsettings can be established to limit its performance degradation due to\\nviolation of the Markov assumption. Empirical evaluation on real-world\\ntemperature and plankton density field data shows that our Markov-based policy\\ncan generally achieve active sampling performance comparable to that of the\\nwidely-used non-Markovian greedy policies under less favorable realistic field\\nconditions and task settings while enjoying significant computational gain over\\nthem.',\n",
       "    'author': [{'name': 'Kian Hsiang Low'},\n",
       "     {'name': 'John M. Dolan'},\n",
       "     {'name': 'Pradeep Khosla'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10th International Conference on Autonomous Agents and Multiagent\\n  Systems (AAMAS 2011), Extended version with proofs, 11 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1101.5632v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1101.5632v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'G.3; I.2.8; I.2.9',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.11699v2',\n",
       "    'updated': '2020-02-10T21:06:43Z',\n",
       "    'published': '2019-11-26T17:08:40Z',\n",
       "    'title': 'Multi-Vehicle Mixed-Reality Reinforcement Learning for Autonomous\\n  Multi-Lane Driving',\n",
       "    'summary': 'Autonomous driving promises to transform road transport. Multi-vehicle and\\nmulti-lane scenarios, however, present unique challenges due to constrained\\nnavigation and unpredictable vehicle interactions. Learning-based\\nmethods---such as deep reinforcement learning---are emerging as a promising\\napproach to automatically design intelligent driving policies that can cope\\nwith these challenges. Yet, the process of safely learning multi-vehicle\\ndriving behaviours is hard: while collisions---and their near-avoidance---are\\nessential to the learning process, directly executing immature policies on\\nautonomous vehicles raises considerable safety concerns. In this article, we\\npresent a safe and efficient framework that enables the learning of driving\\npolicies for autonomous vehicles operating in a shared workspace, where the\\nabsence of collisions cannot be guaranteed. Key to our learning procedure is a\\nsim2real approach that uses real-world online policy adaptation in a\\nmixed-reality setup, where other vehicles and static obstacles exist in the\\nvirtual domain. This allows us to perform safe learning by simulating (and\\nlearning from) collisions between the learning agent(s) and other objects in\\nvirtual reality. Our results demonstrate that, after only a few runs in\\nmixed-reality, collisions are significantly reduced.',\n",
       "    'author': [{'name': 'Rupert Mitchell',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'University of Cambridge'}},\n",
       "     {'name': 'Jenny Fletcher',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'University of Cambridge'}},\n",
       "     {'name': 'Jacopo Panerati',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'University of Cambridge'}},\n",
       "     {'name': 'Amanda Prorok',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'University of Cambridge'}}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages, 8 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.11699v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.11699v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.6; I.2.9', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2001.10208v1',\n",
       "    'updated': '2020-01-28T08:37:33Z',\n",
       "    'published': '2020-01-28T08:37:33Z',\n",
       "    'title': 'Towards Learning Multi-agent Negotiations via Self-Play',\n",
       "    'summary': 'Making sophisticated, robust, and safe sequential decisions is at the heart\\nof intelligent systems. This is especially critical for planning in complex\\nmulti-agent environments, where agents need to anticipate other agents\\'\\nintentions and possible future actions. Traditional methods formulate the\\nproblem as a Markov Decision Process, but the solutions often rely on various\\nassumptions and become brittle when presented with corner cases. In contrast,\\ndeep reinforcement learning (Deep RL) has been very effective at finding\\npolicies by simultaneously exploring, interacting, and learning from\\nenvironments. Leveraging the powerful Deep RL paradigm, we demonstrate that an\\niterative procedure of self-play can create progressively more diverse\\nenvironments, leading to the learning of sophisticated and robust multi-agent\\npolicies. We demonstrate this in a challenging multi-agent simulation of\\nmerging traffic, where agents must interact and negotiate with others in order\\nto successfully merge on or off the road. While the environment starts off\\nsimple, we increase its complexity by iteratively adding an increasingly\\ndiverse set of agents to the agent \"zoo\" as training progresses. Qualitatively,\\nwe find that through self-play, our policies automatically learn interesting\\nbehaviors such as defensive driving, overtaking, yielding, and the use of\\nsignal lights to communicate intentions to other agents. In addition,\\nquantitatively, we show a dramatic improvement of the success rate of merging\\nmaneuvers from 63% to over 98%.',\n",
       "    'author': {'name': 'Yichuan Charlie Tang'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Autonomous Driving Workshop, IEEE International Conference on\\n  Computer Vision (ICCV 2019)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2001.10208v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2001.10208v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.06906v2',\n",
       "    'updated': '2020-11-09T05:34:13Z',\n",
       "    'published': '2020-03-15T19:49:20Z',\n",
       "    'title': 'Model-based Reinforcement Learning for Decentralized Multiagent\\n  Rendezvous',\n",
       "    'summary': 'Collaboration requires agents to align their goals on the fly. Underlying the\\nhuman ability to align goals with other agents is their ability to predict the\\nintentions of others and actively update their own plans. We propose\\nhierarchical predictive planning (HPP), a model-based reinforcement learning\\nmethod for decentralized multiagent rendezvous. Starting with pretrained,\\nsingle-agent point to point navigation policies and using noisy,\\nhigh-dimensional sensor inputs like lidar, we first learn via self-supervision\\nmotion predictions of all agents on the team. Next, HPP uses the prediction\\nmodels to propose and evaluate navigation subgoals for completing the\\nrendezvous task without explicit communication among agents. We evaluate HPP in\\na suite of unseen environments, with increasing complexity and numbers of\\nobstacles. We show that HPP outperforms alternative reinforcement learning,\\npath planning, and heuristic-based baselines on challenging, unseen\\nenvironments. Experiments in the real world demonstrate successful transfer of\\nthe prediction models from sim to real world without any additional\\nfine-tuning. Altogether, HPP removes the need for a centralized operator in\\nmultiagent systems by combining model-based RL and inference methods, enabling\\nagents to dynamically align plans.',\n",
       "    'author': [{'name': 'Rose E. Wang'},\n",
       "     {'name': 'J. Chase Kew'},\n",
       "     {'name': 'Dennis Lee'},\n",
       "     {'name': 'Tsang-Wei Edward Lee'},\n",
       "     {'name': 'Tingnan Zhang'},\n",
       "     {'name': 'Brian Ichter'},\n",
       "     {'name': 'Jie Tan'},\n",
       "     {'name': 'Aleksandra Faust'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2020. The video is available at: https://youtu.be/-ydXHUtPzWE'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.06906v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.06906v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2005.05420v2',\n",
       "    'updated': '2020-09-11T21:14:15Z',\n",
       "    'published': '2020-05-11T20:42:29Z',\n",
       "    'title': 'Mobile Robot Path Planning in Dynamic Environments through Globally\\n  Guided Reinforcement Learning',\n",
       "    'summary': 'Path planning for mobile robots in large dynamic environments is a\\nchallenging problem, as the robots are required to efficiently reach their\\ngiven goals while simultaneously avoiding potential conflicts with other robots\\nor dynamic objects. In the presence of dynamic obstacles, traditional solutions\\nusually employ re-planning strategies, which re-call a planning algorithm to\\nsearch for an alternative path whenever the robot encounters a conflict.\\nHowever, such re-planning strategies often cause unnecessary detours. To\\naddress this issue, we propose a learning-based technique that exploits\\nenvironmental spatio-temporal information. Different from existing\\nlearning-based methods, we introduce a globally guided reinforcement learning\\napproach (G2RL), which incorporates a novel reward structure that generalizes\\nto arbitrary environments. We apply G2RL to solve the multi-robot path planning\\nproblem in a fully distributed reactive manner. We evaluate our method across\\ndifferent map types, obstacle densities, and the number of robots. Experimental\\nresults show that G2RL generalizes well, outperforming existing distributed\\nmethods, and performing very similarly to fully centralized state-of-the-art\\nbenchmarks.',\n",
       "    'author': [{'name': 'Binyu Wang'},\n",
       "     {'name': 'Zhe Liu'},\n",
       "     {'name': 'Qingbiao Li'},\n",
       "     {'name': 'Amanda Prorok'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 4 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2005.05420v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2005.05420v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2006.15807v2',\n",
       "    'updated': '2020-12-12T20:52:26Z',\n",
       "    'published': '2020-06-29T04:55:59Z',\n",
       "    'title': 'Using Reinforcement Learning to Herd a Robotic Swarm to a Target\\n  Distribution',\n",
       "    'summary': 'In this paper, we present a reinforcement learning approach to designing a\\ncontrol policy for a \"leader\" agent that herds a swarm of \"follower\" agents,\\nvia repulsive interactions, as quickly as possible to a target probability\\ndistribution over a strongly connected graph. The leader control policy is a\\nfunction of the swarm distribution, which evolves over time according to a\\nmean-field model in the form of an ordinary difference equation. The dependence\\nof the policy on agent populations at each graph vertex, rather than on\\nindividual agent activity, simplifies the observations required by the leader\\nand enables the control strategy to scale with the number of agents. Two\\nTemporal-Difference learning algorithms, SARSA and Q-Learning, are used to\\ngenerate the leader control policy based on the follower agent distribution and\\nthe leader\\'s location on the graph. A simulation environment corresponding to a\\ngrid graph with 4 vertices was used to train and validate the control policies\\nfor follower agent populations ranging from 10 to 100. Finally, the control\\npolicies trained on 100 simulated agents were used to successfully redistribute\\na physical swarm of 10 small robots to a target distribution among 4 spatial\\nregions.',\n",
       "    'author': [{'name': 'Zahi M. Kakish'},\n",
       "     {'name': 'Karthik Elamvazhuthi'},\n",
       "     {'name': 'Spring Berman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Paper was submitted to Conference on Robot Learning 2019 and IEEE\\n  Robotics and Automation Letters 2020 Revised, updated, and submitted to\\n  DARS/SWARMS 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2006.15807v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2006.15807v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.02616v2',\n",
       "    'updated': '2020-11-04T18:01:46Z',\n",
       "    'published': '2020-08-06T12:48:08Z',\n",
       "    'title': 'The Emergence of Adversarial Communication in Multi-Agent Reinforcement\\n  Learning',\n",
       "    'summary': 'Many real-world problems require the coordination of multiple autonomous\\nagents. Recent work has shown the promise of Graph Neural Networks (GNNs) to\\nlearn explicit communication strategies that enable complex multi-agent\\ncoordination. These works use models of cooperative multi-agent systems whereby\\nagents strive to achieve a shared global goal. When considering agents with\\nself-interested local objectives, the standard design choice is to model these\\nas separate learning systems (albeit sharing the same environment). Such a\\ndesign choice, however, precludes the existence of a single, differentiable\\ncommunication channel, and consequently prohibits the learning of inter-agent\\ncommunication strategies. In this work, we address this gap by presenting a\\nlearning model that accommodates individual non-shared rewards and a\\ndifferentiable communication channel that is common among all agents. We focus\\non the case where agents have self-interested objectives, and develop a\\nlearning algorithm that elicits the emergence of adversarial communications. We\\nperform experiments on multi-agent coverage and path planning problems, and\\nemploy a post-hoc interpretability technique to visualize the messages that\\nagents communicate to each other. We show how a single self-interested agent is\\ncapable of learning highly manipulative communication strategies that allows it\\nto significantly outperform a cooperative team of agents.',\n",
       "    'author': [{'name': 'Jan Blumenkamp'}, {'name': 'Amanda Prorok'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to Conference on Robot Learning (CoRL) 2020. Camera-ready\\n  version incorporating rebuttal'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.02616v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.02616v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.00508v1',\n",
       "    'updated': '2020-12-01T14:21:58Z',\n",
       "    'published': '2020-12-01T14:21:58Z',\n",
       "    'title': 'Gaussian Process Based Message Filtering for Robust Multi-Agent\\n  Cooperation in the Presence of Adversarial Communication',\n",
       "    'summary': \"In this paper, we consider the problem of providing robustness to adversarial\\ncommunication in multi-agent systems. Specifically, we propose a solution\\ntowards robust cooperation, which enables the multi-agent system to maintain\\nhigh performance in the presence of anonymous non-cooperative agents that\\ncommunicate faulty, misleading or manipulative information. In pursuit of this\\ngoal, we propose a communication architecture based on Graph Neural Networks\\n(GNNs), which is amenable to a novel Gaussian Process (GP)-based probabilistic\\nmodel characterizing the mutual information between the simultaneous\\ncommunications of different agents due to their physical proximity and relative\\nposition. This model allows agents to locally compute approximate posterior\\nprobabilities, or confidences, that any given one of their communication\\npartners is being truthful. These confidences can be used as weights in a\\nmessage filtering scheme, thereby suppressing the influence of suspicious\\ncommunication on the receiving agent's decisions. In order to assess the\\nefficacy of our method, we introduce a taxonomy of non-cooperative agents,\\nwhich distinguishes them by the amount of information available to them. We\\ndemonstrate in two distinct experiments that our method performs well across\\nthis taxonomy, outperforming alternative methods. For all but the best informed\\nadversaries, our filtering method is able to reduce the impact that\\nnon-cooperative agents cause, reducing it to the point of negligibility, and\\nwith negligible cost to performance in the absence of adversaries.\",\n",
       "    'author': [{'name': 'Rupert Mitchell'},\n",
       "     {'name': 'Jan Blumenkamp'},\n",
       "     {'name': 'Amanda Prorok'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.00508v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.00508v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.12070v1',\n",
       "    'updated': '2021-03-22T14:46:43Z',\n",
       "    'published': '2021-03-22T14:46:43Z',\n",
       "    'title': 'Learning to Robustly Negotiate Bi-Directional Lane Usage in\\n  High-Conflict Driving Scenarios',\n",
       "    'summary': 'Recently, autonomous driving has made substantial progress in addressing the\\nmost common traffic scenarios like intersection navigation and lane changing.\\nHowever, most of these successes have been limited to scenarios with\\nwell-defined traffic rules and require minimal negotiation with other vehicles.\\nIn this paper, we introduce a previously unconsidered, yet everyday,\\nhigh-conflict driving scenario requiring negotiations between agents of equal\\nrights and priorities. There exists no centralized control structure and we do\\nnot allow communications. Therefore, it is unknown if other drivers are willing\\nto cooperate, and if so to what extent. We train policies to robustly negotiate\\nwith opposing vehicles of an unobservable degree of cooperativeness using\\nmulti-agent reinforcement learning (MARL). We propose Discrete Asymmetric Soft\\nActor-Critic (DASAC), a maximum-entropy off-policy MARL algorithm allowing for\\ncentralized training with decentralized execution. We show that using DASAC we\\nare able to successfully negotiate and traverse the scenario considered over\\n99% of the time. Our agents are robust to an unknown timing of opponent\\ndecisions, an unobservable degree of cooperativeness of the opposing vehicle,\\nand previously unencountered policies. Furthermore, they learn to exhibit\\nhuman-like behaviors such as defensive driving, anticipating solution options\\nand interpreting the behavior of other agents.',\n",
       "    'author': [{'name': 'Christoph Killing'},\n",
       "     {'name': 'Adam Villaflor'},\n",
       "     {'name': 'John M. Dolan'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICRA48506.2021.9561071'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICRA48506.2021.9561071',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2103.12070v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.12070v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 7 figures'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.03546v1',\n",
       "    'updated': '2021-05-08T01:04:51Z',\n",
       "    'published': '2021-05-08T01:04:51Z',\n",
       "    'title': 'Scalable, Decentralized Multi-Agent Reinforcement Learning Methods\\n  Inspired by Stigmergy and Ant Colonies',\n",
       "    'summary': 'Bolstering multi-agent learning algorithms to tackle complex coordination and\\ncontrol tasks has been a long-standing challenge of on-going research. Numerous\\nmethods have been proposed to help reduce the effects of non-stationarity and\\nunscalability. In this work, we investigate a novel approach to decentralized\\nmulti-agent learning and planning that attempts to address these two\\nchallenges. In particular, this method is inspired by the cohesion,\\ncoordination, and behavior of ant colonies. As a result, these algorithms are\\ndesigned to be naturally scalable to systems with numerous agents. While no\\noptimality is guaranteed, the method is intended to work well in practice and\\nscale better in efficacy with the number of agents present than others. The\\napproach combines single-agent RL and an ant-colony-inspired decentralized,\\nstigmergic algorithm for multi-agent path planning and environment\\nmodification. Specifically, we apply this algorithm in a setting where agents\\nmust navigate to a goal location, learning to push rectangular boxes into holes\\nto yield new traversable pathways. It is shown that while the approach yields\\npromising success in this particular environment, it may not be as easily\\ngeneralized to others. The algorithm designed is notably scalable to numerous\\nagents but is limited in its performance due to its relatively simplistic,\\nrule-based approach. Furthermore, the composability of RL-trained policies is\\ncalled into question, where, while policies are successful in their training\\nenvironments, applying trained policies to a larger-scale, multi-agent\\nframework results in unpredictable behavior.',\n",
       "    'author': {'name': 'Austin Anhkhoi Nguyen'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '50 pages, 40 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.03546v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.03546v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.04678v1',\n",
       "    'updated': '2021-05-06T03:01:46Z',\n",
       "    'published': '2021-05-06T03:01:46Z',\n",
       "    'title': 'Incentivizing Efficient Equilibria in Traffic Networks with Mixed\\n  Autonomy',\n",
       "    'summary': \"Traffic congestion has large economic and social costs. The introduction of\\nautonomous vehicles can potentially reduce this congestion by increasing road\\ncapacity via vehicle platooning and by creating an avenue for influencing\\npeople's choice of routes. We consider a network of parallel roads with two\\nmodes of transportation: (i) human drivers, who will choose the quickest route\\navailable to them, and (ii) a ride hailing service, which provides an array of\\nautonomous vehicle route options, each with different prices, to users. We\\nformalize a model of vehicle flow in mixed autonomy and a model of how\\nautonomous service users make choices between routes with different prices and\\nlatencies. Developing an algorithm to learn the preferences of the users, we\\nformulate a planning optimization that chooses prices to maximize a social\\nobjective. We demonstrate the benefit of the proposed scheme by comparing the\\nresults to theoretical benchmarks which we show can be efficiently calculated.\",\n",
       "    'author': [{'name': 'Erdem Bıyık'},\n",
       "     {'name': 'Daniel A. Lazar'},\n",
       "     {'name': 'Ramtin Pedarsani'},\n",
       "     {'name': 'Dorsa Sadigh'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 7 figures, 2 tables. To appear at IEEE Transactions on\\n  Control of Network Systems (TCNS). arXiv admin note: substantial text overlap\\n  with arXiv:1904.02209'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.04678v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.04678v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.09232v3',\n",
       "    'updated': '2022-02-10T16:08:04Z',\n",
       "    'published': '2021-07-20T02:40:19Z',\n",
       "    'title': 'Using reinforcement learning to autonomously identify the source of\\n  errors for agents in a group mission',\n",
       "    'summary': 'When agents are swarmed to execute a mission, there is often a sudden failure\\nof some of the agents observed from the command base. Solely relying on the\\ncommunication between the command base and the concerning agent, it is\\ngenerally difficult to determine whether the failure is caused by actuators\\n(hypothesis, $h_a$) or sensors (hypothesis, $h_s$) However, by instigating\\ncollusion between the agents, we can pinpoint the cause of the failure, that\\nis, for $h_a$, we expect to detect corresponding displacements while for $h_a$\\nwe do not. We recommend that such swarm strategies applied to grasp the\\nsituation be autonomously generated by artificial intelligence (AI). Preferable\\nactions (${e.g.,}$ the collision) for the distinction will be those maximizing\\nthe difference between the expected behaviors for each hypothesis, as a value\\nfunction. Such actions exist, however, only very sparsely in the whole\\npossibilities, for which the conventional search based on gradient methods does\\nnot make sense. To mitigate the abovementioned shortcoming, we (successfully)\\napplied the reinforcement learning technique, achieving the maximization of\\nsuch a sparse value function. Machine learning was concluded autonomously.The\\ncolliding action is the basis of distinguishing the hypothesizes. To pinpoint\\nthe agent with the actuator error via this action, the agents behave as if they\\nare assisting the malfunctioning one to achieve a given mission.',\n",
       "    'author': [{'name': 'Keishu Utimula'},\n",
       "     {'name': 'Ken-taro Hayaschi'},\n",
       "     {'name': 'Trevor J. Bihl'},\n",
       "     {'name': 'Kousuke Nakano'},\n",
       "     {'name': 'Kenta Hongo'},\n",
       "     {'name': 'Ryo Maezono'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 2 figure. References added. It has been edited in English'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.09232v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.09232v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.06514v1',\n",
       "    'updated': '2021-09-14T08:18:47Z',\n",
       "    'published': '2021-09-14T08:18:47Z',\n",
       "    'title': 'Vision Transformer for Learning Driving Policies in Complex Multi-Agent\\n  Environments',\n",
       "    'summary': \"Driving in a complex urban environment is a difficult task that requires a\\ncomplex decision policy. In order to make informed decisions, one needs to gain\\nan understanding of the long-range context and the importance of other\\nvehicles. In this work, we propose to use Vision Transformer (ViT) to learn a\\ndriving policy in urban settings with birds-eye-view (BEV) input images. The\\nViT network learns the global context of the scene more effectively than with\\nearlier proposed Convolutional Neural Networks (ConvNets). Furthermore, ViT's\\nattention mechanism helps to learn an attention map for the scene which allows\\nthe ego car to determine which surrounding cars are important to its next\\ndecision. We demonstrate that a DQN agent with a ViT backbone outperforms\\nbaseline algorithms with ConvNet backbones pre-trained in various ways. In\\nparticular, the proposed method helps reinforcement learning algorithms to\\nlearn faster, with increased performance and less data than baselines.\",\n",
       "    'author': [{'name': 'Eshagh Kargar'}, {'name': 'Ville Kyrki'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.06514v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.06514v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.15266v3',\n",
       "    'updated': '2022-05-31T12:56:48Z',\n",
       "    'published': '2021-09-30T17:06:39Z',\n",
       "    'title': 'Modeling Interactions of Autonomous Vehicles and Pedestrians with Deep\\n  Multi-Agent Reinforcement Learning for Collision Avoidance',\n",
       "    'summary': \"Reliable pedestrian crash avoidance mitigation (PCAM) systems are crucial\\ncomponents of safe autonomous vehicles (AVs). The nature of the\\nvehicle-pedestrian interaction where decisions of one agent directly affect the\\nother agent's optimal behavior, and vice versa, is a challenging yet often\\nneglected aspect of such systems. We address this issue by modeling a Markov\\ndecision process (MDP) for a simulated AV-pedestrian interaction at an unmarked\\ncrosswalk. The AV's PCAM decision policy is learned through deep reinforcement\\nlearning (DRL). Since modeling pedestrians realistically is challenging, we\\ncompare two levels of intelligent pedestrian behavior. While the baseline model\\nfollows a predefined strategy, our advanced pedestrian model is defined as a\\nsecond DRL agent. This model captures continuous learning and the uncertainty\\ninherent in human behavior, making the AV-pedestrian interaction a deep\\nmulti-agent reinforcement learning (DMARL) problem. We benchmark the developed\\nPCAM systems according to the collision rate and the resulting traffic flow\\nefficiency with a focus on the influence of observation uncertainty on the\\ndecision-making of the agents. The results show that the AV is able to\\ncompletely mitigate collisions under the majority of the investigated\\nconditions and that the DRL pedestrian model learns an intelligent crossing\\nbehavior.\",\n",
       "    'author': [{'name': 'Raphael Trumpp'},\n",
       "     {'name': 'Harald Bayerlein'},\n",
       "     {'name': 'David Gesbert'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/IV51971.2022.9827451'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/IV51971.2022.9827451',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2109.15266v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.15266v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Intelligent Vehicles Symposium 2022'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.08229v1',\n",
       "    'updated': '2021-10-05T16:46:04Z',\n",
       "    'published': '2021-10-05T16:46:04Z',\n",
       "    'title': 'Influencing Towards Stable Multi-Agent Interactions',\n",
       "    'summary': \"Learning in multi-agent environments is difficult due to the non-stationarity\\nintroduced by an opponent's or partner's changing behaviors. Instead of\\nreactively adapting to the other agent's (opponent or partner) behavior, we\\npropose an algorithm to proactively influence the other agent's strategy to\\nstabilize -- which can restrain the non-stationarity caused by the other agent.\\nWe learn a low-dimensional latent representation of the other agent's strategy\\nand the dynamics of how the latent strategy evolves with respect to our robot's\\nbehavior. With this learned dynamics model, we can define an unsupervised\\nstability reward to train our robot to deliberately influence the other agent\\nto stabilize towards a single strategy. We demonstrate the effectiveness of\\nstabilizing in improving efficiency of maximizing the task reward in a variety\\nof simulated environments, including autonomous driving, emergent\\ncommunication, and robotic manipulation. We show qualitative results on our\\nwebsite: https://sites.google.com/view/stable-marl/.\",\n",
       "    'author': [{'name': 'Woodrow Z. Wang'},\n",
       "     {'name': 'Andy Shih'},\n",
       "     {'name': 'Annie Xie'},\n",
       "     {'name': 'Dorsa Sadigh'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '15 pages, 5 figures, Published as an Oral at Conference on Robot\\n  Learning (CoRL) 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.08229v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.08229v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.10823v1',\n",
       "    'updated': '2022-03-21T09:16:56Z',\n",
       "    'published': '2022-03-21T09:16:56Z',\n",
       "    'title': 'Long Short-Term Memory for Spatial Encoding in Multi-Agent Path Planning',\n",
       "    'summary': 'Reinforcement learning-based path planning for multi-agent systems of varying\\nsize constitutes a research topic with increasing significance as progress in\\ndomains such as urban air mobility and autonomous aerial vehicles continues.\\nReinforcement learning with continuous state and action spaces is used to train\\na policy network that accommodates desirable path planning behaviors and can be\\nused for time-critical applications. A Long Short-Term Memory module is\\nproposed to encode an unspecified number of states for a varying, indefinite\\nnumber of agents. The described training strategies and policy architecture\\nlead to a guidance that scales to an infinite number of agents and unlimited\\nphysical dimensions, although training takes place at a smaller scale. The\\nguidance is implemented on a low-cost, off-the-shelf onboard computer. The\\nfeasibility of the proposed approach is validated by presenting flight test\\nresults of up to four drones, autonomously navigating collision-free in a\\nreal-world environment.',\n",
       "    'author': [{'name': 'Marc R. Schlichting'},\n",
       "     {'name': 'Stefan Notter'},\n",
       "     {'name': 'Walter Fichter'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.2514/1.G006129'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.2514/1.G006129',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2203.10823v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.10823v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'For associated source code, see\\n  https://github.com/MarcSchlichting/LSTMSpatialEncoding , For associated video\\n  of flight test, see https://schlichting.page.link/lstm_flight_test , 17\\n  pages, 11 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'AIAA Journal of Guidance, Control, and Dynamics, March 2022'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.8; I.2.9; I.2.11; J.2',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.03516v1',\n",
       "    'updated': '2022-04-07T15:34:19Z',\n",
       "    'published': '2022-04-07T15:34:19Z',\n",
       "    'title': 'Distributed Reinforcement Learning for Robot Teams: A Review',\n",
       "    'summary': 'Purpose of review: Recent advances in sensing, actuation, and computation\\nhave opened the door to multi-robot systems consisting of hundreds/thousands of\\nrobots, with promising applications to automated manufacturing, disaster\\nrelief, harvesting, last-mile delivery, port/airport operations, or search and\\nrescue. The community has leveraged model-free multi-agent reinforcement\\nlearning (MARL) to devise efficient, scalable controllers for multi-robot\\nsystems (MRS). This review aims to provide an analysis of the state-of-the-art\\nin distributed MARL for multi-robot cooperation.\\n  Recent findings: Decentralized MRS face fundamental challenges, such as\\nnon-stationarity and partial observability. Building upon the \"centralized\\ntraining, decentralized execution\" paradigm, recent MARL approaches include\\nindependent learning, centralized critic, value decomposition, and\\ncommunication learning approaches. Cooperative behaviors are demonstrated\\nthrough AI benchmarks and fundamental real-world robotic capabilities such as\\nmulti-robot motion/path planning.\\n  Summary: This survey reports the challenges surrounding decentralized\\nmodel-free MARL for multi-robot cooperation and existing classes of approaches.\\nWe present benchmarks and robotic applications along with a discussion on\\ncurrent open avenues for research.',\n",
       "    'author': [{'name': 'Yutong Wang'},\n",
       "     {'name': 'Mehul Damani'},\n",
       "     {'name': 'Pamela Wang'},\n",
       "     {'name': 'Yuhong Cao'},\n",
       "     {'name': 'Guillaume Sartoretti'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Preprint of the paper submitted to Springer's Current Robotics\\n  Reports\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.03516v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.03516v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.00233v2',\n",
       "    'updated': '2023-01-02T18:38:48Z',\n",
       "    'published': '2022-06-01T04:57:50Z',\n",
       "    'title': 'DM$^2$: Decentralized Multi-Agent Reinforcement Learning for\\n  Distribution Matching',\n",
       "    'summary': 'Current approaches to multi-agent cooperation rely heavily on centralized\\nmechanisms or explicit communication protocols to ensure convergence. This\\npaper studies the problem of distributed multi-agent learning without resorting\\nto centralized components or explicit communication. It examines the use of\\ndistribution matching to facilitate the coordination of independent agents. In\\nthe proposed scheme, each agent independently minimizes the distribution\\nmismatch to the corresponding component of a target visitation distribution.\\nThe theoretical analysis shows that under certain conditions, each agent\\nminimizing its individual distribution mismatch allows the convergence to the\\njoint policy that generated the target distribution. Further, if the target\\ndistribution is from a joint policy that optimizes a cooperative task, the\\noptimal policy for a combination of this task reward and the distribution\\nmatching reward is the same joint policy. This insight is used to formulate a\\npractical algorithm (DM$^2$), in which each individual agent matches a target\\ndistribution derived from concurrently sampled trajectories from a joint expert\\npolicy. Experimental validation on the StarCraft domain shows that combining\\n(1) a task reward, and (2) a distribution matching reward for expert\\ndemonstrations for the same task, allows agents to outperform a naive\\ndistributed baseline. Additional experiments probe the conditions under which\\nexpert demonstrations need to be sampled to obtain the learning benefits.',\n",
       "    'author': [{'name': 'Caroline Wang'},\n",
       "     {'name': 'Ishan Durugkar'},\n",
       "     {'name': 'Elad Liebman'},\n",
       "     {'name': 'Peter Stone'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.00233v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.00233v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.0; I.2.8; I.2.9; I.2.11',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.08351v1',\n",
       "    'updated': '2022-09-17T15:24:37Z',\n",
       "    'published': '2022-09-17T15:24:37Z',\n",
       "    'title': 'Sample-Efficient Multi-Agent Reinforcement Learning with Demonstrations\\n  for Flocking Control',\n",
       "    'summary': 'Flocking control is a significant problem in multi-agent systems such as\\nmulti-agent unmanned aerial vehicles and multi-agent autonomous underwater\\nvehicles, which enhances the cooperativity and safety of agents. In contrast to\\ntraditional methods, multi-agent reinforcement learning (MARL) solves the\\nproblem of flocking control more flexibly. However, methods based on MARL\\nsuffer from sample inefficiency, since they require a huge number of\\nexperiences to be collected from interactions between agents and the\\nenvironment. We propose a novel method Pretraining with Demonstrations for MARL\\n(PwD-MARL), which can utilize non-expert demonstrations collected in advance\\nwith traditional methods to pretrain agents. During the process of pretraining,\\nagents learn policies from demonstrations by MARL and behavior cloning\\nsimultaneously, and are prevented from overfitting demonstrations. By\\npretraining with non-expert demonstrations, PwD-MARL improves sample efficiency\\nin the process of online MARL with a warm start. Experiments show that PwD-MARL\\nimproves sample efficiency and policy performance in the problem of flocking\\ncontrol, even with bad or few demonstrations.',\n",
       "    'author': [{'name': 'Yunbo Qiu'},\n",
       "     {'name': 'Yuzhu Zhan'},\n",
       "     {'name': 'Yue Jin'},\n",
       "     {'name': 'Jian Wang'},\n",
       "     {'name': 'Xudong Zhang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by IEEE Vehicular Technology Conference (VTC) 2022-Fall'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.08351v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.08351v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.03649v1',\n",
       "    'updated': '2022-10-07T15:56:59Z',\n",
       "    'published': '2022-10-07T15:56:59Z',\n",
       "    'title': 'How to Enable Uncertainty Estimation in Proximal Policy Optimization',\n",
       "    'summary': 'While deep reinforcement learning (RL) agents have showcased strong results\\nacross many domains, a major concern is their inherent opaqueness and the\\nsafety of such systems in real-world use cases. To overcome these issues, we\\nneed agents that can quantify their uncertainty and detect out-of-distribution\\n(OOD) states. Existing uncertainty estimation techniques, like Monte-Carlo\\nDropout or Deep Ensembles, have not seen widespread adoption in on-policy deep\\nRL. We posit that this is due to two reasons: concepts like uncertainty and OOD\\nstates are not well defined compared to supervised learning, especially for\\non-policy RL methods. Secondly, available implementations and comparative\\nstudies for uncertainty estimation methods in RL have been limited. To overcome\\nthe first gap, we propose definitions of uncertainty and OOD for Actor-Critic\\nRL algorithms, namely, proximal policy optimization (PPO), and present possible\\napplicable measures. In particular, we discuss the concepts of value and policy\\nuncertainty. The second point is addressed by implementing different\\nuncertainty estimation methods and comparing them across a number of\\nenvironments. The OOD detection performance is evaluated via a custom\\nevaluation benchmark of in-distribution (ID) and OOD states for various RL\\nenvironments. We identify a trade-off between reward and OOD detection\\nperformance. To overcome this, we formulate a Pareto optimization problem in\\nwhich we simultaneously optimize for reward and OOD detection performance. We\\nshow experimentally that the recently proposed method of Masksembles strikes a\\nfavourable balance among the survey methods, enabling high-quality uncertainty\\nestimation and OOD detection while matching the performance of original RL\\nagents.',\n",
       "    'author': [{'name': 'Eugene Bykovets'},\n",
       "     {'name': 'Yannick Metz'},\n",
       "     {'name': 'Mennatallah El-Assady'},\n",
       "     {'name': 'Daniel A. Keim'},\n",
       "     {'name': 'Joachim M. Buhmann'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.03649v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.03649v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2; I.2.6; I.2.8; I.2.9; I.2.10',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.07882v1',\n",
       "    'updated': '2022-11-15T04:15:03Z',\n",
       "    'published': '2022-11-15T04:15:03Z',\n",
       "    'title': 'Explainable Action Advising for Multi-Agent Reinforcement Learning',\n",
       "    'summary': \"Action advising is a knowledge transfer technique for reinforcement learning\\nbased on the teacher-student paradigm. An expert teacher provides advice to a\\nstudent during training in order to improve the student's sample efficiency and\\npolicy performance. Such advice is commonly given in the form of state-action\\npairs. However, it makes it difficult for the student to reason with and apply\\nto novel states. We introduce Explainable Action Advising, in which the teacher\\nprovides action advice as well as associated explanations indicating why the\\naction was chosen. This allows the student to self-reflect on what it has\\nlearned, enabling advice generalization and leading to improved sample\\nefficiency and learning performance - even in environments where the teacher is\\nsub-optimal. We empirically show that our framework is effective in both\\nsingle-agent and multi-agent scenarios, yielding improved policy returns and\\nconvergence rates when compared to state-of-the-art methods.\",\n",
       "    'author': [{'name': 'Yue Guo'},\n",
       "     {'name': 'Joseph Campbell'},\n",
       "     {'name': 'Simon Stepputtis'},\n",
       "     {'name': 'Ruiyu Li'},\n",
       "     {'name': 'Dana Hughes'},\n",
       "     {'name': 'Fei Fang'},\n",
       "     {'name': 'Katia Sycara'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.07882v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.07882v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.11498v1',\n",
       "    'updated': '2022-12-22T06:18:41Z',\n",
       "    'published': '2022-12-22T06:18:41Z',\n",
       "    'title': 'Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with\\n  Robotic and Human Co-Workers',\n",
       "    'summary': 'This project leverages advances in multi-agent reinforcement learning (MARL)\\nto improve the efficiency and flexibility of order-picking systems for\\ncommercial warehouses. We envision a warehouse of the future in which dozens of\\nmobile robots and human pickers work together to collect and deliver items\\nwithin the warehouse. The fundamental problem we tackle, called the\\norder-picking problem, is how these worker agents must coordinate their\\nmovement and actions in the warehouse to maximise performance (e.g. order\\nthroughput) under given resource constraints. Established industry methods\\nusing heuristic approaches require large engineering efforts to optimise for\\ninnately variable warehouse configurations. In contrast, the MARL framework can\\nbe flexibly applied to any warehouse configuration (e.g. size, layout,\\nnumber/types of workers, item replenishment frequency) and the agents learn via\\na process of trial-and-error how to optimally cooperate with one another. This\\npaper details the current status of the R&D effort initiated by Dematic and the\\nUniversity of Edinburgh towards a general-purpose and scalable MARL solution\\nfor the order-picking problem in realistic warehouses.',\n",
       "    'author': [{'name': 'Aleksandar Krnjaic'},\n",
       "     {'name': 'Jonathan D. Thomas'},\n",
       "     {'name': 'Georgios Papoudakis'},\n",
       "     {'name': 'Lukas Schäfer'},\n",
       "     {'name': 'Peter Börsting'},\n",
       "     {'name': 'Stefano V. Albrecht'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.11498v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.11498v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.05223v1',\n",
       "    'updated': '2023-01-12T18:59:34Z',\n",
       "    'published': '2023-01-12T18:59:34Z',\n",
       "    'title': 'NOPA: Neurally-guided Online Probabilistic Assistance for Building\\n  Socially Intelligent Home Assistants',\n",
       "    'summary': \"In this work, we study how to build socially intelligent robots to assist\\npeople in their homes. In particular, we focus on assistance with online goal\\ninference, where robots must simultaneously infer humans' goals and how to help\\nthem achieve those goals. Prior assistance methods either lack the adaptivity\\nto adjust helping strategies (i.e., when and how to help) in response to\\nuncertainty about goals or the scalability to conduct fast inference in a large\\ngoal space. Our NOPA (Neurally-guided Online Probabilistic Assistance) method\\naddresses both of these challenges. NOPA consists of (1) an online goal\\ninference module combining neural goal proposals with inverse planning and\\nparticle filtering for robust inference under uncertainty, and (2) a helping\\nplanner that discovers valuable subgoals to help with and is aware of the\\nuncertainty in goal inference. We compare NOPA against multiple baselines in a\\nnew embodied AI assistance challenge: Online Watch-And-Help, in which a helper\\nagent needs to simultaneously watch a main agent's action, infer its goal, and\\nhelp perform a common household task faster in realistic virtual home\\nenvironments. Experiments show that our helper agent robustly updates its goal\\ninference and adapts its helping plans to the changing level of uncertainty.\",\n",
       "    'author': [{'name': 'Xavier Puig'},\n",
       "     {'name': 'Tianmin Shu'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Antonio Torralba'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project website: https://www.tshu.io/online_watch_and_help. Code:\\n  https://github.com/xavierpuigf/online_watch_and_help'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.05223v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.05223v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.07137v1',\n",
       "    'updated': '2023-01-17T19:05:17Z',\n",
       "    'published': '2023-01-17T19:05:17Z',\n",
       "    'title': 'Heterogeneous Multi-Robot Reinforcement Learning',\n",
       "    'summary': 'Cooperative multi-robot tasks can benefit from heterogeneity in the robots\\'\\nphysical and behavioral traits. In spite of this, traditional Multi-Agent\\nReinforcement Learning (MARL) frameworks lack the ability to explicitly\\naccommodate policy heterogeneity, and typically constrain agents to share\\nneural network parameters. This enforced homogeneity limits application in\\ncases where the tasks benefit from heterogeneous behaviors. In this paper, we\\ncrystallize the role of heterogeneity in MARL policies. Towards this end, we\\nintroduce Heterogeneous Graph Neural Network Proximal Policy Optimization\\n(HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a\\nGraph Neural Network for differentiable inter-agent communication. HetGPPO\\nallows communicating agents to learn heterogeneous behaviors while enabling\\nfully decentralized training in partially observable environments. We\\ncomplement this with a taxonomical overview that exposes more heterogeneity\\nclasses than previously identified. To motivate the need for our model, we\\npresent a characterization of techniques that homogeneous models can leverage\\nto emulate heterogeneous behavior, and show how this \"apparent heterogeneity\"\\nis brittle in real-world conditions. Through simulations and real-world\\nexperiments, we show that: (i) when homogeneous methods fail due to strong\\nheterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous\\nmethods are able to learn apparently heterogeneous behaviors, HetGPPO achieves\\nhigher resilience to both training and deployment noise.',\n",
       "    'author': [{'name': 'Matteo Bettini'},\n",
       "     {'name': 'Ajay Shankar'},\n",
       "     {'name': 'Amanda Prorok'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.07137v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.07137v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.08451v1',\n",
       "    'updated': '2023-01-20T07:22:24Z',\n",
       "    'published': '2023-01-20T07:22:24Z',\n",
       "    'title': 'Accelerating Multi-Agent Planning Using Graph Transformers with Bounded\\n  Suboptimality',\n",
       "    'summary': 'Conflict-Based Search is one of the most popular methods for multi-agent path\\nfinding. Though it is complete and optimal, it does not scale well. Recent\\nworks have been proposed to accelerate it by introducing various heuristics.\\nHowever, whether these heuristics can apply to non-grid-based problem settings\\nwhile maintaining their effectiveness remains an open question. In this work,\\nwe find that the answer is prone to be no. To this end, we propose a\\nlearning-based component, i.e., the Graph Transformer, as a heuristic function\\nto accelerate the planning. The proposed method is provably complete and\\nbounded-suboptimal with any desired factor. We conduct extensive experiments on\\ntwo environments with dense graphs. Results show that the proposed Graph\\nTransformer can be trained in problem instances with relatively few agents and\\ngeneralizes well to a larger number of agents, while achieving better\\nperformance than state-of-the-art methods.',\n",
       "    'author': [{'name': 'Chenning Yu'},\n",
       "     {'name': 'Qingbiao Li'},\n",
       "     {'name': 'Sicun Gao'},\n",
       "     {'name': 'Amanda Prorok'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by ICRA 2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.08451v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.08451v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1605.09507v3',\n",
       "    'updated': '2016-12-26T12:29:26Z',\n",
       "    'published': '2016-05-31T07:11:18Z',\n",
       "    'title': 'Deep convolutional neural networks for predominant instrument\\n  recognition in polyphonic music',\n",
       "    'summary': 'Identifying musical instruments in polyphonic music recordings is a\\nchallenging but important problem in the field of music information retrieval.\\nIt enables music search by instrument, helps recognize musical genres, or can\\nmake music transcription easier and more accurate. In this paper, we present a\\nconvolutional neural network framework for predominant instrument recognition\\nin real-world polyphonic music. We train our network from fixed-length music\\nexcerpts with a single-labeled predominant instrument and estimate an arbitrary\\nnumber of predominant instruments from an audio signal with a variable length.\\nTo obtain the audio-excerpt-wise result, we aggregate multiple outputs from\\nsliding windows over the test audio. In doing so, we investigated two different\\naggregation methods: one takes the average for each instrument and the other\\ntakes the instrument-wise sum followed by normalization. In addition, we\\nconducted extensive experiments on several important factors that affect the\\nperformance, including analysis window size, identification threshold, and\\nactivation functions for neural networks to find the optimal set of parameters.\\nUsing a dataset of 10k audio excerpts from 11 instruments for evaluation, we\\nfound that convolutional neural networks are more robust than conventional\\nmethods that exploit spectral features and source separation with support\\nvector machines. Experimental results showed that the proposed convolutional\\nnetwork architecture obtained an F1 measure of 0.602 for micro and 0.503 for\\nmacro, respectively, achieving 19.6% and 16.4% in performance improvement\\ncompared with other state-of-the-art algorithms.',\n",
       "    'author': [{'name': 'Yoonchang Han'},\n",
       "     {'name': 'Jaehun Kim'},\n",
       "     {'name': 'Kyogu Lee'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TASLP.2016.2632307'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TASLP.2016.2632307',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1605.09507v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1605.09507v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '13 pages, 7 figures, accepted for publication in IEEE/ACM\\n  Transactions on Audio, Speech, and Language Processing on 16-Nov-2016. This\\n  is initial submission version. Fully edited version is available at\\n  http://ieeexplore.ieee.org/document/7755799/'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in: IEEE/ACM Transactions on Audio, Speech, and Language\\n  Processing ( Volume: 25, Issue: 1, Jan. 2017 ) Page(s): 208 - 221'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1608.04363v2',\n",
       "    'updated': '2016-11-28T17:48:04Z',\n",
       "    'published': '2016-08-15T18:57:10Z',\n",
       "    'title': 'Deep Convolutional Neural Networks and Data Augmentation for\\n  Environmental Sound Classification',\n",
       "    'summary': 'The ability of deep convolutional neural networks (CNN) to learn\\ndiscriminative spectro-temporal patterns makes them well suited to\\nenvironmental sound classification. However, the relative scarcity of labeled\\ndata has impeded the exploitation of this family of high-capacity models. This\\nstudy has two primary contributions: first, we propose a deep convolutional\\nneural network architecture for environmental sound classification. Second, we\\npropose the use of audio data augmentation for overcoming the problem of data\\nscarcity and explore the influence of different augmentations on the\\nperformance of the proposed CNN architecture. Combined with data augmentation,\\nthe proposed model produces state-of-the-art results for environmental sound\\nclassification. We show that the improved performance stems from the\\ncombination of a deep, high-capacity model and an augmented training set: this\\ncombination outperforms both the proposed CNN without augmentation and a\\n\"shallow\" dictionary learning model with augmentation. Finally, we examine the\\ninfluence of each augmentation on the model\\'s classification accuracy for each\\nclass, and observe that the accuracy for each class is influenced differently\\nby each augmentation, suggesting that the performance of the model could be\\nimproved further by applying class-conditional data augmentation.',\n",
       "    'author': [{'name': 'Justin Salamon'}, {'name': 'Juan Pablo Bello'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LSP.2017.2657381'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LSP.2017.2657381',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1608.04363v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1608.04363v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted November 2016, IEEE Signal Processing Letters. Copyright\\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\\n  be obtained for all other uses, in any current or future media, including\\n  reprinting/republishing this material, creating new collective works, for\\n  resale or redistribution, or reuse of any copyrighted component of this work\\n  in other works'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.00528v2',\n",
       "    'updated': '2021-04-19T03:25:50Z',\n",
       "    'published': '2021-03-31T04:09:30Z',\n",
       "    'title': 'OutlierNets: Highly Compact Deep Autoencoder Network Architectures for\\n  On-Device Acoustic Anomaly Detection',\n",
       "    'summary': 'Human operators often diagnose industrial machinery via anomalous sounds.\\nAutomated acoustic anomaly detection can lead to reliable maintenance of\\nmachinery. However, deep learning-driven anomaly detection methods often\\nrequire an extensive amount of computational resources which prohibits their\\ndeployment in factories. Here we explore a machine-driven design exploration\\nstrategy to create OutlierNets, a family of highly compact deep convolutional\\nautoencoder network architectures featuring as few as 686 parameters, model\\nsizes as small as 2.7 KB, and as low as 2.8 million FLOPs, with a detection\\naccuracy matching or exceeding published architectures with as many as 4\\nmillion parameters. Furthermore, CPU-accelerated latency experiments show that\\nthe OutlierNet architectures can achieve as much as 21x lower latency than\\npublished networks.',\n",
       "    'author': [{'name': 'Saad Abbasi'},\n",
       "     {'name': 'Mahmoud Famouri'},\n",
       "     {'name': 'Mohammad Javad Shafiee'},\n",
       "     {'name': 'Alexander Wong'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.00528v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.00528v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1809.00543v1',\n",
       "    'updated': '2018-09-03T10:44:28Z',\n",
       "    'published': '2018-09-03T10:44:28Z',\n",
       "    'title': 'Learning Vision-based Cohesive Flight in Drone Swarms',\n",
       "    'summary': 'This paper presents a data-driven approach to learning vision-based\\ncollective behavior from a simple flocking algorithm. We simulate a swarm of\\nquadrotor drones and formulate the controller as a regression problem in which\\nwe generate 3D velocity commands directly from raw camera images. The dataset\\nis created by simultaneously acquiring omnidirectional images and computing the\\ncorresponding control command from the flocking algorithm. We show that a\\nconvolutional neural network trained on the visual inputs of the drone can\\nlearn not only robust collision avoidance but also coherence of the flock in a\\nsample-efficient manner. The neural controller effectively learns to localize\\nother agents in the visual input, which we show by visualizing the regions with\\nthe most influence on the motion of an agent. This weakly supervised saliency\\nmap can be computed efficiently and may be used as a prior for subsequent\\ndetection and relative localization of other agents. We remove the dependence\\non sharing positions among flock members by taking only local visual\\ninformation into account for control. Our work can therefore be seen as the\\nfirst step towards a fully decentralized, vision-based flock without the need\\nfor communication or visual markers to aid detection of other agents.',\n",
       "    'author': [{'name': 'Fabian Schilling'},\n",
       "     {'name': 'Julien Lecoeur'},\n",
       "     {'name': 'Fabrizio Schiano'},\n",
       "     {'name': 'Dario Floreano'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1809.00543v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1809.00543v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1908.02999v1',\n",
       "    'updated': '2019-08-08T10:19:48Z',\n",
       "    'published': '2019-08-08T10:19:48Z',\n",
       "    'title': 'Learning Vision-based Flight in Drone Swarms by Imitation',\n",
       "    'summary': 'Decentralized drone swarms deployed today either rely on sharing of positions\\namong agents or detecting swarm members with the help of visual markers. This\\nwork proposes an entirely visual approach to coordinate markerless drone swarms\\nbased on imitation learning. Each agent is controlled by a small and efficient\\nconvolutional neural network that takes raw omnidirectional images as inputs\\nand predicts 3D velocity commands that match those computed by a flocking\\nalgorithm. We start training in simulation and propose a simple yet effective\\nunsupervised domain adaptation approach to transfer the learned controller to\\nthe real world. We further train the controller with data collected in our\\nmotion capture hall. We show that the convolutional neural network trained on\\nthe visual inputs of the drone can learn not only robust inter-agent collision\\navoidance but also cohesion of the swarm in a sample-efficient manner. The\\nneural controller effectively learns to localize other agents in the visual\\ninput, which we show by visualizing the regions with the most influence on the\\nmotion of an agent. We remove the dependence on sharing positions among swarm\\nmembers by taking only local visual information into account for control. Our\\nwork can therefore be seen as the first step towards a fully decentralized,\\nvision-based swarm without the need for communication or visual markers.',\n",
       "    'author': [{'name': 'Fabian Schilling'},\n",
       "     {'name': 'Julien Lecoeur'},\n",
       "     {'name': 'Fabrizio Schiano'},\n",
       "     {'name': 'Dario Floreano'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 8 figures, accepted for publication in the IEEE Robotics and\\n  Automation Letters (RA-L) on July 28, 2019. arXiv admin note: substantial\\n  text overlap with arXiv:1809.00543'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1908.02999v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1908.02999v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2002.06241v1',\n",
       "    'updated': '2020-02-14T20:11:13Z',\n",
       "    'published': '2020-02-14T20:11:13Z',\n",
       "    'title': 'Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein\\n  Graph Double-Attention Network',\n",
       "    'summary': 'Effective understanding of the environment and accurate trajectory prediction\\nof surrounding dynamic obstacles are indispensable for intelligent mobile\\nsystems (like autonomous vehicles and social robots) to achieve safe and\\nhigh-quality planning when they navigate in highly interactive and crowded\\nscenarios. Due to the existence of frequent interactions and uncertainty in the\\nscene evolution, it is desired for the prediction system to enable relational\\nreasoning on different entities and provide a distribution of future\\ntrajectories for each agent. In this paper, we propose a generic generative\\nneural system (called Social-WaGDAT) for multi-agent trajectory prediction,\\nwhich makes a step forward to explicit interaction modeling by incorporating\\nrelational inductive biases with a dynamic graph representation and leverages\\nboth trajectory and scene context information. We also employ an efficient\\nkinematic constraint layer applied to vehicle trajectory prediction which not\\nonly ensures physical feasibility but also enhances model performance. The\\nproposed system is evaluated on three public benchmark datasets for trajectory\\nprediction, where the agents cover pedestrians, cyclists and on-road vehicles.\\nThe experimental results demonstrate that our model achieves better performance\\nthan various baseline approaches in terms of prediction accuracy.',\n",
       "    'author': [{'name': 'Jiachen Li'},\n",
       "     {'name': 'Hengbo Ma'},\n",
       "     {'name': 'Zhihao Zhang'},\n",
       "     {'name': 'Masayoshi Tomizuka'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2002.06241v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2002.06241v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.13924v4',\n",
       "    'updated': '2020-10-22T16:02:44Z',\n",
       "    'published': '2020-03-31T02:49:23Z',\n",
       "    'title': 'EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational\\n  Reasoning',\n",
       "    'summary': 'Multi-agent interacting systems are prevalent in the world, from pure\\nphysical systems to complicated social dynamic systems. In many applications,\\neffective understanding of the situation and accurate trajectory prediction of\\ninteractive agents play a significant role in downstream tasks, such as\\ndecision making and planning. In this paper, we propose a generic trajectory\\nforecasting framework (named EvolveGraph) with explicit relational structure\\nrecognition and prediction via latent interaction graphs among multiple\\nheterogeneous, interactive agents. Considering the uncertainty of future\\nbehaviors, the model is designed to provide multi-modal prediction hypotheses.\\nSince the underlying interactions may evolve even with abrupt changes, and\\ndifferent modalities of evolution may lead to different outcomes, we address\\nthe necessity of dynamic relational reasoning and adaptively evolving the\\ninteraction graphs. We also introduce a double-stage training pipeline which\\nnot only improves training efficiency and accelerates convergence, but also\\nenhances model performance. The proposed framework is evaluated on both\\nsynthetic physics simulations and multiple real-world benchmark datasets in\\nvarious areas. The experimental results illustrate that our approach achieves\\nstate-of-the-art performance in terms of prediction accuracy.',\n",
       "    'author': [{'name': 'Jiachen Li'},\n",
       "     {'name': 'Fan Yang'},\n",
       "     {'name': 'Masayoshi Tomizuka'},\n",
       "     {'name': 'Chiho Choi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2020. Website:\\n  https://jiachenli94.github.io/publications/Evolvegraph/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.13924v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.13924v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2006.13164v3',\n",
       "    'updated': '2021-04-03T13:32:03Z',\n",
       "    'published': '2020-06-23T17:07:00Z',\n",
       "    'title': 'Joint Object Detection and Multi-Object Tracking with Graph Neural\\n  Networks',\n",
       "    'summary': 'Object detection and data association are critical components in multi-object\\ntracking (MOT) systems. Despite the fact that the two components are dependent\\non each other, prior works often design detection and data association modules\\nseparately which are trained with separate objectives. As a result, one cannot\\nback-propagate the gradients and optimize the entire MOT system, which leads to\\nsub-optimal performance. To address this issue, recent works simultaneously\\noptimize detection and data association modules under a joint MOT framework,\\nwhich has shown improved performance in both modules. In this work, we propose\\na new instance of joint MOT approach based on Graph Neural Networks (GNNs). The\\nkey idea is that GNNs can model relations between variable-sized objects in\\nboth the spatial and temporal domains, which is essential for learning\\ndiscriminative features for detection and data association. Through extensive\\nexperiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of\\nour GNN-based joint MOT approach and show state-of-the-art performance for both\\ndetection and MOT tasks. Our code is available at:\\nhttps://github.com/yongxinw/GSDT',\n",
       "    'author': [{'name': 'Yongxin Wang'},\n",
       "     {'name': 'Kris Kitani'},\n",
       "     {'name': 'Xinshuo Weng'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in International Conference on Robotics and Automation\\n  (ICRA), 2021. Code is released here: https://github.com/yongxinw/GSDT'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2006.13164v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2006.13164v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.11598v1',\n",
       "    'updated': '2020-08-25T16:54:46Z',\n",
       "    'published': '2020-08-25T16:54:46Z',\n",
       "    'title': 'End-to-End 3D Multi-Object Tracking and Trajectory Forecasting',\n",
       "    'summary': '3D multi-object tracking (MOT) and trajectory forecasting are two critical\\ncomponents in modern 3D perception systems. We hypothesize that it is\\nbeneficial to unify both tasks under one framework to learn a shared feature\\nrepresentation of agent interaction. To evaluate this hypothesis, we propose a\\nunified solution for 3D MOT and trajectory forecasting which also incorporates\\ntwo additional novel computational units. First, we employ a feature\\ninteraction technique by introducing Graph Neural Networks (GNNs) to capture\\nthe way in which multiple agents interact with one another. The GNN is able to\\nmodel complex hierarchical interactions, improve the discriminative feature\\nlearning for MOT association, and provide socially-aware context for trajectory\\nforecasting. Second, we use a diversity sampling function to improve the\\nquality and diversity of our forecasted trajectories. The learned sampling\\nfunction is trained to efficiently extract a variety of outcomes from a\\ngenerative trajectory distribution and helps avoid the problem of generating\\nmany duplicate trajectory samples. We show that our method achieves\\nstate-of-the-art performance on the KITTI dataset. Our project website is at\\nhttp://www.xinshuoweng.com/projects/GNNTrkForecast.',\n",
       "    'author': [{'name': 'Xinshuo Weng'},\n",
       "     {'name': 'Ye Yuan'},\n",
       "     {'name': 'Kris Kitani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Extended abstract. The first two authors contributed equally. Project\\n  website: http://www.xinshuoweng.com/projects/GNNTrkForecast. arXiv admin\\n  note: substantial text overlap with arXiv:2003.07847'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.11598v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.11598v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.01245v2',\n",
       "    'updated': '2021-02-16T10:13:38Z',\n",
       "    'published': '2020-12-02T14:44:40Z',\n",
       "    'title': 'Vision-based Drone Flocking in Outdoor Environments',\n",
       "    'summary': 'Decentralized deployment of drone swarms usually relies on inter-agent\\ncommunication or visual markers that are mounted on the vehicles to simplify\\ntheir mutual detection. This letter proposes a vision-based detection and\\ntracking algorithm that enables groups of drones to navigate without\\ncommunication or visual markers. We employ a convolutional neural network to\\ndetect and localize nearby agents onboard the quadcopters in real-time. Rather\\nthan manually labeling a dataset, we automatically annotate images to train the\\nneural network using background subtraction by systematically flying a\\nquadcopter in front of a static camera. We use a multi-agent state tracker to\\nestimate the relative positions and velocities of nearby agents, which are\\nsubsequently fed to a flocking algorithm for high-level control. The drones are\\nequipped with multiple cameras to provide omnidirectional visual inputs. The\\ncamera setup ensures the safety of the flock by avoiding blind spots regardless\\nof the agent configuration. We evaluate the approach with a group of three real\\nquadcopters that are controlled using the proposed vision-based flocking\\nalgorithm. The results show that the drones can safely navigate in an outdoor\\nenvironment despite substantial background clutter and difficult lighting\\nconditions. The source code, image dataset, and trained detection model are\\navailable at https://github.com/lis-epfl/vswarm.',\n",
       "    'author': [{'name': 'Fabian Schilling'},\n",
       "     {'name': 'Fabrizio Schiano'},\n",
       "     {'name': 'Dario Floreano'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 8 figures, accepted for publication in the IEEE Robotics and\\n  Automation Letters (RA-L) on February 2, 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.01245v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.01245v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.12738v1',\n",
       "    'updated': '2019-04-26T05:22:29Z',\n",
       "    'published': '2019-04-26T05:22:29Z',\n",
       "    'title': 'Self Training Autonomous Driving Agent',\n",
       "    'summary': \"Intrinsically, driving is a Markov Decision Process which suits well the\\nreinforcement learning paradigm. In this paper, we propose a novel agent which\\nlearns to drive a vehicle without any human assistance. We use the concept of\\nreinforcement learning and evolutionary strategies to train our agent in a 2D\\nsimulation environment. Our model's architecture goes beyond the World Model's\\nby introducing difference images in the auto encoder. This novel involvement of\\ndifference images in the auto-encoder gives better representation of the latent\\nspace with respect to the motion of vehicle and helps an autonomous agent to\\nlearn more efficiently how to drive a vehicle. Results show that our method\\nrequires fewer (96% less) total agents, (87.5% less) agents per generations,\\n(70% less) generations and (90% less) rollouts than the original architecture\\nwhile achieving the same accuracy of the original.\",\n",
       "    'author': [{'name': 'Shashank Kotyan'},\n",
       "     {'name': 'Danilo Vasconcellos Vargas'},\n",
       "     {'name': 'Venkanna U'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.12738v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.12738v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.15864v3',\n",
       "    'updated': '2022-10-19T20:39:47Z',\n",
       "    'published': '2022-05-30T14:30:45Z',\n",
       "    'title': 'Braille Letter Reading: A Benchmark for Spatio-Temporal Pattern\\n  Recognition on Neuromorphic Hardware',\n",
       "    'summary': \"Spatio-temporal pattern recognition is a fundamental ability of the brain\\nwhich is required for numerous real-world activities. Recent deep learning\\napproaches have reached outstanding accuracies in such tasks, but their\\nimplementation on conventional embedded solutions is still very computationally\\nand energy expensive. Tactile sensing in robotic applications is a\\nrepresentative example where real-time processing and energy efficiency are\\nrequired. Following a brain-inspired computing approach, we propose a new\\nbenchmark for spatio-temporal tactile pattern recognition at the edge through\\nBraille letter reading. We recorded a new Braille letters dataset based on the\\ncapacitive tactile sensors of the iCub robot's fingertip. We then investigated\\nthe importance of spatial and temporal information as well as the impact of\\nevent-based encoding on spike-based computation. Afterward, we trained and\\ncompared feedforward and recurrent Spiking Neural Networks (SNNs) offline using\\nBackpropagation Through Time (BPTT) with surrogate gradients, then we deployed\\nthem on the Intel Loihi neuromorphic chip for fast and efficient inference. We\\ncompared our approach to standard classifiers, in particular to the Long\\nShort-Term Memory (LSTM) deployed on the embedded NVIDIA Jetson GPU, in terms\\nof classification accuracy, power, energy consumption, and delay. Our results\\nshow that the LSTM reaches ~97% of accuracy, outperforming the recurrent SNN by\\n~17% when using continuous frame-based data instead of event-based inputs.\\nHowever, the recurrent SNN on Loihi with event-based inputs is ~500 times more\\nenergy-efficient than the LSTM on Jetson, requiring a total power of only ~30\\nmW. This work proposes a new benchmark for tactile sensing and highlights the\\nchallenges and opportunities of event-based encoding, neuromorphic hardware,\\nand spike-based computing for spatio-temporal pattern recognition at the edge.\",\n",
       "    'author': [{'name': 'Simon F Muller-Cleve'},\n",
       "     {'name': 'Vittorio Fra'},\n",
       "     {'name': 'Lyes Khacef'},\n",
       "     {'name': 'Alejandro Pequeno-Zurro'},\n",
       "     {'name': 'Daniel Klepatsch'},\n",
       "     {'name': 'Evelina Forno'},\n",
       "     {'name': 'Diego G Ivanovich'},\n",
       "     {'name': 'Shavika Rastogi'},\n",
       "     {'name': 'Gianvito Urgese'},\n",
       "     {'name': 'Friedemann Zenke'},\n",
       "     {'name': 'Chiara Bartolozzi'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.15864v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.15864v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.02000v2',\n",
       "    'updated': '2022-10-10T16:44:13Z',\n",
       "    'published': '2022-09-05T14:57:03Z',\n",
       "    'title': 'Neuromorphic Visual Odometry with Resonator Networks',\n",
       "    'summary': 'Autonomous agents require self-localization to navigate in unknown\\nenvironments. They can use Visual Odometry (VO) to estimate self-motion and\\nlocalize themselves using visual sensors. This motion-estimation strategy is\\nnot compromised by drift as inertial sensors or slippage as wheel encoders.\\nHowever, VO with conventional cameras is computationally demanding, limiting\\nits application in systems with strict low-latency, -memory, and -energy\\nrequirements. Using event-based cameras and neuromorphic computing hardware\\noffers a promising low-power solution to the VO problem. However, conventional\\nalgorithms for VO are not readily convertible to neuromorphic hardware. In this\\nwork, we present a VO algorithm built entirely of neuronal building blocks\\nsuitable for neuromorphic implementation. The building blocks are groups of\\nneurons representing vectors in the computational framework of Vector Symbolic\\nArchitecture (VSA) which was proposed as an abstraction layer to program\\nneuromorphic hardware. The VO network we propose generates and stores a working\\nmemory of the presented visual environment. It updates this working memory\\nwhile at the same time estimating the changing location and orientation of the\\ncamera. We demonstrate how VSA can be leveraged as a computing paradigm for\\nneuromorphic robotics. Moreover, our results represent an important step\\ntowards using neuromorphic computing hardware for fast and power-efficient VO\\nand the related task of simultaneous localization and mapping (SLAM). We\\nvalidate this approach experimentally in a simple robotic task and with an\\nevent-based dataset, demonstrating state-of-the-art performance in these\\nsettings.',\n",
       "    'author': [{'name': 'Alpha Renner'},\n",
       "     {'name': 'Lazar Supic'},\n",
       "     {'name': 'Andreea Danielescu'},\n",
       "     {'name': 'Giacomo Indiveri'},\n",
       "     {'name': 'E. Paxon Frady'},\n",
       "     {'name': 'Friedrich T. Sommer'},\n",
       "     {'name': 'Yulia Sandamirskaya'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages, 5 figures, minor changes'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.02000v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.02000v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.4.9', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1409.8484v1',\n",
       "    'updated': '2014-09-30T11:10:23Z',\n",
       "    'published': '2014-09-30T11:10:23Z',\n",
       "    'title': 'An agent-driven semantical identifier using radial basis neural networks\\n  and reinforcement learning',\n",
       "    'summary': 'Due to the huge availability of documents in digital form, and the deception\\npossibility raise bound to the essence of digital documents and the way they\\nare spread, the authorship attribution problem has constantly increased its\\nrelevance. Nowadays, authorship attribution,for both information retrieval and\\nanalysis, has gained great importance in the context of security, trust and\\ncopyright preservation. This work proposes an innovative multi-agent driven\\nmachine learning technique that has been developed for authorship attribution.\\nBy means of a preprocessing for word-grouping and time-period related analysis\\nof the common lexicon, we determine a bias reference level for the recurrence\\nfrequency of the words within analysed texts, and then train a Radial Basis\\nNeural Networks (RBPNN)-based classifier to identify the correct author. The\\nmain advantage of the proposed approach lies in the generality of the semantic\\nanalysis, which can be applied to different contexts and lexical domains,\\nwithout requiring any modification. Moreover, the proposed system is able to\\nincorporate an external input, meant to tune the classifier, and then\\nself-adjust by means of continuous learning reinforcement.',\n",
       "    'author': [{'name': 'Christian Napoli'},\n",
       "     {'name': 'Giuseppe Pappalardo'},\n",
       "     {'name': 'Emiliano Tramontana'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.13140/2.1.1446.7843'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.13140/2.1.1446.7843',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1409.8484v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1409.8484v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published on: Proceedings of the XV Workshop \"Dagli Oggetti agli\\n  Agenti\" (WOA 2014), Catania, Italy, Sepember. 25-26, 2014'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the XV Workshop \"Dagli Oggetti agli Agenti\" (WOA\\n  2014), on CEUR-WS, volume 1260, ISSN: 1613-073, Catania, Italy, Sepember.\\n  25-26, 2014. http://ceur-ws.org/Vol-1260/'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T01, 68T05, 68T10, 68T50, 68U15',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'C.2.1; I.2.6; I.2.7',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.04232v6',\n",
       "    'updated': '2020-08-31T21:45:47Z',\n",
       "    'published': '2019-05-10T16:00:09Z',\n",
       "    'title': 'Automatic Programming of Cellular Automata and Artificial Neural\\n  Networks Guided by Philosophy',\n",
       "    'summary': 'Many computer models such as cellular automata and artificial neural networks\\nhave been developed and successfully applied. However, in some cases, these\\nmodels might be restrictive on the possible solutions or their solutions might\\nbe difficult to interpret. To overcome this problem, we outline a new approach,\\nthe so-called allagmatic method, that automatically programs and executes\\nmodels with as little limitations as possible while maintaining human\\ninterpretability. Earlier we described a metamodel and its building blocks\\naccording to the philosophical concepts of structure (spatial dimension) and\\noperation (temporal dimension). They are entity, milieu, and update function\\nthat together abstractly describe cellular automata, artificial neural\\nnetworks, and possibly any kind of computer model. By automatically combining\\nthese building blocks in an evolutionary computation, interpretability might be\\nincreased by the relationship to the metamodel, and models might be translated\\ninto more interpretable models via the metamodel. We propose generic and\\nobject-oriented programming to implement the entities and their milieus as\\ndynamic and generic arrays and the update function as a method. We show two\\nexperiments where a simple cellular automaton and an artificial neural network\\nare automatically programmed, compiled, and executed. A target state is\\nsuccessfully evolved and learned in the cellular automaton and artificial\\nneural network, respectively. We conclude that the allagmatic method can create\\nand execute cellular automaton and artificial neural network models in an\\nautomated manner with the guidance of philosophy.',\n",
       "    'author': [{'name': 'Patrik Christen'}, {'name': 'Olivier Del Fabbro'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/978-3-030-48332-6'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/978-3-030-48332-6',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1905.04232v6',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.04232v6',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 1 figure'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Rolf Dornberger, editor, New Trends in Business Information\\n  Systems and Technology: Digital Innovation and Digital Business\\n  Transformation, pages 131-146. Springer, Cham, 2020'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.PL', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1804.02884v1',\n",
       "    'updated': '2018-04-09T09:45:29Z',\n",
       "    'published': '2018-04-09T09:45:29Z',\n",
       "    'title': 'Policy Gradient With Value Function Approximation For Collective\\n  Multiagent Planning',\n",
       "    'summary': 'Decentralized (PO)MDPs provide an expressive framework for sequential\\ndecision making in a multiagent system. Given their computational complexity,\\nrecent research has focused on tractable yet practical subclasses of\\nDec-POMDPs. We address such a subclass called CDEC-POMDP where the collective\\nbehavior of a population of agents affects the joint-reward and environment\\ndynamics. Our main contribution is an actor-critic (AC) reinforcement learning\\nmethod for optimizing CDEC-POMDP policies. Vanilla AC has slow convergence for\\nlarger problems. To address this, we show how a particular decomposition of the\\napproximate action-value function over agents leads to effective updates, and\\nalso derive a new way to train the critic based on local reward signals.\\nComparisons on a synthetic benchmark and a real-world taxi fleet optimization\\nproblem show that our new AC approach provides better quality solutions than\\nprevious best approaches.',\n",
       "    'author': [{'name': 'Duc Thien Nguyen'},\n",
       "     {'name': 'Akshat Kumar'},\n",
       "     {'name': 'Hoong Chuin Lau'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1804.02884v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1804.02884v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1806.06464v2',\n",
       "    'updated': '2018-07-31T21:36:26Z',\n",
       "    'published': '2018-06-17T23:29:19Z',\n",
       "    'title': 'Learning Policy Representations in Multiagent Systems',\n",
       "    'summary': 'Modeling agent behavior is central to understanding the emergence of complex\\nphenomena in multiagent systems. Prior work in agent modeling has largely been\\ntask-specific and driven by hand-engineering domain-specific prior knowledge.\\nWe propose a general learning framework for modeling agent behavior in any\\nmultiagent system using only a handful of interaction data. Our framework casts\\nagent modeling as a representation learning problem. Consequently, we construct\\na novel objective inspired by imitation learning and agent identification and\\ndesign an algorithm for unsupervised learning of representations of agent\\npolicies. We demonstrate empirically the utility of the proposed framework in\\n(i) a challenging high-dimensional competitive environment for continuous\\ncontrol and (ii) a cooperative environment for communication, on supervised\\npredictive tasks, unsupervised clustering, and policy optimization using deep\\nreinforcement learning.',\n",
       "    'author': [{'name': 'Aditya Grover'},\n",
       "     {'name': 'Maruan Al-Shedivat'},\n",
       "     {'name': 'Jayesh K. Gupta'},\n",
       "     {'name': 'Yura Burda'},\n",
       "     {'name': 'Harrison Edwards'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1806.06464v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1806.06464v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1809.07098v1',\n",
       "    'updated': '2018-09-19T09:38:20Z',\n",
       "    'published': '2018-09-19T09:38:20Z',\n",
       "    'title': 'Novelty-organizing team of classifiers in noisy and dynamic environments',\n",
       "    'summary': 'In the real world, the environment is constantly changing with the input\\nvariables under the effect of noise. However, few algorithms were shown to be\\nable to work under those circumstances. Here, Novelty-Organizing Team of\\nClassifiers (NOTC) is applied to the continuous action mountain car as well as\\ntwo variations of it: a noisy mountain car and an unstable weather mountain\\ncar. These problems take respectively noise and change of problem dynamics into\\naccount. Moreover, NOTC is compared with NeuroEvolution of Augmenting\\nTopologies (NEAT) in these problems, revealing a trade-off between the\\napproaches. While NOTC achieves the best performance in all of the problems,\\nNEAT needs less trials to converge. It is demonstrated that NOTC achieves\\nbetter performance because of its division of the input space (creating easier\\nproblems). Unfortunately, this division of input space also requires a bit of\\ntime to bootstrap.',\n",
       "    'author': [{'name': 'Danilo Vasconcellos Vargas'},\n",
       "     {'name': 'Hirotaka Takano'},\n",
       "     {'name': 'Junichi Murata'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/CEC.2015.7257254'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/CEC.2015.7257254',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1809.07098v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1809.07098v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2015 IEEE Congress on Evolutionary Computation (CEC)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.01470v3',\n",
       "    'updated': '2020-07-10T13:31:16Z',\n",
       "    'published': '2019-06-04T14:18:47Z',\n",
       "    'title': 'Options as responses: Grounding behavioural hierarchies in multi-agent\\n  RL',\n",
       "    'summary': \"This paper investigates generalisation in multi-agent games, where the\\ngenerality of the agent can be evaluated by playing against opponents it hasn't\\nseen during training. We propose two new games with concealed information and\\ncomplex, non-transitive reward structure (think rock/paper/scissors). It turns\\nout that most current deep reinforcement learning methods fail to efficiently\\nexplore the strategy space, thus learning policies that generalise poorly to\\nunseen opponents. We then propose a novel hierarchical agent architecture,\\nwhere the hierarchy is grounded in the game-theoretic structure of the game --\\nthe top level chooses strategic responses to opponents, while the low level\\nimplements them into policy over primitive actions. This grounding facilitates\\ncredit assignment across the levels of hierarchy. Our experiments show that the\\nproposed hierarchical agent is capable of generalisation to unseen opponents,\\nwhile conventional baselines fail to generalise whatsoever.\",\n",
       "    'author': [{'name': 'Alexander Sasha Vezhnevets'},\n",
       "     {'name': 'Yuhuai Wu'},\n",
       "     {'name': 'Remi Leblond'},\n",
       "     {'name': 'Joel Z. Leibo'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'First two authors contributed equally'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Machine Learning 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.01470v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.01470v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.07707v1',\n",
       "    'updated': '2020-04-16T15:32:52Z',\n",
       "    'published': '2020-04-16T15:32:52Z',\n",
       "    'title': 'Analyzing Reinforcement Learning Benchmarks with Random Weight Guessing',\n",
       "    'summary': \"We propose a novel method for analyzing and visualizing the complexity of\\nstandard reinforcement learning (RL) benchmarks based on score distributions. A\\nlarge number of policy networks are generated by randomly guessing their\\nparameters, and then evaluated on the benchmark task; the study of their\\naggregated results provide insights into the benchmark complexity. Our method\\nguarantees objectivity of evaluation by sidestepping learning altogether: the\\npolicy network parameters are generated using Random Weight Guessing (RWG),\\nmaking our method agnostic to (i) the classic RL setup, (ii) any learning\\nalgorithm, and (iii) hyperparameter tuning. We show that this approach isolates\\nthe environment complexity, highlights specific types of challenges, and\\nprovides a proper foundation for the statistical analysis of the task's\\ndifficulty. We test our approach on a variety of classic control benchmarks\\nfrom the OpenAI Gym, where we show that small untrained networks can provide a\\nrobust baseline for a variety of tasks. The networks generated often show good\\nperformance even without gradual learning, incidentally highlighting the\\ntriviality of a few popular benchmarks.\",\n",
       "    'author': [{'name': 'Declan Oller'},\n",
       "     {'name': 'Tobias Glasmachers'},\n",
       "     {'name': 'Giuseppe Cuccu'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2004.07707v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.07707v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.10113v1',\n",
       "    'updated': '2022-04-26T22:41:17Z',\n",
       "    'published': '2022-04-26T22:41:17Z',\n",
       "    'title': 'Evolutionary Multi-Armed Bandits with Genetic Thompson Sampling',\n",
       "    'summary': \"As two popular schools of machine learning, online learning and evolutionary\\ncomputations have become two important driving forces behind real-world\\ndecision making engines for applications in biomedicine, economics, and\\nengineering fields. Although there are prior work that utilizes bandits to\\nimprove evolutionary algorithms' optimization process, it remains a field of\\nblank on how evolutionary approach can help improve the sequential decision\\nmaking tasks of online learning agents such as the multi-armed bandits. In this\\nwork, we propose the Genetic Thompson Sampling, a bandit algorithm that keeps a\\npopulation of agents and update them with genetic principles such as elite\\nselection, crossover and mutations. Empirical results in multi-armed bandit\\nsimulation environments and a practical epidemic control problem suggest that\\nby incorporating the genetic algorithm into the bandit algorithm, our method\\nsignificantly outperforms the baselines in nonstationary settings. Lastly, we\\nintroduce EvoBandit, a web-based interactive visualization to guide the readers\\nthrough the entire learning process and perform lightweight evaluations on the\\nfly. We hope to engage researchers into this growing field of research with\\nthis investigation.\",\n",
       "    'author': {'name': 'Baihan Lin'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceeding of IEEE CEC 2022. This work is one of the first works to\\n  solve the online learning problems with distributed evolutionary\\n  optimizations, and extends our prior work on contextual bandits (e.g.\\n  arXiv:2106.15808) by testing against similar simulated and real-world\\n  scenarios. Codes at https://github.com/doerlbh/BanditZoo'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.10113v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.10113v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1712.00948v5',\n",
       "    'updated': '2019-09-03T21:05:21Z',\n",
       "    'published': '2017-12-04T08:18:08Z',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight',\n",
       "    'summary': 'Hierarchical agents have the potential to solve sequential decision making\\ntasks with greater sample efficiency than their non-hierarchical counterparts\\nbecause hierarchical agents can break down tasks into sets of subtasks that\\nonly require short sequences of decisions. In order to realize this potential\\nof faster learning, hierarchical agents need to be able to learn their multiple\\nlevels of policies in parallel so these simpler subproblems can be solved\\nsimultaneously. Yet, learning multiple levels of policies in parallel is hard\\nbecause it is inherently unstable: changes in a policy at one level of the\\nhierarchy may cause changes in the transition and reward functions at higher\\nlevels in the hierarchy, making it difficult to jointly learn multiple levels\\nof policies. In this paper, we introduce a new Hierarchical Reinforcement\\nLearning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome\\nthe instability issues that arise when agents try to jointly learn multiple\\nlevels of policies. The main idea behind HAC is to train each level of the\\nhierarchy independently of the lower levels by training each level as if the\\nlower level policies are already optimal. We demonstrate experimentally in both\\ngrid world and simulated robotics domains that our approach can significantly\\naccelerate learning relative to other non-hierarchical and hierarchical\\nmethods. Indeed, our framework is the first to successfully learn 3-level\\nhierarchies in parallel in tasks with continuous state and action spaces.',\n",
       "    'author': [{'name': 'Andrew Levy'},\n",
       "     {'name': 'George Konidaris'},\n",
       "     {'name': 'Robert Platt'},\n",
       "     {'name': 'Kate Saenko'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICLR 2019 Accepted Paper'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1712.00948v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1712.00948v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/cs/0204043v1',\n",
       "    'updated': '2002-04-20T05:02:53Z',\n",
       "    'published': '2002-04-20T05:02:53Z',\n",
       "    'title': 'Learning from Scarce Experience',\n",
       "    'summary': 'Searching the space of policies directly for the optimal policy has been one\\npopular method for solving partially observable reinforcement learning\\nproblems. Typically, with each change of the target policy, its value is\\nestimated from the results of following that very policy. This requires a large\\nnumber of interactions with the environment as different polices are\\nconsidered. We present a family of algorithms based on likelihood ratio\\nestimation that use data gathered when executing one policy (or collection of\\npolicies) to estimate the value of a different policy. The algorithms combine\\nestimation and optimization stages. The former utilizes experience to build a\\nnon-parametric representation of an optimized function. The latter performs\\noptimization on this estimate. We show positive empirical results and provide\\nthe sample complexity bound.',\n",
       "    'author': [{'name': 'Leonid Peshkin'}, {'name': 'Christian R. Shelton'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages 4 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/cs/0204043v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/cs/0204043v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2; I.2.8; I.2.11; I.2.6; G.1.6',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1207.4931v1',\n",
       "    'updated': '2012-07-20T12:15:12Z',\n",
       "    'published': '2012-07-20T12:15:12Z',\n",
       "    'title': 'Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural\\n  Network',\n",
       "    'summary': 'The paper presents the electronic design and motion planning of a robot based\\non decision making regarding its straight motion and precise turn using\\nArtificial Neural Network (ANN). The ANN helps in learning of robot so that it\\nperforms motion autonomously. The weights calculated are implemented in\\nmicrocontroller. The performance has been tested to be excellent.',\n",
       "    'author': [{'name': 'G. N. Tripathi'}, {'name': 'V. Rihani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 4 figures, 1 table, 1 graph chart, ITCA-2012, Chennai, India'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1207.4931v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1207.4931v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1407.3269v1',\n",
       "    'updated': '2014-07-11T14:21:22Z',\n",
       "    'published': '2014-07-11T14:21:22Z',\n",
       "    'title': 'Multiple chaotic central pattern generators with learning for legged\\n  locomotion and malfunction compensation',\n",
       "    'summary': \"An originally chaotic system can be controlled into various periodic\\ndynamics. When it is implemented into a legged robot's locomotion control as a\\ncentral pattern generator (CPG), sophisticated gait patterns arise so that the\\nrobot can perform various walking behaviors. However, such a single chaotic CPG\\ncontroller has difficulties dealing with leg malfunction. Specifically, in the\\nscenarios presented here, its movement permanently deviates from the desired\\ntrajectory. To address this problem, we extend the single chaotic CPG to\\nmultiple CPGs with learning. The learning mechanism is based on a simulated\\nannealing algorithm. In a normal situation, the CPGs synchronize and their\\ndynamics are identical. With leg malfunction or disability, the CPGs lose\\nsynchronization leading to independent dynamics. In this case, the learning\\nmechanism is applied to automatically adjust the remaining legs' oscillation\\nfrequencies so that the robot adapts its locomotion to deal with the\\nmalfunction. As a consequence, the trajectory produced by the multiple chaotic\\nCPGs resembles the original trajectory far better than the one produced by only\\na single CPG. The performance of the system is evaluated first in a physical\\nsimulation of a quadruped as well as a hexapod robot and finally in a real\\nsix-legged walking machine called AMOSII. The experimental results presented\\nhere reveal that using multiple CPGs with learning is an effective approach for\\nadaptive locomotion generation where, for instance, different body parts have\\nto perform independent movements for malfunction compensation.\",\n",
       "    'author': [{'name': 'Guanjiao Ren'},\n",
       "     {'name': 'Weihai Chen'},\n",
       "     {'name': 'Sakyasingha Dasgupta'},\n",
       "     {'name': 'Christoph Kolodziejski'},\n",
       "     {'name': 'Florentin Wörgötter'},\n",
       "     {'name': 'Poramate Manoonpong'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.ins.2014.05.001'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.ins.2014.05.001',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1407.3269v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1407.3269v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '48 pages, 16 figures, Information Sciences 2014'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.6', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1703.07326v3',\n",
       "    'updated': '2017-12-04T21:53:23Z',\n",
       "    'published': '2017-03-21T17:22:29Z',\n",
       "    'title': 'One-Shot Imitation Learning',\n",
       "    'summary': 'Imitation learning has been commonly applied to solve different tasks in\\nisolation. This usually requires either careful feature engineering, or a\\nsignificant number of samples. This is far from what we desire: ideally, robots\\nshould be able to learn from very few demonstrations of any given task, and\\ninstantly generalize to new situations of the same task, without requiring\\ntask-specific engineering. In this paper, we propose a meta-learning framework\\nfor achieving such capability, which we call one-shot imitation learning.\\n  Specifically, we consider the setting where there is a very large set of\\ntasks, and each task has many instantiations. For example, a task could be to\\nstack all blocks on a table into a single tower, another task could be to place\\nall blocks on a table into two-block towers, etc. In each case, different\\ninstances of the task would consist of different sets of blocks with different\\ninitial states. At training time, our algorithm is presented with pairs of\\ndemonstrations for a subset of all tasks. A neural net is trained that takes as\\ninput one demonstration and the current state (which initially is the initial\\nstate of the other demonstration of the pair), and outputs an action with the\\ngoal that the resulting sequence of states and actions matches as closely as\\npossible with the second demonstration. At test time, a demonstration of a\\nsingle instance of a new task is presented, and the neural net is expected to\\nperform well on new instances of this new task. The use of soft attention\\nallows the model to generalize to conditions and tasks unseen in the training\\ndata. We anticipate that by training this model on a much greater variety of\\ntasks and settings, we will obtain a general system that can turn any\\ndemonstrations into robust policies that can accomplish an overwhelming variety\\nof tasks.\\n  Videos available at https://bit.ly/nips2017-oneshot .',\n",
       "    'author': [{'name': 'Yan Duan'},\n",
       "     {'name': 'Marcin Andrychowicz'},\n",
       "     {'name': 'Bradly C. Stadie'},\n",
       "     {'name': 'Jonathan Ho'},\n",
       "     {'name': 'Jonas Schneider'},\n",
       "     {'name': 'Ilya Sutskever'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Wojciech Zaremba'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1703.07326v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1703.07326v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1704.03012v1',\n",
       "    'updated': '2017-04-10T18:41:28Z',\n",
       "    'published': '2017-04-10T18:41:28Z',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning',\n",
       "    'summary': 'Deep reinforcement learning has achieved many impressive results in recent\\nyears. However, tasks with sparse rewards or long horizons continue to pose\\nsignificant challenges. To tackle these important problems, we propose a\\ngeneral framework that first learns useful skills in a pre-training\\nenvironment, and then leverages the acquired skills for learning faster in\\ndownstream tasks. Our approach brings together some of the strengths of\\nintrinsic motivation and hierarchical methods: the learning of useful skill is\\nguided by a single proxy reward, the design of which requires very minimal\\ndomain knowledge about the downstream tasks. Then a high-level policy is\\ntrained on top of these skills, providing a significant improvement of the\\nexploration and allowing to tackle sparse rewards in the downstream tasks. To\\nefficiently pre-train a large span of skills, we use Stochastic Neural Networks\\ncombined with an information-theoretic regularizer. Our experiments show that\\nthis combination is effective in learning a wide span of interpretable skills\\nin a sample-efficient way, and can significantly boost the learning performance\\nuniformly across a wide range of downstream tasks.',\n",
       "    'author': [{'name': 'Carlos Florensa'},\n",
       "     {'name': 'Yan Duan'},\n",
       "     {'name': 'Pieter Abbeel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published as a conference paper at ICLR 2017'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Learning Representations 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1704.03012v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1704.03012v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1707.01495v3',\n",
       "    'updated': '2018-02-23T10:04:20Z',\n",
       "    'published': '2017-07-05T17:55:53Z',\n",
       "    'title': 'Hindsight Experience Replay',\n",
       "    'summary': 'Dealing with sparse rewards is one of the biggest challenges in Reinforcement\\nLearning (RL). We present a novel technique called Hindsight Experience Replay\\nwhich allows sample-efficient learning from rewards which are sparse and binary\\nand therefore avoid the need for complicated reward engineering. It can be\\ncombined with an arbitrary off-policy RL algorithm and may be seen as a form of\\nimplicit curriculum.\\n  We demonstrate our approach on the task of manipulating objects with a\\nrobotic arm. In particular, we run experiments on three different tasks:\\npushing, sliding, and pick-and-place, in each case using only binary rewards\\nindicating whether or not the task is completed. Our ablation studies show that\\nHindsight Experience Replay is a crucial ingredient which makes training\\npossible in these challenging environments. We show that our policies trained\\non a physics simulation can be deployed on a physical robot and successfully\\ncomplete the task.',\n",
       "    'author': [{'name': 'Marcin Andrychowicz'},\n",
       "     {'name': 'Filip Wolski'},\n",
       "     {'name': 'Alex Ray'},\n",
       "     {'name': 'Jonas Schneider'},\n",
       "     {'name': 'Rachel Fong'},\n",
       "     {'name': 'Peter Welinder'},\n",
       "     {'name': 'Bob McGrew'},\n",
       "     {'name': 'Josh Tobin'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Wojciech Zaremba'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1707.01495v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1707.01495v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1707.05300v3',\n",
       "    'updated': '2018-07-23T10:10:17Z',\n",
       "    'published': '2017-07-17T17:53:54Z',\n",
       "    'title': 'Reverse Curriculum Generation for Reinforcement Learning',\n",
       "    'summary': \"Many relevant tasks require an agent to reach a certain state, or to\\nmanipulate objects into a desired configuration. For example, we might want a\\nrobot to align and assemble a gear onto an axle or insert and turn a key in a\\nlock. These goal-oriented tasks present a considerable challenge for\\nreinforcement learning, since their natural reward function is sparse and\\nprohibitive amounts of exploration are required to reach the goal and receive\\nsome learning signal. Past approaches tackle these problems by exploiting\\nexpert demonstrations or by manually designing a task-specific reward shaping\\nfunction to guide the learning agent. Instead, we propose a method to learn\\nthese tasks without requiring any prior knowledge other than obtaining a single\\nstate in which the task is achieved. The robot is trained in reverse, gradually\\nlearning to reach the goal from a set of start states increasingly far from the\\ngoal. Our method automatically generates a curriculum of start states that\\nadapts to the agent's performance, leading to efficient training on\\ngoal-oriented tasks. We demonstrate our approach on difficult simulated\\nnavigation and fine-grained manipulation problems, not solvable by\\nstate-of-the-art reinforcement learning methods.\",\n",
       "    'author': [{'name': 'Carlos Florensa'},\n",
       "     {'name': 'David Held'},\n",
       "     {'name': 'Markus Wulfmeier'},\n",
       "     {'name': 'Michael Zhang'},\n",
       "     {'name': 'Pieter Abbeel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at the 1st Conference on Robot Learning (CoRL 2017)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1707.05300v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1707.05300v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.10089v2',\n",
       "    'updated': '2018-02-25T07:48:19Z',\n",
       "    'published': '2017-09-28T17:51:48Z',\n",
       "    'title': 'Overcoming Exploration in Reinforcement Learning with Demonstrations',\n",
       "    'summary': 'Exploration in environments with sparse rewards has been a persistent problem\\nin reinforcement learning (RL). Many tasks are natural to specify with a sparse\\nreward, and manually shaping a reward function can result in suboptimal\\nperformance. However, finding a non-zero reward is exponentially more difficult\\nwith increasing task horizon or action dimensionality. This puts many\\nreal-world tasks out of practical reach of RL methods. In this work, we use\\ndemonstrations to overcome the exploration problem and successfully learn to\\nperform long-horizon, multi-step robotics tasks with continuous control such as\\nstacking blocks with a robot arm. Our method, which builds on top of Deep\\nDeterministic Policy Gradients and Hindsight Experience Replay, provides an\\norder of magnitude of speedup over RL on simulated robotics tasks. It is simple\\nto implement and makes only the additional assumption that we can collect a\\nsmall set of demonstrations. Furthermore, our method is able to solve tasks not\\nsolvable by either RL or behavior cloning alone, and often ends up\\noutperforming the demonstrator policy.',\n",
       "    'author': [{'name': 'Ashvin Nair'},\n",
       "     {'name': 'Bob McGrew'},\n",
       "     {'name': 'Marcin Andrychowicz'},\n",
       "     {'name': 'Wojciech Zaremba'},\n",
       "     {'name': 'Pieter Abbeel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, ICRA 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.10089v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.10089v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1711.06006v3',\n",
       "    'updated': '2019-02-20T10:46:44Z',\n",
       "    'published': '2017-11-16T10:05:31Z',\n",
       "    'title': 'Hindsight policy gradients',\n",
       "    'summary': 'A reinforcement learning agent that needs to pursue different goals across\\nepisodes requires a goal-conditional policy. In addition to their potential to\\ngeneralize desirable behavior to unseen goals, such policies may also enable\\nhigher-level planning based on subgoals. In sparse-reward environments, the\\ncapacity to exploit information about the degree to which an arbitrary goal has\\nbeen achieved while another goal was intended appears crucial to enable sample\\nefficient learning. However, reinforcement learning agents have only recently\\nbeen endowed with such capacity for hindsight. In this paper, we demonstrate\\nhow hindsight can be introduced to policy gradient methods, generalizing this\\nidea to a broad class of successful algorithms. Our experiments on a diverse\\nselection of sparse-reward environments show that hindsight leads to a\\nremarkable increase in sample efficiency.',\n",
       "    'author': [{'name': 'Paulo Rauber'},\n",
       "     {'name': 'Avinash Ummadisingu'},\n",
       "     {'name': 'Filipe Mutz'},\n",
       "     {'name': 'Juergen Schmidhuber'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICLR 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1711.06006v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1711.06006v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1712.05249v1',\n",
       "    'updated': '2017-12-14T14:31:51Z',\n",
       "    'published': '2017-12-14T14:31:51Z',\n",
       "    'title': 'Proximodistal Exploration in Motor Learning as an Emergent Property of\\n  Optimization',\n",
       "    'summary': 'To harness the complexity of their high-dimensional bodies during\\nsensorimotor development, infants are guided by patterns of freezing and\\nfreeing of degrees of freedom. For instance, when learning to reach, infants\\nfree the degrees of freedom in their arm proximodistally, i.e. from joints that\\nare closer to the body to those that are more distant. Here, we formulate and\\nstudy computationally the hypothesis that such patterns can emerge\\nspontaneously as the result of a family of stochastic optimization processes\\n(evolution strategies with covariance-matrix adaptation), without an innate\\nencoding of a maturational schedule. In particular, we present simulated\\nexperiments with an arm where a computational learner progressively acquires\\nreaching skills through adaptive exploration, and we show that a proximodistal\\norganization appears spontaneously, which we denote PDFF (ProximoDistal\\nFreezing and Freeing of degrees of freedom). We also compare this emergent\\norganization between different arm morphologies -- from human-like to quite\\nunnatural ones -- to study the effect of different kinematic structures on the\\nemergence of PDFF. Keywords: human motor learning; proximo-distal exploration;\\nstochastic optimization; modelling; evolution strategies; cross-entropy\\nmethods; policy search; morphology.}',\n",
       "    'author': [{'name': 'Freek Stulp'}, {'name': 'Pierre-Yves Oudeyer'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1712.05249v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1712.05249v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1809.10283v3',\n",
       "    'updated': '2022-07-25T09:14:25Z',\n",
       "    'published': '2018-09-26T12:23:19Z',\n",
       "    'title': 'Adding Neural Network Controllers to Behavior Trees without Destroying\\n  Performance Guarantees',\n",
       "    'summary': 'In this paper, we show how Behavior Trees that have performance guarantees,\\nin terms of safety and goal convergence, can be extended with components that\\nwere designed using machine learning, without destroying those performance\\nguarantees.\\n  Machine learning approaches such as reinforcement learning or learning from\\ndemonstration can be very appealing to AI designers that want efficient and\\nrealistic behaviors in their agents. However, those algorithms seldom provide\\nguarantees for solving the given task in all different situations while keeping\\nthe agent safe. Instead, such guarantees are often easier to find for manually\\ndesigned model-based approaches. In this paper we exploit the modularity of\\nbehavior trees to extend a given design with an efficient, but possibly\\nunreliable, machine learning component in a way that preserves the guarantees.\\nThe approach is illustrated with an inverted pendulum example.',\n",
       "    'author': [{'name': 'Christopher Iliffe Sprague'},\n",
       "     {'name': 'Petter Ögren'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted as Regular Paper to The 61th IEEE Conference on Decision and\\n  Control (CDC 2022)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1809.10283v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1809.10283v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1901.00943v1',\n",
       "    'updated': '2019-01-03T23:49:09Z',\n",
       "    'published': '2019-01-03T23:49:09Z',\n",
       "    'title': 'Self-supervised Learning of Image Embedding for Continuous Control',\n",
       "    'summary': 'Operating directly from raw high dimensional sensory inputs like images is\\nstill a challenge for robotic control. Recently, Reinforcement Learning methods\\nhave been proposed to solve specific tasks end-to-end, from pixels to torques.\\nHowever, these approaches assume the access to a specified reward which may\\nrequire specialized instrumentation of the environment. Furthermore, the\\nobtained policy and representations tend to be task specific and may not\\ntransfer well. In this work we investigate completely self-supervised learning\\nof a general image embedding and control primitives, based on finding the\\nshortest time to reach any state. We also introduce a new structure for the\\nstate-action value function that builds a connection between model-free and\\nmodel-based methods, and improves the performance of the learning algorithm. We\\nexperimentally demonstrate these findings in three simulated robotic tasks.',\n",
       "    'author': [{'name': 'Carlos Florensa'},\n",
       "     {'name': 'Jonas Degrave'},\n",
       "     {'name': 'Nicolas Heess'},\n",
       "     {'name': 'Jost Tobias Springenberg'},\n",
       "     {'name': 'Martin Riedmiller'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Contributed talk at Inference to Control workshop at NeurIPS2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1901.00943v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1901.00943v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1907.06511v4',\n",
       "    'updated': '2021-04-06T17:00:42Z',\n",
       "    'published': '2019-07-10T16:57:50Z',\n",
       "    'title': 'Reinforcement Learning with Chromatic Networks for Compact Architecture\\n  Search',\n",
       "    'summary': 'We present a neural architecture search algorithm to construct compact\\nreinforcement learning (RL) policies, by combining ENAS and ES in a highly\\nscalable and intuitive way. By defining the combinatorial search space of NAS\\nto be the set of different edge-partitionings (colorings) into same-weight\\nclasses, we represent compact architectures via efficient learned\\nedge-partitionings. For several RL tasks, we manage to learn colorings\\ntranslating to effective policies parameterized by as few as $17$ weight\\nparameters, providing >90% compression over vanilla policies and 6x compression\\nover state-of-the-art compact policies based on Toeplitz matrices, while still\\nmaintaining good reward. We believe that our work is one of the first attempts\\nto propose a rigorous approach to training structured neural network\\narchitectures for RL problems that are of interest especially in mobile\\nrobotics with limited storage and computational resources.',\n",
       "    'author': [{'name': 'Xingyou Song'},\n",
       "     {'name': 'Krzysztof Choromanski'},\n",
       "     {'name': 'Jack Parker-Holder'},\n",
       "     {'name': 'Yunhao Tang'},\n",
       "     {'name': 'Wenbo Gao'},\n",
       "     {'name': 'Aldo Pacchiano'},\n",
       "     {'name': 'Tamas Sarlos'},\n",
       "     {'name': 'Deepali Jain'},\n",
       "     {'name': 'Yuxiang Yang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at ICLR 2020 Neural Architecture Search Workshop. This\\n  paper is deprecated; please see arXiv:2101.07415 for the newer version'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1907.06511v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1907.06511v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1907.07029v3',\n",
       "    'updated': '2020-03-03T22:25:25Z',\n",
       "    'published': '2019-07-16T14:26:13Z',\n",
       "    'title': 'Adaptive Prior Selection for Repertoire-based Online Adaptation in\\n  Robotics',\n",
       "    'summary': 'Repertoire-based learning is a data-efficient adaptation approach based on a\\ntwo-step process in which (1) a large and diverse set of policies is learned in\\nsimulation, and (2) a planning or learning algorithm chooses the most\\nappropriate policies according to the current situation (e.g., a damaged robot,\\na new object, etc.). In this paper, we relax the assumption of previous works\\nthat a single repertoire is enough for adaptation. Instead, we generate\\nrepertoires for many different situations (e.g., with a missing leg, on\\ndifferent floors, etc.) and let our algorithm selects the most useful prior.\\nOur main contribution is an algorithm, APROL (Adaptive Prior selection for\\nRepertoire-based Online Learning) to plan the next action by incorporating\\nthese priors when the robot has no information about the current situation. We\\nevaluate APROL on two simulated tasks: (1) pushing unknown objects of various\\nshapes and sizes with a robotic arm and (2) a goal reaching task with a damaged\\nhexapod robot. We compare with \"Reset-free Trial and Error\" (RTE) and various\\nsingle repertoire-based baselines. The results show that APROL solves both the\\ntasks in less interaction time than the baselines. Additionally, we demonstrate\\nAPROL on a real, damaged hexapod that quickly learns to pick compensatory\\npolicies to reach a goal by avoiding obstacles in the path.',\n",
       "    'author': [{'name': 'Rituraj Kaushik'},\n",
       "     {'name': 'Pierre Desreumaux'},\n",
       "     {'name': 'Jean-Baptiste Mouret'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.3389/frobt.2019.00151'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.3389/frobt.2019.00151',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1907.07029v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1907.07029v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Frontiers in Robotics and AI. Vol. 6, p. 151, 2020. Video :\\n  http://tiny.cc/aprol_video'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Frontiers in Robotics and AI. 6 (2020) 151'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.05508v4',\n",
       "    'updated': '2020-05-04T09:20:08Z',\n",
       "    'published': '2019-09-12T08:47:44Z',\n",
       "    'title': 'Unsupervised Learning and Exploration of Reachable Outcome Space',\n",
       "    'summary': 'Performing Reinforcement Learning in sparse rewards settings, with very\\nlittle prior knowledge, is a challenging problem since there is no signal to\\nproperly guide the learning process. In such situations, a good search strategy\\nis fundamental. At the same time, not having to adapt the algorithm to every\\nsingle problem is very desirable. Here we introduce TAXONS, a Task Agnostic\\neXploration of Outcome spaces through Novelty and Surprise algorithm. Based on\\na population-based divergent-search approach, it learns a set of diverse\\npolicies directly from high-dimensional observations, without any task-specific\\ninformation. TAXONS builds a repertoire of policies while training an\\nautoencoder on the high-dimensional observation of the final state of the\\nsystem to build a low-dimensional outcome space. The learned outcome space,\\ncombined with the reconstruction error, is used to drive the search for new\\npolicies. Results show that TAXONS can find a diverse set of controllers,\\ncovering a good part of the ground-truth outcome space, while having no\\ninformation about such space.',\n",
       "    'author': [{'name': 'Giuseppe Paolo'},\n",
       "     {'name': 'Alban Laflaquière'},\n",
       "     {'name': 'Alexandre Coninx'},\n",
       "     {'name': 'Stephane Doncieux'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at IEEE International Conference on Robotics and Automation\\n  (ICRA) 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.05508v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.05508v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.01431v1',\n",
       "    'updated': '2020-03-03T10:29:02Z',\n",
       "    'published': '2020-03-03T10:29:02Z',\n",
       "    'title': 'Embodied Synaptic Plasticity with Online Reinforcement learning',\n",
       "    'summary': 'The endeavor to understand the brain involves multiple collaborating research\\nfields. Classically, synaptic plasticity rules derived by theoretical\\nneuroscientists are evaluated in isolation on pattern classification tasks.\\nThis contrasts with the biological brain which purpose is to control a body in\\nclosed-loop. This paper contributes to bringing the fields of computational\\nneuroscience and robotics closer together by integrating open-source software\\ncomponents from these two fields. The resulting framework allows to evaluate\\nthe validity of biologically-plausibe plasticity models in closed-loop robotics\\nenvironments. We demonstrate this framework to evaluate Synaptic Plasticity\\nwith Online REinforcement learning (SPORE), a reward-learning rule based on\\nsynaptic sampling, on two visuomotor tasks: reaching and lane following. We\\nshow that SPORE is capable of learning to perform policies within the course of\\nsimulated hours for both tasks. Provisional parameter explorations indicate\\nthat the learning rate and the temperature driving the stochastic processes\\nthat govern synaptic learning dynamics need to be regulated for performance\\nimprovements to be retained. We conclude by discussing the recent deep\\nreinforcement learning techniques which would be beneficial to increase the\\nfunctionality of SPORE on visuomotor tasks.',\n",
       "    'author': [{'name': 'Jacques Kaiser'},\n",
       "     {'name': 'Michael Hoff'},\n",
       "     {'name': 'Andreas Konle'},\n",
       "     {'name': 'J. Camilo Vasquez Tieck'},\n",
       "     {'name': 'David Kappel'},\n",
       "     {'name': 'Daniel Reichard'},\n",
       "     {'name': 'Anand Subramoney'},\n",
       "     {'name': 'Robert Legenstein'},\n",
       "     {'name': 'Arne Roennau'},\n",
       "     {'name': 'Wolfgang Maass'},\n",
       "     {'name': 'Rudiger Dillmann'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.3389/fnbot.2019.00081'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.3389/fnbot.2019.00081',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2003.01431v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.01431v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '18 pages, 5 figures, published in frontiers in neurorobotics'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Frontiers in neurorobotics, volume 13, p81, 2019'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2005.05151v1',\n",
       "    'updated': '2020-05-11T14:43:55Z',\n",
       "    'published': '2020-05-11T14:43:55Z',\n",
       "    'title': 'Autonomous learning and chaining of motor primitives using the Free\\n  Energy Principle',\n",
       "    'summary': 'In this article, we apply the Free-Energy Principle to the question of motor\\nprimitives learning. An echo-state network is used to generate motor\\ntrajectories. We combine this network with a perception module and a controller\\nthat can influence its dynamics. This new compound network permits the\\nautonomous learning of a repertoire of motor trajectories. To evaluate the\\nrepertoires built with our method, we exploit them in a handwriting task where\\nprimitives are chained to produce long-range sequences.',\n",
       "    'author': [{'name': 'Louis Annabi',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ETIS'}},\n",
       "     {'name': 'Alexandre Pitti',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ETIS'}},\n",
       "     {'name': 'Mathias Quoy',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ETIS'}}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2005.05151v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2005.05151v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2005.06223v1',\n",
       "    'updated': '2020-05-13T09:29:40Z',\n",
       "    'published': '2020-05-13T09:29:40Z',\n",
       "    'title': 'DREAM Architecture: a Developmental Approach to Open-Ended Learning in\\n  Robotics',\n",
       "    'summary': 'Robots are still limited to controlled conditions, that the robot designer\\nknows with enough details to endow the robot with the appropriate models or\\nbehaviors. Learning algorithms add some flexibility with the ability to\\ndiscover the appropriate behavior given either some demonstrations or a reward\\nto guide its exploration with a reinforcement learning algorithm. Reinforcement\\nlearning algorithms rely on the definition of state and action spaces that\\ndefine reachable behaviors. Their adaptation capability critically depends on\\nthe representations of these spaces: small and discrete spaces result in fast\\nlearning while large and continuous spaces are challenging and either require a\\nlong training period or prevent the robot from converging to an appropriate\\nbehavior. Beside the operational cycle of policy execution and the learning\\ncycle, which works at a slower time scale to acquire new policies, we introduce\\nthe redescription cycle, a third cycle working at an even slower time scale to\\ngenerate or adapt the required representations to the robot, its environment\\nand the task. We introduce the challenges raised by this cycle and we present\\nDREAM (Deferred Restructuring of Experience in Autonomous Machines), a\\ndevelopmental cognitive architecture to bootstrap this redescription process\\nstage by stage, build new state representations with appropriate motivations,\\nand transfer the acquired knowledge across domains or tasks or even across\\nrobots. We describe results obtained so far with this approach and end up with\\na discussion of the questions it raises in Neuroscience.',\n",
       "    'author': [{'name': 'Stephane Doncieux',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Nicolas Bredeche',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Léni Le Goff',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Benoît Girard',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Alexandre Coninx',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Olivier Sigaud',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Mehdi Khamassi',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Natalia Díaz-Rodríguez',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'U2IS'}},\n",
       "     {'name': 'David Filliat',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'U2IS'}},\n",
       "     {'name': 'Timothy Hospedales',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ICSA'}},\n",
       "     {'name': 'A. Eiben',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'VU'}},\n",
       "     {'name': 'Richard Duro'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2005.06223v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2005.06223v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2005.06224v1',\n",
       "    'updated': '2020-05-13T09:32:07Z',\n",
       "    'published': '2020-05-13T09:32:07Z',\n",
       "    'title': 'Novelty Search makes Evolvability Inevitable',\n",
       "    'summary': 'Evolvability is an important feature that impacts the ability of evolutionary\\nprocesses to find interesting novel solutions and to deal with changing\\nconditions of the problem to solve. The estimation of evolvability is not\\nstraightforward and is generally too expensive to be directly used as selective\\npressure in the evolutionary process. Indirectly promoting evolvability as a\\nside effect of other easier and faster to compute selection pressures would\\nthus be advantageous. In an unbounded behavior space, it has already been shown\\nthat evolvable individuals naturally appear and tend to be selected as they are\\nmore likely to invade empty behavior niches. Evolvability is thus a natural\\nbyproduct of the search in this context. However, practical agents and\\nenvironments often impose limits on the reach-able behavior space. How do these\\nboundaries impact evolvability? In this context, can evolvability still be\\npromoted without explicitly rewarding it? We show that Novelty Search\\nimplicitly creates a pressure for high evolvability even in bounded behavior\\nspaces, and explore the reasons for such a behavior. More precisely we show\\nthat, throughout the search, the dynamic evaluation of novelty rewards\\nindividuals which are very mobile in the behavior space, which in turn promotes\\nevolvability.',\n",
       "    'author': [{'name': 'Stephane Doncieux',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Giuseppe Paolo',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Alban Laflaquière',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Alexandre Coninx',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2005.06224v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2005.06224v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.04589v1',\n",
       "    'updated': '2020-08-11T09:02:18Z',\n",
       "    'published': '2020-08-11T09:02:18Z',\n",
       "    'title': 'Model-Based Quality-Diversity Search for Efficient Robot Learning',\n",
       "    'summary': 'Despite recent progress in robot learning, it still remains a challenge to\\nprogram a robot to deal with open-ended object manipulation tasks. One approach\\nthat was recently used to autonomously generate a repertoire of diverse skills\\nis a novelty based Quality-Diversity~(QD) algorithm. However, as most\\nevolutionary algorithms, QD suffers from sample-inefficiency and, thus, it is\\nchallenging to apply it in real-world scenarios. This paper tackles this\\nproblem by integrating a neural network that predicts the behavior of the\\nperturbed parameters into a novelty based QD algorithm. In the proposed\\nModel-based Quality-Diversity search (M-QD), the network is trained\\nconcurrently to the repertoire and is used to avoid executing unpromising\\nactions in the novelty search process. Furthermore, it is used to adapt the\\nskills of the final repertoire in order to generalize the skills to different\\nscenarios. Our experiments show that enhancing a QD algorithm with such a\\nforward model improves the sample-efficiency and performance of the\\nevolutionary process and the skill adaptation.',\n",
       "    'author': [{'name': 'Leon Keller'},\n",
       "     {'name': 'Daniel Tanneberg'},\n",
       "     {'name': 'Svenja Stark'},\n",
       "     {'name': 'Jan Peters'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE/RSJ International Conference on Intelligent Robots and Systems\\n  (IROS) 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.04589v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.04589v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.04700v1',\n",
       "    'updated': '2020-12-08T19:37:58Z',\n",
       "    'published': '2020-12-08T19:37:58Z',\n",
       "    'title': 'Emergence of Different Modes of Tool Use in a Reaching and Dragging Task',\n",
       "    'summary': 'Tool use is an important milestone in the evolution of intelligence. In this\\npaper, we investigate different modes of tool use that emerge in a reaching and\\ndragging task. In this task, a jointed arm with a gripper must grab a tool (T,\\nI, or L-shaped) and drag an object down to the target location (the bottom of\\nthe arena). The simulated environment had real physics such as gravity and\\nfriction. We trained a deep-reinforcement learning based controller (with raw\\nvisual and proprioceptive input) with minimal reward shaping information to\\ntackle this task. We observed the emergence of a wide range of unexpected\\nbehaviors, not directly encoded in the motor primitives or reward functions.\\nExamples include hitting the object to the target location, correcting error of\\ninitial contact, throwing the tool toward the object, as well as normal\\nexpected behavior such as wide sweep. Also, we further analyzed these behaviors\\nbased on the type of tool and the initial position of the target object. Our\\nresults show a rich repertoire of behaviors, beyond the basic built-in\\nmechanisms of the deep reinforcement learning method we used.',\n",
       "    'author': [{'name': 'Khuong Nguyen'}, {'name': 'Yoonsuck Choe'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages, 10 figures, 14 pdf-embedded animations'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.04700v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.04700v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.01431v2',\n",
       "    'updated': '2021-02-03T09:07:53Z',\n",
       "    'published': '2021-02-02T11:04:22Z',\n",
       "    'title': 'Predicting the Time Until a Vehicle Changes the Lane Using LSTM-based\\n  Recurrent Neural Networks',\n",
       "    'summary': 'To plan safe and comfortable trajectories for automated vehicles on highways,\\naccurate predictions of traffic situations are needed. So far, a lot of\\nresearch effort has been spent on detecting lane change maneuvers rather than\\non estimating the point in time a lane change actually happens. In practice,\\nhowever, this temporal information might be even more useful. This paper deals\\nwith the development of a system that accurately predicts the time to the next\\nlane change of surrounding vehicles on highways using long short-term\\nmemory-based recurrent neural networks. An extensive evaluation based on a\\nlarge real-world data set shows that our approach is able to make reliable\\npredictions, even in the most challenging situations, with a root mean squared\\nerror around 0.7 seconds. Already 3.5 seconds prior to lane changes the\\npredictions become highly accurate, showing a median error of less than 0.25\\nseconds. In summary, this article forms a fundamental step towards downstreamed\\nhighly accurate position predictions.',\n",
       "    'author': [{'name': 'Florian Wirthmüller'},\n",
       "     {'name': 'Marvin Klimke'},\n",
       "     {'name': 'Julian Schlechtriemen'},\n",
       "     {'name': 'Jochen Hipp'},\n",
       "     {'name': 'Manfred Reichert'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2021.3058930'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2021.3058930',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2102.01431v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.01431v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'the article has been accepted for publication in IEEE Robotics and\\n  Automation Letters (RA-L); the article has been submitted to RA-L with IEEE\\n  ICRA conference option; if the article will be presented during the\\n  conference will be decided independently; 8 pages, 5 figures, 6 tables'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.03140v2',\n",
       "    'updated': '2021-04-16T08:33:19Z',\n",
       "    'published': '2021-02-05T12:34:54Z',\n",
       "    'title': 'Sparse Reward Exploration via Novelty Search and Emitters',\n",
       "    'summary': 'Reward-based optimization algorithms require both exploration, to find\\nrewards, and exploitation, to maximize performance. The need for efficient\\nexploration is even more significant in sparse reward settings, in which\\nperformance feedback is given sparingly, thus rendering it unsuitable for\\nguiding the search process. In this work, we introduce the SparsE Reward\\nExploration via Novelty and Emitters (SERENE) algorithm, capable of efficiently\\nexploring a search space, as well as optimizing rewards found in potentially\\ndisparate areas. Contrary to existing emitters-based approaches, SERENE\\nseparates the search space exploration and reward exploitation into two\\nalternating processes. The first process performs exploration through Novelty\\nSearch, a divergent search algorithm. The second one exploits discovered reward\\nareas through emitters, i.e. local instances of population-based optimization\\nalgorithms. A meta-scheduler allocates a global computational budget by\\nalternating between the two processes, ensuring the discovery and efficient\\nexploitation of disjoint reward areas. SERENE returns both a collection of\\ndiverse solutions covering the search space and a collection of high-performing\\nsolutions for each distinct reward area. We evaluate SERENE on various sparse\\nreward environments and show it compares favorably to existing baselines.',\n",
       "    'author': [{'name': 'Giuseppe Paolo',\n",
       "      'arxiv:affiliation': [{'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'ISIR'},\n",
       "       {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'SBRE'}]},\n",
       "     {'name': 'Alexandre Coninx',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Stephane Doncieux',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ISIR'}},\n",
       "     {'name': 'Alban Laflaquière',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'SBRE'}}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1145/3449639.3459314'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1145/3449639.3459314',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2102.03140v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.03140v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In 2021 Genetic and Evolutionary Computation Conference (GECCO 21),\\n  July, 2021, Lille, France. ACM, New York, NY, USA, 11 pages'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.04909v3',\n",
       "    'updated': '2022-02-28T17:28:51Z',\n",
       "    'published': '2021-03-08T17:15:23Z',\n",
       "    'title': 'Latent Imagination Facilitates Zero-Shot Transfer in Autonomous Racing',\n",
       "    'summary': 'World models learn behaviors in a latent imagination space to enhance the\\nsample-efficiency of deep reinforcement learning (RL) algorithms. While\\nlearning world models for high-dimensional observations (e.g., pixel inputs)\\nhas become practicable on standard RL benchmarks and some games, their\\neffectiveness in real-world robotics applications has not been explored. In\\nthis paper, we investigate how such agents generalize to real-world autonomous\\nvehicle control tasks, where advanced model-free deep RL algorithms fail. In\\nparticular, we set up a series of time-lap tasks for an F1TENTH racing robot,\\nequipped with a high-dimensional LiDAR sensor, on a set of test tracks with a\\ngradual increase in their complexity. In this continuous-control setting, we\\nshow that model-based agents capable of learning in imagination substantially\\noutperform model-free agents with respect to performance, sample efficiency,\\nsuccessful task completion, and generalization. Moreover, we show that the\\ngeneralization ability of model-based agents strongly depends on the choice of\\ntheir observation model. We provide extensive empirical evidence for the\\neffectiveness of world models provided with long enough memory horizons in\\nsim2real tasks.',\n",
       "    'author': [{'name': 'Axel Brunnbauer'},\n",
       "     {'name': 'Luigi Berducci'},\n",
       "     {'name': 'Andreas Brandstätter'},\n",
       "     {'name': 'Mathias Lechner'},\n",
       "     {'name': 'Ramin Hasani'},\n",
       "     {'name': 'Daniela Rus'},\n",
       "     {'name': 'Radu Grosu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This paper is accepted for presentation at the International\\n  Conference on Robotics and Automation (ICRA), 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.04909v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.04909v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.05648v3',\n",
       "    'updated': '2022-02-16T18:11:35Z',\n",
       "    'published': '2021-06-10T10:40:18Z',\n",
       "    'title': 'Unsupervised Behaviour Discovery with Quality-Diversity Optimisation',\n",
       "    'summary': 'Quality-Diversity algorithms refer to a class of evolutionary algorithms\\ndesigned to find a collection of diverse and high-performing solutions to a\\ngiven problem. In robotics, such algorithms can be used for generating a\\ncollection of controllers covering most of the possible behaviours of a robot.\\nTo do so, these algorithms associate a behavioural descriptor to each of these\\nbehaviours. Each behavioural descriptor is used for estimating the novelty of\\none behaviour compared to the others. In most existing algorithms, the\\nbehavioural descriptor needs to be hand-coded, thus requiring prior knowledge\\nabout the task to solve. In this paper, we introduce: Autonomous Robots\\nRealising their Abilities, an algorithm that uses a dimensionality reduction\\ntechnique to automatically learn behavioural descriptors based on raw sensory\\ndata. The performance of this algorithm is assessed on three robotic tasks in\\nsimulation. The experimental results show that it performs similarly to\\ntraditional hand-coded approaches without the requirement to provide any\\nhand-coded behavioural descriptor. In the collection of diverse and\\nhigh-performing solutions, it also manages to find behaviours that are novel\\nwith respect to more features than its hand-coded baselines. Finally, we\\nintroduce a variant of the algorithm which is robust to the dimensionality of\\nthe behavioural descriptor space.',\n",
       "    'author': [{'name': 'Luca Grillotti'}, {'name': 'Antoine Cully'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.05648v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.05648v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.08314v2',\n",
       "    'updated': '2021-08-16T19:08:43Z',\n",
       "    'published': '2021-06-15T17:45:32Z',\n",
       "    'title': 'Causal Navigation by Continuous-time Neural Networks',\n",
       "    'summary': 'Imitation learning enables high-fidelity, vision-based learning of policies\\nwithin rich, photorealistic environments. However, such techniques often rely\\non traditional discrete-time neural models and face difficulties in\\ngeneralizing to domain shifts by failing to account for the causal\\nrelationships between the agent and the environment. In this paper, we propose\\na theoretical and experimental framework for learning causal representations\\nusing continuous-time neural networks, specifically over their discrete-time\\ncounterparts. We evaluate our method in the context of visual-control learning\\nof drones over a series of complex tasks, ranging from short- and long-term\\nnavigation, to chasing static and dynamic objects through photorealistic\\nenvironments. Our results demonstrate that causal continuous-time deep models\\ncan perform robust navigation tasks, where advanced recurrent models fail.\\nThese models learn complex causal control representations directly from raw\\nvisual inputs and scale to solve a variety of tasks using imitation learning.',\n",
       "    'author': [{'name': 'Charles Vorbach'},\n",
       "     {'name': 'Ramin Hasani'},\n",
       "     {'name': 'Alexander Amini'},\n",
       "     {'name': 'Mathias Lechner'},\n",
       "     {'name': 'Daniela Rus'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '24 Pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.08314v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.08314v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.08522v1',\n",
       "    'updated': '2021-09-16T08:35:35Z',\n",
       "    'published': '2021-09-16T08:35:35Z',\n",
       "    'title': 'Dynamics-Aware Quality-Diversity for Efficient Learning of Skill\\n  Repertoires',\n",
       "    'summary': 'Quality-Diversity (QD) algorithms are powerful exploration algorithms that\\nallow robots to discover large repertoires of diverse and high-performing\\nskills. However, QD algorithms are sample inefficient and require millions of\\nevaluations. In this paper, we propose Dynamics-Aware Quality-Diversity\\n(DA-QD), a framework to improve the sample efficiency of QD algorithms through\\nthe use of dynamics models. We also show how DA-QD can then be used for\\ncontinual acquisition of new skill repertoires. To do so, we incrementally\\ntrain a deep dynamics model from experience obtained when performing skill\\ndiscovery using QD. We can then perform QD exploration in imagination with an\\nimagined skill repertoire. We evaluate our approach on three robotic\\nexperiments. First, our experiments show DA-QD is 20 times more sample\\nefficient than existing QD approaches for skill discovery. Second, we\\ndemonstrate learning an entirely new skill repertoire in imagination to perform\\nzero-shot learning. Finally, we show how DA-QD is useful and effective for\\nsolving a long horizon navigation task and for damage adaptation in the real\\nworld. Videos and source code are available at:\\nhttps://sites.google.com/view/da-qd.',\n",
       "    'author': [{'name': 'Bryan Lim'},\n",
       "     {'name': 'Luca Grillotti'},\n",
       "     {'name': 'Lorenzo Bernasconi'},\n",
       "     {'name': 'Antoine Cully'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICRA46639.2022.9811559'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICRA46639.2022.9811559',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2109.08522v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.08522v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.05437v2',\n",
       "    'updated': '2022-11-26T19:51:25Z',\n",
       "    'published': '2021-10-11T17:26:55Z',\n",
       "    'title': 'Autonomous Racing using a Hybrid Imitation-Reinforcement Learning\\n  Architecture',\n",
       "    'summary': 'In this work, we present a rigorous end-to-end control strategy for\\nautonomous vehicles aimed at minimizing lap times in a time attack racing\\nevent. We also introduce AutoRACE Simulator developed as a part of this\\nresearch project, which was employed to simulate accurate vehicular and\\nenvironmental dynamics along with realistic audio-visual effects. We adopted a\\nhybrid imitation-reinforcement learning architecture and crafted a novel reward\\nfunction to train a deep neural network policy to drive (using imitation\\nlearning) and race (using reinforcement learning) a car autonomously in less\\nthan 20 hours. Deployment results were reported as a direct comparison of 10\\nautonomous laps against 100 manual laps by 10 different human players. The\\nautonomous agent not only exhibited superior performance by gaining 0.96\\nseconds over the best manual lap, but it also dominated the human players by\\n1.46 seconds with regard to the mean lap time. This dominance could be\\njustified in terms of better trajectory optimization and lower reaction time of\\nthe autonomous agent.',\n",
       "    'author': [{'name': 'Chinmay Vilas Samak'},\n",
       "     {'name': 'Tanmay Vilas Samak'},\n",
       "     {'name': 'Sivanathan Kandhasamy'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.05437v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.05437v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.01919v1',\n",
       "    'updated': '2021-11-02T22:21:11Z',\n",
       "    'published': '2021-11-02T22:21:11Z',\n",
       "    'title': 'Discovering and Exploiting Sparse Rewards in a Learned Behavior Space',\n",
       "    'summary': 'Learning optimal policies in sparse rewards settings is difficult as the\\nlearning agent has little to no feedback on the quality of its actions. In\\nthese situations, a good strategy is to focus on exploration, hopefully leading\\nto the discovery of a reward signal to improve on. A learning algorithm capable\\nof dealing with this kind of settings has to be able to (1) explore possible\\nagent behaviors and (2) exploit any possible discovered reward. Efficient\\nexploration algorithms have been proposed that require to define a behavior\\nspace, that associates to an agent its resulting behavior in a space that is\\nknown to be worth exploring. The need to define this space is a limitation of\\nthese algorithms. In this work, we introduce STAX, an algorithm designed to\\nlearn a behavior space on-the-fly and to explore it while efficiently\\noptimizing any reward discovered. It does so by separating the exploration and\\nlearning of the behavior space from the exploitation of the reward through an\\nalternating two-steps process. In the first step, STAX builds a repertoire of\\ndiverse policies while learning a low-dimensional representation of the\\nhigh-dimensional observations generated during the policies evaluation. In the\\nexploitation step, emitters are used to optimize the performance of the\\ndiscovered rewarding solutions. Experiments conducted on three different sparse\\nreward environments show that STAX performs comparably to existing baselines\\nwhile requiring much less prior information about the task as it autonomously\\nbuilds the behavior space.',\n",
       "    'author': [{'name': 'Giuseppe Paolo'},\n",
       "     {'name': 'Alexandre Coninx'},\n",
       "     {'name': 'Alban Laflaquière'},\n",
       "     {'name': 'Stephane Doncieux'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '25 pages. Under review for the Evolutionary Computation Journal, MIT\\n  Press'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.01919v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.01919v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.09746v1',\n",
       "    'updated': '2022-01-19T15:54:39Z',\n",
       "    'published': '2022-01-19T15:54:39Z',\n",
       "    'title': 'Reinforcement Learning Textbook',\n",
       "    'summary': 'This textbook covers principles behind main modern deep reinforcement\\nlearning algorithms that achieved breakthrough results in many domains from\\ngame AI to robotics. All required theory is explained with proofs using unified\\nnotation and emphasize on the differences between different types of algorithms\\nand the reasons why they are constructed the way they are.',\n",
       "    'author': {'name': 'Sergey Ivanov'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The text is in Russian'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.09746v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.09746v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.13248v1',\n",
       "    'updated': '2022-01-27T16:40:36Z',\n",
       "    'published': '2022-01-27T16:40:36Z',\n",
       "    'title': 'SafeAPT: Safe Simulation-to-Real Robot Learning using Diverse Policies\\n  Learned in Simulation',\n",
       "    'summary': 'The framework of Simulation-to-real learning, i.e, learning policies in\\nsimulation and transferring those policies to the real world is one of the most\\npromising approaches towards data-efficient learning in robotics. However, due\\nto the inevitable reality gap between the simulation and the real world, a\\npolicy learned in the simulation may not always generate a safe behaviour on\\nthe real robot. As a result, during adaptation of the policy in the real world,\\nthe robot may damage itself or cause harm to its surroundings. In this work, we\\nintroduce a novel learning algorithm called SafeAPT that leverages a diverse\\nrepertoire of policies evolved in the simulation and transfers the most\\npromising safe policy to the real robot through episodic interaction. To\\nachieve this, SafeAPT iteratively learns a probabilistic reward model as well\\nas a safety model using real-world observations combined with simulated\\nexperiences as priors. Then, it performs Bayesian optimization on the\\nrepertoire with the reward model while maintaining the specified safety\\nconstraint using the safety model. SafeAPT allows a robot to adapt to a wide\\nrange of goals safely with the same repertoire of policies evolved in the\\nsimulation. We compare SafeAPT with several baselines, both in simulated and\\nreal robotic experiments and show that SafeAPT finds high-performance policies\\nwithin a few minutes in the real world while minimizing safety violations\\nduring the interactions.',\n",
       "    'author': [{'name': 'Rituraj Kaushik'},\n",
       "     {'name': 'Karol Arndt'},\n",
       "     {'name': 'Ville Kyrki'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Under review. For video of the paper http://tiny.cc/safeAPT'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.13248v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.13248v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.01258v3',\n",
       "    'updated': '2022-10-10T16:30:12Z',\n",
       "    'published': '2022-02-02T19:44:17Z',\n",
       "    'title': 'Accelerated Quality-Diversity through Massive Parallelism',\n",
       "    'summary': 'Quality-Diversity (QD) optimization algorithms are a well-known approach to\\ngenerate large collections of diverse and high-quality solutions. However,\\nderived from evolutionary computation, QD algorithms are population-based\\nmethods which are known to be data-inefficient and requires large amounts of\\ncomputational resources. This makes QD algorithms slow when used in\\napplications where solution evaluations are computationally costly. A common\\napproach to speed up QD algorithms is to evaluate solutions in parallel, for\\ninstance by using physical simulators in robotics. Yet, this approach is\\nlimited to several dozen of parallel evaluations as most physics simulators can\\nonly be parallelized more with a greater number of CPUs. With recent advances\\nin simulators that run on accelerators, thousands of evaluations can now be\\nperformed in parallel on single GPU/TPU. In this paper, we present QDax, an\\naccelerated implementation of MAP-Elites which leverages massive parallelism on\\naccelerators to make QD algorithms more accessible. We show that QD algorithms\\nare ideal candidates to take advantage of progress in hardware acceleration. We\\ndemonstrate that QD algorithms can scale with massive parallelism to be run at\\ninteractive timescales without any significant effect on the performance.\\nResults across standard optimization functions and four neuroevolution\\nbenchmark environments shows that experiment runtimes are reduced by two\\nfactors of magnitudes, turning days of computation into minutes. More\\nsurprising, we observe that reducing the number of generations by two orders of\\nmagnitude, and thus having significantly shorter lineage does not impact the\\nperformance of QD algorithms. These results show that QD can now benefit from\\nhardware acceleration, which contributed significantly to the bloom of deep\\nlearning.',\n",
       "    'author': [{'name': 'Bryan Lim'},\n",
       "     {'name': 'Maxime Allard'},\n",
       "     {'name': 'Luca Grillotti'},\n",
       "     {'name': 'Antoine Cully'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.01258v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.01258v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.03655v1',\n",
       "    'updated': '2022-04-07T14:07:51Z',\n",
       "    'published': '2022-04-07T14:07:51Z',\n",
       "    'title': 'Learning to Walk Autonomously via Reset-Free Quality-Diversity',\n",
       "    'summary': 'Quality-Diversity (QD) algorithms can discover large and complex behavioural\\nrepertoires consisting of both diverse and high-performing skills. However, the\\ngeneration of behavioural repertoires has mainly been limited to simulation\\nenvironments instead of real-world learning. This is because existing QD\\nalgorithms need large numbers of evaluations as well as episodic resets, which\\nrequire manual human supervision and interventions. This paper proposes\\nReset-Free Quality-Diversity optimization (RF-QD) as a step towards autonomous\\nlearning for robotics in open-ended environments. We build on Dynamics-Aware\\nQuality-Diversity (DA-QD) and introduce a behaviour selection policy that\\nleverages the diversity of the imagined repertoire and environmental\\ninformation to intelligently select of behaviours that can act as automatic\\nresets. We demonstrate this through a task of learning to walk within defined\\ntraining zones with obstacles. Our experiments show that we can learn full\\nrepertoires of legged locomotion controllers autonomously without manual resets\\nwith high sample efficiency in spite of harsh safety constraints. Finally,\\nusing an ablation of different target objectives, we show that it is important\\nfor RF-QD to have diverse types solutions available for the behaviour selection\\npolicy over solutions optimised with a specific objective. Videos and code\\navailable at https://sites.google.com/view/rf-qd.',\n",
       "    'author': [{'name': 'Bryan Lim'},\n",
       "     {'name': 'Alexander Reichenbach'},\n",
       "     {'name': 'Antoine Cully'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1145/3512290.3528715'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1145/3512290.3528715',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2204.03655v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.03655v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.05726v1',\n",
       "    'updated': '2022-04-12T11:44:01Z',\n",
       "    'published': '2022-04-12T11:44:01Z',\n",
       "    'title': 'Hierarchical Quality-Diversity for Online Damage Recovery',\n",
       "    'summary': 'Adaptation capabilities, like damage recovery, are crucial for the deployment\\nof robots in complex environments. Several works have demonstrated that using\\nrepertoires of pre-trained skills can enable robots to adapt to unforeseen\\nmechanical damages in a few minutes. These adaptation capabilities are directly\\nlinked to the behavioural diversity in the repertoire. The more alternatives\\nthe robot has to execute a skill, the better are the chances that it can adapt\\nto a new situation. However, solving complex tasks, like maze navigation,\\nusually requires multiple different skills. Finding a large behavioural\\ndiversity for these multiple skills often leads to an intractable exponential\\ngrowth of the number of required solutions. In this paper, we introduce the\\nHierarchical Trial and Error algorithm, which uses a hierarchical behavioural\\nrepertoire to learn diverse skills and leverages them to make the robot more\\nadaptive to different situations. We show that the hierarchical decomposition\\nof skills enables the robot to learn more complex behaviours while keeping the\\nlearning of the repertoire tractable. The experiments with a hexapod robot show\\nthat our method solves maze navigation tasks with 20% less actions in the most\\nchallenging scenarios than the best baseline while having 57% less complete\\nfailures.',\n",
       "    'author': [{'name': 'Maxime Allard'},\n",
       "     {'name': 'Simón C. Smith'},\n",
       "     {'name': 'Konstantinos Chatzilygeroudis'},\n",
       "     {'name': 'Antoine Cully'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1145/3512290.3528751'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1145/3512290.3528751',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2204.05726v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.05726v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at GECCO 2022'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.09828v1',\n",
       "    'updated': '2022-04-21T00:29:38Z',\n",
       "    'published': '2022-04-21T00:29:38Z',\n",
       "    'title': 'Relevance-guided Unsupervised Discovery of Abilities with\\n  Quality-Diversity Algorithms',\n",
       "    'summary': 'Quality-Diversity algorithms provide efficient mechanisms to generate large\\ncollections of diverse and high-performing solutions, which have shown to be\\ninstrumental for solving downstream tasks. However, most of those algorithms\\nrely on a behavioural descriptor to characterise the diversity that is\\nhand-coded, hence requiring prior knowledge about the considered tasks. In this\\nwork, we introduce Relevance-guided Unsupervised Discovery of Abilities; a\\nQuality-Diversity algorithm that autonomously finds a behavioural\\ncharacterisation tailored to the task at hand. In particular, our method\\nintroduces a custom diversity metric that leads to higher densities of\\nsolutions near the areas of interest in the learnt behavioural descriptor\\nspace. We evaluate our approach on a simulated robotic environment, where the\\nrobot has to autonomously discover its abilities based on its full sensory\\ndata. We evaluated the algorithms on three tasks: navigation to random targets,\\nmoving forward with a high velocity, and performing half-rolls. The\\nexperimental results show that our method manages to discover collections of\\nsolutions that are not only diverse, but also well-adapted to the considered\\ndownstream task.',\n",
       "    'author': [{'name': 'Luca Grillotti'}, {'name': 'Antoine Cully'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1145/3512290.3528837'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1145/3512290.3528837',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2204.09828v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.09828v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at GECCO 2022'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.01080v1',\n",
       "    'updated': '2022-07-23T12:15:43Z',\n",
       "    'published': '2022-07-23T12:15:43Z',\n",
       "    'title': 'Event-Driven Tactile Learning with Location Spiking Neurons',\n",
       "    'summary': 'The sense of touch is essential for a variety of daily tasks. New advances in\\nevent-based tactile sensors and Spiking Neural Networks (SNNs) spur the\\nresearch in event-driven tactile learning. However, SNN-enabled event-driven\\ntactile learning is still in its infancy due to the limited representative\\nabilities of existing spiking neurons and high spatio-temporal complexity in\\nthe data. In this paper, to improve the representative capabilities of existing\\nspiking neurons, we propose a novel neuron model called \"location spiking\\nneuron\", which enables us to extract features of event-based data in a novel\\nway. Moreover, based on the classical Time Spike Response Model (TSRM), we\\ndevelop a specific location spiking neuron model - Location Spike Response\\nModel (LSRM) that serves as a new building block of SNNs. Furthermore, we\\npropose a hybrid model which combines an SNN with TSRM neurons and an SNN with\\nLSRM neurons to capture the complex spatio-temporal dependencies in the data.\\nExtensive experiments demonstrate the significant improvements of our models\\nover other works on event-driven tactile learning and show the superior energy\\nefficiency of our models and location spiking neurons, which may unlock their\\npotential on neuromorphic hardware.',\n",
       "    'author': [{'name': 'Peng Kang'},\n",
       "     {'name': 'Srutarshi Banerjee'},\n",
       "     {'name': 'Henry Chopp'},\n",
       "     {'name': 'Aggelos Katsaggelos'},\n",
       "     {'name': 'Oliver Cossairt'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'accepted by IJCNN 2022 (oral), the source code is available at\\n  https://github.com/pkang2017/TactileLocNeurons'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.01080v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.01080v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.09007v1',\n",
       "    'updated': '2022-09-19T13:34:18Z',\n",
       "    'published': '2022-09-19T13:34:18Z',\n",
       "    'title': 'Comparative Study of Q-Learning and NeuroEvolution of Augmenting\\n  Topologies for Self Driving Agents',\n",
       "    'summary': 'Autonomous driving vehicles have been of keen interest ever since automation\\nof various tasks started. Humans are prone to exhaustion and have a slow\\nresponse time on the road, and on top of that driving is already quite a\\ndangerous task with around 1.35 million road traffic incident deaths each year.\\nIt is expected that autonomous driving can reduce the number of driving\\naccidents around the world which is why this problem has been of keen interest\\nfor researchers. Currently, self-driving vehicles use different algorithms for\\nvarious sub-problems in making the vehicle autonomous. We will focus\\nreinforcement learning algorithms, more specifically Q-learning algorithms and\\nNeuroEvolution of Augment Topologies (NEAT), a combination of evolutionary\\nalgorithms and artificial neural networks, to train a model agent to learn how\\nto drive on a given path. This paper will focus on drawing a comparison between\\nthe two aforementioned algorithms.',\n",
       "    'author': [{'name': 'Arhum Ishtiaq'},\n",
       "     {'name': 'Maheen Anees'},\n",
       "     {'name': 'Sara Mahmood'},\n",
       "     {'name': 'Neha Jafry'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.09007v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.09007v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.04277v3',\n",
       "    'updated': '2022-12-19T08:06:25Z',\n",
       "    'published': '2022-10-09T14:49:27Z',\n",
       "    'title': 'Boost Event-Driven Tactile Learning with Location Spiking Neurons',\n",
       "    'summary': 'Tactile sensing is essential for a variety of daily tasks. And recent\\nadvances in event-driven tactile sensors and Spiking Neural Networks (SNNs)\\nspur the research in related fields. However, SNN-enabled event-driven tactile\\nlearning is still in its infancy due to the limited representation abilities of\\nexisting spiking neurons and high spatio-temporal complexity in the\\nevent-driven tactile data. In this paper, to improve the representation\\ncapability of existing spiking neurons, we propose a novel neuron model called\\n\"location spiking neuron\", which enables us to extract features of event-based\\ndata in a novel way. Specifically, based on the classical Time Spike Response\\nModel (TSRM), we develop the Location Spike Response Model (LSRM). In addition,\\nbased on the most commonly-used Time Leaky Integrate-and-Fire (TLIF) model, we\\ndevelop the Location Leaky Integrate-and-Fire (LLIF) model. Moreover, to\\ndemonstrate the representation effectiveness of our proposed neurons and\\ncapture the complex spatio-temporal dependencies in the event-driven tactile\\ndata, we exploit the location spiking neurons to propose two hybrid models for\\nevent-driven tactile learning. Specifically, the first hybrid model combines a\\nfully-connected SNN with TSRM neurons and a fully-connected SNN with LSRM\\nneurons. And the second hybrid model fuses the spatial spiking graph neural\\nnetwork with TLIF neurons and the temporal spiking graph neural network with\\nLLIF neurons. Extensive experiments demonstrate the significant improvements of\\nour models over the state-of-the-art methods on event-driven tactile learning.\\nMoreover, compared to the counterpart artificial neural networks (ANNs), our\\nSNN models are 10x to 100x energy-efficient, which shows the superior energy\\nefficiency of our models and may bring new opportunities to the spike-based\\nlearning community and neuromorphic engineering.',\n",
       "    'author': [{'name': 'Peng Kang'},\n",
       "     {'name': 'Srutarshi Banerjee'},\n",
       "     {'name': 'Henry Chopp'},\n",
       "     {'name': 'Aggelos Katsaggelos'},\n",
       "     {'name': 'Oliver Cossairt'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Under review. Please note that this paper is a journal extension of\\n  our previous conference paper: arXiv:2209.01080. Please check what we added\\n  in the introduction part'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.04277v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.04277v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.04819v1',\n",
       "    'updated': '2022-10-10T16:31:11Z',\n",
       "    'published': '2022-10-10T16:31:11Z',\n",
       "    'title': 'Efficient Learning of Locomotion Skills through the Discovery of Diverse\\n  Environmental Trajectory Generator Priors',\n",
       "    'summary': 'Data-driven learning based methods have recently been particularly successful\\nat learning robust locomotion controllers for a variety of unstructured\\nterrains. Prior work has shown that incorporating good locomotion priors in the\\nform of trajectory generators (TGs) is effective at efficiently learning\\ncomplex locomotion skills. However, defining a good, single TG as\\ntasks/environments become increasingly more complex remains a challenging\\nproblem as it requires extensive tuning and risks reducing the effectiveness of\\nthe prior. In this paper, we present Evolved Environmental Trajectory\\nGenerators (EETG), a method that learns a diverse set of specialised locomotion\\npriors using Quality-Diversity algorithms while maintaining a single policy\\nwithin the Policies Modulating TG (PMTG) architecture. The results demonstrate\\nthat EETG enables a quadruped robot to successfully traverse a wide range of\\nenvironments, such as slopes, stairs, rough terrain, and balance beams. Our\\nexperiments show that learning a diverse set of specialized TG priors is\\nsignificantly (5 times) more efficient than using a single, fixed prior when\\ndealing with a wide range of environments.',\n",
       "    'author': [{'name': 'Shikha Surana'},\n",
       "     {'name': 'Bryan Lim'},\n",
       "     {'name': 'Antoine Cully'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.04819v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.04819v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.09918v1',\n",
       "    'updated': '2022-10-18T15:02:41Z',\n",
       "    'published': '2022-10-18T15:02:41Z',\n",
       "    'title': 'Online Damage Recovery for Physical Robots with Hierarchical\\n  Quality-Diversity',\n",
       "    'summary': 'In real-world environments, robots need to be resilient to damages and robust\\nto unforeseen scenarios. Quality-Diversity (QD) algorithms have been\\nsuccessfully used to make robots adapt to damages in seconds by leveraging a\\ndiverse set of learned skills. A high diversity of skills increases the chances\\nof a robot to succeed at overcoming new situations since there are more\\npotential alternatives to solve a new task.However, finding and storing a large\\nbehavioural diversity of multiple skills often leads to an increase in\\ncomputational complexity. Furthermore, robot planning in a large skill space is\\nan additional challenge that arises with an increased number of skills.\\nHierarchical structures can help reducing this search and storage complexity by\\nbreaking down skills into primitive skills. In this paper, we introduce the\\nHierarchical Trial and Error algorithm, which uses a hierarchical behavioural\\nrepertoire to learn diverse skills and leverages them to make the robot adapt\\nquickly in the physical world. We show that the hierarchical decomposition of\\nskills enables the robot to learn more complex behaviours while keeping the\\nlearning of the repertoire tractable. Experiments with a hexapod robot show\\nthat our method solves a maze navigation tasks with 20% less actions in\\nsimulation, and 43% less actions in the physical world, for the most\\nchallenging scenarios than the best baselines while having 78% less complete\\nfailures.',\n",
       "    'author': [{'name': 'Maxime Allard'},\n",
       "     {'name': 'Simón C. Smith'},\n",
       "     {'name': 'Konstantinos Chatzilygeroudis'},\n",
       "     {'name': 'Bryan Lim'},\n",
       "     {'name': 'Antoine Cully'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: substantial text overlap with arXiv:2204.05726'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.09918v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.09918v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.02193v1',\n",
       "    'updated': '2022-11-04T00:14:42Z',\n",
       "    'published': '2022-11-04T00:14:42Z',\n",
       "    'title': 'Benchmarking Quality-Diversity Algorithms on Neuroevolution for\\n  Reinforcement Learning',\n",
       "    'summary': 'We present a Quality-Diversity benchmark suite for Deep Neuroevolution in\\nReinforcement Learning domains for robot control. The suite includes the\\ndefinition of tasks, environments, behavioral descriptors, and fitness. We\\nspecify different benchmarks based on the complexity of both the task and the\\nagent controlled by a deep neural network. The benchmark uses standard\\nQuality-Diversity metrics, including coverage, QD-score, maximum fitness, and\\nan archive profile metric to quantify the relation between coverage and\\nfitness. We also present how to quantify the robustness of the solutions with\\nrespect to environmental stochasticity by introducing corrected versions of the\\nsame metrics. We believe that our benchmark is a valuable tool for the\\ncommunity to compare and improve their findings. The source code is available\\nonline: https://github.com/adaptive-intelligent-robotics/QDax',\n",
       "    'author': [{'name': 'Manon Flageat'},\n",
       "     {'name': 'Bryan Lim'},\n",
       "     {'name': 'Luca Grillotti'},\n",
       "     {'name': 'Maxime Allard'},\n",
       "     {'name': 'Simón C. Smith'},\n",
       "     {'name': 'Antoine Cully'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at GECCO Workshop on Quality Diversity Algorithm Benchmarks'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.02193v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.02193v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.15451v1',\n",
       "    'updated': '2022-11-22T16:57:52Z',\n",
       "    'published': '2022-11-22T16:57:52Z',\n",
       "    'title': 'Discovering Unsupervised Behaviours from Full-State Trajectories',\n",
       "    'summary': 'Improving open-ended learning capabilities is a promising approach to enable\\nrobots to face the unbounded complexity of the real-world. Among existing\\nmethods, the ability of Quality-Diversity algorithms to generate large\\ncollections of diverse and high-performing skills is instrumental in this\\ncontext. However, most of those algorithms rely on a hand-coded behavioural\\ndescriptor to characterise the diversity, hence requiring prior knowledge about\\nthe considered tasks. In this work, we propose an additional analysis of\\nAutonomous Robots Realising their Abilities; a Quality-Diversity algorithm that\\nautonomously finds behavioural characterisations. We evaluate this approach on\\na simulated robotic environment, where the robot has to autonomously discover\\nits abilities from its full-state trajectories. All algorithms were applied to\\nthree tasks: navigation, moving forward with a high velocity, and performing\\nhalf-rolls. The experimental results show that the algorithm under study\\ndiscovers autonomously collections of solutions that are diverse with respect\\nto all tasks. More specifically, the analysed approach autonomously finds\\npolicies that make the robot move to diverse positions, but also utilise its\\nlegs in diverse ways, and even perform half-rolls.',\n",
       "    'author': [{'name': 'Luca Grillotti'}, {'name': 'Antoine Cully'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at the Workshop on Agent Learning in Open-Endedness (ALOE)\\n  at ICLR 2022. arXiv admin note: substantial text overlap with\\n  arXiv:2204.09828'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.15451v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.15451v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.04359v1',\n",
       "    'updated': '2022-12-08T15:56:13Z',\n",
       "    'published': '2022-12-08T15:56:13Z',\n",
       "    'title': 'HERD: Continuous Human-to-Robot Evolution for Learning from Human\\n  Demonstration',\n",
       "    'summary': 'The ability to learn from human demonstration endows robots with the ability\\nto automate various tasks. However, directly learning from human demonstration\\nis challenging since the structure of the human hand can be very different from\\nthe desired robot gripper. In this work, we show that manipulation skills can\\nbe transferred from a human to a robot through the use of micro-evolutionary\\nreinforcement learning, where a five-finger human dexterous hand robot\\ngradually evolves into a commercial robot, while repeated interacting in a\\nphysics simulator to continuously update the policy that is first learned from\\nhuman demonstration. To deal with the high dimensions of robot parameters, we\\npropose an algorithm for multi-dimensional evolution path searching that allows\\njoint optimization of both the robot evolution path and the policy. Through\\nexperiments on human object manipulation datasets, we show that our framework\\ncan efficiently transfer the expert human agent policy trained from human\\ndemonstrations in diverse modalities to target commercial robots.',\n",
       "    'author': [{'name': 'Xingyu Liu'},\n",
       "     {'name': 'Deepak Pathak'},\n",
       "     {'name': 'Kris M. Kitani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.04359v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.04359v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.12620v1',\n",
       "    'updated': '2022-12-24T00:00:53Z',\n",
       "    'published': '2022-12-24T00:00:53Z',\n",
       "    'title': 'Mantis: Enabling Energy-Efficient Autonomous Mobile Agents with Spiking\\n  Neural Networks',\n",
       "    'summary': 'Autonomous mobile agents such as unmanned aerial vehicles (UAVs) and mobile\\nrobots have shown huge potential for improving human productivity. These mobile\\nagents require low power/energy consumption to have a long lifespan since they\\nare usually powered by batteries. These agents also need to adapt to\\nchanging/dynamic environments, especially when deployed in far or dangerous\\nlocations, thus requiring efficient online learning capabilities. These\\nrequirements can be fulfilled by employing Spiking Neural Networks (SNNs) since\\nSNNs offer low power/energy consumption due to sparse computations and\\nefficient online learning due to bio-inspired learning mechanisms. However, a\\nmethodology is still required to employ appropriate SNN models on autonomous\\nmobile agents. Towards this, we propose a Mantis methodology to systematically\\nemploy SNNs on autonomous mobile agents to enable energy-efficient processing\\nand adaptive capabilities in dynamic environments. The key ideas of our Mantis\\ninclude the optimization of SNN operations, the employment of a bio-plausible\\nonline learning mechanism, and the SNN model selection. The experimental\\nresults demonstrate that our methodology maintains high accuracy with a\\nsignificantly smaller memory footprint and energy consumption (i.e., 3.32x\\nmemory reduction and 2.9x energy saving for an SNN model with 8-bit weights)\\ncompared to the baseline network with 32-bit weights. In this manner, our\\nMantis enables the employment of SNNs for resource- and energy-constrained\\nmobile agents.',\n",
       "    'author': [{'name': 'Rachmad Vidya Wicaksana Putra'},\n",
       "     {'name': 'Muhammad Shafique'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear at the 2023 International Conference on Automation,\\n  Robotics and Applications (ICARA), February 2023, Abu Dhabi, UAE. arXiv admin\\n  note: text overlap with arXiv:2206.08656'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.12620v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.12620v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.12586v1',\n",
       "    'updated': '2020-12-23T10:33:22Z',\n",
       "    'published': '2020-12-23T10:33:22Z',\n",
       "    'title': 'Distributed Adaptive Control: An ideal Cognitive Architecture candidate\\n  for managing a robotic recycling plant',\n",
       "    'summary': 'In the past decade, society has experienced notable growth in a variety of\\ntechnological areas. However, the Fourth Industrial Revolution has not been\\nembraced yet. Industry 4.0 imposes several challenges which include the\\nnecessity of new architectural models to tackle the uncertainty that open\\nenvironments represent to cyber-physical systems (CPS). Waste Electrical and\\nElectronic Equipment (WEEE) recycling plants stand for one of such open\\nenvironments. Here, CPSs must work harmoniously in a changing environment,\\ninteracting with similar and not so similar CPSs, and adaptively collaborating\\nwith human workers. In this paper, we support the Distributed Adaptive Control\\n(DAC) theory as a suitable Cognitive Architecture for managing a recycling\\nplant. Specifically, a recursive implementation of DAC (between both\\nsingle-agent and large-scale levels) is proposed to meet the expected demands\\nof the European Project HR-Recycler. Additionally, with the aim of having a\\nrealistic benchmark for future implementations of the recursive DAC, a\\nmicro-recycling plant prototype is presented.',\n",
       "    'author': [{'name': 'Oscar Guerrero-Rosado'}, {'name': 'Paul Verschure'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 2 figures, Living Machines conference 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.12586v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.12586v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1506.03899v1',\n",
       "    'updated': '2015-06-12T05:45:36Z',\n",
       "    'published': '2015-06-12T05:45:36Z',\n",
       "    'title': 'Place classification with a graph regularized deep neural network model',\n",
       "    'summary': 'Place classification is a fundamental ability that a robot should possess to\\ncarry out effective human-robot interactions. It is a nontrivial classification\\nproblem which has attracted many research. In recent years, there is a high\\nexploitation of Artificial Intelligent algorithms in robotics applications.\\nInspired by the recent successes of deep learning methods, we propose an\\nend-to-end learning approach for the place classification problem. With the\\ndeep architectures, this methodology automatically discovers features and\\ncontributes in general to higher classification accuracies. The pipeline of our\\napproach is composed of three parts. Firstly, we construct multiple layers of\\nlaser range data to represent the environment information in different levels\\nof granularity. Secondly, each layer of data is fed into a deep neural network\\nmodel for classification, where a graph regularization is imposed to the deep\\narchitecture for keeping local consistency between adjacent samples. Finally,\\nthe predicted labels obtained from all the layers are fused based on confidence\\ntrees to maximize the overall confidence. Experimental results validate the\\neffective- ness of our end-to-end place classification framework in which both\\nthe multi-layer structure and the graph regularization promote the\\nclassification performance. Furthermore, results show that the features\\nautomatically learned from the raw input range data can achieve competitive\\nresults to the features constructed based on statistical and geometrical\\ninformation.',\n",
       "    'author': [{'name': 'Yiyi Liao'},\n",
       "     {'name': 'Sarath Kodagoda'},\n",
       "     {'name': 'Yue Wang'},\n",
       "     {'name': 'Lei Shi'},\n",
       "     {'name': 'Yong Liu'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1506.03899v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1506.03899v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1704.07911v1',\n",
       "    'updated': '2017-04-25T21:25:41Z',\n",
       "    'published': '2017-04-25T21:25:41Z',\n",
       "    'title': 'Explaining How a Deep Neural Network Trained with End-to-End Learning\\n  Steers a Car',\n",
       "    'summary': \"As part of a complete software stack for autonomous driving, NVIDIA has\\ncreated a neural-network-based system, known as PilotNet, which outputs\\nsteering angles given images of the road ahead. PilotNet is trained using road\\nimages paired with the steering angles generated by a human driving a\\ndata-collection car. It derives the necessary domain knowledge by observing\\nhuman drivers. This eliminates the need for human engineers to anticipate what\\nis important in an image and foresee all the necessary rules for safe driving.\\nRoad tests demonstrated that PilotNet can successfully perform lane keeping in\\na wide variety of driving conditions, regardless of whether lane markings are\\npresent or not.\\n  The goal of the work described here is to explain what PilotNet learns and\\nhow it makes its decisions. To this end we developed a method for determining\\nwhich elements in the road image most influence PilotNet's steering decision.\\nResults show that PilotNet indeed learns to recognize relevant objects on the\\nroad.\\n  In addition to learning the obvious features such as lane markings, edges of\\nroads, and other cars, PilotNet learns more subtle features that would be hard\\nto anticipate and program by engineers, for example, bushes lining the edge of\\nthe road and atypical vehicle classes.\",\n",
       "    'author': [{'name': 'Mariusz Bojarski'},\n",
       "     {'name': 'Philip Yeres'},\n",
       "     {'name': 'Anna Choromanska'},\n",
       "     {'name': 'Krzysztof Choromanski'},\n",
       "     {'name': 'Bernhard Firner'},\n",
       "     {'name': 'Lawrence Jackel'},\n",
       "     {'name': 'Urs Muller'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1704.07911v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1704.07911v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.07043v2',\n",
       "    'updated': '2022-09-19T08:56:04Z',\n",
       "    'published': '2022-09-15T04:51:42Z',\n",
       "    'title': 'Towards self-attention based visual navigation in the real world',\n",
       "    'summary': \"Vision guided navigation requires processing complex visual information to\\ninform task-orientated decisions. Applications include autonomous robots,\\nself-driving cars, and assistive vision for humans. A key element is the\\nextraction and selection of relevant features in pixel space upon which to base\\naction choices, for which Machine Learning techniques are well suited. However,\\nDeep Reinforcement Learning agents trained in simulation often exhibit\\nunsatisfactory results when deployed in the real-world due to perceptual\\ndifferences known as the $\\\\textit{reality gap}$. An approach that is yet to be\\nexplored to bridge this gap is self-attention. In this paper we (1) perform a\\nsystematic exploration of the hyperparameter space for self-attention based\\nnavigation of 3D environments and qualitatively appraise behaviour observed\\nfrom different hyperparameter sets, including their ability to generalise; (2)\\npresent strategies to improve the agents' generalisation abilities and\\nnavigation behaviour; and (3) show how models trained in simulation are capable\\nof processing real world images meaningfully in real time. To our knowledge,\\nthis is the first demonstration of a self-attention based agent successfully\\ntrained in navigating a 3D action space, using less than 4000 parameters.\",\n",
       "    'author': [{'name': 'Jaime Ruiz-Serra'},\n",
       "     {'name': 'Jack White'},\n",
       "     {'name': 'Stephen Petrie'},\n",
       "     {'name': 'Tatiana Kameneva'},\n",
       "     {'name': 'Chris McCarthy'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to The 2022 Australian Conference on Robotics and\\n  Automation (ACRA 2022)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.07043v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.07043v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1401.5535v2',\n",
       "    'updated': '2014-01-23T09:19:10Z',\n",
       "    'published': '2014-01-22T01:35:59Z',\n",
       "    'title': 'Learning Mid-Level Features and Modeling Neuron Selectivity for Image\\n  Classification',\n",
       "    'summary': 'We now know that mid-level features can greatly enhance the performance of\\nimage learning, but how to automatically learn the image features efficiently\\nand in an unsupervised manner is still an open question. In this paper, we\\npresent a very efficient mid-level feature learning approach (MidFea), which\\nonly involves simple operations such as $k$-means clustering, convolution,\\npooling, vector quantization and random projection. We explain why this simple\\nmethod generates the desired features, and argue that there is no need to spend\\nmuch time in learning low-level feature extractors. Furthermore, to boost the\\nperformance, we propose to model the neuron selectivity (NS) principle by\\nbuilding an additional layer over the mid-level features before feeding the\\nfeatures into the classifier. We show that the NS-layer learns\\ncategory-specific neurons with both bottom-up inference and top-down analysis,\\nand thus supports fast inference for a query image. We run extensive\\nexperiments on several public databases to demonstrate that our approach can\\nachieve state-of-the-art performances for face recognition, gender\\nclassification, age estimation and object categorization. In particular, we\\ndemonstrate that our approach is more than an order of magnitude faster than\\nsome recently proposed sparse coding based methods.',\n",
       "    'author': [{'name': 'Shu Kong'},\n",
       "     {'name': 'Zhuolin Jiang'},\n",
       "     {'name': 'Qiang Yang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '19 pages, 14 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1401.5535v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1401.5535v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1507.06821v2',\n",
       "    'updated': '2015-08-18T13:04:29Z',\n",
       "    'published': '2015-07-24T12:20:19Z',\n",
       "    'title': 'Multimodal Deep Learning for Robust RGB-D Object Recognition',\n",
       "    'summary': 'Robust object recognition is a crucial ingredient of many, if not all,\\nreal-world robotics applications. This paper leverages recent progress on\\nConvolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture\\nfor object recognition. Our architecture is composed of two separate CNN\\nprocessing streams - one for each modality - which are consecutively combined\\nwith a late fusion network. We focus on learning with imperfect sensor data, a\\ntypical problem in real-world robotics tasks. For accurate learning, we\\nintroduce a multi-stage training methodology and two crucial ingredients for\\nhandling depth data with CNNs. The first, an effective encoding of depth\\ninformation for CNNs that enables learning without the need for large depth\\ndatasets. The second, a data augmentation scheme for robust learning with depth\\nimages by corrupting them with realistic noise patterns. We present\\nstate-of-the-art results on the RGB-D object dataset and show recognition in\\nchallenging RGB-D real-world noisy settings.',\n",
       "    'author': [{'name': 'Andreas Eitel'},\n",
       "     {'name': 'Jost Tobias Springenberg'},\n",
       "     {'name': 'Luciano Spinello'},\n",
       "     {'name': 'Martin Riedmiller'},\n",
       "     {'name': 'Wolfram Burgard'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Final version submitted to IROS'2015, results unchanged,\\n  reformulation of some text passages in abstract and introduction\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1507.06821v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1507.06821v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1507.08286v1',\n",
       "    'updated': '2015-07-29T20:11:12Z',\n",
       "    'published': '2015-07-29T20:11:12Z',\n",
       "    'title': 'Deep Learning for Single-View Instance Recognition',\n",
       "    'summary': 'Deep learning methods have typically been trained on large datasets in which\\nmany training examples are available. However, many real-world product datasets\\nhave only a small number of images available for each product. We explore the\\nuse of deep learning methods for recognizing object instances when we have only\\na single training example per class. We show that feedforward neural networks\\noutperform state-of-the-art methods for recognizing objects from novel\\nviewpoints even when trained from just a single image per object. To further\\nimprove our performance on this task, we propose to take advantage of a\\nsupplementary dataset in which we observe a separate set of objects from\\nmultiple viewpoints. We introduce a new approach for training deep learning\\nmethods for instance recognition with limited training data, in which we use an\\nauxiliary multi-view dataset to train our network to be robust to viewpoint\\nchanges. We find that this approach leads to a more robust classifier for\\nrecognizing objects from novel viewpoints, outperforming previous\\nstate-of-the-art approaches including keypoint-matching, template-based\\ntechniques, and sparse coding.',\n",
       "    'author': [{'name': 'David Held'},\n",
       "     {'name': 'Sebastian Thrun'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '16 pages, 15 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1507.08286v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1507.08286v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1511.05298v3',\n",
       "    'updated': '2016-04-11T19:00:24Z',\n",
       "    'published': '2015-11-17T07:49:58Z',\n",
       "    'title': 'Structural-RNN: Deep Learning on Spatio-Temporal Graphs',\n",
       "    'summary': 'Deep Recurrent Neural Network architectures, though remarkably capable at\\nmodeling sequences, lack an intuitive high-level spatio-temporal structure.\\nThat is while many problems in computer vision inherently have an underlying\\nhigh-level structure and can benefit from it. Spatio-temporal graphs are a\\npopular tool for imposing such high-level intuitions in the formulation of real\\nworld problems. In this paper, we propose an approach for combining the power\\nof high-level spatio-temporal graphs and sequence learning success of Recurrent\\nNeural Networks~(RNNs). We develop a scalable method for casting an arbitrary\\nspatio-temporal graph as a rich RNN mixture that is feedforward, fully\\ndifferentiable, and jointly trainable. The proposed method is generic and\\nprincipled as it can be used for transforming any spatio-temporal graph through\\nemploying a certain set of well defined steps. The evaluations of the proposed\\napproach on a diverse set of problems, ranging from modeling human motion to\\nobject interactions, shows improvement over the state-of-the-art with a large\\nmargin. We expect this method to empower new approaches to problem formulation\\nthrough high-level spatio-temporal graphs and Recurrent Neural Networks.',\n",
       "    'author': [{'name': 'Ashesh Jain'},\n",
       "     {'name': 'Amir R. Zamir'},\n",
       "     {'name': 'Silvio Savarese'},\n",
       "     {'name': 'Ashutosh Saxena'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2016 (Oral)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1511.05298v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1511.05298v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1603.02636v2',\n",
       "    'updated': '2016-12-05T18:06:28Z',\n",
       "    'published': '2016-03-08T19:39:19Z',\n",
       "    'title': 'DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range\\n  Data',\n",
       "    'summary': 'We introduce the DROW detector, a deep learning based detector for 2D range\\ndata. Laser scanners are lighting invariant, provide accurate range data, and\\ntypically cover a large field of view, making them interesting sensors for\\nrobotics applications. So far, research on detection in laser range data has\\nbeen dominated by hand-crafted features and boosted classifiers, potentially\\nlosing performance due to suboptimal design choices. We propose a Convolutional\\nNeural Network (CNN) based detector for this task. We show how to effectively\\napply CNNs for detection in 2D range data, and propose a depth preprocessing\\nstep and voting scheme that significantly improve CNN performance. We\\ndemonstrate our approach on wheelchairs and walkers, obtaining state of the art\\ndetection results. Apart from the training data, none of our design choices\\nlimits the detector to these two classes, though. We provide a ROS node for our\\ndetector and release our dataset containing 464k laser scans, out of which 24k\\nwere annotated.',\n",
       "    'author': [{'name': 'Lucas Beyer'},\n",
       "     {'name': 'Alexander Hermans'},\n",
       "     {'name': 'Bastian Leibe'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Lucas Beyer and Alexander Hermans contributed equally'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1603.02636v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1603.02636v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1604.04528v1',\n",
       "    'updated': '2016-04-15T14:55:27Z',\n",
       "    'published': '2016-04-15T14:55:27Z',\n",
       "    'title': 'Tracking Human-like Natural Motion Using Deep Recurrent Neural Networks',\n",
       "    'summary': 'Kinect skeleton tracker is able to achieve considerable human body tracking\\nperformance in convenient and a low-cost manner. However, The tracker often\\ncaptures unnatural human poses such as discontinuous and vibrated motions when\\nself-occlusions occur. A majority of approaches tackle this problem by using\\nmultiple Kinect sensors in a workspace. Combination of the measurements from\\ndifferent sensors is then conducted in Kalman filter framework or optimization\\nproblem is formulated for sensor fusion. However, these methods usually require\\nheuristics to measure reliability of measurements observed from each Kinect\\nsensor. In this paper, we developed a method to improve Kinect skeleton using\\nsingle Kinect sensor, in which supervised learning technique was employed to\\ncorrect unnatural tracking motions. Specifically, deep recurrent neural\\nnetworks were used for improving joint positions and velocities of Kinect\\nskeleton, and three methods were proposed to integrate the refined positions\\nand velocities for further enhancement. Moreover, we suggested a novel measure\\nto evaluate naturalness of captured motions. We evaluated the proposed approach\\nby comparison with the ground truth obtained using a commercial optical\\nmaker-based motion capture system.',\n",
       "    'author': [{'name': 'Youngbin Park'},\n",
       "     {'name': 'Sungphill Moon'},\n",
       "     {'name': 'Il Hong Suh'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'submitted to ECCV 2016'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1604.04528v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1604.04528v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1704.03847v1',\n",
       "    'updated': '2017-04-12T17:12:57Z',\n",
       "    'published': '2017-04-12T17:12:57Z',\n",
       "    'title': 'Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark',\n",
       "    'summary': 'This paper presents a new 3D point cloud classification benchmark data set\\nwith over four billion manually labelled points, meant as input for data-hungry\\n(deep) learning methods. We also discuss first submissions to the benchmark\\nthat use deep convolutional neural networks (CNNs) as a work horse, which\\nalready show remarkable performance improvements over state-of-the-art. CNNs\\nhave become the de-facto standard for many tasks in computer vision and machine\\nlearning like semantic segmentation or object detection in images, but have no\\nyet led to a true breakthrough for 3D point cloud labelling tasks due to lack\\nof training data. With the massive data set presented in this paper, we aim at\\nclosing this data gap to help unleash the full potential of deep learning\\nmethods for 3D labelling tasks. Our semantic3D.net data set consists of dense\\npoint clouds acquired with static terrestrial laser scanners. It contains 8\\nsemantic classes and covers a wide range of urban outdoor scenes: churches,\\nstreets, railroad tracks, squares, villages, soccer fields and castles. We\\ndescribe our labelling interface and show that our data set provides more dense\\nand complete point clouds with much higher overall number of labelled points\\ncompared to those already available to the research community. We further\\nprovide baseline method descriptions and comparison between methods submitted\\nto our online system. We hope semantic3D.net will pave the way for deep\\nlearning methods in 3D point cloud labelling to learn richer, more general 3D\\nrepresentations, and first submissions after only a few months indicate that\\nthis might indeed be the case.',\n",
       "    'author': [{'name': 'Timo Hackel'},\n",
       "     {'name': 'Nikolay Savinov'},\n",
       "     {'name': 'Lubor Ladicky'},\n",
       "     {'name': 'Jan D. Wegner'},\n",
       "     {'name': 'Konrad Schindler'},\n",
       "     {'name': 'Marc Pollefeys'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ISPRS Annals. The benchmark website is available at\\n  http://www.semantic3d.net/ . The baseline code is available at\\n  https://github.com/nsavinov/semantic3dnet'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1704.03847v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1704.03847v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1710.08247v1',\n",
       "    'updated': '2017-10-23T13:01:05Z',\n",
       "    'published': '2017-10-23T13:01:05Z',\n",
       "    'title': 'Generic 3D Representation via Pose Estimation and Matching',\n",
       "    'summary': 'Though a large body of computer vision research has investigated developing\\ngeneric semantic representations, efforts towards developing a similar\\nrepresentation for 3D has been limited. In this paper, we learn a generic 3D\\nrepresentation through solving a set of foundational proxy 3D tasks:\\nobject-centric camera pose estimation and wide baseline feature matching. Our\\nmethod is based upon the premise that by providing supervision over a set of\\ncarefully selected foundational tasks, generalization to novel tasks and\\nabstraction capabilities can be achieved. We empirically show that the internal\\nrepresentation of a multi-task ConvNet trained to solve the above core problems\\ngeneralizes to novel 3D tasks (e.g., scene layout estimation, object pose\\nestimation, surface normal estimation) without the need for fine-tuning and\\nshows traits of abstraction abilities (e.g., cross-modality pose estimation).\\nIn the context of the core supervised tasks, we demonstrate our representation\\nachieves state-of-the-art wide baseline feature matching results without\\nrequiring apriori rectification (unlike SIFT and the majority of learned\\nfeatures). We also show 6DOF camera pose estimation given a pair local image\\npatches. The accuracy of both supervised tasks come comparable to humans.\\nFinally, we contribute a large-scale dataset composed of object-centric street\\nview scenes along with point correspondences and camera pose information, and\\nconclude with a discussion on the learned representation and open research\\nquestions.',\n",
       "    'author': [{'name': 'Amir R. Zamir'},\n",
       "     {'name': 'Tilman Wekel'},\n",
       "     {'name': 'Pulkit Argrawal'},\n",
       "     {'name': 'Colin Weil'},\n",
       "     {'name': 'Jitendra Malik'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/978-3-319-46487-9_33'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/978-3-319-46487-9_33',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1710.08247v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1710.08247v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in ECCV16. See the project website\\n  http://3drepresentation.stanford.edu/ and dataset website\\n  https://github.com/amir32002/3D_Street_View'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ECCV 2016 535-553'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.08324v2',\n",
       "    'updated': '2020-02-17T20:53:30Z',\n",
       "    'published': '2019-12-18T00:18:17Z',\n",
       "    'title': 'Analysing Deep Reinforcement Learning Agents Trained with Domain\\n  Randomisation',\n",
       "    'summary': 'Deep reinforcement learning has the potential to train robots to perform\\ncomplex tasks in the real world without requiring accurate models of the robot\\nor its environment. A practical approach is to train agents in simulation, and\\nthen transfer them to the real world. One popular method for achieving\\ntransferability is to use domain randomisation, which involves randomly\\nperturbing various aspects of a simulated environment in order to make trained\\nagents robust to the reality gap. However, less work has gone into\\nunderstanding such agents - which are deployed in the real world - beyond task\\nperformance. In this work we examine such agents, through qualitative and\\nquantitative comparisons between agents trained with and without visual domain\\nrandomisation. We train agents for Fetch and Jaco robots on a visuomotor\\ncontrol task and evaluate how well they generalise using different testing\\nconditions. Finally, we investigate the internals of the trained agents by\\nusing a suite of interpretability techniques. Our results show that the primary\\noutcome of domain randomisation is more robust, entangled representations,\\naccompanied with larger weights with greater spatial structure; moreover, the\\ntypes of changes are heavily influenced by the task setup and presence of\\nadditional proprioceptive inputs. Additionally, we demonstrate that our domain\\nrandomised agents require higher sample complexity, can overfit and more\\nheavily rely on recurrent processing. Furthermore, even with an improved\\nsaliency method introduced in this work, we show that qualitative studies may\\nnot always correspond with quantitative measures, necessitating the combination\\nof inspection tools in order to provide sufficient insights into the behaviour\\nof trained agents.',\n",
       "    'author': [{'name': 'Tianhong Dai'},\n",
       "     {'name': 'Kai Arulkumaran'},\n",
       "     {'name': 'Tamara Gerbert'},\n",
       "     {'name': 'Samyakh Tukra'},\n",
       "     {'name': 'Feryal Behbahani'},\n",
       "     {'name': 'Anil Anthony Bharath'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.neucom.2022.04.005'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.neucom.2022.04.005',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1912.08324v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.08324v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.11121v1',\n",
       "    'updated': '2019-12-23T21:45:50Z',\n",
       "    'published': '2019-12-23T21:45:50Z',\n",
       "    'title': 'Learning to Navigate Using Mid-Level Visual Priors',\n",
       "    'summary': 'How much does having visual priors about the world (e.g. the fact that the\\nworld is 3D) assist in learning to perform downstream motor tasks (e.g.\\nnavigating a complex environment)? What are the consequences of not utilizing\\nsuch visual priors in learning? We study these questions by integrating a\\ngeneric perceptual skill set (a distance estimator, an edge detector, etc.)\\nwithin a reinforcement learning framework (see Fig. 1). This skill set\\n(\"mid-level vision\") provides the policy with a more processed state of the\\nworld compared to raw images.\\n  Our large-scale study demonstrates that using mid-level vision results in\\npolicies that learn faster, generalize better, and achieve higher final\\nperformance, when compared to learning from scratch and/or using\\nstate-of-the-art visual and non-visual representation learning methods. We show\\nthat conventional computer vision objectives are particularly effective in this\\nregard and can be conveniently integrated into reinforcement learning\\nframeworks. Finally, we found that no single visual representation was\\nuniversally useful for all downstream tasks, hence we computationally derive a\\ntask-agnostic set of representations optimized to support arbitrary downstream\\ntasks.',\n",
       "    'author': [{'name': 'Alexander Sax'},\n",
       "     {'name': 'Jeffrey O. Zhang'},\n",
       "     {'name': 'Bradley Emi'},\n",
       "     {'name': 'Amir Zamir'},\n",
       "     {'name': 'Silvio Savarese'},\n",
       "     {'name': 'Leonidas Guibas'},\n",
       "     {'name': 'Jitendra Malik'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In Conference on Robot Learning, 2019. See project website and demos\\n  at http://perceptual.actor/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1912.11121v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.11121v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.13503v4',\n",
       "    'updated': '2020-07-31T00:44:06Z',\n",
       "    'published': '2019-12-31T18:52:32Z',\n",
       "    'title': 'Side-Tuning: A Baseline for Network Adaptation via Additive Side\\n  Networks',\n",
       "    'summary': 'When training a neural network for a desired task, one may prefer to adapt a\\npre-trained network rather than starting from randomly initialized weights.\\nAdaptation can be useful in cases when training data is scarce, when a single\\nlearner needs to perform multiple tasks, or when one wishes to encode priors in\\nthe network. The most commonly employed approaches for network adaptation are\\nfine-tuning and using the pre-trained network as a fixed feature extractor,\\namong others.\\n  In this paper, we propose a straightforward alternative: side-tuning.\\nSide-tuning adapts a pre-trained network by training a lightweight \"side\"\\nnetwork that is fused with the (unchanged) pre-trained network via summation.\\nThis simple method works as well as or better than existing solutions and it\\nresolves some of the basic issues with fine-tuning, fixed features, and other\\ncommon approaches. In particular, side-tuning is less prone to overfitting, is\\nasymptotically consistent, and does not suffer from catastrophic forgetting in\\nincremental learning. We demonstrate the performance of side-tuning under a\\ndiverse set of scenarios, including incremental learning (iCIFAR, iTaskonomy),\\nreinforcement learning, imitation learning (visual navigation in Habitat), NLP\\nquestion-answering (SQuAD v2), and single-task transfer learning (Taskonomy),\\nwith consistently promising results.',\n",
       "    'author': [{'name': 'Jeffrey O Zhang'},\n",
       "     {'name': 'Alexander Sax'},\n",
       "     {'name': 'Amir Zamir'},\n",
       "     {'name': 'Leonidas Guibas'},\n",
       "     {'name': 'Jitendra Malik'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In ECCV 2020 (Spotlight). For more, see project website and code at\\n  http://sidetuning.berkeley.edu'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1912.13503v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.13503v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.04767v4',\n",
       "    'updated': '2021-08-05T10:31:35Z',\n",
       "    'published': '2020-10-09T19:03:15Z',\n",
       "    'title': 'Robust Behavioral Cloning for Autonomous Vehicles using End-to-End\\n  Imitation Learning',\n",
       "    'summary': 'In this work, we present a lightweight pipeline for robust behavioral cloning\\nof a human driver using end-to-end imitation learning. The proposed pipeline\\nwas employed to train and deploy three distinct driving behavior models onto a\\nsimulated vehicle. The training phase comprised of data collection, balancing,\\naugmentation, preprocessing and training a neural network, following which, the\\ntrained model was deployed onto the ego vehicle to predict steering commands\\nbased on the feed from an onboard camera. A novel coupled control law was\\nformulated to generate longitudinal control commands on-the-go based on the\\npredicted steering angle and other parameters such as actual speed of the ego\\nvehicle and the prescribed constraints for speed and steering. We analyzed\\ncomputational efficiency of the pipeline and evaluated robustness of the\\ntrained models through exhaustive experimentation during the deployment phase.\\nWe also compared our approach against state-of-the-art implementation in order\\nto comment on its validity.',\n",
       "    'author': [{'name': 'Tanmay Vilas Samak'},\n",
       "     {'name': 'Chinmay Vilas Samak'},\n",
       "     {'name': 'Sivanathan Kandhasamy'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.4271/12-04-03-0023'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.4271/12-04-03-0023',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2010.04767v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.04767v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at SAE International Journal of Connected and Automated\\n  Vehicles'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'SAE International Journal of Connected and Automated Vehicles,\\n  vol. 4, no. 3, pp. 279-295, 2021'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.14633v1',\n",
       "    'updated': '2021-03-26T17:51:43Z',\n",
       "    'published': '2021-03-26T17:51:43Z',\n",
       "    'title': 'Visionary: Vision architecture discovery for robot learning',\n",
       "    'summary': 'We propose a vision-based architecture search algorithm for robot\\nmanipulation learning, which discovers interactions between low dimension\\naction inputs and high dimensional visual inputs. Our approach automatically\\ndesigns architectures while training on the task - discovering novel ways of\\ncombining and attending image feature representations with actions as well as\\nfeatures from previous layers. The obtained new architectures demonstrate\\nbetter task success rates, in some cases with a large margin, compared to a\\nrecent high performing baseline. Our real robot experiments also confirm that\\nit improves grasping performance by 6%. This is the first approach to\\ndemonstrate a successful neural architecture search and attention connectivity\\nsearch for a real-robot task.',\n",
       "    'author': [{'name': 'Iretiayo Akinola'},\n",
       "     {'name': 'Anelia Angelova'},\n",
       "     {'name': 'Yao Lu'},\n",
       "     {'name': 'Yevgen Chebotar'},\n",
       "     {'name': 'Dmitry Kalashnikov'},\n",
       "     {'name': 'Jacob Varley'},\n",
       "     {'name': 'Julian Ibarz'},\n",
       "     {'name': 'Michael S. Ryoo'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.14633v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.14633v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.03722v1',\n",
       "    'updated': '2022-04-07T20:21:30Z',\n",
       "    'published': '2022-04-07T20:21:30Z',\n",
       "    'title': 'Automated Design of Salient Object Detection Algorithms with Brain\\n  Programming',\n",
       "    'summary': \"Despite recent improvements in computer vision, artificial visual systems'\\ndesign is still daunting since an explanation of visual computing algorithms\\nremains elusive. Salient object detection is one problem that is still open due\\nto the difficulty of understanding the brain's inner workings. Progress on this\\nresearch area follows the traditional path of hand-made designs using\\nneuroscience knowledge. In recent years two different approaches based on\\ngenetic programming appear to enhance their technique. One follows the idea of\\ncombining previous hand-made methods through genetic programming and fuzzy\\nlogic. The other approach consists of improving the inner computational\\nstructures of basic hand-made models through artificial evolution. This\\nresearch work proposes expanding the artificial dorsal stream using a recent\\nproposal to solve salient object detection problems. This approach uses the\\nbenefits of the two main aspects of this research area: fixation prediction and\\ndetection of salient objects. We decided to apply the fusion of visual saliency\\nand image segmentation algorithms as a template. The proposed methodology\\ndiscovers several critical structures in the template through artificial\\nevolution. We present results on a benchmark designed by experts with\\noutstanding results in comparison with the state-of-the-art.\",\n",
       "    'author': [{'name': 'Gustavo Olague'},\n",
       "     {'name': 'Jose Armando Menendez-Clavijo'},\n",
       "     {'name': 'Matthieu Olague'},\n",
       "     {'name': 'Arturo Ocampo'},\n",
       "     {'name': 'Gerardo Ibarra-Vazquez'},\n",
       "     {'name': 'Rocio Ochoa'},\n",
       "     {'name': 'Roberto Pineda'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '35 pages, 5 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.03722v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.03722v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.14612v2',\n",
       "    'updated': '2022-11-02T08:26:18Z',\n",
       "    'published': '2022-10-26T10:39:59Z',\n",
       "    'title': 'Analyzing Deep Learning Representations of Point Clouds for Real-Time\\n  In-Vehicle LiDAR Perception',\n",
       "    'summary': \"LiDAR sensors are an integral part of modern autonomous vehicles as they\\nprovide an accurate, high-resolution 3D representation of the vehicle's\\nsurroundings. However, it is computationally difficult to make use of the\\never-increasing amounts of data from multiple high-resolution LiDAR sensors. As\\nframe-rates, point cloud sizes and sensor resolutions increase, real-time\\nprocessing of these point clouds must still extract semantics from this\\nincreasingly precise picture of the vehicle's environment. One deciding factor\\nof the run-time performance and accuracy of deep neural networks operating on\\nthese point clouds is the underlying data representation and the way it is\\ncomputed. In this work, we examine the relationship between the computational\\nrepresentations used in neural networks and their performance characteristics.\\nTo this end, we propose a novel computational taxonomy of LiDAR point cloud\\nrepresentations used in modern deep neural networks for 3D point cloud\\nprocessing. Using this taxonomy, we perform a structured analysis of different\\nfamilies of approaches. Thereby, we uncover common advantages and limitations\\nin terms of computational efficiency, memory requirements, and representational\\ncapacity as measured by semantic segmentation performance. Finally, we provide\\nsome insights and guidance for future developments in neural point cloud\\nprocessing methods.\",\n",
       "    'author': [{'name': 'Marc Uecker'},\n",
       "     {'name': 'Tobias Fleck'},\n",
       "     {'name': 'Marcel Pflugfelder'},\n",
       "     {'name': 'J. Marius Zöllner'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at the NeurIPS 2022 Workshop on Machine Learning for\\n  Autonomous Driving (ML4AD)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.14612v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.14612v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T07', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.10; I.2.9; I.5.1',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2007.04979v1',\n",
       "    'updated': '2020-07-09T17:59:57Z',\n",
       "    'published': '2020-07-09T17:59:57Z',\n",
       "    'title': 'A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied\\n  Tasks',\n",
       "    'summary': \"Autonomous agents must learn to collaborate. It is not scalable to develop a\\nnew centralized agent every time a task's difficulty outpaces a single agent's\\nabilities. While multi-agent collaboration research has flourished in\\ngridworld-like environments, relatively little work has considered visually\\nrich domains. Addressing this, we introduce the novel task FurnMove in which\\nagents work together to move a piece of furniture through a living room to a\\ngoal. Unlike existing tasks, FurnMove requires agents to coordinate at every\\ntimestep. We identify two challenges when training agents to complete FurnMove:\\nexisting decentralized action sampling procedures do not permit expressive\\njoint action policies and, in tasks requiring close coordination, the number of\\nfailed actions dominates successful actions. To confront these challenges we\\nintroduce SYNC-policies (synchronize your actions coherently) and CORDIAL\\n(coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58%\\ncompletion rate on FurnMove, an impressive absolute gain of 25 percentage\\npoints over competitive decentralized baselines. Our dataset, code, and\\npretrained models are available at https://unnat.github.io/cordial-sync .\",\n",
       "    'author': [{'name': 'Unnat Jain'},\n",
       "     {'name': 'Luca Weihs'},\n",
       "     {'name': 'Eric Kolve'},\n",
       "     {'name': 'Ali Farhadi'},\n",
       "     {'name': 'Svetlana Lazebnik'},\n",
       "     {'name': 'Aniruddha Kembhavi'},\n",
       "     {'name': 'Alexander Schwing'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ECCV 2020 (spotlight); Project page:\\n  https://unnat.github.io/cordial-sync'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2007.04979v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2007.04979v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.00752v1',\n",
       "    'updated': '2021-08-02T09:56:10Z',\n",
       "    'published': '2021-08-02T09:56:10Z',\n",
       "    'title': 'Flip Learning: Erase to Segment',\n",
       "    'summary': 'Nodule segmentation from breast ultrasound images is challenging yet\\nessential for the diagnosis. Weakly-supervised segmentation (WSS) can help\\nreduce time-consuming and cumbersome manual annotation. Unlike existing\\nweakly-supervised approaches, in this study, we propose a novel and general WSS\\nframework called Flip Learning, which only needs the box annotation.\\nSpecifically, the target in the label box will be erased gradually to flip the\\nclassification tag, and the erased region will be considered as the\\nsegmentation result finally. Our contribution is three-fold. First, our\\nproposed approach erases on superpixel level using a Multi-agent Reinforcement\\nLearning framework to exploit the prior boundary knowledge and accelerate the\\nlearning process. Second, we design two rewards: classification score and\\nintensity distribution reward, to avoid under- and over-segmentation,\\nrespectively. Third, we adopt a coarse-to-fine learning strategy to reduce the\\nresidual errors and improve the segmentation performance. Extensively validated\\non a large dataset, our proposed approach achieves competitive performance and\\nshows great potential to narrow the gap between fully-supervised and\\nweakly-supervised learning.',\n",
       "    'author': [{'name': 'Yuhao Huang'},\n",
       "     {'name': 'Xin Yang'},\n",
       "     {'name': 'Yuxin Zou'},\n",
       "     {'name': 'Chaoyu Chen'},\n",
       "     {'name': 'Jian Wang'},\n",
       "     {'name': 'Haoran Dou'},\n",
       "     {'name': 'Nishant Ravikumar'},\n",
       "     {'name': 'Alejandro F Frangi'},\n",
       "     {'name': 'Jianqiao Zhou'},\n",
       "     {'name': 'Dong Ni'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by MICCAI 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.00752v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.00752v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.05769v1',\n",
       "    'updated': '2021-10-12T06:56:11Z',\n",
       "    'published': '2021-10-12T06:56:11Z',\n",
       "    'title': 'Interpretation of Emergent Communication in Heterogeneous Collaborative\\n  Embodied Agents',\n",
       "    'summary': 'Communication between embodied AI agents has received increasing attention in\\nrecent years. Despite its use, it is still unclear whether the learned\\ncommunication is interpretable and grounded in perception. To study the\\ngrounding of emergent forms of communication, we first introduce the\\ncollaborative multi-object navigation task CoMON. In this task, an oracle agent\\nhas detailed environment information in the form of a map. It communicates with\\na navigator agent that perceives the environment visually and is tasked to find\\na sequence of goals. To succeed at the task, effective communication is\\nessential. CoMON hence serves as a basis to study different communication\\nmechanisms between heterogeneous agents, that is, agents with different\\ncapabilities and roles. We study two common communication mechanisms and\\nanalyze their communication patterns through an egocentric and spatial lens. We\\nshow that the emergent communication can be grounded to the agent observations\\nand the spatial structure of the 3D environment. Video summary:\\nhttps://youtu.be/kLv2rxO9t0g',\n",
       "    'author': [{'name': 'Shivansh Patel'},\n",
       "     {'name': 'Saim Wani'},\n",
       "     {'name': 'Unnat Jain'},\n",
       "     {'name': 'Alexander Schwing'},\n",
       "     {'name': 'Svetlana Lazebnik'},\n",
       "     {'name': 'Manolis Savva'},\n",
       "     {'name': 'Angel X. Chang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page: https://shivanshpatel35.github.io/comon/ ; the first\\n  three authors contributed equally'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.05769v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.05769v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.07891v1',\n",
       "    'updated': '2022-12-15T15:20:58Z',\n",
       "    'published': '2022-12-15T15:20:58Z',\n",
       "    'title': 'Emergent Behaviors in Multi-Agent Target Acquisition',\n",
       "    'summary': \"Only limited studies and superficial evaluations are available on agents'\\nbehaviors and roles within a Multi-Agent System (MAS). We simulate a MAS using\\nReinforcement Learning (RL) in a pursuit-evasion (a.k.a predator-prey pursuit)\\ngame, which shares task goals with target acquisition, and we create different\\nadversarial scenarios by replacing RL-trained pursuers' policies with two\\ndistinct (non-RL) analytical strategies. Using heatmaps of agents' positions\\n(state-space variable) over time, we are able to categorize an RL-trained\\nevader's behaviors. The novelty of our approach entails the creation of an\\ninfluential feature set that reveals underlying data regularities, which allow\\nus to classify an agent's behavior. This classification may aid in catching the\\n(enemy) targets by enabling us to identify and predict their behaviors, and\\nwhen extended to pursuers, this approach towards identifying teammates'\\nbehavior may allow agents to coordinate more effectively.\",\n",
       "    'author': [{'name': 'Piyush K. Sharma'},\n",
       "     {'name': 'Erin Zaroukian'},\n",
       "     {'name': 'Derrik E. Asher'},\n",
       "     {'name': 'Bryson Howell'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1117/12.2618646'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1117/12.2618646',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2212.07891v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.07891v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This article appeared in the news at:\\n  https://www.army.mil/article/258408/u_s_army_scientists_invent_a_method_to_characterize_ai_behavior'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in:Proceedings Volume 12113, Artificial Intelligence and\\n  Machine Learning for Multi-Domain Operations Applications IV; 1211314 (6 June\\n  2022), SPIE Defense + Commercial Sensing, 2022, Orlando, Florida, United\\n  States'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.00931v2',\n",
       "    'updated': '2021-10-13T17:26:12Z',\n",
       "    'published': '2021-04-14T17:59:57Z',\n",
       "    'title': 'GridToPix: Training Embodied Agents with Minimal Supervision',\n",
       "    'summary': \"While deep reinforcement learning (RL) promises freedom from hand-labeled\\ndata, great successes, especially for Embodied AI, require significant work to\\ncreate supervision via carefully shaped rewards. Indeed, without shaped\\nrewards, i.e., with only terminal rewards, present-day Embodied AI results\\ndegrade significantly across Embodied AI problems from single-agent\\nHabitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent\\nAI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent\\nGoogle Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1).\\nAs training from shaped rewards doesn't scale to more realistic tasks, the\\ncommunity needs to improve the success of training with terminal rewards. For\\nthis we propose GridToPix: 1) train agents with terminal rewards in gridworlds\\nthat generically mirror Embodied AI environments, i.e., they are independent of\\nthe task; 2) distill the learned policy into agents that reside in complex\\nvisual worlds. Despite learning from only terminal rewards with identical\\nmodels and RL algorithms, GridToPix significantly improves results across\\ntasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture\\nMoving (success improves from 1% to 25%) to football gameplay (game score\\nimproves from 0.1 to 0.6). GridToPix even helps to improve the results of\\nshaped reward training.\",\n",
       "    'author': [{'name': 'Unnat Jain'},\n",
       "     {'name': 'Iou-Jen Liu'},\n",
       "     {'name': 'Svetlana Lazebnik'},\n",
       "     {'name': 'Aniruddha Kembhavi'},\n",
       "     {'name': 'Luca Weihs'},\n",
       "     {'name': 'Alexander Schwing'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page: https://unnat.github.io/gridtopix/ ; last two authors\\n  contributed equally'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.00931v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.00931v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.02067v2',\n",
       "    'updated': '2021-11-09T16:35:33Z',\n",
       "    'published': '2021-06-03T18:17:55Z',\n",
       "    'title': 'Learning to Draw: Emergent Communication through Sketching',\n",
       "    'summary': 'Evidence that visual communication preceded written language and provided a\\nbasis for it goes back to prehistory, in forms such as cave and rock paintings\\ndepicting traces of our distant ancestors. Emergent communication research has\\nsought to explore how agents can learn to communicate in order to\\ncollaboratively solve tasks. Existing research has focused on language, with a\\nlearned communication channel transmitting sequences of discrete tokens between\\nthe agents. In this work, we explore a visual communication channel between\\nagents that are allowed to draw with simple strokes. Our agents are\\nparameterised by deep neural networks, and the drawing procedure is\\ndifferentiable, allowing for end-to-end training. In the framework of a\\nreferential communication game, we demonstrate that agents can not only\\nsuccessfully learn to communicate by drawing, but with appropriate inductive\\nbiases, can do so in a fashion that humans can interpret. We hope to encourage\\nfuture research to consider visual communication as a more flexible and\\ndirectly interpretable alternative of training collaborative agents.',\n",
       "    'author': [{'name': 'Daniela Mihai'}, {'name': 'Jonathon Hare'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.02067v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.02067v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.10553v1',\n",
       "    'updated': '2022-07-21T15:51:30Z',\n",
       "    'published': '2022-07-21T15:51:30Z',\n",
       "    'title': 'The MABe22 Benchmarks for Representation Learning of Multi-Agent\\n  Behavior',\n",
       "    'summary': 'Real-world behavior is often shaped by complex interactions between multiple\\nagents. To scalably study multi-agent behavior, advances in unsupervised and\\nself-supervised learning have enabled a variety of different behavioral\\nrepresentations to be learned from trajectory data. To date, there does not\\nexist a unified set of benchmarks that can enable comparing methods\\nquantitatively and systematically across a broad set of behavior analysis\\nsettings. We aim to address this by introducing a large-scale, multi-agent\\ntrajectory dataset from real-world behavioral neuroscience experiments that\\ncovers a range of behavior analysis tasks. Our dataset consists of trajectory\\ndata from common model organisms, with 9.6 million frames of mouse data and 4.4\\nmillion frames of fly data, in a variety of experimental settings, such as\\ndifferent strains, lengths of interaction, and optogenetic stimulation. A\\nsubset of the frames also consist of expert-annotated behavior labels.\\nImprovements on our dataset corresponds to behavioral representations that work\\nacross multiple organisms and is able to capture differences for common\\nbehavior analysis tasks.',\n",
       "    'author': [{'name': 'Jennifer J. Sun'},\n",
       "     {'name': 'Andrew Ulmer'},\n",
       "     {'name': 'Dipam Chakraborty'},\n",
       "     {'name': 'Brian Geuther'},\n",
       "     {'name': 'Edward Hayes'},\n",
       "     {'name': 'Heng Jia'},\n",
       "     {'name': 'Vivek Kumar'},\n",
       "     {'name': 'Zachary Partridge'},\n",
       "     {'name': 'Alice Robie'},\n",
       "     {'name': 'Catherine E. Schretter'},\n",
       "     {'name': 'Chao Sun'},\n",
       "     {'name': 'Keith Sheppard'},\n",
       "     {'name': 'Param Uttarwar'},\n",
       "     {'name': 'Pietro Perona'},\n",
       "     {'name': 'Yisong Yue'},\n",
       "     {'name': 'Kristin Branson'},\n",
       "     {'name': 'Ann Kennedy'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project website:\\n  https://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.10553v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.10553v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.01447v1',\n",
       "    'updated': '2020-11-03T03:27:18Z',\n",
       "    'published': '2020-11-03T03:27:18Z',\n",
       "    'title': 'A Two-Stage Approach to Device-Robust Acoustic Scene Classification',\n",
       "    'summary': 'To improve device robustness, a highly desirable key feature of a competitive\\ndata-driven acoustic scene classification (ASC) system, a novel two-stage\\nsystem based on fully convolutional neural networks (CNNs) is proposed. Our\\ntwo-stage system leverages on an ad-hoc score combination based on two CNN\\nclassifiers: (i) the first CNN classifies acoustic inputs into one of three\\nbroad classes, and (ii) the second CNN classifies the same inputs into one of\\nten finer-grained classes. Three different CNN architectures are explored to\\nimplement the two-stage classifiers, and a frequency sub-sampling scheme is\\ninvestigated. Moreover, novel data augmentation schemes for ASC are also\\ninvestigated. Evaluated on DCASE 2020 Task 1a, our results show that the\\nproposed ASC system attains a state-of-the-art accuracy on the development set,\\nwhere our best system, a two-stage fusion of CNN ensembles, delivers a 81.9%\\naverage accuracy among multi-device test data, and it obtains a significant\\nimprovement on unseen devices. Finally, neural saliency analysis with class\\nactivation mapping (CAM) gives new insights on the patterns learnt by our\\nmodels.',\n",
       "    'author': [{'name': 'Hu Hu'},\n",
       "     {'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Xianjun Xia'},\n",
       "     {'name': 'Xue Bai'},\n",
       "     {'name': 'Xin Tang'},\n",
       "     {'name': 'Yajian Wang'},\n",
       "     {'name': 'Shutong Niu'},\n",
       "     {'name': 'Li Chai'},\n",
       "     {'name': 'Juanjuan Li'},\n",
       "     {'name': 'Hongning Zhu'},\n",
       "     {'name': 'Feng Bao'},\n",
       "     {'name': 'Yuanjun Zhao'},\n",
       "     {'name': 'Sabato Marco Siniscalchi'},\n",
       "     {'name': 'Yannan Wang'},\n",
       "     {'name': 'Jun Du'},\n",
       "     {'name': 'Chin-Hui Lee'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICASSP39728.2021.9414835'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICASSP39728.2021.9414835',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2011.01447v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.01447v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICASSP 2021. Code available:\\n  https://github.com/MihawkHu/DCASE2020_task1'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICASSP 2021-2021 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.08532v1',\n",
       "    'updated': '2022-02-17T09:17:58Z',\n",
       "    'published': '2022-02-17T09:17:58Z',\n",
       "    'title': 'Mitigating Closed-model Adversarial Examples with Bayesian Neural\\n  Modeling for Enhanced End-to-End Speech Recognition',\n",
       "    'summary': 'In this work, we aim to enhance the system robustness of end-to-end automatic\\nspeech recognition (ASR) against adversarially-noisy speech examples. We focus\\non a rigorous and empirical \"closed-model adversarial robustness\" setting\\n(e.g., on-device or cloud applications). The adversarial noise is only\\ngenerated by closed-model optimization (e.g., evolutionary and zeroth-order\\nestimation) without accessing gradient information of a targeted ASR model\\ndirectly. We propose an advanced Bayesian neural network (BNN) based\\nadversarial detector, which could model latent distributions against adaptive\\nadversarial perturbation with divergence measurement. We further simulate\\ndeployment scenarios of RNN Transducer, Conformer, and wav2vec-2.0 based ASR\\nsystems with the proposed adversarial detection system. Leveraging the proposed\\nBNN based detection system, we improve detection rate by +2.77 to +5.42%\\n(relative +3.03 to +6.26%) and reduce the word error rate by 5.02 to 7.47% on\\nLibriSpeech datasets compared to the current model enhancement methods against\\nthe adversarial speech examples.',\n",
       "    'author': [{'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Zeeshan Ahmed'},\n",
       "     {'name': 'Yile Gu'},\n",
       "     {'name': 'Joseph Szurley'},\n",
       "     {'name': 'Roger Ren'},\n",
       "     {'name': 'Linda Liu'},\n",
       "     {'name': 'Andreas Stolcke'},\n",
       "     {'name': 'Ivan Bulyko'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICASSP 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.08532v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.08532v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.AS',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.AS',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.01317v1',\n",
       "    'updated': '2022-11-02T17:38:33Z',\n",
       "    'published': '2022-11-02T17:38:33Z',\n",
       "    'title': 'Low-Resource Music Genre Classification with Advanced Neural Model\\n  Reprogramming',\n",
       "    'summary': 'Transfer learning (TL) approaches have shown promising results when handling\\ntasks with limited training data. However, considerable memory and\\ncomputational resources are often required for fine-tuning pre-trained neural\\nnetworks with target domain data. In this work, we introduce a novel method for\\nleveraging pre-trained models for low-resource (music) classification based on\\nthe concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a\\npre-trained model from a source domain to a target domain by modifying the\\ninput of a frozen pre-trained model. In addition to the known,\\ninput-independent, reprogramming method, we propose an advanced reprogramming\\nparadigm: Input-dependent NMR, to increase adaptability to complex input data\\nsuch as musical audio. Experimental results suggest that a neural model\\npre-trained on large-scale datasets can successfully perform music genre\\nclassification by using this reprogramming method. The two proposed\\nInput-dependent NMR TL methods outperform fine-tuning-based TL methods on a\\nsmall genre classification dataset.',\n",
       "    'author': [{'name': 'Yun-Ning Hung'},\n",
       "     {'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Pin-Yu Chen'},\n",
       "     {'name': 'Alexander Lerch'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICASSP 2023. Some experimental results were reduced due\\n  to the space limit. The implementation will be available at\\n  https://github.com/biboamy/music-repro'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.01317v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.01317v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.10942v2',\n",
       "    'updated': '2020-02-10T09:36:23Z',\n",
       "    'published': '2019-10-24T06:54:36Z',\n",
       "    'title': 'A Recurrent Variational Autoencoder for Speech Enhancement',\n",
       "    'summary': 'This paper presents a generative approach to speech enhancement based on a\\nrecurrent variational autoencoder (RVAE). The deep generative speech model is\\ntrained using clean speech signals only, and it is combined with a nonnegative\\nmatrix factorization noise model for speech enhancement. We propose a\\nvariational expectation-maximization algorithm where the encoder of the RVAE is\\nfine-tuned at test time, to approximate the distribution of the latent\\nvariables given the noisy speech observations. Compared with previous\\napproaches based on feed-forward fully-connected architectures, the proposed\\nrecurrent deep generative speech model induces a posterior temporal dynamic\\nover the latent variables, which is shown to improve the speech enhancement\\nresults.',\n",
       "    'author': [{'name': 'Simon Leglaive',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'IETR'}},\n",
       "     {'name': 'Xavier Alameda-Pineda',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'PERCEPTION'}},\n",
       "     {'name': 'Laurent Girin',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'GIPSA-CRISSP, PERCEPTION'}},\n",
       "     {'name': 'Radu Horaud',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'PERCEPTION'}}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICASSP 2020 - 2020 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), May 2020, Barcelona, Spain'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1910.10942v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.10942v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.01271v2',\n",
       "    'updated': '2021-06-15T06:09:42Z',\n",
       "    'published': '2021-04-02T23:10:57Z',\n",
       "    'title': 'PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation\\n  of Teacher Ensembles for Spoken Command Classification',\n",
       "    'summary': 'We propose using an adversarial autoencoder (AAE) to replace generative\\nadversarial network (GAN) in the private aggregation of teacher ensembles\\n(PATE), a solution for ensuring differential privacy in speech applications.\\nThe AAE architecture allows us to obtain good synthetic speech leveraging upon\\na discriminative training of latent vectors. Such synthetic speech is used to\\nbuild a privacy-preserving classifier when non-sensitive data is not\\nsufficiently available in the public domain. This classifier follows the PATE\\nscheme that uses an ensemble of noisy outputs to label the synthetic samples\\nand guarantee $\\\\varepsilon$-differential privacy (DP) on its derived\\nclassifiers. Our proposed framework thus consists of an AAE-based generator and\\na PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands\\nDataset Version II, the proposed PATE-AAE improves the average classification\\naccuracy by +$2.11\\\\%$ and +$6.60\\\\%$, respectively, when compared with\\nalternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while\\nmaintaining a strong level of privacy target at $\\\\varepsilon$=0.01 with a fixed\\n$\\\\delta$=10$^{-5}$.',\n",
       "    'author': [{'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Sabato Marco Siniscalchi'},\n",
       "     {'name': 'Chin-Hui Lee'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.21437/Interspeech.2021-640'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.21437/Interspeech.2021-640',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2104.01271v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.01271v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to Interspeech 2021'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proc. Interspeech 2021'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.00687v1',\n",
       "    'updated': '2021-06-01T18:00:02Z',\n",
       "    'published': '2021-06-01T18:00:02Z',\n",
       "    'title': 'Online Detection of Vibration Anomalies Using Balanced Spiking Neural\\n  Networks',\n",
       "    'summary': 'Vibration patterns yield valuable information about the health state of a\\nrunning machine, which is commonly exploited in predictive maintenance tasks\\nfor large industrial systems. However, the overhead, in terms of size,\\ncomplexity and power budget, required by classical methods to exploit this\\ninformation is often prohibitive for smaller-scale applications such as\\nautonomous cars, drones or robotics. Here we propose a neuromorphic approach to\\nperform vibration analysis using spiking neural networks that can be applied to\\na wide range of scenarios. We present a spike-based end-to-end pipeline able to\\ndetect system anomalies from vibration data, using building blocks that are\\ncompatible with analog-digital neuromorphic circuits. This pipeline operates in\\nan online unsupervised fashion, and relies on a cochlea model, on feedback\\nadaptation and on a balanced spiking neural network. We show that the proposed\\nmethod achieves state-of-the-art performance or better against two publicly\\navailable data sets. Further, we demonstrate a working proof-of-concept\\nimplemented on an asynchronous neuromorphic processor device. This work\\nrepresents a significant step towards the design and implementation of\\nautonomous low-power edge-computing devices for online vibration monitoring.',\n",
       "    'author': [{'name': 'Nik Dennler'},\n",
       "     {'name': 'Germain Haessig'},\n",
       "     {'name': 'Matteo Cartiglia'},\n",
       "     {'name': 'Giacomo Indiveri'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This work is presented at the 2021 IEEE AICAS'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.00687v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.00687v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.09296v3',\n",
       "    'updated': '2022-01-14T16:43:19Z',\n",
       "    'published': '2021-06-17T07:59:15Z',\n",
       "    'title': 'Voice2Series: Reprogramming Acoustic Models for Time Series\\n  Classification',\n",
       "    'summary': 'Learning to classify time series with limited data is a practical yet\\nchallenging problem. Current methods are primarily based on hand-designed\\nfeature extraction rules or domain-specific data augmentation. Motivated by the\\nadvances in deep speech processing models and the fact that voice data are\\nunivariate temporal signals, in this paper, we propose Voice2Series (V2S), a\\nnovel end-to-end approach that reprograms acoustic models for time series\\nclassification, through input transformation learning and output label mapping.\\nLeveraging the representation learning power of a large-scale pre-trained\\nspeech processing model, on 30 different time series tasks we show that V2S\\nperforms competitive results on 19 time series classification tasks. We further\\nprovide a theoretical justification of V2S by proving its population risk is\\nupper bounded by the source risk and a Wasserstein distance accounting for\\nfeature alignment via reprogramming. Our results offer new and effective means\\nto time series classification.',\n",
       "    'author': [{'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Yun-Yun Tsai'},\n",
       "     {'name': 'Pin-Yu Chen'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Updated version with a correction. The full draft was submitted in\\n  Jan 2021. The Voice2Series project initially was launched in Sep 2020.\\n  Accepted to ICML 2021, 16 Pages'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the 38th International Conference on Machine\\n  Learning 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.09296v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.09296v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.03894v3',\n",
       "    'updated': '2022-12-20T12:44:24Z',\n",
       "    'published': '2021-10-08T05:07:35Z',\n",
       "    'title': 'Neural Model Reprogramming with Similarity Based Mapping for\\n  Low-Resource Spoken Command Classification',\n",
       "    'summary': 'In this study, we propose a novel adversarial reprogramming (AR) approach for\\nlow-resource spoken command recognition (SCR), and build an AR-SCR system. The\\nAR procedure aims to modify the acoustic signals (from the target domain) to\\nrepurpose a pretrained SCR model (from the source domain). To solve the label\\nmismatches between source and target domains, and further improve the stability\\nof AR, we propose a novel similarity-based label mapping technique to align\\nclasses. In addition, the transfer learning (TL) technique is combined with the\\noriginal AR process to improve the model adaptation capability. We evaluate the\\nproposed AR-SCR system on three low-resource SCR datasets, including Arabic,\\nLithuanian, and dysarthric Mandarin speech. Experimental results show that with\\na pretrained AM trained on a large-scale English dataset, the proposed AR-SCR\\nsystem outperforms the current state-of-the-art results on Arabic and\\nLithuanian speech commands datasets, with only a limited amount of training\\ndata.',\n",
       "    'author': [{'name': 'Hao Yen'},\n",
       "     {'name': 'Pin-Jui Ku'},\n",
       "     {'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Hu Hu'},\n",
       "     {'name': 'Sabato Marco Siniscalchi'},\n",
       "     {'name': 'Pin-Yu Chen'},\n",
       "     {'name': 'Yu Tsao'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICASSP 2023. The draft has been updated on its new\\n  reprogramming findings with data augmentation results (8.7% to 10.9%\\n  relatively improvements)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.03894v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.03894v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.AS',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.AS',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.08598v2',\n",
       "    'updated': '2022-02-20T18:32:04Z',\n",
       "    'published': '2021-10-16T15:54:01Z',\n",
       "    'title': 'A Variational Bayesian Approach to Learning Latent Variables for\\n  Acoustic Knowledge Transfer',\n",
       "    'summary': 'We propose a variational Bayesian (VB) approach to learning distributions of\\nlatent variables in deep neural network (DNN) models for cross-domain knowledge\\ntransfer, to address acoustic mismatches between training and testing\\nconditions. Instead of carrying out point estimation in conventional maximum a\\nposteriori estimation with a risk of having a curse of dimensionality in\\nestimating a huge number of model parameters, we focus our attention on\\nestimating a manageable number of latent variables of DNNs via a VB inference\\nframework. To accomplish model transfer, knowledge learnt from a source domain\\nis encoded in prior distributions of latent variables and optimally combined,\\nin a Bayesian sense, with a small set of adaptation data from a target domain\\nto approximate the corresponding posterior distributions. Experimental results\\non device adaptation in acoustic scene classification show that our proposed VB\\napproach can obtain good improvements on target devices, and consistently\\noutperforms 13 state-of-the-art knowledge transfer algorithms.',\n",
       "    'author': [{'name': 'Hu Hu'},\n",
       "     {'name': 'Sabato Marco Siniscalchi'},\n",
       "     {'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Chin-Hui Lee'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICASSP 2022. Code is available at\\n  https://github.com/MihawkHu/ASC_Knowledge_Transfer'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.08598v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.08598v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.AS',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.AS',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.01189v1',\n",
       "    'updated': '2022-11-02T15:03:50Z',\n",
       "    'published': '2022-11-02T15:03:50Z',\n",
       "    'title': 'Inference and Denoise: Causal Inference-based Neural Speech Enhancement',\n",
       "    'summary': 'This study addresses the speech enhancement (SE) task within the causal\\ninference paradigm by modeling the noise presence as an intervention. Based on\\nthe potential outcome framework, the proposed causal inference-based speech\\nenhancement (CISE) separates clean and noisy frames in an intervened noisy\\nspeech using a noise detector and assigns both sets of frames to two mask-based\\nenhancement modules (EMs) to perform noise-conditional SE. Specifically, we use\\nthe presence of noise as guidance for EM selection during training, and the\\nnoise detector selects the enhancement module according to the prediction of\\nthe presence of noise for each frame. Moreover, we derived a SE-specific\\naverage treatment effect to quantify the causal effect adequately. Experimental\\nevidence demonstrates that CISE outperforms a non-causal mask-based SE approach\\nin the studied settings and has better performance and efficiency than more\\ncomplex SE models.',\n",
       "    'author': [{'name': 'Tsun-An Hsieh'},\n",
       "     {'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Pin-Yu Chen'},\n",
       "     {'name': 'Sabato Marco Siniscalchi'},\n",
       "     {'name': 'Yu Tsao'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICASSP 2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.01189v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.01189v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.AS',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.AS',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.01839v1',\n",
       "    'updated': '2022-11-03T14:20:32Z',\n",
       "    'published': '2022-11-03T14:20:32Z',\n",
       "    'title': 'HyperSound: Generating Implicit Neural Representations of Audio Signals\\n  with Hypernetworks',\n",
       "    'summary': 'Implicit neural representations (INRs) are a rapidly growing research field,\\nwhich provides alternative ways to represent multimedia signals. Recent\\napplications of INRs include image super-resolution, compression of\\nhigh-dimensional signals, or 3D rendering. However, these solutions usually\\nfocus on visual data, and adapting them to the audio domain is not trivial.\\nMoreover, it requires a separately trained model for every data sample. To\\naddress this limitation, we propose HyperSound, a meta-learning method\\nleveraging hypernetworks to produce INRs for audio signals unseen at training\\ntime. We show that our approach can reconstruct sound waves with quality\\ncomparable to other state-of-the-art models.',\n",
       "    'author': [{'name': 'Filip Szatkowski'},\n",
       "     {'name': 'Karol J. Piczak'},\n",
       "     {'name': 'Przemysław Spurek'},\n",
       "     {'name': 'Jacek Tabor'},\n",
       "     {'name': 'Tomasz Trzciński'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.01839v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.01839v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.07851v1',\n",
       "    'updated': '2023-01-19T02:37:56Z',\n",
       "    'published': '2023-01-19T02:37:56Z',\n",
       "    'title': 'From English to More Languages: Parameter-Efficient Model Reprogramming\\n  for Cross-Lingual Speech Recognition',\n",
       "    'summary': 'In this work, we propose a new parameter-efficient learning framework based\\non neural model reprogramming for cross-lingual speech recognition, which can\\n\\\\textbf{re-purpose} well-trained English automatic speech recognition (ASR)\\nmodels to recognize the other languages. We design different auxiliary neural\\narchitectures focusing on learnable pre-trained feature enhancement that, for\\nthe first time, empowers model reprogramming on ASR. Specifically, we\\ninvestigate how to select trainable components (i.e., encoder) of a\\nconformer-based RNN-Transducer, as a frozen pre-trained backbone. Experiments\\non a seven-language multilingual LibriSpeech speech (MLS) task show that model\\nreprogramming only requires 4.2% (11M out of 270M) to 6.8% (45M out of 660M) of\\nits original trainable parameters from a full ASR model to perform competitive\\nresults in a range of 11.9% to 8.1% WER averaged across different languages. In\\naddition, we discover different setups to make large-scale pre-trained ASR\\nsucceed in both monolingual and multilingual speech recognition. Our methods\\noutperform existing ASR tuning architectures and their extension with\\nself-supervised losses (e.g., w2v-bert) in terms of lower WER and better\\ntraining efficiency.',\n",
       "    'author': [{'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Bo Li'},\n",
       "     {'name': 'Yu Zhang'},\n",
       "     {'name': 'Nanxin Chen'},\n",
       "     {'name': 'Rohit Prabhavalkar'},\n",
       "     {'name': 'Tara N. Sainath'},\n",
       "     {'name': 'Trevor Strohman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICASSP 2023. The project was initiated in May 2022\\n  during a research internship at Google Research'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.07851v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.07851v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1206.6230v2',\n",
       "    'updated': '2012-06-28T04:21:18Z',\n",
       "    'published': '2012-06-27T11:11:55Z',\n",
       "    'title': 'Decentralized Data Fusion and Active Sensing with Mobile Sensors for\\n  Modeling and Predicting Spatiotemporal Traffic Phenomena',\n",
       "    'summary': 'The problem of modeling and predicting spatiotemporal traffic phenomena over\\nan urban road network is important to many traffic applications such as\\ndetecting and forecasting congestion hotspots. This paper presents a\\ndecentralized data fusion and active sensing (D2FAS) algorithm for mobile\\nsensors to actively explore the road network to gather and assimilate the most\\ninformative data for predicting the traffic phenomenon. We analyze the time and\\ncommunication complexity of D2FAS and demonstrate that it can scale well with a\\nlarge number of observations and sensors. We provide a theoretical guarantee on\\nits predictive performance to be equivalent to that of a sophisticated\\ncentralized sparse approximation for the Gaussian process (GP) model: The\\ncomputation of such a sparse approximate GP model can thus be parallelized and\\ndistributed among the mobile sensors (in a Google-like MapReduce paradigm),\\nthereby achieving efficient and scalable prediction. We also theoretically\\nguarantee its active sensing performance that improves under various practical\\nenvironmental conditions. Empirical evaluation on real-world urban road network\\ndata shows that our D2FAS algorithm is significantly more time-efficient and\\nscalable than state-of-the-art centralized algorithms while achieving\\ncomparable predictive performance.',\n",
       "    'author': [{'name': 'Jie Chen'},\n",
       "     {'name': 'Kian Hsiang Low'},\n",
       "     {'name': 'Colin Keng-Yan Tan'},\n",
       "     {'name': 'Ali Oran'},\n",
       "     {'name': 'Patrick Jaillet'},\n",
       "     {'name': 'John M. Dolan'},\n",
       "     {'name': 'Gaurav S. Sukhatme'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '28th Conference on Uncertainty in Artificial Intelligence (UAI 2012),\\n  Extended version with proofs, 13 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1206.6230v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1206.6230v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.DC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.12204v3',\n",
       "    'updated': '2019-09-30T18:44:13Z',\n",
       "    'published': '2019-05-29T04:02:41Z',\n",
       "    'title': 'Learning scalable and transferable multi-robot/machine sequential\\n  assignment planning via graph embedding',\n",
       "    'summary': 'Can the success of reinforcement learning methods for simple combinatorial\\noptimization problems be extended to multi-robot sequential assignment\\nplanning? In addition to the challenge of achieving near-optimal performance in\\nlarge problems, transferability to an unseen number of robots and tasks is\\nanother key challenge for real-world applications. In this paper, we suggest a\\nmethod that achieves the first success in both challenges for robot/machine\\nscheduling problems.\\n  Our method comprises of three components. First, we show a robot scheduling\\nproblem can be expressed as a random probabilistic graphical model (PGM). We\\ndevelop a mean-field inference method for random PGM and use it for Q-function\\ninference. Second, we show that transferability can be achieved by carefully\\ndesigning two-step sequential encoding of problem state. Third, we resolve the\\ncomputational scalability issue of fitted Q-iteration by suggesting a heuristic\\nauction-based Q-iteration fitting method enabled by transferability we\\nachieved.\\n  We apply our method to discrete-time, discrete space problems (Multi-Robot\\nReward Collection (MRRC)) and scalably achieve 97% optimality with\\ntransferability. This optimality is maintained under stochastic contexts. By\\nextending our method to continuous time, continuous space formulation, we claim\\nto be the first learning-based method with scalable performance among\\nmulti-machine scheduling problems; our method scalability achieves comparable\\nperformance to popular metaheuristics in Identical parallel machine scheduling\\n(IPMS) problems.',\n",
       "    'author': [{'name': 'Hyunwook Kang'},\n",
       "     {'name': 'Aydar Mynbay'},\n",
       "     {'name': 'James R. Morrison'},\n",
       "     {'name': 'Jinkyoo Park'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.12204v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.12204v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2006.01022v2',\n",
       "    'updated': '2020-06-27T10:25:27Z',\n",
       "    'published': '2020-06-01T15:39:58Z',\n",
       "    'title': 'A novel approach for multi-agent cooperative pursuit to capture grouped\\n  evaders',\n",
       "    'summary': \"An approach of mobile multi-agent pursuit based on application of\\nself-organizing feature map (SOFM) and along with that reinforcement learning\\nbased on agent group role membership function (AGRMF) model is proposed. This\\nmethod promotes dynamic organization of the pursuers' groups and also makes\\npursuers' group evader according to their desire based on SOFM and AGRMF\\ntechniques. This helps to overcome the shortcomings of the pursuers that they\\ncannot fully reorganize when the goal is too independent in process of AGRMF\\nmodels operation. Besides, we also discuss a new reward function. After the\\nformation of the group, reinforcement learning is applied to get the optimal\\nsolution for each agent. The results of each step in capturing process will\\nfinally affect the AGR membership function to speed up the convergence of the\\ncompetitive neural network. The experiments result shows that this approach is\\nmore effective for the mobile agents to capture evaders.\",\n",
       "    'author': [{'name': 'Muhammad Zuhair Qadir'},\n",
       "     {'name': 'Songhao Piao'},\n",
       "     {'name': 'Haiyang Jiang'},\n",
       "     {'name': 'Mohammed El Habib Souidi'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/s11227-018-2591-3'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/s11227-018-2591-3',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2006.01022v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2006.01022v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"published paper's draft version\"},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Journal of Supercomputing, J Supercomput 76 (2020)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2007.05096v2',\n",
       "    'updated': '2020-08-14T18:08:25Z',\n",
       "    'published': '2020-07-09T22:16:45Z',\n",
       "    'title': 'Multi-Agent Routing Value Iteration Network',\n",
       "    'summary': 'In this paper we tackle the problem of routing multiple agents in a\\ncoordinated manner. This is a complex problem that has a wide range of\\napplications in fleet management to achieve a common goal, such as mapping from\\na swarm of robots and ride sharing. Traditional methods are typically not\\ndesigned for realistic environments hich contain sparsely connected graphs and\\nunknown traffic, and are often too slow in runtime to be practical. In\\ncontrast, we propose a graph neural network based model that is able to perform\\nmulti-agent routing based on learned value iteration in a sparsely connected\\ngraph with dynamically changing traffic conditions. Moreover, our learned\\ncommunication module enables the agents to coordinate online and adapt to\\nchanges more effectively. We created a simulated environment to mimic realistic\\nmapping performed by autonomous vehicles with unknown minimum edge coverage and\\ntraffic conditions; our approach significantly outperforms traditional solvers\\nboth in terms of total cost and runtime. We also show that our model trained\\nwith only two agents on graphs with a maximum of 25 nodes can easily generalize\\nto situations with more agents and/or nodes.',\n",
       "    'author': [{'name': 'Quinlan Sykora'},\n",
       "     {'name': 'Mengye Ren'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at ICML 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2007.05096v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2007.05096v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.11; I.2.6', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.00165v2',\n",
       "    'updated': '2021-10-27T23:29:08Z',\n",
       "    'published': '2020-10-31T02:06:07Z',\n",
       "    'title': 'FireCommander: An Interactive, Probabilistic Multi-agent Environment for\\n  Heterogeneous Robot Teams',\n",
       "    'summary': \"The purpose of this tutorial is to help individuals use the\\n\\\\underline{FireCommander} game environment for research applications. The\\nFireCommander is an interactive, probabilistic joint perception-action\\nreconnaissance environment in which a composite team of agents (e.g., robots)\\ncooperate to fight dynamic, propagating firespots (e.g., targets). In\\nFireCommander game, a team of agents must be tasked to optimally deal with a\\nwildfire situation in an environment with propagating fire areas and some\\nfacilities such as houses, hospitals, power stations, etc. The team of agents\\ncan accomplish their mission by first sensing (e.g., estimating fire states),\\ncommunicating the sensed fire-information among each other and then taking\\naction to put the firespots out based on the sensed information (e.g., dropping\\nwater on estimated fire locations). The FireCommander environment can be useful\\nfor research topics spanning a wide range of applications from Reinforcement\\nLearning (RL) and Learning from Demonstration (LfD), to Coordination,\\nPsychology, Human-Robot Interaction (HRI) and Teaming. There are four important\\nfacets of the FireCommander environment that overall, create a non-trivial\\ngame: (1) Complex Objectives: Multi-objective Stochastic Environment,\\n(2)Probabilistic Environment: Agents' actions result in probabilistic\\nperformance, (3) Hidden Targets: Partially Observable Environment and, (4)\\nUni-task Robots: Perception-only and Action-only agents. The FireCommander\\nenvironment is first-of-its-kind in terms of including Perception-only and\\nAction-only agents for coordination. It is a general multi-purpose game that\\ncan be useful in a variety of combinatorial optimization problems and\\nstochastic games, such as applications of Reinforcement Learning (RL), Learning\\nfrom Demonstration (LfD) and Inverse RL (iRL).\",\n",
       "    'author': [{'name': 'Esmaeil Seraj'},\n",
       "     {'name': 'Xiyang Wu'},\n",
       "     {'name': 'Matthew Gombolay'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.00165v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.00165v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.04679v4',\n",
       "    'updated': '2021-08-20T00:25:08Z',\n",
       "    'published': '2021-05-25T22:46:36Z',\n",
       "    'title': 'Self-Adaptive Swarm System (SASS)',\n",
       "    'summary': 'Distributed artificial intelligence (DAI) studies artificial intelligence\\nentities working together to reason, plan, solve problems, organize behaviors\\nand strategies, make collective decisions and learn. This Ph.D. research\\nproposes a principled Multi-Agent Systems (MAS) cooperation framework --\\nSelf-Adaptive Swarm System (SASS) -- to bridge the fourth level automation gap\\nbetween perception, communication, planning, execution, decision-making, and\\nlearning.',\n",
       "    'author': {'name': 'Qin Yang'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The Camera-ready version for IJCAI 2021 Doctoral Consortium'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.04679v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.04679v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.00751v2',\n",
       "    'updated': '2021-12-16T14:25:51Z',\n",
       "    'published': '2021-10-02T08:17:30Z',\n",
       "    'title': 'Partner-Aware Algorithms in Decentralized Cooperative Bandit Teams',\n",
       "    'summary': 'When humans collaborate with each other, they often make decisions by\\nobserving others and considering the consequences that their actions may have\\non the entire team, instead of greedily doing what is best for just themselves.\\nWe would like our AI agents to effectively collaborate in a similar way by\\ncapturing a model of their partners. In this work, we propose and analyze a\\ndecentralized Multi-Armed Bandit (MAB) problem with coupled rewards as an\\nabstraction of more general multi-agent collaboration. We demonstrate that\\nna\\\\\"ive extensions of single-agent optimal MAB algorithms fail when applied for\\ndecentralized bandit teams. Instead, we propose a Partner-Aware strategy for\\njoint sequential decision-making that extends the well-known single-agent Upper\\nConfidence Bound algorithm. We analytically show that our proposed strategy\\nachieves logarithmic regret, and provide extensive experiments involving\\nhuman-AI and human-robot collaboration to validate our theoretical findings.\\nOur results show that the proposed partner-aware strategy outperforms other\\nknown methods, and our human subject studies suggest humans prefer to\\ncollaborate with AI agents implementing our partner-aware strategy.',\n",
       "    'author': [{'name': 'Erdem Bıyık'},\n",
       "     {'name': 'Anusha Lalitha'},\n",
       "     {'name': 'Rajarshi Saha'},\n",
       "     {'name': 'Andrea Goldsmith'},\n",
       "     {'name': 'Dorsa Sadigh'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages, 13 figures. To be presented at \"Thirty-Sixth AAAI\\n  Conference on Artificial Intelligence (AAAI) 2022\". Also presented at\\n  \"Artificial Intelligence for Human-Robot Interaction (AI-HRI) at AAAI Fall\\n  Symposium Series 2021\"'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.00751v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.00751v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.04175v1',\n",
       "    'updated': '2019-11-11T10:55:25Z',\n",
       "    'published': '2019-11-11T10:55:25Z',\n",
       "    'title': 'Multi-Agent Connected Autonomous Driving using Deep Reinforcement\\n  Learning',\n",
       "    'summary': 'The capability to learn and adapt to changes in the driving environment is\\ncrucial for developing autonomous driving systems that are scalable beyond\\ngeo-fenced operational design domains. Deep Reinforcement Learning (RL)\\nprovides a promising and scalable framework for developing adaptive learning\\nbased solutions. Deep RL methods usually model the problem as a (Partially\\nObservable) Markov Decision Process in which an agent acts in a stationary\\nenvironment to learn an optimal behavior policy. However, driving involves\\ncomplex interaction between multiple, intelligent (artificial or human) agents\\nin a highly non-stationary environment. In this paper, we propose the use of\\nPartially Observable Markov Games(POSG) for formulating the connected\\nautonomous driving problems with realistic assumptions. We provide a taxonomy\\nof multi-agent learning environments based on the nature of tasks, nature of\\nagents and the nature of the environment to help in categorizing various\\nautonomous driving problems that can be addressed under the proposed\\nformulation. As our main contributions, we provide MACAD-Gym, a Multi-Agent\\nConnected, Autonomous Driving agent learning platform for furthering research\\nin this direction. Our MACAD-Gym platform provides an extensible set of\\nConnected Autonomous Driving (CAD) simulation environments that enable the\\nresearch and development of Deep RL- based integrated sensing, perception,\\nplanning and control algorithms for CAD systems with unlimited operational\\ndesign domain under realistic, multi-agent settings. We also share the\\nMACAD-Agents that were trained successfully using the MACAD-Gym platform to\\nlearn control policies for multiple vehicle agents in a partially observable,\\nstop-sign controlled, 3-way urban intersection environment with raw (camera)\\nsensor observations.',\n",
       "    'author': {'name': 'Praveen Palanisamy'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted, Machine Learning for Autonomous Driving Workshop at the\\n  33rd Conference on Neural Information Processing Systems(NeurIPS 2019)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.04175v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.04175v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '93C85, 68T40, 68T05',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.11; I.2.9; I.2.6',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.06380v1',\n",
       "    'updated': '2022-10-12T16:33:34Z',\n",
       "    'published': '2022-10-12T16:33:34Z',\n",
       "    'title': 'Near-Optimal Multi-Agent Learning for Safe Coverage Control',\n",
       "    'summary': \"In multi-agent coverage control problems, agents navigate their environment\\nto reach locations that maximize the coverage of some density. In practice, the\\ndensity is rarely known $\\\\textit{a priori}$, further complicating the original\\nNP-hard problem. Moreover, in many applications, agents cannot visit arbitrary\\nlocations due to $\\\\textit{a priori}$ unknown safety constraints. In this paper,\\nwe aim to efficiently learn the density to approximately solve the coverage\\nproblem while preserving the agents' safety. We first propose a conditionally\\nlinear submodular coverage function that facilitates theoretical analysis.\\nUtilizing this structure, we develop MacOpt, a novel algorithm that efficiently\\ntrades off the exploration-exploitation dilemma due to partial observability,\\nand show that it achieves sublinear regret. Next, we extend results on\\nsingle-agent safe exploration to our multi-agent setting and propose SafeMac\\nfor safe coverage and exploration. We analyze SafeMac and give first of its\\nkind results: near optimal coverage in finite time while provably guaranteeing\\nsafety. We extensively evaluate our algorithms on synthetic and real problems,\\nincluding a bio-diversity monitoring task under safety constraints, where\\nSafeMac outperforms competing methods.\",\n",
       "    'author': [{'name': 'Manish Prajapat'},\n",
       "     {'name': 'Matteo Turchetta'},\n",
       "     {'name': 'Melanie N. Zeilinger'},\n",
       "     {'name': 'Andreas Krause'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at NeurIPS 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.06380v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.06380v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'math.OC', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2002.11882v2',\n",
       "    'updated': '2021-02-23T12:12:47Z',\n",
       "    'published': '2020-02-27T02:38:21Z',\n",
       "    'title': 'A Visual Communication Map for Multi-Agent Deep Reinforcement Learning',\n",
       "    'summary': 'Deep reinforcement learning has been applied successfully to solve various\\nreal-world problems and the number of its applications in the multi-agent\\nsettings has been increasing. Multi-agent learning distinctly poses significant\\nchallenges in the effort to allocate a concealed communication medium. Agents\\nreceive thorough knowledge from the medium to determine subsequent actions in a\\ndistributed nature. Apparently, the goal is to leverage the cooperation of\\nmultiple agents to achieve a designated objective efficiently. Recent studies\\ntypically combine a specialized neural network with reinforcement learning to\\nenable communication between agents. This approach, however, limits the number\\nof agents or necessitates the homogeneity of the system. In this paper, we have\\nproposed a more scalable approach that not only deals with a great number of\\nagents but also enables collaboration between dissimilar functional agents and\\ncompatibly combined with any deep reinforcement learning methods. Specifically,\\nwe create a global communication map to represent the status of each agent in\\nthe system visually. The visual map and the environmental state are fed to a\\nshared-parameter network to train multiple agents concurrently. Finally, we\\nselect the Asynchronous Advantage Actor-Critic (A3C) algorithm to demonstrate\\nour proposed scheme, namely Visual communication map for Multi-agent A3C\\n(VMA3C). Simulation results show that the use of visual communication map\\nimproves the performance of A3C regarding learning speed, reward achievement,\\nand robustness in multi-agent problems.',\n",
       "    'author': [{'name': 'Ngoc Duy Nguyen'},\n",
       "     {'name': 'Thanh Thi Nguyen'},\n",
       "     {'name': 'Doug Creighton'},\n",
       "     {'name': 'Saeid Nahavandi'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.13140/RG.2.2.13433.62563'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.13140/RG.2.2.13433.62563',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2002.11882v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2002.11882v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.00882v1',\n",
       "    'updated': '2021-09-02T12:43:35Z',\n",
       "    'published': '2021-09-02T12:43:35Z',\n",
       "    'title': 'MACRPO: Multi-Agent Cooperative Recurrent Policy Optimization',\n",
       "    'summary': \"This work considers the problem of learning cooperative policies in\\nmulti-agent settings with partially observable and non-stationary environments\\nwithout a communication channel. We focus on improving information sharing\\nbetween agents and propose a new multi-agent actor-critic method called\\n\\\\textit{Multi-Agent Cooperative Recurrent Proximal Policy Optimization}\\n(MACRPO). We propose two novel ways of integrating information across agents\\nand time in MACRPO: First, we use a recurrent layer in critic's network\\narchitecture and propose a new framework to use a meta-trajectory to train the\\nrecurrent layer. This allows the network to learn the cooperation and dynamics\\nof interactions between agents, and also handle partial observability. Second,\\nwe propose a new advantage function that incorporates other agents' rewards and\\nvalue functions. We evaluate our algorithm on three challenging multi-agent\\nenvironments with continuous and discrete action spaces, Deepdrive-Zero,\\nMulti-Walker, and Particle environment. We compare the results with several\\nablations and state-of-the-art multi-agent algorithms such as QMIX and MADDPG\\nand also single-agent methods with shared parameters between agents such as\\nIMPALA and APEX. The results show superior performance against other\\nalgorithms. The code is available online at\\nhttps://github.com/kargarisaac/macrpo.\",\n",
       "    'author': [{'name': 'Eshagh Kargar'}, {'name': 'Ville Kyrki'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.00882v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.00882v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1607.02303v2',\n",
       "    'updated': '2016-08-15T18:05:00Z',\n",
       "    'published': '2016-07-08T10:39:05Z',\n",
       "    'title': 'CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label\\n  Tree Embeddings for Audio Scene Recognition',\n",
       "    'summary': 'We describe in this report our audio scene recognition system submitted to\\nthe DCASE 2016 challenge. Firstly, given the label set of the scenes, a label\\ntree is automatically constructed. This category taxonomy is then used in the\\nfeature extraction step in which an audio scene instance is represented by a\\nlabel tree embedding image. Different convolutional neural networks, which are\\ntailored for the task at hand, are finally learned on top of the image features\\nfor scene recognition. Our system reaches an overall recognition accuracy of\\n81.2% and 83.3% and outperforms the DCASE 2016 baseline with absolute\\nimprovements of 8.7% and 6.1% on the development and test data, respectively.',\n",
       "    'author': [{'name': 'Huy Phan'},\n",
       "     {'name': 'Lars Hertel'},\n",
       "     {'name': 'Marco Maass'},\n",
       "     {'name': 'Philipp Koch'},\n",
       "     {'name': 'Alfred Mertins'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Task1 technical report for the DCASE2016 challenge. arXiv admin note:\\n  text overlap with arXiv:1606.07908'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1607.02303v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1607.02303v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.11797v2',\n",
       "    'updated': '2018-05-31T03:49:25Z',\n",
       "    'published': '2018-05-30T04:15:58Z',\n",
       "    'title': 'Grow and Prune Compact, Fast, and Accurate LSTMs',\n",
       "    'summary': \"Long short-term memory (LSTM) has been widely used for sequential data\\nmodeling. Researchers have increased LSTM depth by stacking LSTM cells to\\nimprove performance. This incurs model redundancy, increases run-time delay,\\nand makes the LSTMs more prone to overfitting. To address these problems, we\\npropose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original\\none level non-linear control gates. H-LSTM increases accuracy while employing\\nfewer external stacked layers, thus reducing the number of parameters and\\nrun-time latency significantly. We employ grow-and-prune (GP) training to\\niteratively adjust the hidden layers through gradient-based growth and\\nmagnitude-based pruning of connections. This learns both the weights and the\\ncompact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for\\nimage captioning and speech recognition applications. For the NeuralTalk\\narchitecture on the MSCOCO dataset, our three models reduce the number of\\nparameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time\\nlatency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2\\narchitecture on the AN4 dataset, our two models reduce the number of parameters\\nby 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate\\nfrom 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast,\\nand accurate.\",\n",
       "    'author': [{'name': 'Xiaoliang Dai'},\n",
       "     {'name': 'Hongxu Yin'},\n",
       "     {'name': 'Niraj K. Jha'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1805.11797v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.11797v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.15377v2',\n",
       "    'updated': '2022-12-08T11:00:05Z',\n",
       "    'published': '2022-11-23T09:57:17Z',\n",
       "    'title': 'Whose Emotion Matters? Speaker Detection without Prior Knowledge',\n",
       "    'summary': 'The task of emotion recognition in conversations (ERC) benefits from the\\navailability of multiple modalities, as offered, for example, in the\\nvideo-based MELD dataset. However, only a few research approaches use both\\nacoustic and visual information from the MELD videos. There are two reasons for\\nthis: First, label-to-video alignments in MELD are noisy, making those videos\\nan unreliable source of emotional speech data. Second, conversations can\\ninvolve several people in the same scene, which requires the detection of the\\nperson speaking the utterance. In this paper we demonstrate that by using\\nrecent automatic speech recognition and active speaker detection models, we are\\nable to realign the videos of MELD, and capture the facial expressions from\\nuttering speakers in 96.92% of the utterances provided in MELD. Experiments\\nwith a self-supervised voice recognition model indicate that the realigned MELD\\nvideos more closely match the corresponding utterances offered in the dataset.\\nFinally, we devise a model for emotion recognition in conversations trained on\\nthe face and audio information of the MELD realigned videos, which outperforms\\nstate-of-the-art models for ERC based on vision alone. This indicates that\\nactive speaker detection is indeed effective for extracting facial expressions\\nfrom the uttering speakers, and that faces provide more informative visual cues\\nthan the visual features state-of-the-art models have been using so far.',\n",
       "    'author': [{'name': 'Hugo Carneiro'},\n",
       "     {'name': 'Cornelius Weber'},\n",
       "     {'name': 'Stefan Wermter'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '22 pages, 8 figures, 6 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.15377v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.15377v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.AS',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.AS',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T20', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.0', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.12147v2',\n",
       "    'updated': '2020-10-30T08:59:08Z',\n",
       "    'published': '2019-12-18T12:19:27Z',\n",
       "    'title': 'Cooperative Perception for 3D Object Detection in Driving Scenarios\\n  using Infrastructure Sensors',\n",
       "    'summary': '3D object detection is a common function within the perception system of an\\nautonomous vehicle and outputs a list of 3D bounding boxes around objects of\\ninterest. Various 3D object detection methods have relied on fusion of\\ndifferent sensor modalities to overcome limitations of individual sensors.\\nHowever, occlusion, limited field-of-view and low-point density of the sensor\\ndata cannot be reliably and cost-effectively addressed by multi-modal sensing\\nfrom a single point of view. Alternatively, cooperative perception incorporates\\ninformation from spatially diverse sensors distributed around the environment\\nas a way to mitigate these limitations. This article proposes two schemes for\\ncooperative 3D object detection using single modality sensors. The early fusion\\nscheme combines point clouds from multiple spatially diverse sensing points of\\nview before detection. In contrast, the late fusion scheme fuses the\\nindependently detected bounding boxes from multiple spatially diverse sensors.\\nWe evaluate the performance of both schemes, and their hybrid combination,\\nusing a synthetic cooperative dataset created in two complex driving scenarios,\\na T-junction and a roundabout. The evaluation shows that the early fusion\\napproach outperforms late fusion by a significant margin at the cost of higher\\ncommunication bandwidth. The results demonstrate that cooperative perception\\ncan recall more than 95% of the objects as opposed to 30% for single-point\\nsensing in the most challenging scenario. To provide practical insights into\\nthe deployment of such system, we report how the number of sensors and their\\nconfiguration impact the detection performance of the system.',\n",
       "    'author': [{'name': 'Eduardo Arnold'},\n",
       "     {'name': 'Mehrdad Dianati'},\n",
       "     {'name': 'Robert de Temple'},\n",
       "     {'name': 'Saber Fallah'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TITS.2020.3028424'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TITS.2020.3028424',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1912.12147v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.12147v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '13 pages, 4 tables, 7 figures. Published in IEEE Transactions on\\n  Intelligent Transportation Systems'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.00997v2',\n",
       "    'updated': '2019-12-06T23:36:01Z',\n",
       "    'published': '2019-11-04T00:42:01Z',\n",
       "    'title': 'Multiple Futures Prediction',\n",
       "    'summary': \"Temporal prediction is critical for making intelligent and robust decisions\\nin complex dynamic environments. Motion prediction needs to model the\\ninherently uncertain future which often contains multiple potential outcomes,\\ndue to multi-agent interactions and the latent goals of others. Towards these\\ngoals, we introduce a probabilistic framework that efficiently learns latent\\nvariables to jointly model the multi-step future motions of agents in a scene.\\nOur framework is data-driven and learns semantically meaningful latent\\nvariables to represent the multimodal future, without requiring explicit\\nlabels. Using a dynamic attention-based state encoder, we learn to encode the\\npast as well as the future interactions among agents, efficiently scaling to\\nany number of agents. Finally, our model can be used for planning via computing\\na conditional probability density over the trajectories of other agents given a\\nhypothetical rollout of the 'self' agent. We demonstrate our algorithms by\\npredicting vehicle trajectories of both simulated and real data, demonstrating\\nthe state-of-the-art results on several vehicle trajectory datasets.\",\n",
       "    'author': [{'name': 'Yichuan Charlie Tang'},\n",
       "     {'name': 'Ruslan Salakhutdinov'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In proceedings of NeurIPS 2019, Vancouver, British Columbia, Canada'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.00997v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.00997v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.13044v6',\n",
       "    'updated': '2021-01-02T13:24:01Z',\n",
       "    'published': '2019-11-29T10:42:10Z',\n",
       "    'title': 'Learning Structured Representations of Spatial and Interactive Dynamics\\n  for Trajectory Prediction in Crowded Scenes',\n",
       "    'summary': \"Context plays a significant role in the generation of motion for dynamic\\nagents in interactive environments. This work proposes a modular method that\\nutilises a learned model of the environment for motion prediction. This\\nmodularity explicitly allows for unsupervised adaptation of trajectory\\nprediction models to unseen environments and new tasks by relying on unlabelled\\nimage data only. We model both the spatial and dynamic aspects of a given\\nenvironment alongside the per agent motions. This results in more informed\\nmotion prediction and allows for performance comparable to the\\nstate-of-the-art. We highlight the model's prediction capability using a\\nbenchmark pedestrian prediction problem and a robot manipulation task and show\\nthat we can transfer the predictor across these tasks in a completely\\nunsupervised way. The proposed approach allows for robust and label efficient\\nforward modelling, and relaxes the need for full model re-training in new\\nenvironments.\",\n",
       "    'author': [{'name': 'Todor Davchev'},\n",
       "     {'name': 'Michael Burke'},\n",
       "     {'name': 'Subramanian Ramamoorthy'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2020.3047778'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2020.3047778',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1911.13044v6',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.13044v6',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters 2021'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.09506v1',\n",
       "    'updated': '2020-08-20T17:55:41Z',\n",
       "    'published': '2020-08-20T17:55:41Z',\n",
       "    'title': 'Graph Neural Networks for 3D Multi-Object Tracking',\n",
       "    'summary': '3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work\\noften uses a tracking-by-detection pipeline, where the feature of each object\\nis extracted independently to compute an affinity matrix. Then, the affinity\\nmatrix is passed to the Hungarian algorithm for data association. A key process\\nof this pipeline is to learn discriminative features for different objects in\\norder to reduce confusion during data association. To that end, we propose two\\ninnovative techniques: (1) instead of obtaining the features for each object\\nindependently, we propose a novel feature interaction mechanism by introducing\\nGraph Neural Networks; (2) instead of obtaining the features from either 2D or\\n3D space as in prior work, we propose a novel joint feature extractor to learn\\nappearance and motion features from 2D and 3D space. Through experiments on the\\nKITTI dataset, our proposed method achieves state-of-the-art 3D MOT\\nperformance. Our project website is at\\nhttp://www.xinshuoweng.com/projects/GNN3DMOT.',\n",
       "    'author': [{'name': 'Xinshuo Weng'},\n",
       "     {'name': 'Yongxin Wang'},\n",
       "     {'name': 'Yunze Man'},\n",
       "     {'name': 'Kris Kitani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ECCV 2020 workshop paper. Project website:\\n  http://www.xinshuoweng.com/projects/GNN3DMOT. arXiv admin note: substantial\\n  text overlap with arXiv:2006.07327'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.09506v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.09506v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1710.00489v1',\n",
       "    'updated': '2017-10-02T05:18:12Z',\n",
       "    'published': '2017-10-02T05:18:12Z',\n",
       "    'title': 'SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning\\n  and Control',\n",
       "    'summary': 'In this work, we present an approach to deep visuomotor control using\\nstructured deep dynamics models. Our deep dynamics model, a variant of\\nSE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an\\nencoder-decoder structure. Unlike prior work, our dynamics model is structured:\\ngiven an input scene, our network explicitly learns to segment salient parts\\nand predict their pose-embedding along with their motion modeled as a change in\\nthe pose space due to the applied actions. We train our model using a pair of\\npoint clouds separated by an action and show that given supervision only in the\\nform of point-wise data associations between the frames our network is able to\\nlearn a meaningful segmentation of the scene along with consistent poses. We\\nfurther show that our model can be used for closed-loop control directly in the\\nlearned low-dimensional pose space, where the actions are computed by\\nminimizing error in the pose space using gradient-based methods, similar to\\ntraditional model-based control. We present results on controlling a Baxter\\nrobot from raw depth data in simulation and in the real world and compare\\nagainst two baseline deep networks. Our method runs in real-time, achieves good\\nprediction of scene dynamics and outperforms the baseline methods on multiple\\ncontrol runs. Video results can be found at:\\nhttps://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/',\n",
       "    'author': [{'name': 'Arunkumar Byravan'},\n",
       "     {'name': 'Felix Leeb'},\n",
       "     {'name': 'Franziska Meier'},\n",
       "     {'name': 'Dieter Fox'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, Initial submission to IEEE International Conference on\\n  Robotics and Automation (ICRA) 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1710.00489v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1710.00489v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.00342v2',\n",
       "    'updated': '2020-10-14T15:35:11Z',\n",
       "    'published': '2020-02-29T20:24:11Z',\n",
       "    'title': 'Robust Robotic Pouring using Audition and Haptics',\n",
       "    'summary': 'Robust and accurate estimation of liquid height lies as an essential part of\\npouring tasks for service robots. However, vision-based methods often fail in\\noccluded conditions while audio-based methods cannot work well in a noisy\\nenvironment. We instead propose a multimodal pouring network (MP-Net) that is\\nable to robustly predict liquid height by conditioning on both audition and\\nhaptics input. MP-Net is trained on a self-collected multimodal pouring\\ndataset. This dataset contains 300 robot pouring recordings with audio and\\nforce/torque measurements for three types of target containers. We also augment\\nthe audio data by inserting robot noise. We evaluated MP-Net on our collected\\ndataset and a wide variety of robot experiments. Both network training results\\nand robot experiments demonstrate that MP-Net is robust against noise and\\nchanges to the task and environment. Moreover, we further combine the predicted\\nheight and force data to estimate the shape of the target container.',\n",
       "    'author': [{'name': 'Hongzhuo Liang'},\n",
       "     {'name': 'Chuangchuang Zhou'},\n",
       "     {'name': 'Shuang Li'},\n",
       "     {'name': 'Xiaojian Ma'},\n",
       "     {'name': 'Norman Hendrich'},\n",
       "     {'name': 'Timo Gerkmann'},\n",
       "     {'name': 'Fuchun Sun'},\n",
       "     {'name': 'Marcus Stoffel'},\n",
       "     {'name': 'Jianwei Zhang'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/IROS45743.2020.9340859'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/IROS45743.2020.9340859',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2003.00342v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.00342v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'accepted by IROS2020'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2020 IEEE/RSJ International Conference on Intelligent Robots and\\n  Systems (IROS)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1311.1761v1',\n",
       "    'updated': '2013-11-07T17:39:31Z',\n",
       "    'published': '2013-11-07T17:39:31Z',\n",
       "    'title': 'Exploring Deep and Recurrent Architectures for Optimal Control',\n",
       "    'summary': 'Sophisticated multilayer neural networks have achieved state of the art\\nresults on multiple supervised tasks. However, successful applications of such\\nmultilayer networks to control have so far been limited largely to the\\nperception portion of the control pipeline. In this paper, we explore the\\napplication of deep and recurrent neural networks to a continuous,\\nhigh-dimensional locomotion task, where the network is used to represent a\\ncontrol policy that maps the state of the system (represented by joint angles)\\ndirectly to the torques at each joint. By using a recent reinforcement learning\\nalgorithm called guided policy search, we can successfully train neural network\\ncontrollers with thousands of parameters, allowing us to compare a variety of\\narchitectures. We discuss the differences between the locomotion control task\\nand previous supervised perception tasks, present experimental results\\ncomparing various architectures, and discuss future directions in the\\napplication of techniques from deep learning to the problem of optimal control.',\n",
       "    'author': {'name': 'Sergey Levine'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Appears in the Neural Information Processing Systems (NIPS 2013)\\n  Workshop on Deep Learning'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1311.1761v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1311.1761v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1407.3501v4',\n",
       "    'updated': '2015-05-27T22:43:04Z',\n",
       "    'published': '2014-07-13T19:06:08Z',\n",
       "    'title': 'Robots that can adapt like animals',\n",
       "    'summary': 'As robots leave the controlled environments of factories to autonomously\\nfunction in more complex, natural environments, they will have to respond to\\nthe inevitable fact that they will become damaged. However, while animals can\\nquickly adapt to a wide variety of injuries, current robots cannot \"think\\noutside the box\" to find a compensatory behavior when damaged: they are limited\\nto their pre-specified self-sensing abilities, can diagnose only anticipated\\nfailure modes, and require a pre-programmed contingency plan for every type of\\npotential damage, an impracticality for complex robots. Here we introduce an\\nintelligent trial and error algorithm that allows robots to adapt to damage in\\nless than two minutes, without requiring self-diagnosis or pre-specified\\ncontingency plans. Before deployment, a robot exploits a novel algorithm to\\ncreate a detailed map of the space of high-performing behaviors: This map\\nrepresents the robot\\'s intuitions about what behaviors it can perform and their\\nvalue. If the robot is damaged, it uses these intuitions to guide a\\ntrial-and-error learning algorithm that conducts intelligent experiments to\\nrapidly discover a compensatory behavior that works in spite of the damage.\\nExperiments reveal successful adaptations for a legged robot injured in five\\ndifferent ways, including damaged, broken, and missing legs, and for a robotic\\narm with joints broken in 14 different ways. This new technique will enable\\nmore robust, effective, autonomous robots, and suggests principles that animals\\nmay use to adapt to injury.',\n",
       "    'author': [{'name': 'Antoine Cully'},\n",
       "     {'name': 'Jeff Clune'},\n",
       "     {'name': 'Danesh Tarapore'},\n",
       "     {'name': 'Jean-Baptiste Mouret'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1038/nature14422'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1038/nature14422',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1407.3501v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1407.3501v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1506.04089v4',\n",
       "    'updated': '2015-12-17T17:57:42Z',\n",
       "    'published': '2015-06-12T18:05:00Z',\n",
       "    'title': 'Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to\\n  Action Sequences',\n",
       "    'summary': 'We propose a neural sequence-to-sequence model for direction following, a\\ntask that is essential to realizing effective autonomous agents. Our\\nalignment-based encoder-decoder model with long short-term memory recurrent\\nneural networks (LSTM-RNN) translates natural language instructions to action\\nsequences based upon a representation of the observable world state. We\\nintroduce a multi-level aligner that empowers our model to focus on sentence\\n\"regions\" salient to the current world state by using multiple abstractions of\\nthe input sentence. In contrast to existing methods, our model uses no\\nspecialized linguistic resources (e.g., parsers) or task-specific annotations\\n(e.g., seed lexicons). It is therefore generalizable, yet still achieves the\\nbest results reported to-date on a benchmark single-sentence dataset and\\ncompetitive results for the limited-training multi-sentence setting. We analyze\\nour model through a series of ablations that elucidate the contributions of the\\nprimary components of our model.',\n",
       "    'author': [{'name': 'Hongyuan Mei'},\n",
       "     {'name': 'Mohit Bansal'},\n",
       "     {'name': 'Matthew R. Walter'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear at AAAI 2016 (and an extended version of a NIPS 2015\\n  Multimodal Machine Learning workshop paper)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1506.04089v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1506.04089v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CL',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CL',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1706.01905v2',\n",
       "    'updated': '2018-01-31T09:05:10Z',\n",
       "    'published': '2017-06-06T18:09:29Z',\n",
       "    'title': 'Parameter Space Noise for Exploration',\n",
       "    'summary': \"Deep reinforcement learning (RL) methods generally engage in exploratory\\nbehavior through noise injection in the action space. An alternative is to add\\nnoise directly to the agent's parameters, which can lead to more consistent\\nexploration and a richer set of behaviors. Methods such as evolutionary\\nstrategies use parameter perturbations, but discard all temporal structure in\\nthe process and require significantly more samples. Combining parameter noise\\nwith traditional RL methods allows to combine the best of both worlds. We\\ndemonstrate that both off- and on-policy methods benefit from this approach\\nthrough experimental comparison of DQN, DDPG, and TRPO on high-dimensional\\ndiscrete action environments as well as continuous control tasks. Our results\\nshow that RL with parameter noise learns more efficiently than traditional RL\\nwith action space noise and evolutionary strategies individually.\",\n",
       "    'author': [{'name': 'Matthias Plappert'},\n",
       "     {'name': 'Rein Houthooft'},\n",
       "     {'name': 'Prafulla Dhariwal'},\n",
       "     {'name': 'Szymon Sidor'},\n",
       "     {'name': 'Richard Y. Chen'},\n",
       "     {'name': 'Xi Chen'},\n",
       "     {'name': 'Tamim Asfour'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Marcin Andrychowicz'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Updated to camera-ready ICLR submission'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1706.01905v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1706.01905v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1706.02490v1',\n",
       "    'updated': '2017-06-08T09:31:42Z',\n",
       "    'published': '2017-06-08T09:31:42Z',\n",
       "    'title': 'Where is my forearm? Clustering of body parts from simultaneous tactile\\n  and linguistic input using sequential mapping',\n",
       "    'summary': 'Humans and animals are constantly exposed to a continuous stream of sensory\\ninformation from different modalities. At the same time, they form more\\ncompressed representations like concepts or symbols. In species that use\\nlanguage, this process is further structured by this interaction, where a\\nmapping between the sensorimotor concepts and linguistic elements needs to be\\nestablished. There is evidence that children might be learning language by\\nsimply disambiguating potential meanings based on multiple exposures to\\nutterances in different contexts (cross-situational learning). In existing\\nmodels, the mapping between modalities is usually found in a single step by\\ndirectly using frequencies of referent and meaning co-occurrences. In this\\npaper, we present an extension of this one-step mapping and introduce a newly\\nproposed sequential mapping algorithm together with a publicly available Matlab\\nimplementation. For demonstration, we have chosen a less typical scenario:\\ninstead of learning to associate objects with their names, we focus on body\\nrepresentations. A humanoid robot is receiving tactile stimulations on its\\nbody, while at the same time listening to utterances of the body part names\\n(e.g., hand, forearm and torso). With the goal at arriving at the correct \"body\\ncategories\", we demonstrate how a sequential mapping algorithm outperforms\\none-step mapping. In addition, the effect of data set size and noise in the\\nlinguistic input are studied.',\n",
       "    'author': [{'name': 'Karla Stepanova'},\n",
       "     {'name': 'Matej Hoffmann'},\n",
       "     {'name': 'Zdenek Straka'},\n",
       "     {'name': 'Frederico B. Klein'},\n",
       "     {'name': 'Angelo Cangelosi'},\n",
       "     {'name': 'Michal Vavrecka'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'pp. 155-162'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1706.02490v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1706.02490v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.06917v2',\n",
       "    'updated': '2018-03-13T16:39:40Z',\n",
       "    'published': '2017-09-20T15:03:47Z',\n",
       "    'title': 'Using Parameterized Black-Box Priors to Scale Up Model-Based Policy\\n  Search for Robotics',\n",
       "    'summary': 'The most data-efficient algorithms for reinforcement learning in robotics are\\nmodel-based policy search algorithms, which alternate between learning a\\ndynamical model of the robot and optimizing a policy to maximize the expected\\nreturn given the model and its uncertainties. Among the few proposed\\napproaches, the recently introduced Black-DROPS algorithm exploits a black-box\\noptimization algorithm to achieve both high data-efficiency and good\\ncomputation times when several cores are used; nevertheless, like all\\nmodel-based policy search approaches, Black-DROPS does not scale to high\\ndimensional state/action spaces. In this paper, we introduce a new model\\nlearning procedure in Black-DROPS that leverages parameterized black-box priors\\nto (1) scale up to high-dimensional systems, and (2) be robust to large\\ninaccuracies of the prior information. We demonstrate the effectiveness of our\\napproach with the \"pendubot\" swing-up task in simulation and with a physical\\nhexapod robot (48D state space, 18D action space) that has to walk forward as\\nfast as possible. The results show that our new algorithm is more\\ndata-efficient than previous model-based policy search algorithms (with and\\nwithout priors) and that it can allow a physical 6-legged robot to learn new\\ngaits in only 16 to 30 seconds of interaction time.',\n",
       "    'author': [{'name': 'Konstantinos Chatzilygeroudis'},\n",
       "     {'name': 'Jean-Baptiste Mouret'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at ICRA 2018; 8 pages, 4 figures, 2 algorithms, 1 table;\\n  Video at https://youtu.be/HFkZkhGGzTo ; Spotlight ICRA presentation at\\n  https://youtu.be/_MZYDhfWeLc'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.06917v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.06917v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.06919v2',\n",
       "    'updated': '2018-03-13T16:45:49Z',\n",
       "    'published': '2017-09-20T15:04:50Z',\n",
       "    'title': 'Bayesian Optimization with Automatic Prior Selection for Data-Efficient\\n  Direct Policy Search',\n",
       "    'summary': 'One of the most interesting features of Bayesian optimization for direct\\npolicy search is that it can leverage priors (e.g., from simulation or from\\nprevious tasks) to accelerate learning on a robot. In this paper, we are\\ninterested in situations for which several priors exist but we do not know in\\nadvance which one fits best the current situation. We tackle this problem by\\nintroducing a novel acquisition function, called Most Likely Expected\\nImprovement (MLEI), that combines the likelihood of the priors and the expected\\nimprovement. We evaluate this new acquisition function on a transfer learning\\ntask for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has\\nto learn to walk on flat ground and on stairs, with priors corresponding to\\ndifferent stairs and different kinds of damages. Our results show that MLEI\\neffectively identifies and exploits the priors, even when there is no obvious\\nmatch between the current situations and the priors.',\n",
       "    'author': [{'name': 'Rémi Pautrat'},\n",
       "     {'name': 'Konstantinos Chatzilygeroudis'},\n",
       "     {'name': 'Jean-Baptiste Mouret'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at ICRA 2018; 8 pages, 4 figures, 1 algorithm; Video at\\n  https://youtu.be/xo8mUIZTvNE ; Spotlight ICRA presentation\\n  https://youtu.be/iiVaV-U6Kqo'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.06919v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.06919v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.08180v2',\n",
       "    'updated': '2019-03-08T17:52:47Z',\n",
       "    'published': '2018-05-21T17:02:53Z',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Hindsight',\n",
       "    'summary': 'Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency\\nwhen rewards are delayed and sparse. We introduce a solution that enables\\nagents to learn temporally extended actions at multiple levels of abstraction\\nin a sample efficient and automated fashion. Our approach combines universal\\nvalue functions and hindsight learning, allowing agents to learn policies\\nbelonging to different time scales in parallel. We show that our method\\nsignificantly accelerates learning in a variety of discrete and continuous\\ntasks.',\n",
       "    'author': [{'name': 'Andrew Levy'},\n",
       "     {'name': 'Robert Platt'},\n",
       "     {'name': 'Kate Saenko'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Duplicate. See arXiv:1712.00948 \"Learning Multi-Level Hierarchies\\n  with Hindsight\" for latest version'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1805.08180v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.08180v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1806.09351v3',\n",
       "    'updated': '2020-03-03T22:57:46Z',\n",
       "    'published': '2018-06-25T09:46:47Z',\n",
       "    'title': 'Multi-objective Model-based Policy Search for Data-efficient Learning\\n  with Sparse Rewards',\n",
       "    'summary': 'The most data-efficient algorithms for reinforcement learning in robotics are\\nmodel-based policy search algorithms, which alternate between learning a\\ndynamical model of the robot and optimizing a policy to maximize the expected\\nreturn given the model and its uncertainties. However, the current algorithms\\nlack an effective exploration strategy to deal with sparse or misleading reward\\nscenarios: if they do not experience any state with a positive reward during\\nthe initial random exploration, it is very unlikely to solve the problem. Here,\\nwe propose a novel model-based policy search algorithm, Multi-DEX, that\\nleverages a learned dynamical model to efficiently explore the task space and\\nsolve tasks with sparse rewards in a few episodes. To achieve this, we frame\\nthe policy search problem as a multi-objective, model-based policy optimization\\nproblem with three objectives: (1) generate maximally novel state trajectories,\\n(2) maximize the expected return and (3) keep the system in state-space regions\\nfor which the model is as accurate as possible. We then optimize these\\nobjectives using a Pareto-based multi-objective optimization algorithm. The\\nexperiments show that Multi-DEX is able to solve sparse reward scenarios (with\\na simulated robotic arm) in much lower interaction time than VIME, TRPO,\\nGEP-PG, CMA-ES and Black-DROPS.',\n",
       "    'author': [{'name': 'Rituraj Kaushik'},\n",
       "     {'name': 'Konstantinos Chatzilygeroudis'},\n",
       "     {'name': 'Jean-Baptiste Mouret'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Conference on Robot Learning (CoRL)- 2018; Code at\\n  https://github.com/resibots/kaushik_2018_multi-dex ; Video at\\n  https://youtu.be/9ZLwUxAAq6M'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the Conference on Robot Learning, PMLR 87:839-855,\\n  2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1806.09351v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1806.09351v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.01521v3',\n",
       "    'updated': '2018-11-04T16:36:39Z',\n",
       "    'published': '2018-07-04T11:23:57Z',\n",
       "    'title': 'Curiosity Driven Exploration of Learned Disentangled Goal Spaces',\n",
       "    'summary': 'Intrinsically motivated goal exploration processes enable agents to\\nautonomously sample goals to explore efficiently complex environments with\\nhigh-dimensional continuous actions. They have been applied successfully to\\nreal world robots to discover repertoires of policies producing a wide\\ndiversity of effects. Often these algorithms relied on engineered goal spaces\\nbut it was recently shown that one can use deep representation learning\\nalgorithms to learn an adequate goal space in simple environments. However, in\\nthe case of more complex environments containing multiple objects or\\ndistractors, an efficient exploration requires that the structure of the goal\\nspace reflects the one of the environment. In this paper we show that using a\\ndisentangled goal space leads to better exploration performances than an\\nentangled goal space. We further show that when the representation is\\ndisentangled, one can leverage it by sampling goals that maximize learning\\nprogress in a modular manner. Finally, we show that the measure of learning\\nprogress, used to drive curiosity-driven exploration, can be used\\nsimultaneously to discover abstract independently controllable features of the\\nenvironment.',\n",
       "    'author': [{'name': 'Adrien Laversanne-Finot'},\n",
       "     {'name': 'Alexandre Péré'},\n",
       "     {'name': 'Pierre-Yves Oudeyer'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The code used in the experiments is available at\\n  https://github.com/flowersteam/Curiosity_Driven_Goal_Exploration'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of The 2nd Conference on Robot Learning, PMLR\\n  87:487-504, 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.01521v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.01521v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1809.04423v2',\n",
       "    'updated': '2019-11-16T13:12:37Z',\n",
       "    'published': '2018-09-11T15:05:12Z',\n",
       "    'title': 'Can a Compact Neuronal Circuit Policy be Re-purposed to Learn Simple\\n  Robotic Control?',\n",
       "    'summary': 'We propose a neural information processing system which is obtained by\\nre-purposing the function of a biological neural circuit model, to govern\\nsimulated and real-world control tasks. Inspired by the structure of the\\nnervous system of the soil-worm, C. elegans, we introduce Neuronal Circuit\\nPolicies (NCPs), defined as the model of biological neural circuits\\nreparameterized for the control of an alternative task. We learn instances of\\nNCPs to control a series of robotic tasks, including the autonomous parking of\\na real-world rover robot. For reconfiguration of the purpose of the neural\\ncircuit, we adopt a search-based optimization algorithm. Neuronal circuit\\npolicies perform on par and in some cases surpass the performance of\\ncontemporary deep learning models with the advantage leveraging significantly\\nfewer learnable parameters and realizing interpretable dynamics at the\\ncell-level.',\n",
       "    'author': [{'name': 'Ramin Hasani'},\n",
       "     {'name': 'Mathias Lechner'},\n",
       "     {'name': 'Alexander Amini'},\n",
       "     {'name': 'Daniela Rus'},\n",
       "     {'name': 'Radu Grosu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: substantial text overlap with arXiv:1803.08554'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1809.04423v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1809.04423v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1812.03955v1',\n",
       "    'updated': '2018-12-10T18:09:10Z',\n",
       "    'published': '2018-12-10T18:09:10Z',\n",
       "    'title': 'Improving Model-Based Control and Active Exploration with Reconstruction\\n  Uncertainty Optimization',\n",
       "    'summary': 'Model based predictions of future trajectories of a dynamical system often\\nsuffer from inaccuracies, forcing model based control algorithms to re-plan\\noften, thus being computationally expensive, suboptimal and not reliable. In\\nthis work, we propose a model agnostic method for estimating the uncertainty of\\na model?s predictions based on reconstruction error, using it in control and\\nexploration. As our experiments show, this uncertainty estimation can be used\\nto improve control performance on a wide variety of environments by choosing\\npredictions of which the model is confident. It can also be used for active\\nlearning to explore more efficiently the environment by planning for\\ntrajectories with high uncertainty, allowing faster model learning.',\n",
       "    'author': [{'name': 'Norman Di Palo'}, {'name': 'Harri Valpola'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1812.03955v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1812.03955v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1901.09006v2',\n",
       "    'updated': '2019-10-07T10:12:47Z',\n",
       "    'published': '2019-01-25T18:11:52Z',\n",
       "    'title': 'On the Limitations of Representing Functions on Sets',\n",
       "    'summary': 'Recent work on the representation of functions on sets has considered the use\\nof summation in a latent space to enforce permutation invariance. In\\nparticular, it has been conjectured that the dimension of this latent space may\\nremain fixed as the cardinality of the sets under consideration increases.\\nHowever, we demonstrate that the analysis leading to this conjecture requires\\nmappings which are highly discontinuous and argue that this is only of limited\\npractical use. Motivated by this observation, we prove that an implementation\\nof this model via continuous mappings (as provided by e.g. neural networks or\\nGaussian processes) actually imposes a constraint on the dimensionality of the\\nlatent space. Practical universal function representation for set inputs can\\nonly be achieved with a latent dimension at least the size of the maximum\\nnumber of input elements.',\n",
       "    'author': [{'name': 'Edward Wagstaff'},\n",
       "     {'name': 'Fabian B. Fuchs'},\n",
       "     {'name': 'Martin Engelcke'},\n",
       "     {'name': 'Ingmar Posner'},\n",
       "     {'name': 'Michael Osborne'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at the International Conference on Machine Learning (2019)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1901.09006v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1901.09006v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.03967v1',\n",
       "    'updated': '2019-06-10T13:31:12Z',\n",
       "    'published': '2019-06-10T13:31:12Z',\n",
       "    'title': 'Autonomous Goal Exploration using Learned Goal Spaces for Visuomotor\\n  Skill Acquisition in Robots',\n",
       "    'summary': 'The automatic and efficient discovery of skills, without supervision, for\\nlong-living autonomous agents, remains a challenge of Artificial Intelligence.\\nIntrinsically Motivated Goal Exploration Processes give learning agents a\\nhuman-inspired mechanism to sequentially select goals to achieve. This approach\\ngives a new perspective on the lifelong learning problem, with promising\\nresults on both simulated and real-world experiments. Until recently, those\\nalgorithms were restricted to domains with experimenter-knowledge, since the\\nGoal Space used by the agents was built on engineered feature extractors. The\\nrecent advances of deep representation learning, enables new ways of designing\\nthose feature extractors, using directly the agent experience. Recent work has\\nshown the potential of those methods on simple yet challenging simulated\\ndomains. In this paper, we present recent results showing the applicability of\\nthose principles on a real-world robotic setup, where a 6-joint robotic arm\\nlearns to manipulate a ball inside an arena, by choosing goals in a space\\nlearned from its past experience.',\n",
       "    'author': [{'name': 'Adrien Laversanne-Finot'},\n",
       "     {'name': 'Alexandre Péré'},\n",
       "     {'name': 'Pierre-Yves Oudeyer'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.03967v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.03967v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.01575v1',\n",
       "    'updated': '2019-09-04T06:58:32Z',\n",
       "    'published': '2019-09-04T06:58:32Z',\n",
       "    'title': 'Learning sparse representations in reinforcement learning',\n",
       "    'summary': \"Reinforcement learning (RL) algorithms allow artificial agents to improve\\ntheir selection of actions to increase rewarding experiences in their\\nenvironments. Temporal Difference (TD) Learning -- a model-free RL method -- is\\na leading account of the midbrain dopamine system and the basal ganglia in\\nreinforcement learning. These algorithms typically learn a mapping from the\\nagent's current sensed state to a selected action (known as a policy function)\\nvia learning a value function (expected future rewards). TD Learning methods\\nhave been very successful on a broad range of control tasks, but learning can\\nbecome intractably slow as the state space of the environment grows. This has\\nmotivated methods that learn internal representations of the agent's state,\\neffectively reducing the size of the state space and restructuring state\\nrepresentations in order to support generalization. However, TD Learning\\ncoupled with an artificial neural network, as a function approximator, has been\\nshown to fail to learn some fairly simple control tasks, challenging this\\nexplanation of reward-based learning. We hypothesize that such failures do not\\narise in the brain because of the ubiquitous presence of lateral inhibition in\\nthe cortex, producing sparse distributed internal representations that support\\nthe learning of expected future reward. The sparse conjunctive representations\\ncan avoid catastrophic interference while still supporting generalization. We\\nprovide support for this conjecture through computational simulations,\\ndemonstrating the benefits of learned sparse representations for three\\nproblematic classic control tasks: Puddle-world, Mountain-car, and Acrobot.\",\n",
       "    'author': [{'name': 'Jacob Rafati'}, {'name': 'David C. Noelle'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.01575v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.01575v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.06832v1',\n",
       "    'updated': '2019-11-15T19:01:21Z',\n",
       "    'published': '2019-11-15T19:01:21Z',\n",
       "    'title': 'Data-efficient Co-Adaptation of Morphology and Behaviour with Deep\\n  Reinforcement Learning',\n",
       "    'summary': 'Humans and animals are capable of quickly learning new behaviours to solve\\nnew tasks. Yet, we often forget that they also rely on a highly specialized\\nmorphology that co-adapted with motor control throughout thousands of years.\\nAlthough compelling, the idea of co-adapting morphology and behaviours in\\nrobots is often unfeasible because of the long manufacturing times, and the\\nneed to re-design an appropriate controller for each morphology. In this paper,\\nwe propose a novel approach to automatically and efficiently co-adapt a robot\\nmorphology and its controller. Our approach is based on recent advances in deep\\nreinforcement learning, and specifically the soft actor critic algorithm. Key\\nto our approach is the possibility of leveraging previously tested morphologies\\nand behaviors to estimate the performance of new candidate morphologies. As\\nsuch, we can make full use of the information available for making more\\ninformed decisions, with the ultimate goal of achieving a more data-efficient\\nco-adaptation (i.e., reducing the number of morphologies and behaviors tested).\\nSimulated experiments show that our approach requires drastically less design\\nprototypes to find good morphology-behaviour combinations, making this method\\nparticularly suitable for future co-adaptation of robot designs in the real\\nworld.',\n",
       "    'author': [{'name': 'Kevin Sebastian Luck'},\n",
       "     {'name': 'Heni Ben Amor'},\n",
       "     {'name': 'Roberto Calandra'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for the Conference on Robot Learning 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.06832v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.06832v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2002.11708v1',\n",
       "    'updated': '2020-02-26T18:57:05Z',\n",
       "    'published': '2020-02-26T18:57:05Z',\n",
       "    'title': 'Generalized Hindsight for Reinforcement Learning',\n",
       "    'summary': 'One of the key reasons for the high sample complexity in reinforcement\\nlearning (RL) is the inability to transfer knowledge from one task to another.\\nIn standard multi-task RL settings, low-reward data collected while trying to\\nsolve one task provides little to no signal for solving that particular task\\nand is hence effectively wasted. However, we argue that this data, which is\\nuninformative for one task, is likely a rich source of information for other\\ntasks. To leverage this insight and efficiently reuse data, we present\\nGeneralized Hindsight: an approximate inverse reinforcement learning technique\\nfor relabeling behaviors with the right tasks. Intuitively, given a behavior\\ngenerated under one task, Generalized Hindsight returns a different task that\\nthe behavior is better suited for. Then, the behavior is relabeled with this\\nnew task before being used by an off-policy RL optimizer. Compared to standard\\nrelabeling techniques, Generalized Hindsight provides a substantially more\\nefficient reuse of samples, which we empirically demonstrate on a suite of\\nmulti-task navigation and manipulation tasks. Videos and code can be accessed\\nhere: https://sites.google.com/view/generalized-hindsight.',\n",
       "    'author': [{'name': 'Alexander C. Li'},\n",
       "     {'name': 'Lerrel Pinto'},\n",
       "     {'name': 'Pieter Abbeel'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2002.11708v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2002.11708v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.10423v1',\n",
       "    'updated': '2020-03-23T17:49:39Z',\n",
       "    'published': '2020-03-23T17:49:39Z',\n",
       "    'title': 'Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement\\n  Learning',\n",
       "    'summary': 'In multi-agent games, the complexity of the environment can grow\\nexponentially as the number of agents increases, so it is particularly\\nchallenging to learn good policies when the agent population is large. In this\\npaper, we introduce Evolutionary Population Curriculum (EPC), a curriculum\\nlearning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by\\nprogressively increasing the population of training agents in a stage-wise\\nmanner. Furthermore, EPC uses an evolutionary approach to fix an objective\\nmisalignment issue throughout the curriculum: agents successfully trained in an\\nearly stage with a small population are not necessarily the best candidates for\\nadapting to later stages with scaled populations. Concretely, EPC maintains\\nmultiple sets of agents in each stage, performs mix-and-match and fine-tuning\\nover these sets and promotes the sets of agents with the best adaptability to\\nthe next stage. We implement EPC on a popular MARL algorithm, MADDPG, and\\nempirically show that our approach consistently outperforms baselines by a\\nlarge margin as the number of agents grows exponentially.',\n",
       "    'author': [{'name': 'Qian Long'},\n",
       "     {'name': 'Zihan Zhou'},\n",
       "     {'name': 'Abhibav Gupta'},\n",
       "     {'name': 'Fei Fang'},\n",
       "     {'name': 'Yi Wu'},\n",
       "     {'name': 'Xiaolong Wang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The project page is https://sites.google.com/view/epciclr2020 .The\\n  source code is released at https://github.com/qian18long/epciclr2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.10423v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.10423v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.11947v1',\n",
       "    'updated': '2020-04-24T19:15:06Z',\n",
       "    'published': '2020-04-24T19:15:06Z',\n",
       "    'title': 'Symbolic Regression Driven by Training Data and Prior Knowledge',\n",
       "    'summary': 'In symbolic regression, the search for analytic models is typically driven\\npurely by the prediction error observed on the training data samples. However,\\nwhen the data samples do not sufficiently cover the input space, the prediction\\nerror does not provide sufficient guidance toward desired models. Standard\\nsymbolic regression techniques then yield models that are partially incorrect,\\nfor instance, in terms of their steady-state characteristics or local behavior.\\nIf these properties were considered already during the search process, more\\naccurate and relevant models could be produced. We propose a multi-objective\\nsymbolic regression approach that is driven by both the training data and the\\nprior knowledge of the properties the desired model should manifest. The\\nproperties given in the form of formal constraints are internally represented\\nby a set of discrete data samples on which candidate models are exactly\\nchecked. The proposed approach was experimentally evaluated on three test\\nproblems with results clearly demonstrating its capability to evolve realistic\\nmodels that fit the training data well while complying with the prior knowledge\\nof the desired model characteristics at the same time. It outperforms standard\\nsymbolic regression by several orders of magnitude in terms of the mean squared\\ndeviation from a reference model.',\n",
       "    'author': [{'name': 'J. Kubalík'},\n",
       "     {'name': 'E. Derner'},\n",
       "     {'name': 'R. Babuška'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1145/3377930.3390152'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1145/3377930.3390152',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2004.11947v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.11947v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 4 figures, 3 tables'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2006.12453v8',\n",
       "    'updated': '2022-02-15T21:44:27Z',\n",
       "    'published': '2020-06-22T17:35:53Z',\n",
       "    'title': 'Fanoos: Multi-Resolution, Multi-Strength, Interactive Explanations for\\n  Learned Systems',\n",
       "    'summary': 'Machine learning is becoming increasingly important to control the behavior\\nof safety and financially critical components in sophisticated environments,\\nwhere the inability to understand learned components in general, and neural\\nnets in particular, poses serious obstacles to their adoption. Explainability\\nand interpretability methods for learned systems have gained considerable\\nacademic attention, but the focus of current approaches on only one aspect of\\nexplanation, at a fixed level of abstraction, and limited if any formal\\nguarantees, prevents those explanations from being digestible by the relevant\\nstakeholders (e.g., end users, certification authorities, engineers) with their\\ndiverse backgrounds and situation-specific needs. We introduce Fanoos, a\\nframework for combining formal verification techniques, heuristic search, and\\nuser interaction to explore explanations at the desired level of granularity\\nand fidelity. We demonstrate the ability of Fanoos to produce and adjust the\\nabstractness of explanations in response to user requests on a learned\\ncontroller for an inverted double pendulum and on a learned CPU usage model.',\n",
       "    'author': [{'name': 'David Bayani',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Carnegie Mellon University'}},\n",
       "     {'name': 'Stefan Mitsch',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Carnegie Mellon University'}}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '60 pages, 20 pages main body, 128 references, 3 figures, 5 tables, 12\\n  pseudocode blocks Update 24 Sep. 2020 : Added a pointer to further, external\\n  content: Append Section E. Update 20 Mar. 2021: Substantial additions.\\n  Further explanations of process, with far more pseudocode. Some corrections\\n  to a previous description; see errata section. Also briefly describe a few\\n  implemented extensions'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2006.12453v8',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2006.12453v8',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'D.2.1; D.2.2; D.2.4; F.3.1; H.1.2; H.2.8; H.3.1; H.4.2; H.5.2; I.2;\\n  I.2.0; I.2.1; I.2.2; I.2.3; I.2.6; I.2.8; I.2.9; I.5.1; I.5.2; I.5.5; I.6',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.09377v1',\n",
       "    'updated': '2020-08-21T08:59:28Z',\n",
       "    'published': '2020-08-21T08:59:28Z',\n",
       "    'title': 'Curriculum Learning with Hindsight Experience Replay for Sequential\\n  Object Manipulation Tasks',\n",
       "    'summary': \"Learning complex tasks from scratch is challenging and often impossible for\\nhumans as well as for artificial agents. A curriculum can be used instead,\\nwhich decomposes a complex task (target task) into a sequence of source tasks\\n(the curriculum). Each source task is a simplified version of the next source\\ntask with increasing complexity. Learning then occurs gradually by training on\\neach source task while using knowledge from the curriculum's prior source\\ntasks. In this study, we present a new algorithm that combines curriculum\\nlearning with Hindsight Experience Replay (HER), to learn sequential object\\nmanipulation tasks for multiple goals and sparse feedback. The algorithm\\nexploits the recurrent structure inherent in many object manipulation tasks and\\nimplements the entire learning process in the original simulation without\\nadjusting it to each source task. We have tested our algorithm on three\\nchallenging throwing tasks and show vast improvements compared to vanilla-HER.\",\n",
       "    'author': [{'name': 'Binyamin Manela'}, {'name': 'Armin Biess'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: text overlap with arXiv:2001.03877'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.09377v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.09377v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.09004v1',\n",
       "    'updated': '2020-11-17T23:40:47Z',\n",
       "    'published': '2020-11-17T23:40:47Z',\n",
       "    'title': 'Explaining Conditions for Reinforcement Learning Behaviors from Real and\\n  Imagined Data',\n",
       "    'summary': 'The deployment of reinforcement learning (RL) in the real world comes with\\nchallenges in calibrating user trust and expectations. As a step toward\\ndeveloping RL systems that are able to communicate their competencies, we\\npresent a method of generating human-interpretable abstract behavior models\\nthat identify the experiential conditions leading to different task execution\\nstrategies and outcomes. Our approach consists of extracting experiential\\nfeatures from state representations, abstracting strategy descriptors from\\ntrajectories, and training an interpretable decision tree that identifies the\\nconditions most predictive of different RL behaviors. We demonstrate our method\\non trajectory data generated from interactions with the environment and on\\nimagined trajectory data that comes from a trained probabilistic world model in\\na model-based RL setting.',\n",
       "    'author': [{'name': 'Aastha Acharya'},\n",
       "     {'name': 'Rebecca Russell'},\n",
       "     {'name': 'Nisar R. Ahmed'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to the Workshop on Challenges of Real-World RL at NeurIPS\\n  2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.09004v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.09004v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.03748v1',\n",
       "    'updated': '2021-06-07T16:15:46Z',\n",
       "    'published': '2021-06-07T16:15:46Z',\n",
       "    'title': 'Towards robust and domain agnostic reinforcement learning competitions',\n",
       "    'summary': 'Reinforcement learning competitions have formed the basis for standard\\nresearch benchmarks, galvanized advances in the state-of-the-art, and shaped\\nthe direction of the field. Despite this, a majority of challenges suffer from\\nthe same fundamental problems: participant solutions to the posed challenge are\\nusually domain-specific, biased to maximally exploit compute resources, and not\\nguaranteed to be reproducible. In this paper, we present a new framework of\\ncompetition design that promotes the development of algorithms that overcome\\nthese barriers. We propose four central mechanisms for achieving this end:\\nsubmission retraining, domain randomization, desemantization through domain\\nobfuscation, and the limitation of competition compute and environment-sample\\nbudget. To demonstrate the efficacy of this design, we proposed, organized, and\\nran the MineRL 2020 Competition on Sample-Efficient Reinforcement Learning. In\\nthis work, we describe the organizational outcomes of the competition and show\\nthat the resulting participant submissions are reproducible, non-specific to\\nthe competition environment, and sample/resource efficient, despite the\\ndifficult competition task.',\n",
       "    'author': [{'name': 'William Hebgen Guss'},\n",
       "     {'name': 'Stephanie Milani'},\n",
       "     {'name': 'Nicholay Topin'},\n",
       "     {'name': 'Brandon Houghton'},\n",
       "     {'name': 'Sharada Mohanty'},\n",
       "     {'name': 'Andrew Melnik'},\n",
       "     {'name': 'Augustin Harter'},\n",
       "     {'name': 'Benoit Buschmaas'},\n",
       "     {'name': 'Bjarne Jaster'},\n",
       "     {'name': 'Christoph Berganski'},\n",
       "     {'name': 'Dennis Heitkamp'},\n",
       "     {'name': 'Marko Henning'},\n",
       "     {'name': 'Helge Ritter'},\n",
       "     {'name': 'Chengjie Wu'},\n",
       "     {'name': 'Xiaotian Hao'},\n",
       "     {'name': 'Yiming Lu'},\n",
       "     {'name': 'Hangyu Mao'},\n",
       "     {'name': 'Yihuan Mao'},\n",
       "     {'name': 'Chao Wang'},\n",
       "     {'name': 'Michal Opanowicz'},\n",
       "     {'name': 'Anssi Kanervisto'},\n",
       "     {'name': 'Yanick Schraner'},\n",
       "     {'name': 'Christian Scheller'},\n",
       "     {'name': 'Xiren Zhou'},\n",
       "     {'name': 'Lu Liu'},\n",
       "     {'name': 'Daichi Nishio'},\n",
       "     {'name': 'Toi Tsuneda'},\n",
       "     {'name': 'Karolis Ramanauskas'},\n",
       "     {'name': 'Gabija Juceviciute'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '20 pages, several figures, published PMLR'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.03748v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.03748v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.13898v2',\n",
       "    'updated': '2022-03-02T20:24:33Z',\n",
       "    'published': '2021-06-25T22:08:51Z',\n",
       "    'title': 'Closed-form Continuous-time Neural Models',\n",
       "    'summary': \"Continuous-time neural processes are performant sequential decision-makers\\nthat are built by differential equations (DE). However, their expressive power\\nwhen they are deployed on computers is bottlenecked by numerical DE solvers.\\nThis limitation has significantly slowed down the scaling and understanding of\\nnumerous natural physical phenomena such as the dynamics of nervous systems.\\nIdeally, we would circumvent this bottleneck by solving the given dynamical\\nsystem in closed form. This is known to be intractable in general. Here, we\\nshow it is possible to closely approximate the interaction between neurons and\\nsynapses -- the building blocks of natural and artificial neural networks --\\nconstructed by liquid time-constant networks (LTCs) efficiently in closed-form.\\nTo this end, we compute a tightly-bounded approximation of the solution of an\\nintegral appearing in LTCs' dynamics, that has had no known closed-form\\nsolution so far. This closed-form solution substantially impacts the design of\\ncontinuous-time and continuous-depth neural models; for instance, since time\\nappears explicitly in closed-form, the formulation relaxes the need for complex\\nnumerical solvers. Consequently, we obtain models that are between one and five\\norders of magnitude faster in training and inference compared to differential\\nequation-based counterparts. More importantly, in contrast to ODE-based\\ncontinuous networks, closed-form networks can scale remarkably well compared to\\nother deep learning instances. Lastly, as these models are derived from liquid\\nnetworks, they show remarkable performance in time series modeling, compared to\\nadvanced recurrent models.\",\n",
       "    'author': [{'name': 'Ramin Hasani'},\n",
       "     {'name': 'Mathias Lechner'},\n",
       "     {'name': 'Alexander Amini'},\n",
       "     {'name': 'Lucas Liebenwein'},\n",
       "     {'name': 'Aaron Ray'},\n",
       "     {'name': 'Max Tschaikowski'},\n",
       "     {'name': 'Gerald Teschl'},\n",
       "     {'name': 'Daniela Rus'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '40 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.13898v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.13898v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'math.DS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.12670v1',\n",
       "    'updated': '2022-03-23T18:52:18Z',\n",
       "    'published': '2022-03-23T18:52:18Z',\n",
       "    'title': 'Competency Assessment for Autonomous Agents using Deep Generative Models',\n",
       "    'summary': 'For autonomous agents to act as trustworthy partners to human users, they\\nmust be able to reliably communicate their competency for the tasks they are\\nasked to perform. Towards this objective, we develop probabilistic world models\\nbased on deep generative modelling that allow for the simulation of agent\\ntrajectories and accurate calculation of tasking outcome probabilities. By\\ncombining the strengths of conditional variational autoencoders with recurrent\\nneural networks, the deep generative world model can probabilistically forecast\\ntrajectories over long horizons to task completion. We show how these\\nforecasted trajectories can be used to calculate outcome probability\\ndistributions, which enable the precise assessment of agent competency for\\nspecific tasks and initial settings.',\n",
       "    'author': [{'name': 'Aastha Acharya'},\n",
       "     {'name': 'Rebecca Russell'},\n",
       "     {'name': 'Nisar R. Ahmed'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.12670v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.12670v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.06650v1',\n",
       "    'updated': '2022-10-13T01:10:41Z',\n",
       "    'published': '2022-10-13T01:10:41Z',\n",
       "    'title': 'Interpreting Neural Policies with Disentangled Tree Representations',\n",
       "    'summary': \"Compact neural networks used in policy learning and closed-loop end-to-end\\ncontrol learn representations from data that encapsulate agent dynamics and\\npotentially the agent-environment's factors of variation. A formal and\\nquantitative understanding and interpretation of these explanatory factors in\\nneural representations is difficult to achieve due to the complex and\\nintertwined correspondence of neural activities with emergent behaviors. In\\nthis paper, we design a new algorithm that programmatically extracts tree\\nrepresentations from compact neural policies, in the form of a set of logic\\nprograms grounded by the world state. To assess how well networks uncover the\\ndynamics of the task and their factors of variation, we introduce\\ninterpretability metrics that measure the disentanglement of learned neural\\ndynamics from a concentration of decisions, mutual information, and modularity\\nperspectives. Moreover, our method allows us to quantify how accurate the\\nextracted decision paths (explanations) are and computes cross-neuron logic\\nconflict. We demonstrate the effectiveness of our approach with several types\\nof compact network architectures on a series of end-to-end learning to control\\ntasks.\",\n",
       "    'author': [{'name': 'Tsun-Hsuan Wang'},\n",
       "     {'name': 'Wei Xiao'},\n",
       "     {'name': 'Tim Seyde'},\n",
       "     {'name': 'Ramin Hasani'},\n",
       "     {'name': 'Daniela Rus'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.06650v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.06650v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.13219v1',\n",
       "    'updated': '2022-11-20T17:13:50Z',\n",
       "    'published': '2022-11-20T17:13:50Z',\n",
       "    'title': 'Automating Rigid Origami Design',\n",
       "    'summary': 'While rigid origami has shown potential in a large diversity of engineering\\napplications, current rigid origami crease pattern designs mostly rely on known\\ntessellations. This leaves a potential gap in performance as the space of\\nrigidly foldable crease patterns is far larger than these tessellations would\\nsuggest. In this work, we build upon the recently developed principle of three\\nunits method to formulate rigid origami design as a discrete optimization\\nproblem. Our implementation allows for a simple definition of diverse\\nobjectives and thereby expands the potential of rigid origami further to\\noptimized, application-specific crease patterns. We benchmark a diverse set of\\nsearch methods in several shape approximation tasks to validate our model and\\nshowcase the flexibility of our formulation through four illustrative case\\nstudies. Results show that using our proposed problem formulation one can\\nsuccessfully approximate a variety of target shapes. Moreover, by specifying\\ncustom reward functions, we can find patterns, which result in novel, foldable\\ndesigns for everyday objects.',\n",
       "    'author': [{'name': 'Jeremia Geiger'},\n",
       "     {'name': 'Karolis Martinkus'},\n",
       "     {'name': 'Oliver Richter'},\n",
       "     {'name': 'Roger Wattenhofer'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.13219v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.13219v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.GR',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.GR',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.07016v1',\n",
       "    'updated': '2023-01-17T17:06:48Z',\n",
       "    'published': '2023-01-17T17:06:48Z',\n",
       "    'title': 'Consciousness is learning: predictive processing systems that learn by\\n  binding may perceive themselves as conscious',\n",
       "    'summary': \"Machine learning algorithms have achieved superhuman performance in specific\\ncomplex domains. Yet learning online from few examples and efficiently\\ngeneralizing across domains remains elusive. In humans such learning proceeds\\nvia declarative memory formation and is closely associated with consciousness.\\nPredictive processing has been advanced as a principled Bayesian inference\\nframework for understanding the cortex as implementing deep generative\\nperceptual models for both sensory data and action control. However, predictive\\nprocessing offers little direct insight into fast compositional learning or the\\nmystery of consciousness. Here we propose that through implementing online\\nlearning by hierarchical binding of unpredicted inferences, a predictive\\nprocessing system may flexibly generalize in novel situations by forming\\nworking memories for perceptions and actions from single examples, which can\\nbecome short- and long-term declarative memories retrievable by associative\\nrecall. We argue that the contents of such working memories are unified yet\\ndifferentiated, can be maintained by selective attention and are consistent\\nwith observations of masking, postdictive perceptual integration, and other\\nparadigm cases of consciousness research. We describe how the brain could have\\nevolved to use perceptual value prediction for reinforcement learning of\\ncomplex action policies simultaneously implementing multiple survival and\\nreproduction strategies. 'Conscious experience' is how such a learning system\\nperceptually represents its own functioning, suggesting an answer to the meta\\nproblem of consciousness. Our proposal naturally unifies feature binding,\\nrecurrent processing, and predictive processing with global workspace, and, to\\na lesser extent, the higher order theories of consciousness.\",\n",
       "    'author': {'name': 'V. A. Aksyuk'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.07016v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.07016v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'q-bio.NC',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'q-bio.NC',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1110.0214v1',\n",
       "    'updated': '2011-10-02T18:59:42Z',\n",
       "    'published': '2011-10-02T18:59:42Z',\n",
       "    'title': 'Eclectic Extraction of Propositional Rules from Neural Networks',\n",
       "    'summary': 'Artificial Neural Network is among the most popular algorithm for supervised\\nlearning. However, Neural Networks have a well-known drawback of being a \"Black\\nBox\" learner that is not comprehensible to the Users. This lack of transparency\\nmakes it unsuitable for many high risk tasks such as medical diagnosis that\\nrequires a rational justification for making a decision. Rule Extraction\\nmethods attempt to curb this limitation by extracting comprehensible rules from\\na trained Network. Many such extraction algorithms have been developed over the\\nyears with their respective strengths and weaknesses. They have been broadly\\ncategorized into three types based on their approach to use internal model of\\nthe Network. Eclectic Methods are hybrid algorithms that combine the other\\napproaches to attain more performance. In this paper, we present an Eclectic\\nmethod called HERETIC. Our algorithm uses Inductive Decision Tree learning\\ncombined with information of the neural network structure for extracting\\nlogical rules. Experiments and theoretical analysis show HERETIC to be better\\nin terms of speed and performance.',\n",
       "    'author': {'name': 'Ridwan Al Iqbal'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICCIT 2011, Dhaka, Bangladesh'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1110.0214v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1110.0214v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1806.08047v2',\n",
       "    'updated': '2018-10-27T05:28:48Z',\n",
       "    'published': '2018-06-21T02:19:50Z',\n",
       "    'title': 'Flexible Neural Representation for Physics Prediction',\n",
       "    'summary': 'Humans have a remarkable capacity to understand the physical dynamics of\\nobjects in their environment, flexibly capturing complex structures and\\ninteractions at multiple levels of detail. Inspired by this ability, we propose\\na hierarchical particle-based object representation that covers a wide variety\\nof types of three-dimensional objects, including both arbitrary rigid\\ngeometrical shapes and deformable materials. We then describe the Hierarchical\\nRelation Network (HRN), an end-to-end differentiable neural network based on\\nhierarchical graph convolution, that learns to predict physical dynamics in\\nthis representation. Compared to other neural network baselines, the HRN\\naccurately handles complex collisions and nonrigid deformations, generating\\nplausible dynamics predictions at long time scales in novel settings, and\\nscaling to large scene configurations. These results demonstrate an\\narchitecture with the potential to form the basis of next-generation physics\\npredictors for use in computer vision, robotics, and quantitative cognitive\\nscience.',\n",
       "    'author': [{'name': 'Damian Mrowca'},\n",
       "     {'name': 'Chengxu Zhuang'},\n",
       "     {'name': 'Elias Wang'},\n",
       "     {'name': 'Nick Haber'},\n",
       "     {'name': 'Li Fei-Fei'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Daniel L. K. Yamins'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '23 pages, 20 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1806.08047v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1806.08047v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.03361v1',\n",
       "    'updated': '2018-07-09T19:53:16Z',\n",
       "    'published': '2018-07-09T19:53:16Z',\n",
       "    'title': 'Weakly-Supervised Convolutional Neural Networks for Multimodal Image\\n  Registration',\n",
       "    'summary': 'One of the fundamental challenges in supervised learning for multimodal image\\nregistration is the lack of ground-truth for voxel-level spatial\\ncorrespondence. This work describes a method to infer voxel-level\\ntransformation from higher-level correspondence information contained in\\nanatomical labels. We argue that such labels are more reliable and practical to\\nobtain for reference sets of image pairs than voxel-level correspondence.\\nTypical anatomical labels of interest may include solid organs, vessels, ducts,\\nstructure boundaries and other subject-specific ad hoc landmarks. The proposed\\nend-to-end convolutional neural network approach aims to predict displacement\\nfields to align multiple labelled corresponding structures for individual image\\npairs during the training, while only unlabelled image pairs are used as the\\nnetwork input for inference. We highlight the versatility of the proposed\\nstrategy, for training, utilising diverse types of anatomical labels, which\\nneed not to be identifiable over all training image pairs. At inference, the\\nresulting 3D deformable image registration algorithm runs in real-time and is\\nfully-automated without requiring any anatomical labels or initialisation.\\nSeveral network architecture variants are compared for registering T2-weighted\\nmagnetic resonance images and 3D transrectal ultrasound images from prostate\\ncancer patients. A median target registration error of 3.6 mm on landmark\\ncentroids and a median Dice of 0.87 on prostate glands are achieved from\\ncross-validation experiments, in which 108 pairs of multimodal images from 76\\npatients were tested with high-quality anatomical labels.',\n",
       "    'author': [{'name': 'Yipeng Hu'},\n",
       "     {'name': 'Marc Modat'},\n",
       "     {'name': 'Eli Gibson'},\n",
       "     {'name': 'Wenqi Li'},\n",
       "     {'name': 'Nooshin Ghavami'},\n",
       "     {'name': 'Ester Bonmati'},\n",
       "     {'name': 'Guotai Wang'},\n",
       "     {'name': 'Steven Bandula'},\n",
       "     {'name': 'Caroline M. Moore'},\n",
       "     {'name': 'Mark Emberton'},\n",
       "     {'name': 'Sébastien Ourselin'},\n",
       "     {'name': 'J. Alison Noble'},\n",
       "     {'name': 'Dean C. Barratt'},\n",
       "     {'name': 'Tom Vercauteren'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.media.2018.07.002'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.media.2018.07.002',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1807.03361v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.03361v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted manuscript in Medical Image Analysis'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.10579v8',\n",
       "    'updated': '2021-05-12T13:20:17Z',\n",
       "    'published': '2019-10-23T14:27:29Z',\n",
       "    'title': 'Autoencoding with a Classifier System',\n",
       "    'summary': 'Autoencoders are data-specific compression algorithms learned automatically\\nfrom examples. The predominant approach has been to construct single large\\nglobal models that cover the domain. However, training and evaluating models of\\nincreasing size comes at the price of additional time and computational cost.\\nConditional computation, sparsity, and model pruning techniques can reduce\\nthese costs while maintaining performance. Learning classifier systems (LCS)\\nare a framework for adaptively subdividing input spaces into an ensemble of\\nsimpler local approximations that together cover the domain. LCS perform\\nconditional computation through the use of a population of individual\\ngating/guarding components, each associated with a local approximation. This\\narticle explores the use of an LCS to adaptively decompose the input domain\\ninto a collection of small autoencoders where local solutions of different\\ncomplexity may emerge. In addition to benefits in convergence time and\\ncomputational cost, it is shown possible to reduce code size as well as the\\nresulting decoder computational cost when compared with the global model\\nequivalent.',\n",
       "    'author': [{'name': 'Richard J. Preen'},\n",
       "     {'name': 'Stewart W. Wilson'},\n",
       "     {'name': 'Larry Bull'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TEVC.2021.3079320'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TEVC.2021.3079320',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1910.10579v8',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.10579v8',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Transactions on Evolutionary Computation (2021)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.05565v1',\n",
       "    'updated': '2020-04-12T08:52:15Z',\n",
       "    'published': '2020-04-12T08:52:15Z',\n",
       "    'title': 'FBNetV2: Differentiable Neural Architecture Search for Spatial and\\n  Channel Dimensions',\n",
       "    'summary': \"Differentiable Neural Architecture Search (DNAS) has demonstrated great\\nsuccess in designing state-of-the-art, efficient neural networks. However,\\nDARTS-based DNAS's search space is small when compared to other search\\nmethods', since all candidate network layers must be explicitly instantiated in\\nmemory. To address this bottleneck, we propose a memory and computationally\\nefficient DNAS variant: DMaskingNAS. This algorithm expands the search space by\\nup to $10^{14}\\\\times$ over conventional DNAS, supporting searches over spatial\\nand channel dimensions that are otherwise prohibitively expensive: input\\nresolution and number of filters. We propose a masking mechanism for feature\\nmap reuse, so that memory and computational costs stay nearly constant as the\\nsearch space expands. Furthermore, we employ effective shape propagation to\\nmaximize per-FLOP or per-parameter accuracy. The searched FBNetV2s yield\\nstate-of-the-art performance when compared with all previous architectures.\\nWith up to 421$\\\\times$ less search cost, DMaskingNAS finds models with 0.9%\\nhigher accuracy, 15% fewer FLOPs than MobileNetV3-Small; and with similar\\naccuracy but 20% fewer FLOPs than Efficient-B0. Furthermore, our FBNetV2\\noutperforms MobileNetV3 by 2.6% in accuracy, with equivalent model size.\\nFBNetV2 models are open-sourced at\\nhttps://github.com/facebookresearch/mobile-vision.\",\n",
       "    'author': [{'name': 'Alvin Wan'},\n",
       "     {'name': 'Xiaoliang Dai'},\n",
       "     {'name': 'Peizhao Zhang'},\n",
       "     {'name': 'Zijian He'},\n",
       "     {'name': 'Yuandong Tian'},\n",
       "     {'name': 'Saining Xie'},\n",
       "     {'name': 'Bichen Wu'},\n",
       "     {'name': 'Matthew Yu'},\n",
       "     {'name': 'Tao Xu'},\n",
       "     {'name': 'Kan Chen'},\n",
       "     {'name': 'Peter Vajda'},\n",
       "     {'name': 'Joseph E. Gonzalez'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 10 figures, accepted to CVPR 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2004.05565v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.05565v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.02909v2',\n",
       "    'updated': '2022-10-18T23:20:41Z',\n",
       "    'published': '2020-12-05T00:32:04Z',\n",
       "    'title': 'What Makes a \"Good\" Data Augmentation in Knowledge Distillation -- A\\n  Statistical Perspective',\n",
       "    'summary': 'Knowledge distillation (KD) is a general neural network training approach\\nthat uses a teacher to guide a student. Existing works mainly study KD from the\\nnetwork output side (e.g., trying to design a better KD loss function), while\\nfew have attempted to understand it from the input side. Especially, its\\ninterplay with data augmentation (DA) has not been well understood. In this\\npaper, we ask: Why do some DA schemes (e.g., CutMix) inherently perform much\\nbetter than others in KD? What makes a \"good\" DA in KD? Our investigation from\\na statistical perspective suggests that a good DA scheme should reduce the\\nvariance of the teacher\\'s mean probability, which will eventually lead to a\\nlower generalization gap for the student. Besides the theoretical\\nunderstanding, we also introduce a new entropy-based data-mixing DA scheme to\\nenhance CutMix. Extensive empirical studies support our claims and demonstrate\\nhow we can harvest considerable performance gains simply by using a better DA\\nscheme in knowledge distillation.',\n",
       "    'author': [{'name': 'Huan Wang'},\n",
       "     {'name': 'Suhas Lohit'},\n",
       "     {'name': 'Mike Jones'},\n",
       "     {'name': 'Yun Fu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by NeurIPS 2022. Project:\\n  https://huanwang.tech/Good-DA-in-KD/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.02909v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.02909v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.01355v1',\n",
       "    'updated': '2021-02-02T07:06:16Z',\n",
       "    'published': '2021-02-02T07:06:16Z',\n",
       "    'title': 'Mining Feature Relationships in Data',\n",
       "    'summary': 'When faced with a new dataset, most practitioners begin by performing\\nexploratory data analysis to discover interesting patterns and characteristics\\nwithin data. Techniques such as association rule mining are commonly applied to\\nuncover relationships between features (attributes) of the data. However,\\nassociation rules are primarily designed for use on binary or categorical data,\\ndue to their use of rule-based machine learning. A large proportion of\\nreal-world data is continuous in nature, and discretisation of such data leads\\nto inaccurate and less informative association rules. In this paper, we propose\\nan alternative approach called feature relationship mining (FRM), which uses a\\ngenetic programming approach to automatically discover symbolic relationships\\nbetween continuous or categorical features in data. To the best of our\\nknowledge, our proposed approach is the first such symbolic approach with the\\ngoal of explicitly discovering relationships between features. Empirical\\ntesting on a variety of real-world datasets shows the proposed method is able\\nto find high-quality, simple feature relationships which can be easily\\ninterpreted and which provide clear and non-trivial insight into data.',\n",
       "    'author': {'name': 'Andrew Lensen'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"16 pages, accepted in EuroGP '21\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.01355v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.01355v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.05451v1',\n",
       "    'updated': '2021-02-10T14:17:51Z',\n",
       "    'published': '2021-02-10T14:17:51Z',\n",
       "    'title': 'Two Novel Performance Improvements for Evolving CNN Topologies',\n",
       "    'summary': 'Convolutional Neural Networks (CNNs) are the state-of-the-art algorithms for\\nthe processing of images. However the configuration and training of these\\nnetworks is a complex task requiring deep domain knowledge, experience and much\\ntrial and error. Using genetic algorithms, competitive CNN topologies for image\\nrecognition can be produced for any specific purpose, however in previous work\\nthis has come at high computational cost. In this work two novel approaches are\\npresented to the utilisation of these algorithms, effective in reducing\\ncomplexity and training time by nearly 20%. This is accomplished via\\nregularisation directly on training time, and the use of partial training to\\nenable early ranking of individual architectures. Both approaches are validated\\non the benchmark CIFAR10 data set, and maintain accuracy.',\n",
       "    'author': [{'name': 'Yaron Strauch',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'University of Southampton'}},\n",
       "     {'name': 'Jo Grundy',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'University of Southampton'}}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to AAAI-21 Workshop W17: Learning Network Architecture\\n  during Training. 5 pages, 4 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.05451v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.05451v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.6; I.2.8; I.2.10; I.5.1; I.5.2; I.6.m',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.06460v3',\n",
       "    'updated': '2022-05-23T21:54:31Z',\n",
       "    'published': '2021-03-11T05:01:52Z',\n",
       "    'title': 'Recent Advances on Neural Network Pruning at Initialization',\n",
       "    'summary': 'Neural network pruning typically removes connections or neurons from a\\npretrained converged model; while a new pruning paradigm, pruning at\\ninitialization (PaI), attempts to prune a randomly initialized network. This\\npaper offers the first survey concentrated on this emerging pruning fashion. We\\nfirst introduce a generic formulation of neural network pruning, followed by\\nthe major classic pruning topics. Then, as the main body of this paper, a\\nthorough and structured literature review of PaI methods is presented,\\nconsisting of two major tracks (sparse training and sparse selection). Finally,\\nwe summarize the surge of PaI compared to PaT and discuss the open problems.\\nApart from the dedicated literature review, this paper also offers a code base\\nfor easy sanity-checking and benchmarking of different PaI methods.',\n",
       "    'author': [{'name': 'Huan Wang'},\n",
       "     {'name': 'Can Qin'},\n",
       "     {'name': 'Yue Bai'},\n",
       "     {'name': 'Yulun Zhang'},\n",
       "     {'name': 'Yun Fu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Accepted in IJCAI'22 Survey Track. Code base:\\n  https://github.com/mingsun-tse/smile-pruning\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.06460v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.06460v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.14399v10',\n",
       "    'updated': '2022-04-05T12:43:01Z',\n",
       "    'published': '2021-05-30T00:55:03Z',\n",
       "    'title': 'Enhanced Isotropy Maximization Loss: Seamless and High-Performance\\n  Out-of-Distribution Detection Simply Replacing the SoftMax Loss',\n",
       "    'summary': \"Current out-of-distribution detection approaches usually present special\\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\\nproduce side effects (e.g., classification accuracy drop and slow/inefficient\\ninferences). Recently, entropic out-of-distribution detection has been proposed\\nas a seamless approach (i.e., a solution that avoids all previously mentioned\\ndrawbacks). The entropic out-of-distribution detection solution uses the IsoMax\\nloss for training and the entropic score for out-of-distribution detection. The\\nIsoMax loss works as a drop-in replacement of the SoftMax loss (i.e., the\\ncombination of the output linear layer, the SoftMax activation, and the\\ncross-entropy loss) because swapping the SoftMax loss with the IsoMax loss\\nrequires no changes in the model's architecture or training\\nprocedures/hyperparameters. In this paper, we perform what we call an\\nisometrization of the distances used in the IsoMax loss. Additionally, we\\npropose replacing the entropic score with the minimum distance score.\\nExperiments showed that these modifications significantly increase\\nout-of-distribution detection performance while keeping the solution seamless.\\nBesides being competitive with or outperforming all major current approaches,\\nthe proposed solution avoids all their current limitations, in addition to\\nbeing much easier to use because only a simple loss replacement for training\\nthe neural network is required. The code to replace the SoftMax loss with the\\nIsoMax+ loss and reproduce the results is available at\\nhttps://github.com/dlmacedo/entropic-out-of-distribution-detection.\",\n",
       "    'author': [{'name': 'David Macêdo'}, {'name': 'Teresa Ludermir'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.14399v10',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.14399v10',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.01544v1',\n",
       "    'updated': '2022-03-03T07:13:39Z',\n",
       "    'published': '2022-03-03T07:13:39Z',\n",
       "    'title': 'Rethinking the role of normalization and residual blocks for spiking\\n  neural networks',\n",
       "    'summary': 'Biologically inspired spiking neural networks (SNNs) are widely used to\\nrealize ultralow-power energy consumption. However, deep SNNs are not easy to\\ntrain due to the excessive firing of spiking neurons in the hidden layers. To\\ntackle this problem, we propose a novel but simple normalization technique\\ncalled postsynaptic potential normalization. This normalization removes the\\nsubtraction term from the standard normalization and uses the second raw moment\\ninstead of the variance as the division term. The spike firing can be\\ncontrolled, enabling the training to proceed appropriating, by conducting this\\nsimple normalization to the postsynaptic potential. The experimental results\\nshow that SNNs with our normalization outperformed other models using other\\nnormalizations. Furthermore, through the pre-activation residual blocks, the\\nproposed model can train with more than 100 layers without other special\\ntechniques dedicated to SNNs.',\n",
       "    'author': [{'name': 'Shin-ichi Ikegawa'},\n",
       "     {'name': 'Ryuji Saiin'},\n",
       "     {'name': 'Yoshihide Sawada'},\n",
       "     {'name': 'Naotake Natori'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages, 9 figures, 3 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.01544v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.01544v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.11887v2',\n",
       "    'updated': '2022-05-19T13:16:23Z',\n",
       "    'published': '2022-04-25T18:00:49Z',\n",
       "    'title': 'Evolutionary latent space search for driving human portrait generation',\n",
       "    'summary': 'This article presents an evolutionary approach for synthetic human portraits\\ngeneration based on the latent space exploration of a generative adversarial\\nnetwork. The idea is to produce different human face images very similar to a\\ngiven target portrait. The approach applies StyleGAN2 for portrait generation\\nand FaceNet for face similarity evaluation. The evolutionary search is based on\\nexploring the real-coded latent space of StyleGAN2. The main results over both\\nsynthetic and real images indicate that the proposed approach generates\\naccurate and diverse solutions, which represent realistic human portraits. The\\nproposed research can contribute to improving the security of face recognition\\nsystems.',\n",
       "    'author': [{'name': 'Benjamín Machín'},\n",
       "     {'name': 'Sergio Nesmachnow'},\n",
       "     {'name': 'Jamal Toutouh'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This paper was accepted and presented during the 2021 IEEE Latin\\n  American Conference on Computational Intelligence (LA-CCI)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.11887v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.11887v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.12755v6',\n",
       "    'updated': '2022-11-15T10:55:02Z',\n",
       "    'published': '2022-05-25T13:10:47Z',\n",
       "    'title': 'An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale\\n  Multitask Learning Systems',\n",
       "    'summary': 'Multitask learning assumes that models capable of learning from multiple\\ntasks can achieve better quality and efficiency via knowledge transfer, a key\\nfeature of human learning. Though, state of the art ML models rely on high\\ncustomization for each task and leverage size and data scale rather than\\nscaling the number of tasks. Also, continual learning, that adds the temporal\\naspect to multitask, is often focused to the study of common pitfalls such as\\ncatastrophic forgetting instead of being studied at a large scale as a critical\\ncomponent to build the next generation artificial intelligence.We propose an\\nevolutionary method capable of generating large scale multitask models that\\nsupport the dynamic addition of new tasks. The generated multitask models are\\nsparsely activated and integrates a task-based routing that guarantees bounded\\ncompute cost and fewer added parameters per task as the model expands.The\\nproposed method relies on a knowledge compartmentalization technique to achieve\\nimmunity against catastrophic forgetting and other common pitfalls such as\\ngradient interference and negative transfer. We demonstrate empirically that\\nthe proposed method can jointly solve and achieve competitive results on\\n69public image classification tasks, for example improving the state of the art\\non a competitive benchmark such as cifar10 by achieving a 15% relative error\\nreduction compared to the best model trained on public data.',\n",
       "    'author': [{'name': 'Andrea Gesmundo'}, {'name': 'Jeff Dean'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.12755v6',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.12755v6',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.01204v2',\n",
       "    'updated': '2022-09-01T00:32:25Z',\n",
       "    'published': '2022-08-02T02:10:00Z',\n",
       "    'title': 'Making a Spiking Net Work: Robust brain-like unsupervised machine\\n  learning',\n",
       "    'summary': 'The surge in interest in Artificial Intelligence (AI) over the past decade\\nhas been driven almost exclusively by advances in Artificial Neural Networks\\n(ANNs). While ANNs set state-of-the-art performance for many previously\\nintractable problems, the use of global gradient descent necessitates large\\ndatasets and computational resources for training, potentially limiting their\\nscalability for real-world domains. Spiking Neural Networks (SNNs) are an\\nalternative to ANNs that use more brain-like artificial neurons and can use\\nlocal unsupervised learning to rapidly discover sparse recognizable features in\\nthe input data. SNNs, however, struggle with dynamical stability and have\\nfailed to match the accuracy of ANNs. Here we show how an SNN can overcome many\\nof the shortcomings that have been identified in the literature, including\\noffering a principled solution to the dynamical \"vanishing spike problem\", to\\noutperform all existing shallow SNNs and equal the performance of an ANN. It\\naccomplishes this while using unsupervised learning with unlabeled data and\\nonly 1/50th of the training epochs (labeled data is used only for a simple\\nlinear readout layer). This result makes SNNs a viable new method for fast,\\naccurate, efficient, explainable, and re-deployable machine learning with\\nunlabeled data.',\n",
       "    'author': [{'name': 'Peter G. Stratton'},\n",
       "     {'name': 'Andrew Wabnitz'},\n",
       "     {'name': 'Chip Essam'},\n",
       "     {'name': 'Allen Cheung'},\n",
       "     {'name': 'Tara J. Hamilton'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages (manuscript), 5 figures, 10 pages (appendix), 11 pages\\n  (extended data)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.01204v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.01204v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.06475v1',\n",
       "    'updated': '2022-07-22T10:58:32Z',\n",
       "    'published': '2022-07-22T10:58:32Z',\n",
       "    'title': 'Guided Evolutionary Neural Architecture Search With Efficient\\n  Performance Estimation',\n",
       "    'summary': 'Neural Architecture Search (NAS) methods have been successfully applied to\\nimage tasks with excellent results. However, NAS methods are often complex and\\ntend to converge to local minima as soon as generated architectures seem to\\nyield good results. This paper proposes GEA, a novel approach for guided NAS.\\nGEA guides the evolution by exploring the search space by generating and\\nevaluating several architectures in each generation at initialisation stage\\nusing a zero-proxy estimator, where only the highest-scoring architecture is\\ntrained and kept for the next generation. Subsequently, GEA continuously\\nextracts knowledge about the search space without increased complexity by\\ngenerating several off-springs from an existing architecture at each\\ngeneration. More, GEA forces exploitation of the most performant architectures\\nby descendant generation while simultaneously driving exploration through\\nparent mutation and favouring younger architectures to the detriment of older\\nones. Experimental results demonstrate the effectiveness of the proposed\\nmethod, and extensive ablation studies evaluate the importance of different\\nparameters. Results show that GEA achieves state-of-the-art results on all data\\nsets of NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101 benchmarks.',\n",
       "    'author': [{'name': 'Vasco Lopes'},\n",
       "     {'name': 'Miguel Santos'},\n",
       "     {'name': 'Bruno Degardin'},\n",
       "     {'name': 'Luís A. Alexandre'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages, 7 figures, 4 tables. arXiv admin note: substantial text\\n  overlap with arXiv:2110.15232'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.06475v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.06475v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.13233v1',\n",
       "    'updated': '2022-09-27T08:10:16Z',\n",
       "    'published': '2022-09-27T08:10:16Z',\n",
       "    'title': 'Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient\\n  Image Classification',\n",
       "    'summary': 'Data-efficient image classification is a challenging task that aims to solve\\nimage classification using small training data. Neural network-based deep\\nlearning methods are effective for image classification, but they typically\\nrequire large-scale training data and have major limitations such as requiring\\nexpertise to design network architectures and having poor interpretability.\\nEvolutionary deep learning is a recent hot topic that combines evolutionary\\ncomputation with deep learning. However, most evolutionary deep learning\\nmethods focus on evolving architectures of neural networks, which still suffer\\nfrom limitations such as poor interpretability. To address this, this paper\\nproposes a new genetic programming-based evolutionary deep learning approach to\\ndata-efficient image classification. The new approach can automatically evolve\\nvariable-length models using many important operators from both image and\\nclassification domains. It can learn different types of image features from\\ncolour or gray-scale images, and construct effective and diverse ensembles for\\nimage classification. A flexible multi-layer representation enables the new\\napproach to automatically construct shallow or deep models/trees for different\\ntasks and perform effective transformations on the input data via multiple\\ninternal nodes. The new approach is applied to solve five image classification\\ntasks with different training set sizes. The results show that it achieves\\nbetter performance in most cases than deep learning methods for data-efficient\\nimage classification. A deep analysis shows that the new approach has good\\nconvergence and evolves models with high interpretability, different\\nlengths/sizes/shapes, and good transferability.',\n",
       "    'author': [{'name': 'Ying Bi'},\n",
       "     {'name': 'Bing Xue'},\n",
       "     {'name': 'Mengjie Zhang'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TEVC.2022.3214503'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TEVC.2022.3214503',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2209.13233v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.13233v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by IEEE Transactions on Evolutionary Computation'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Transactions on Evolutionary Computation, 2022,\\n  https://ieeexplore.ieee.org/document/9919314'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/0911.5372v1',\n",
       "    'updated': '2009-11-28T04:58:38Z',\n",
       "    'published': '2009-11-28T04:58:38Z',\n",
       "    'title': 'Maximin affinity learning of image segmentation',\n",
       "    'summary': 'Images can be segmented by first using a classifier to predict an affinity\\ngraph that reflects the degree to which image pixels must be grouped together\\nand then partitioning the graph to yield a segmentation. Machine learning has\\nbeen applied to the affinity classifier to produce affinity graphs that are\\ngood in the sense of minimizing edge misclassification rates. However, this\\nerror measure is only indirectly related to the quality of segmentations\\nproduced by ultimately partitioning the affinity graph. We present the first\\nmachine learning algorithm for training a classifier to produce affinity graphs\\nthat are good in the sense of producing segmentations that directly minimize\\nthe Rand index, a well known segmentation performance measure. The Rand index\\nmeasures segmentation performance by quantifying the classification of the\\nconnectivity of image pixel pairs after segmentation. By using the simple graph\\npartitioning algorithm of finding the connected components of the thresholded\\naffinity graph, we are able to train an affinity classifier to directly\\nminimize the Rand index of segmentations resulting from the graph partitioning.\\nOur learning algorithm corresponds to the learning of maximin affinities\\nbetween image pixel pairs, which are predictive of the pixel-pair connectivity.',\n",
       "    'author': [{'name': 'Srinivas C. Turaga'},\n",
       "     {'name': 'Kevin L. Briggman'},\n",
       "     {'name': 'Moritz Helmstaedter'},\n",
       "     {'name': 'Winfried Denk'},\n",
       "     {'name': 'H. Sebastian Seung'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/0911.5372v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/0911.5372v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1102.2739v1',\n",
       "    'updated': '2011-02-14T11:40:08Z',\n",
       "    'published': '2011-02-14T11:40:08Z',\n",
       "    'title': 'A General Framework for Development of the Cortex-like Visual Object\\n  Recognition System: Waves of Spikes, Predictive Coding and Universal\\n  Dictionary of Features',\n",
       "    'summary': 'This study is focused on the development of the cortex-like visual object\\nrecognition system. We propose a general framework, which consists of three\\nhierarchical levels (modules). These modules functionally correspond to the V1,\\nV4 and IT areas. Both bottom-up and top-down connections between the\\nhierarchical levels V4 and IT are employed. The higher the degree of matching\\nbetween the input and the preferred stimulus, the shorter the response time of\\nthe neuron. Therefore information about a single stimulus is distributed in\\ntime and is transmitted by the waves of spikes. The reciprocal connections and\\nwaves of spikes implement predictive coding: an initial hypothesis is generated\\non the basis of information delivered by the first wave of spikes and is tested\\nwith the information carried by the consecutive waves. The development is\\nconsidered as extraction and accumulation of features in V4 and objects in IT.\\nOnce stored a feature can be disposed, if rarely activated. This cause update\\nof feature repository. Consequently, objects in IT are also updated. This\\nillustrates the growing process and dynamical change of topological structures\\nof V4, IT and connections between these areas.',\n",
       "    'author': {'name': 'Sergey S. Tarasenko'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1102.2739v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1102.2739v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1103.4487v1',\n",
       "    'updated': '2011-03-23T10:38:50Z',\n",
       "    'published': '2011-03-23T10:38:50Z',\n",
       "    'title': 'Handwritten Digit Recognition with a Committee of Deep Neural Nets on\\n  GPUs',\n",
       "    'summary': 'The competitive MNIST handwritten digit recognition benchmark has a long\\nhistory of broken records since 1998. The most recent substantial improvement\\nby others dates back 7 years (error rate 0.4%) . Recently we were able to\\nsignificantly improve this result, using graphics cards to greatly speed up\\ntraining of simple but deep MLPs, which achieved 0.35%, outperforming all the\\nprevious more complex methods. Here we report another substantial improvement:\\n0.31% obtained using a committee of MLPs.',\n",
       "    'author': [{'name': 'Dan C. Cireşan'},\n",
       "     {'name': 'Ueli Meier'},\n",
       "     {'name': 'Luca M. Gambardella'},\n",
       "     {'name': 'Jürgen Schmidhuber'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 4 figures, 3 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1103.4487v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1103.4487v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1510.04609v1',\n",
       "    'updated': '2015-10-15T16:31:46Z',\n",
       "    'published': '2015-10-15T16:31:46Z',\n",
       "    'title': 'Layer-Specific Adaptive Learning Rates for Deep Networks',\n",
       "    'summary': 'The increasing complexity of deep learning architectures is resulting in\\ntraining time requiring weeks or even months. This slow training is due in part\\nto vanishing gradients, in which the gradients used by back-propagation are\\nextremely large for weights connecting deep layers (layers near the output\\nlayer), and extremely small for shallow layers (near the input layer); this\\nresults in slow learning in the shallow layers. Additionally, it has also been\\nshown that in highly non-convex problems, such as deep neural networks, there\\nis a proliferation of high-error low curvature saddle points, which slows down\\nlearning dramatically. In this paper, we attempt to overcome the two above\\nproblems by proposing an optimization method for training deep neural networks\\nwhich uses learning rates which are both specific to each layer in the network\\nand adaptive to the curvature of the function, increasing the learning rate at\\nlow curvature points. This enables us to speed up learning in the shallow\\nlayers of the network and quickly escape high-error low curvature saddle\\npoints. We test our method on standard image classification datasets such as\\nMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy\\nas well as reduces the required training time over standard algorithms.',\n",
       "    'author': [{'name': 'Bharat Singh'},\n",
       "     {'name': 'Soham De'},\n",
       "     {'name': 'Yangmuzi Zhang'},\n",
       "     {'name': 'Thomas Goldstein'},\n",
       "     {'name': 'Gavin Taylor'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICMLA 2015, deep learning, adaptive learning rates for training,\\n  layer specific learning rate'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1510.04609v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1510.04609v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1511.05547v2',\n",
       "    'updated': '2015-12-09T05:39:43Z',\n",
       "    'published': '2015-11-17T20:53:26Z',\n",
       "    'title': 'Return of Frustratingly Easy Domain Adaptation',\n",
       "    'summary': 'Unlike human learning, machine learning often fails to handle changes between\\ntraining (source) and test (target) input distributions. Such domain shifts,\\ncommon in practical scenarios, severely damage the performance of conventional\\nmachine learning methods. Supervised domain adaptation methods have been\\nproposed for the case when the target data have labels, including some that\\nperform very well despite being \"frustratingly easy\" to implement. However, in\\npractice, the target domain is often unlabeled, requiring unsupervised\\nadaptation. We propose a simple, effective, and efficient method for\\nunsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL\\nminimizes domain shift by aligning the second-order statistics of source and\\ntarget distributions, without requiring any target labels. Even though it is\\nextraordinarily simple--it can be implemented in four lines of Matlab\\ncode--CORAL performs remarkably well in extensive evaluations on standard\\nbenchmark datasets.',\n",
       "    'author': [{'name': 'Baochen Sun'},\n",
       "     {'name': 'Jiashi Feng'},\n",
       "     {'name': 'Kate Saenko'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Fixed typos. Full paper to appear in AAAI-16. Extended Abstract of\\n  the full paper to appear in TASK-CV 2015 workshop'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1511.05547v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1511.05547v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1512.04295v2',\n",
       "    'updated': '2016-01-19T22:56:41Z',\n",
       "    'published': '2015-12-14T13:06:43Z',\n",
       "    'title': 'Origami: A 803 GOp/s/W Convolutional Network Accelerator',\n",
       "    'summary': 'An ever increasing number of computer vision and image/video processing\\nchallenges are being approached using deep convolutional neural networks,\\nobtaining state-of-the-art results in object recognition and detection,\\nsemantic segmentation, action recognition, optical flow and superresolution.\\nHardware acceleration of these algorithms is essential to adopt these\\nimprovements in embedded and mobile computer vision systems. We present a new\\narchitecture, design and implementation as well as the first reported silicon\\nmeasurements of such an accelerator, outperforming previous work in terms of\\npower-, area- and I/O-efficiency. The manufactured device provides up to 196\\nGOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power\\nefficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it\\nthe first architecture scalable to TOp/s performance.',\n",
       "    'author': [{'name': 'Lukas Cavigelli'}, {'name': 'Luca Benini'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TCSVT.2016.2592330'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TCSVT.2016.2592330',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1512.04295v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1512.04295v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'B.7.1; I.2.6', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1605.05359v3',\n",
       "    'updated': '2020-09-21T22:18:31Z',\n",
       "    'published': '2016-05-17T20:44:19Z',\n",
       "    'title': 'Option Discovery in Hierarchical Reinforcement Learning using\\n  Spatio-Temporal Clustering',\n",
       "    'summary': 'This paper introduces an automated skill acquisition framework in\\nreinforcement learning which involves identifying a hierarchical description of\\nthe given task in terms of abstract states and extended actions between\\nabstract states. Identifying such structures present in the task provides ways\\nto simplify and speed up reinforcement learning algorithms. These structures\\nalso help to generalize such algorithms over multiple tasks without relearning\\npolicies from scratch. We use ideas from dynamical systems to find metastable\\nregions in the state space and associate them with abstract states. The\\nspectral clustering algorithm PCCA+ is used to identify suitable abstractions\\naligned to the underlying structure. Skills are defined in terms of the\\nsequence of actions that lead to transitions between such abstract states. The\\nconnectivity information from PCCA+ is used to generate these skills or\\noptions. These skills are independent of the learning task and can be\\nefficiently reused across a variety of tasks defined over the same model. This\\napproach works well even without the exact model of the environment by using\\nsample trajectories to construct an approximate estimate. We also present our\\napproach to scaling the skill acquisition framework to complex tasks with large\\nstate spaces for which we perform state aggregation using the representation\\nlearned from an action conditional video prediction network and use the skill\\nacquisition framework on the aggregated state space.',\n",
       "    'author': [{'name': 'Aravind Srinivas'},\n",
       "     {'name': 'Ramnandan Krishnamurthy'},\n",
       "     {'name': 'Peeyush Kumar'},\n",
       "     {'name': 'Balaraman Ravindran'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Revised version of ICML 16 Abstraction in Reinforcement Learning\\n  workshop paper'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1605.05359v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1605.05359v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1605.06431v2',\n",
       "    'updated': '2016-10-27T00:43:58Z',\n",
       "    'published': '2016-05-20T16:44:03Z',\n",
       "    'title': 'Residual Networks Behave Like Ensembles of Relatively Shallow Networks',\n",
       "    'summary': 'In this work we propose a novel interpretation of residual networks showing\\nthat they can be seen as a collection of many paths of differing length.\\nMoreover, residual networks seem to enable very deep networks by leveraging\\nonly the short paths during training. To support this observation, we rewrite\\nresidual networks as an explicit collection of paths. Unlike traditional\\nmodels, paths through residual networks vary in length. Further, a lesion study\\nreveals that these paths show ensemble-like behavior in the sense that they do\\nnot strongly depend on each other. Finally, and most surprising, most paths are\\nshorter than one might expect, and only the short paths are needed during\\ntraining, as longer paths do not contribute any gradient. For example, most of\\nthe gradient in a residual network with 110 layers comes from paths that are\\nonly 10-34 layers deep. Our results reveal one of the key characteristics that\\nseem to enable the training of very deep networks: Residual networks avoid the\\nvanishing gradient problem by introducing short paths which can carry gradient\\nthroughout the extent of very deep networks.',\n",
       "    'author': [{'name': 'Andreas Veit'},\n",
       "     {'name': 'Michael Wilber'},\n",
       "     {'name': 'Serge Belongie'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NIPS 2016'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1605.06431v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1605.06431v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1605.09304v5',\n",
       "    'updated': '2016-11-23T18:41:12Z',\n",
       "    'published': '2016-05-30T16:22:54Z',\n",
       "    'title': 'Synthesizing the preferred inputs for neurons in neural networks via\\n  deep generator networks',\n",
       "    'summary': 'Deep neural networks (DNNs) have demonstrated state-of-the-art results on\\nmany pattern recognition tasks, especially vision classification problems.\\nUnderstanding the inner workings of such computational brains is both\\nfascinating basic science that is interesting in its own right - similar to why\\nwe study the human brain - and will enable researchers to further improve DNNs.\\nOne path to understanding how a neural network functions internally is to study\\nwhat each of its neurons has learned to detect. One such method is called\\nactivation maximization (AM), which synthesizes an input (e.g. an image) that\\nhighly activates a neuron. Here we dramatically improve the qualitative state\\nof the art of activation maximization by harnessing a powerful, learned prior:\\na deep generator network (DGN). The algorithm (1) generates qualitatively\\nstate-of-the-art synthetic images that look almost real, (2) reveals the\\nfeatures learned by each neuron in an interpretable way, (3) generalizes well\\nto new datasets and somewhat well to different network architectures without\\nrequiring the prior to be relearned, and (4) can be considered as a\\nhigh-quality generative method (in this case, by generating novel, creative,\\ninteresting, recognizable images).',\n",
       "    'author': [{'name': 'Anh Nguyen'},\n",
       "     {'name': 'Alexey Dosovitskiy'},\n",
       "     {'name': 'Jason Yosinski'},\n",
       "     {'name': 'Thomas Brox'},\n",
       "     {'name': 'Jeff Clune'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '29 pages, 35 figures, NIPS camera-ready'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1605.09304v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1605.09304v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1606.02407v1',\n",
       "    'updated': '2016-06-08T05:31:43Z',\n",
       "    'published': '2016-06-08T05:31:43Z',\n",
       "    'title': 'Structured Convolution Matrices for Energy-efficient Deep learning',\n",
       "    'summary': 'We derive a relationship between network representation in energy-efficient\\nneuromorphic architectures and block Toplitz convolutional matrices. Inspired\\nby this connection, we develop deep convolutional networks using a family of\\nstructured convolutional matrices and achieve state-of-the-art trade-off\\nbetween energy efficiency and classification accuracy for well-known image\\nrecognition tasks. We also put forward a novel method to train binary\\nconvolutional networks by utilising an existing connection between\\nnoisy-rectified linear units and binary activations.',\n",
       "    'author': [{'name': 'Rathinakumar Appuswamy'},\n",
       "     {'name': 'Tapan Nayak'},\n",
       "     {'name': 'John Arthur'},\n",
       "     {'name': 'Steven Esser'},\n",
       "     {'name': 'Paul Merolla'},\n",
       "     {'name': 'Jeffrey Mckinstry'},\n",
       "     {'name': 'Timothy Melano'},\n",
       "     {'name': 'Myron Flickner'},\n",
       "     {'name': 'Dharmendra Modha'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1606.02407v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1606.02407v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1607.01719v1',\n",
       "    'updated': '2016-07-06T17:35:55Z',\n",
       "    'published': '2016-07-06T17:35:55Z',\n",
       "    'title': 'Deep CORAL: Correlation Alignment for Deep Domain Adaptation',\n",
       "    'summary': 'Deep neural networks are able to learn powerful representations from large\\nquantities of labeled input data, however they cannot always generalize well\\nacross changes in input distributions. Domain adaptation algorithms have been\\nproposed to compensate for the degradation in performance due to domain shift.\\nIn this paper, we address the case when the target domain is unlabeled,\\nrequiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised\\ndomain adaptation method that aligns the second-order statistics of the source\\nand target distributions with a linear transformation. Here, we extend CORAL to\\nlearn a nonlinear transformation that aligns correlations of layer activations\\nin deep neural networks (Deep CORAL). Experiments on standard benchmark\\ndatasets show state-of-the-art performance.',\n",
       "    'author': [{'name': 'Baochen Sun'}, {'name': 'Kate Saenko'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Extended Abstract'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1607.01719v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1607.01719v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1607.07043v1',\n",
       "    'updated': '2016-07-24T13:39:11Z',\n",
       "    'published': '2016-07-24T13:39:11Z',\n",
       "    'title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "    'summary': '3D action recognition - analysis of human actions based on 3D skeleton data -\\nbecomes popular recently due to its succinctness, robustness, and\\nview-invariant representation. Recent attempts on this problem suggested to\\ndevelop RNN-based learning methods to model the contextual dependency in the\\ntemporal domain. In this paper, we extend this idea to spatio-temporal domains\\nto analyze the hidden sources of action-related information within the input\\ndata over both domains concurrently. Inspired by the graphical structure of the\\nhuman skeleton, we further propose a more powerful tree-structure based\\ntraversal method. To handle the noise and occlusion in 3D skeleton data, we\\nintroduce new gating mechanism within LSTM to learn the reliability of the\\nsequential input data and accordingly adjust its effect on updating the\\nlong-term context information stored in the memory cell. Our method achieves\\nstate-of-the-art performance on 4 challenging benchmark datasets for 3D human\\naction analysis.',\n",
       "    'author': [{'name': 'Jun Liu'},\n",
       "     {'name': 'Amir Shahroudy'},\n",
       "     {'name': 'Dong Xu'},\n",
       "     {'name': 'Gang Wang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1607.07043v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1607.07043v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1611.06791v1',\n",
       "    'updated': '2016-11-21T14:06:48Z',\n",
       "    'published': '2016-11-21T14:06:48Z',\n",
       "    'title': 'Generalized Dropout',\n",
       "    'summary': 'Deep Neural Networks often require good regularizers to generalize well.\\nDropout is one such regularizer that is widely used among Deep Learning\\npractitioners. Recent work has shown that Dropout can also be viewed as\\nperforming Approximate Bayesian Inference over the network parameters. In this\\nwork, we generalize this notion and introduce a rich family of regularizers\\nwhich we call Generalized Dropout. One set of methods in this family, called\\nDropout++, is a version of Dropout with trainable parameters. Classical Dropout\\nemerges as a special case of this method. Another member of this family selects\\nthe width of neural network layers. Experiments show that these methods help in\\nimproving generalization performance over Dropout.',\n",
       "    'author': [{'name': 'Suraj Srinivas'}, {'name': 'R. Venkatesh Babu'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1611.06791v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1611.06791v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1612.01294v1',\n",
       "    'updated': '2016-12-05T10:10:13Z',\n",
       "    'published': '2016-12-05T10:10:13Z',\n",
       "    'title': 'Message Passing Multi-Agent GANs',\n",
       "    'summary': 'Communicating and sharing intelligence among agents is an important facet of\\nachieving Artificial General Intelligence. As a first step towards this\\nchallenge, we introduce a novel framework for image generation: Message Passing\\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\\nrecently been shown to be very effective for image generation and other tasks,\\nthese networks have been limited to mostly single generator-discriminator\\nnetworks. We show that we can obtain multi-agent GANs that communicate through\\nmessage passing to achieve better image generation. The objectives of the\\nindividual agents in this framework are two fold: a co-operation objective and\\na competing objective. The co-operation objective ensures that the message\\nsharing mechanism guides the other generator to generate better than itself\\nwhile the competing objective encourages each generator to generate better than\\nits counterpart. We analyze and visualize the messages that these GANs share\\namong themselves in various scenarios. We quantitatively show that the message\\nsharing formulation serves as a regularizer for the adversarial training.\\nQualitatively, we show that the different generators capture different traits\\nof the underlying data distribution.',\n",
       "    'author': [{'name': 'Arnab Ghosh'},\n",
       "     {'name': 'Viveka Kulharia'},\n",
       "     {'name': 'Vinay Namboodiri'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The first 2 authors contributed equally for this work'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1612.01294v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1612.01294v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1612.02136v5',\n",
       "    'updated': '2017-03-02T06:28:13Z',\n",
       "    'published': '2016-12-07T07:45:38Z',\n",
       "    'title': 'Mode Regularized Generative Adversarial Networks',\n",
       "    'summary': 'Although Generative Adversarial Networks achieve state-of-the-art results on\\na variety of generative tasks, they are regarded as highly unstable and prone\\nto miss modes. We argue that these bad behaviors of GANs are due to the very\\nparticular functional shape of the trained discriminators in high dimensional\\nspaces, which can easily make training stuck or push probability mass in the\\nwrong direction, towards that of higher concentration than that of the data\\ngenerating distribution. We introduce several ways of regularizing the\\nobjective, which can dramatically stabilize the training of GAN models. We also\\nshow that our regularizers can help the fair distribution of probability mass\\nacross the modes of the data generating distribution, during the early phases\\nof training and thus providing a unified solution to the missing modes problem.',\n",
       "    'author': [{'name': 'Tong Che'},\n",
       "     {'name': 'Yanran Li'},\n",
       "     {'name': 'Athul Paul Jacob'},\n",
       "     {'name': 'Yoshua Bengio'},\n",
       "     {'name': 'Wenjie Li'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published as a conference paper at ICLR 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1612.02136v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1612.02136v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1701.05221v5',\n",
       "    'updated': '2017-01-31T12:15:43Z',\n",
       "    'published': '2017-01-18T20:03:12Z',\n",
       "    'title': 'Parsimonious Inference on Convolutional Neural Networks: Learning and\\n  applying on-line kernel activation rules',\n",
       "    'summary': 'A new, radical CNN design approach is presented in this paper, considering\\nthe reduction of the total computational load during inference. This is\\nachieved by a new holistic intervention on both the CNN architecture and the\\ntraining procedure, which targets to the parsimonious inference by learning to\\nexploit or remove the redundant capacity of a CNN architecture. This is\\naccomplished, by the introduction of a new structural element that can be\\ninserted as an add-on to any contemporary CNN architecture, whilst preserving\\nor even improving its recognition accuracy. Our approach formulates a\\nsystematic and data-driven method for developing CNNs that are trained to\\neventually change size and form in real-time during inference, targeting to the\\nsmaller possible computational footprint. Results are provided for the optimal\\nimplementation on a few modern, high-end mobile computing platforms indicating\\na significant speed-up of up to x3 times.',\n",
       "    'author': [{'name': 'I. Theodorakopoulos'},\n",
       "     {'name': 'V. Pothos'},\n",
       "     {'name': 'D. Kastaniotis'},\n",
       "     {'name': 'N. Fragoulis'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '17 pages, 10 figures, 5 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1701.05221v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1701.05221v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T10, 62H30, 68Q32, 68T05, 68Q32, 91E40',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.5; F.1.1; F.4.1; K.3.2; I.4; I.4.8',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1703.03400v3',\n",
       "    'updated': '2017-07-18T16:45:29Z',\n",
       "    'published': '2017-03-09T18:58:03Z',\n",
       "    'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks',\n",
       "    'summary': 'We propose an algorithm for meta-learning that is model-agnostic, in the\\nsense that it is compatible with any model trained with gradient descent and\\napplicable to a variety of different learning problems, including\\nclassification, regression, and reinforcement learning. The goal of\\nmeta-learning is to train a model on a variety of learning tasks, such that it\\ncan solve new learning tasks using only a small number of training samples. In\\nour approach, the parameters of the model are explicitly trained such that a\\nsmall number of gradient steps with a small amount of training data from a new\\ntask will produce good generalization performance on that task. In effect, our\\nmethod trains the model to be easy to fine-tune. We demonstrate that this\\napproach leads to state-of-the-art performance on two few-shot image\\nclassification benchmarks, produces good results on few-shot regression, and\\naccelerates fine-tuning for policy gradient reinforcement learning with neural\\nnetwork policies.',\n",
       "    'author': [{'name': 'Chelsea Finn'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL\\n  results at https://sites.google.com/view/maml, Blog post at\\n  http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1703.03400v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1703.03400v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1704.03079v1',\n",
       "    'updated': '2017-04-10T22:54:38Z',\n",
       "    'published': '2017-04-10T22:54:38Z',\n",
       "    'title': 'WRPN: Training and Inference using Wide Reduced-Precision Networks',\n",
       "    'summary': 'For computer vision applications, prior works have shown the efficacy of\\nreducing the numeric precision of model parameters (network weights) in deep\\nneural networks but also that reducing the precision of activations hurts model\\naccuracy much more than reducing the precision of model parameters. We study\\nschemes to train networks from scratch using reduced-precision activations\\nwithout hurting the model accuracy. We reduce the precision of activation maps\\n(along with model parameters) using a novel quantization scheme and increase\\nthe number of filter maps in a layer, and find that this scheme compensates or\\nsurpasses the accuracy of the baseline full-precision network. As a result, one\\ncan significantly reduce the dynamic memory footprint, memory bandwidth,\\ncomputational energy and speed up the training and inference process with\\nappropriate hardware support. We call our scheme WRPN - wide reduced-precision\\nnetworks. We report results using our proposed schemes and show that our\\nresults are better than previously reported accuracies on ILSVRC-12 dataset\\nwhile being computationally less expensive compared to previously reported\\nreduced-precision networks.',\n",
       "    'author': [{'name': 'Asit Mishra'},\n",
       "     {'name': 'Jeffrey J Cook'},\n",
       "     {'name': 'Eriko Nurvitadhi'},\n",
       "     {'name': 'Debbie Marr'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Under submission to CVPR Workshop'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1704.03079v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1704.03079v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1705.10694v3',\n",
       "    'updated': '2018-02-26T16:51:57Z',\n",
       "    'published': '2017-05-30T15:10:51Z',\n",
       "    'title': 'Deep Learning is Robust to Massive Label Noise',\n",
       "    'summary': 'Deep neural networks trained on large supervised datasets have led to\\nimpressive results in image classification and other tasks. However,\\nwell-annotated datasets can be time-consuming and expensive to collect, lending\\nincreased interest to larger but noisy datasets that are more easily obtained.\\nIn this paper, we show that deep neural networks are capable of generalizing\\nfrom training data for which true labels are massively outnumbered by incorrect\\nlabels. We demonstrate remarkably high test performance after training on\\ncorrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain\\ntest accuracy above 90 percent even after each clean training example has been\\ndiluted with 100 randomly-labeled examples. Such behavior holds across multiple\\npatterns of label noise, even when erroneous labels are biased towards\\nconfusing classes. We show that training in this regime requires a significant\\nbut manageable increase in dataset size that is related to the factor by which\\ncorrect labels have been diluted. Finally, we provide an analysis of our\\nresults that shows how increasing noise decreases the effective batch size.',\n",
       "    'author': [{'name': 'David Rolnick'},\n",
       "     {'name': 'Andreas Veit'},\n",
       "     {'name': 'Serge Belongie'},\n",
       "     {'name': 'Nir Shavit'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1705.10694v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1705.10694v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1707.01357v1',\n",
       "    'updated': '2017-07-05T12:28:43Z',\n",
       "    'published': '2017-07-05T12:28:43Z',\n",
       "    'title': 'Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object\\n  Rotation',\n",
       "    'summary': 'Content-invariance in mapping codes learned by GAEs is a useful feature for\\nvarious relation learning tasks. In this paper we show that the\\ncontent-invariance of mapping codes for images of 2D and 3D rotated objects can\\nbe substantially improved by extending the standard GAE loss (symmetric\\nreconstruction error) with a regularization term that penalizes the symmetric\\ncross-reconstruction error. This error term involves reconstruction of pairs\\nwith mapping codes obtained from other pairs exhibiting similar\\ntransformations. Although this would principally require knowledge of the\\ntransformations exhibited by training pairs, our experiments show that a\\nbootstrapping approach can sidestep this issue, and that the regularization\\nterm can effectively be used in an unsupervised setting.',\n",
       "    'author': [{'name': 'Stefan Lattner'}, {'name': 'Maarten Grachten'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1707.01357v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1707.01357v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1707.03502v2',\n",
       "    'updated': '2017-12-14T03:11:15Z',\n",
       "    'published': '2017-07-12T00:21:04Z',\n",
       "    'title': 'Deep Learning for Sensor-based Activity Recognition: A Survey',\n",
       "    'summary': 'Sensor-based activity recognition seeks the profound high-level knowledge\\nabout human activities from multitudes of low-level sensor readings.\\nConventional pattern recognition approaches have made tremendous progress in\\nthe past years. However, those methods often heavily rely on heuristic\\nhand-crafted feature extraction, which could hinder their generalization\\nperformance. Additionally, existing methods are undermined for unsupervised and\\nincremental learning tasks. Recently, the recent advancement of deep learning\\nmakes it possible to perform automatic high-level feature extraction thus\\nachieves promising performance in many areas. Since then, deep learning based\\nmethods have been widely adopted for the sensor-based activity recognition\\ntasks. This paper surveys the recent advance of deep learning based\\nsensor-based activity recognition. We summarize existing literature from three\\naspects: sensor modality, deep model, and application. We also present detailed\\ninsights on existing work and propose grand challenges for future research.',\n",
       "    'author': [{'name': 'Jindong Wang'},\n",
       "     {'name': 'Yiqiang Chen'},\n",
       "     {'name': 'Shuji Hao'},\n",
       "     {'name': 'Xiaohui Peng'},\n",
       "     {'name': 'Lisha Hu'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.patrec.2018.02.010'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.patrec.2018.02.010',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1707.03502v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1707.03502v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages, 2 figures, and 5 tables; submitted to Pattern Recognition\\n  Letters (second revision)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1708.00631v1',\n",
       "    'updated': '2017-08-02T08:05:09Z',\n",
       "    'published': '2017-08-02T08:05:09Z',\n",
       "    'title': 'On the Importance of Consistency in Training Deep Neural Networks',\n",
       "    'summary': 'We explain that the difficulties of training deep neural networks come from a\\nsyndrome of three consistency issues. This paper describes our efforts in their\\nanalysis and treatment. The first issue is the training speed inconsistency in\\ndifferent layers. We propose to address it with an intuitive,\\nsimple-to-implement, low footprint second-order method. The second issue is the\\nscale inconsistency between the layer inputs and the layer residuals. We\\nexplain how second-order information provides favorable convenience in removing\\nthis roadblock. The third and most challenging issue is the inconsistency in\\nresidual propagation. Based on the fundamental theorem of linear algebra, we\\nprovide a mathematical characterization of the famous vanishing gradient\\nproblem. Thus, an important design principle for future optimization and neural\\nnetwork design is derived. We conclude this paper with the construction of a\\nnovel contractive neural network.',\n",
       "    'author': [{'name': 'Chengxi Ye'},\n",
       "     {'name': 'Yezhou Yang'},\n",
       "     {'name': 'Cornelia Fermuller'},\n",
       "     {'name': 'Yiannis Aloimonos'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1708.00631v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1708.00631v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.03450v1',\n",
       "    'updated': '2017-09-11T15:50:24Z',\n",
       "    'published': '2017-09-11T15:50:24Z',\n",
       "    'title': 'UI-Net: Interactive Artificial Neural Networks for Iterative Image\\n  Segmentation Based on a User Model',\n",
       "    'summary': 'For complex segmentation tasks, fully automatic systems are inherently\\nlimited in their achievable accuracy for extracting relevant objects.\\nEspecially in cases where only few data sets need to be processed for a highly\\naccurate result, semi-automatic segmentation techniques exhibit a clear benefit\\nfor the user. One area of application is medical image processing during an\\nintervention for a single patient. We propose a learning-based cooperative\\nsegmentation approach which includes the computing entity as well as the user\\ninto the task. Our system builds upon a state-of-the-art fully convolutional\\nartificial neural network (FCN) as well as an active user model for training.\\nDuring the segmentation process, a user of the trained system can iteratively\\nadd additional hints in form of pictorial scribbles as seed points into the FCN\\nsystem to achieve an interactive and precise segmentation result. The\\nsegmentation quality of interactive FCNs is evaluated. Iterative FCN approaches\\ncan yield superior results compared to networks without the user input channel\\ncomponent, due to a consistent improvement in segmentation quality after each\\ninteraction.',\n",
       "    'author': [{'name': 'Mario Amrehn'},\n",
       "     {'name': 'Sven Gaube'},\n",
       "     {'name': 'Mathias Unberath'},\n",
       "     {'name': 'Frank Schebesch'},\n",
       "     {'name': 'Tim Horz'},\n",
       "     {'name': 'Maddalena Strumia'},\n",
       "     {'name': 'Stefan Steidl'},\n",
       "     {'name': 'Markus Kowarschik'},\n",
       "     {'name': 'Andreas Maier'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.2312/vcbm.20171248'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.2312/vcbm.20171248',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1709.03450v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.03450v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This work is submitted to the 2017 Eurographics Workshop on Visual\\n  Computing for Biology and Medicine'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Eurographics Workshop on Visual Computing for Biology and Medicine\\n  (2017) 143-147'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T05, 68T45', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.6; I.4.6; I.5.5',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1712.05695v1',\n",
       "    'updated': '2017-12-15T14:56:05Z',\n",
       "    'published': '2017-12-15T14:56:05Z',\n",
       "    'title': 'Lightweight Neural Networks',\n",
       "    'summary': 'Most of the weights in a Lightweight Neural Network have a value of zero,\\nwhile the remaining ones are either +1 or -1. These universal approximators\\nrequire approximately 1.1 bits/weight of storage, posses a quick forward pass\\nand achieve classification accuracies similar to conventional continuous-weight\\nnetworks. Their training regimen focuses on error reduction initially, but\\nlater emphasizes discretization of weights. They ignore insignificant inputs,\\nremove unnecessary weights, and drop unneeded hidden neurons. We have\\nsuccessfully tested them on the MNIST, credit card fraud, and credit card\\ndefaults data sets using networks having 2 to 16 hidden layers and up to 4.4\\nmillion weights.',\n",
       "    'author': {'name': 'Altaf H. Khan'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1712.05695v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1712.05695v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1802.08219v3',\n",
       "    'updated': '2018-05-18T20:09:34Z',\n",
       "    'published': '2018-02-22T18:17:31Z',\n",
       "    'title': 'Tensor field networks: Rotation- and translation-equivariant neural\\n  networks for 3D point clouds',\n",
       "    'summary': 'We introduce tensor field neural networks, which are locally equivariant to\\n3D rotations, translations, and permutations of points at every layer. 3D\\nrotation equivariance removes the need for data augmentation to identify\\nfeatures in arbitrary orientations. Our network uses filters built from\\nspherical harmonics; due to the mathematical consequences of this filter\\nchoice, each layer accepts as input (and guarantees as output) scalars,\\nvectors, and higher-order tensors, in the geometric sense of these terms. We\\ndemonstrate the capabilities of tensor field networks with tasks in geometry,\\nphysics, and chemistry.',\n",
       "    'author': [{'name': 'Nathaniel Thomas'},\n",
       "     {'name': 'Tess Smidt'},\n",
       "     {'name': 'Steven Kearnes'},\n",
       "     {'name': 'Lusann Yang'},\n",
       "     {'name': 'Li Li'},\n",
       "     {'name': 'Kai Kohlhoff'},\n",
       "     {'name': 'Patrick Riley'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'changes for NIPS submission'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1802.08219v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1802.08219v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1803.09760v1',\n",
       "    'updated': '2018-03-26T18:00:07Z',\n",
       "    'published': '2018-03-26T18:00:07Z',\n",
       "    'title': 'Predicting the Future with Transformational States',\n",
       "    'summary': 'An intelligent observer looks at the world and sees not only what is, but\\nwhat is moving and what can be moved. In other words, the observer sees how the\\npresent state of the world can transform in the future. We propose a model that\\npredicts future images by learning to represent the present state and its\\ntransformation given only a sequence of images. To do so, we introduce an\\narchitecture with a latent state composed of two components designed to capture\\n(i) the present image state and (ii) the transformation between present and\\nfuture states, respectively. We couple this latent state with a recurrent\\nneural network (RNN) core that predicts future frames by transforming past\\nstates into future states by applying the accumulated state transformation with\\na learned operator. We describe how this model can be integrated into an\\nencoder-decoder convolutional neural network (CNN) architecture that uses\\nweighted residual connections to integrate representations of the past with\\nrepresentations of the future. Qualitatively, our approach generates image\\nsequences that are stable and capture realistic motion over multiple predicted\\nframes, without requiring adversarial training. Quantitatively, our method\\nachieves prediction results comparable to state-of-the-art results on standard\\nimage prediction benchmarks (Moving MNIST, KTH, and UCF101).',\n",
       "    'author': [{'name': 'Andrew Jaegle'},\n",
       "     {'name': 'Oleh Rybkin'},\n",
       "     {'name': 'Konstantinos G. Derpanis'},\n",
       "     {'name': 'Kostas Daniilidis'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '24 pages, including supplement'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1803.09760v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1803.09760v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.08191v3',\n",
       "    'updated': '2019-01-18T07:58:43Z',\n",
       "    'published': '2018-05-21T17:23:31Z',\n",
       "    'title': 'Hierarchically Structured Reinforcement Learning for Topically Coherent\\n  Visual Story Generation',\n",
       "    'summary': 'We propose a hierarchically structured reinforcement learning approach to\\naddress the challenges of planning for generating coherent multi-sentence\\nstories for the visual storytelling task. Within our framework, the task of\\ngenerating a story given a sequence of images is divided across a two-level\\nhierarchical decoder. The high-level decoder constructs a plan by generating a\\nsemantic concept (i.e., topic) for each image in sequence. The low-level\\ndecoder generates a sentence for each image using a semantic compositional\\nnetwork, which effectively grounds the sentence generation conditioned on the\\ntopic. The two decoders are jointly trained end-to-end using reinforcement\\nlearning. We evaluate our model on the visual storytelling (VIST) dataset.\\nEmpirical results from both automatic and human evaluations demonstrate that\\nthe proposed hierarchically structured reinforced training achieves\\nsignificantly better performance compared to a strong flat deep reinforcement\\nlearning baseline.',\n",
       "    'author': [{'name': 'Qiuyuan Huang'},\n",
       "     {'name': 'Zhe Gan'},\n",
       "     {'name': 'Asli Celikyilmaz'},\n",
       "     {'name': 'Dapeng Wu'},\n",
       "     {'name': 'Jianfeng Wang'},\n",
       "     {'name': 'Xiaodong He'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to AAAI 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1805.08191v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.08191v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.10255v1',\n",
       "    'updated': '2018-05-25T17:12:38Z',\n",
       "    'published': '2018-05-25T17:12:38Z',\n",
       "    'title': 'Parallel Architecture and Hyperparameter Search via Successive Halving\\n  and Classification',\n",
       "    'summary': 'We present a simple and powerful algorithm for parallel black box\\noptimization called Successive Halving and Classification (SHAC). The algorithm\\noperates in $K$ stages of parallel function evaluations and trains a cascade of\\nbinary classifiers to iteratively cull the undesirable regions of the search\\nspace. SHAC is easy to implement, requires no tuning of its own configuration\\nparameters, is invariant to the scale of the objective function and can be\\nbuilt using any choice of binary classifier. We adopt tree-based classifiers\\nwithin SHAC and achieve competitive performance against several strong\\nbaselines for optimizing synthetic functions, hyperparameters and\\narchitectures.',\n",
       "    'author': [{'name': 'Manoj Kumar'},\n",
       "     {'name': 'George E. Dahl'},\n",
       "     {'name': 'Vijay Vasudevan'},\n",
       "     {'name': 'Mohammad Norouzi'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1805.10255v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.10255v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.10795v1',\n",
       "    'updated': '2018-05-28T07:34:14Z',\n",
       "    'published': '2018-05-28T07:34:14Z',\n",
       "    'title': 'Deep Discriminative Latent Space for Clustering',\n",
       "    'summary': 'Clustering is one of the most fundamental tasks in data analysis and machine\\nlearning. It is central to many data-driven applications that aim to separate\\nthe data into groups with similar patterns. Moreover, clustering is a complex\\nprocedure that is affected significantly by the choice of the data\\nrepresentation method. Recent research has demonstrated encouraging clustering\\nresults by learning effectively these representations. In most of these works a\\ndeep auto-encoder is initially pre-trained to minimize a reconstruction loss,\\nand then jointly optimized with clustering centroids in order to improve the\\nclustering objective. Those works focus mainly on the clustering phase of the\\nprocedure, while not utilizing the potential benefit out of the initial phase.\\nIn this paper we propose to optimize an auto-encoder with respect to a\\ndiscriminative pairwise loss function during the auto-encoder pre-training\\nphase. We demonstrate the high accuracy obtained by the proposed method as well\\nas its rapid convergence (e.g. reaching above 92% accuracy on MNIST during the\\npre-training phase, in less than 50 epochs), even with small networks.',\n",
       "    'author': [{'name': 'Elad Tzoreff'},\n",
       "     {'name': 'Olga Kogan'},\n",
       "     {'name': 'Yoni Choukroun'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'A version of this paper has been submitted to NIPS 2018. The paper\\n  contains 9 pages including references, and 4 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1805.10795v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.10795v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1808.06661v2',\n",
       "    'updated': '2018-08-22T01:32:59Z',\n",
       "    'published': '2018-08-20T19:24:45Z',\n",
       "    'title': 'A Hybrid Differential Evolution Approach to Designing Deep Convolutional\\n  Neural Networks for Image Classification',\n",
       "    'summary': 'Convolutional Neural Networks (CNNs) have demonstrated their superiority in\\nimage classification, and evolutionary computation (EC) methods have recently\\nbeen surging to automatically design the architectures of CNNs to save the\\ntedious work of manually designing CNNs. In this paper, a new hybrid\\ndifferential evolution (DE) algorithm with a newly added crossover operator is\\nproposed to evolve the architectures of CNNs of any lengths, which is named\\nDECNN. There are three new ideas in the proposed DECNN method. Firstly, an\\nexisting effective encoding scheme is refined to cater for variable-length CNN\\narchitectures; Secondly, the new mutation and crossover operators are developed\\nfor variable-length DE to optimise the hyperparameters of CNNs; Finally, the\\nnew second crossover is introduced to evolve the depth of the CNN\\narchitectures. The proposed algorithm is tested on six widely-used benchmark\\ndatasets and the results are compared to 12 state-of-the-art methods, which\\nshows the proposed method is vigorously competitive to the state-of-the-art\\nalgorithms. Furthermore, the proposed method is also compared with a method\\nusing particle swarm optimisation with a similar encoding strategy named IPPSO,\\nand the proposed DECNN outperforms IPPSO in terms of the accuracy.',\n",
       "    'author': [{'name': 'Bin Wang'},\n",
       "     {'name': 'Yanan Sun'},\n",
       "     {'name': 'Bing Xue'},\n",
       "     {'name': 'Mengjie Zhang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by The Australasian Joint Conference on Artificial\\n  Intelligence 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1808.06661v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1808.06661v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.13166v1',\n",
       "    'updated': '2018-10-31T09:15:02Z',\n",
       "    'published': '2018-10-31T09:15:02Z',\n",
       "    'title': \"Don't forget, there is more than forgetting: new metrics for Continual\\n  Learning\",\n",
       "    'summary': 'Continual learning consists of algorithms that learn from a stream of\\ndata/tasks continuously and adaptively thought time, enabling the incremental\\ndevelopment of ever more complex knowledge and skills. The lack of consensus in\\nevaluating continual learning algorithms and the almost exclusive focus on\\nforgetting motivate us to propose a more comprehensive set of implementation\\nindependent metrics accounting for several factors we believe have practical\\nimplications worth considering in the deployment of real AI systems that learn\\ncontinually: accuracy or performance over time, backward and forward knowledge\\ntransfer, memory overhead as well as computational efficiency. Drawing\\ninspiration from the standard Multi-Attribute Value Theory (MAVT) we further\\npropose to fuse these metrics into a single score for ranking purposes and we\\nevaluate our proposal with five continual learning strategies on the iCIFAR-100\\ncontinual learning benchmark.',\n",
       "    'author': [{'name': 'Natalia Díaz-Rodríguez'},\n",
       "     {'name': 'Vincenzo Lomonaco'},\n",
       "     {'name': 'David Filliat'},\n",
       "     {'name': 'Davide Maltoni'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.13166v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.13166v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T05, cs.LG, cs.AI, cs.CV, cs.NE, stat.ML',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.13292v5',\n",
       "    'updated': '2019-05-31T12:25:42Z',\n",
       "    'published': '2018-10-31T14:05:44Z',\n",
       "    'title': 'Anomaly Detection With Multiple-Hypotheses Predictions',\n",
       "    'summary': 'In one-class-learning tasks, only the normal case (foreground) can be modeled\\nwith data, whereas the variation of all possible anomalies is too erratic to be\\ndescribed by samples. Thus, due to the lack of representative data, the\\nwide-spread discriminative approaches cannot cover such learning tasks, and\\nrather generative models, which attempt to learn the input density of the\\nforeground, are used. However, generative models suffer from a large input\\ndimensionality (as in images) and are typically inefficient learners. We\\npropose to learn the data distribution of the foreground more efficiently with\\na multi-hypotheses autoencoder. Moreover, the model is criticized by a\\ndiscriminator, which prevents artificial data modes not supported by data, and\\nenforces diversity across hypotheses. Our multiple-hypothesesbased anomaly\\ndetection framework allows the reliable identification of out-of-distribution\\nsamples. For anomaly detection on CIFAR-10, it yields up to 3.9% points\\nimprovement over previously reported results. On a real anomaly detection task,\\nthe approach reduces the error of the baseline models from 6.8% to 1.5%.',\n",
       "    'author': [{'name': 'Duc Tam Nguyen'},\n",
       "     {'name': 'Zhongyu Lou'},\n",
       "     {'name': 'Michael Klar'},\n",
       "     {'name': 'Thomas Brox'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In proceedings of the 36th International Conference on Machine\\n  Learning (ICML), Long Beach, California, PMLR 97, 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.13292v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.13292v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1811.02234v1',\n",
       "    'updated': '2018-11-06T09:01:02Z',\n",
       "    'published': '2018-11-06T09:01:02Z',\n",
       "    'title': 'Semantic bottleneck for computer vision tasks',\n",
       "    'summary': 'This paper introduces a novel method for the representation of images that is\\nsemantic by nature, addressing the question of computation intelligibility in\\ncomputer vision tasks. More specifically, our proposition is to introduce what\\nwe call a semantic bottleneck in the processing pipeline, which is a crossing\\npoint in which the representation of the image is entirely expressed with\\nnatural language , while retaining the efficiency of numerical representations.\\nWe show that our approach is able to generate semantic representations that\\ngive state-of-the-art results on semantic content-based image retrieval and\\nalso perform very well on image classification tasks. Intelligibility is\\nevaluated through user centered experiments for failure detection.',\n",
       "    'author': [{'name': 'Maxime Bucher',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Palaiseau'}},\n",
       "     {'name': 'Stéphane Herbin',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Palaiseau'}},\n",
       "     {'name': 'Frédéric Jurie'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Asian Conference on Computer Vision (ACCV), Dec 2018, Perth,\\n  Australia'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1811.02234v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1811.02234v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.03893v1',\n",
       "    'updated': '2019-03-10T00:51:19Z',\n",
       "    'published': '2019-03-10T00:51:19Z',\n",
       "    'title': 'A Hybrid GA-PSO Method for Evolving Architecture and Short Connections\\n  of Deep Convolutional Neural Networks',\n",
       "    'summary': 'Image classification is a difficult machine learning task, where\\nConvolutional Neural Networks (CNNs) have been applied for over 20 years in\\norder to solve the problem. In recent years, instead of the traditional way of\\nonly connecting the current layer with its next layer, shortcut connections\\nhave been proposed to connect the current layer with its forward layers apart\\nfrom its next layer, which has been proved to be able to facilitate the\\ntraining process of deep CNNs. However, there are various ways to build the\\nshortcut connections, it is hard to manually design the best shortcut\\nconnections when solving a particular problem, especially given the design of\\nthe network architecture is already very challenging.\\n  In this paper, a hybrid evolutionary computation (EC) method is proposed to\\n\\\\textit{automatically} evolve both the architecture of deep CNNs and the\\nshortcut connections. Three major contributions of this work are: Firstly, a\\nnew encoding strategy is proposed to encode a CNN, where the architecture and\\nthe shortcut connections are encoded separately; Secondly, a hybrid two-level\\nEC method, which combines particle swarm optimisation and genetic algorithms,\\nis developed to search for the optimal CNNs; Lastly, an adjustable learning\\nrate is introduced for the fitness evaluations, which provides a better\\nlearning rate for the training process given a fixed number of epochs. The\\nproposed algorithm is evaluated on three widely used benchmark datasets of\\nimage classification and compared with 12 peer Non-EC based competitors and one\\nEC based competitor. The experimental results demonstrate that the proposed\\nmethod outperforms all of the peer competitors in terms of classification\\naccuracy.',\n",
       "    'author': [{'name': 'Bin Wang'},\n",
       "     {'name': 'Yanan Sun'},\n",
       "     {'name': 'Bing Xue'},\n",
       "     {'name': 'Mengjie Zhang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.03893v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.03893v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.09876v1',\n",
       "    'updated': '2019-03-23T20:20:15Z',\n",
       "    'published': '2019-03-23T20:20:15Z',\n",
       "    'title': 'Automated pulmonary nodule detection using 3D deep convolutional neural\\n  networks',\n",
       "    'summary': \"Early detection of pulmonary nodules in computed tomography (CT) images is\\nessential for successful outcomes among lung cancer patients. Much attention\\nhas been given to deep convolutional neural network (DCNN)-based approaches to\\nthis task, but models have relied at least partly on 2D or 2.5D components for\\ninherently 3D data. In this paper, we introduce a novel DCNN approach,\\nconsisting of two stages, that is fully three-dimensional end-to-end and\\nutilizes the state-of-the-art in object detection. First, nodule candidates are\\nidentified with a U-Net-inspired 3D Faster R-CNN trained using online hard\\nnegative mining. Second, false positive reduction is performed by 3D DCNN\\nclassifiers trained on difficult examples produced during candidate screening.\\nFinally, we introduce a method to ensemble models from both stages via\\nconsensus to give the final predictions. By using this framework, we ranked\\nfirst of 2887 teams in Season One of Alibaba's 2017 TianChi AI Competition for\\nHealthcare.\",\n",
       "    'author': [{'name': 'Hao Tang'},\n",
       "     {'name': 'Daniel R. Kim'},\n",
       "     {'name': 'Xiaohui Xie'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI\\n  2018)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.09876v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.09876v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.01390v1',\n",
       "    'updated': '2019-03-27T13:45:49Z',\n",
       "    'published': '2019-03-27T13:45:49Z',\n",
       "    'title': 'Spontaneous Facial Micro-Expression Recognition using 3D Spatiotemporal\\n  Convolutional Neural Networks',\n",
       "    'summary': 'Facial expression recognition in videos is an active area of research in\\ncomputer vision. However, fake facial expressions are difficult to be\\nrecognized even by humans. On the other hand, facial micro-expressions\\ngenerally represent the actual emotion of a person, as it is a spontaneous\\nreaction expressed through human face. Despite of a few attempts made for\\nrecognizing micro-expressions, still the problem is far from being a solved\\nproblem, which is depicted by the poor rate of accuracy shown by the\\nstate-of-the-art methods. A few CNN based approaches are found in the\\nliterature to recognize micro-facial expressions from still images. Whereas, a\\nspontaneous micro-expression video contains multiple frames that have to be\\nprocessed together to encode both spatial and temporal information. This paper\\nproposes two 3D-CNN methods: MicroExpSTCNN and MicroExpFuseNet, for spontaneous\\nfacial micro-expression recognition by exploiting the spatiotemporal\\ninformation in CNN framework. The MicroExpSTCNN considers the full spatial\\ninformation, whereas the MicroExpFuseNet is based on the 3D-CNN feature fusion\\nof the eyes and mouth regions. The experiments are performed over CAS(ME)^2 and\\nSMIC micro-expression databases. The proposed MicroExpSTCNN model outperforms\\nthe state-of-the-art methods.',\n",
       "    'author': [{'name': 'Sai Prasanna Teja Reddy'},\n",
       "     {'name': 'Surya Teja Karri'},\n",
       "     {'name': 'Shiv Ram Dubey'},\n",
       "     {'name': 'Snehasis Mukherjee'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted in 2019 International Joint Conference on Neural Networks\\n  (IJCNN)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.01390v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.01390v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.03816v1',\n",
       "    'updated': '2019-04-08T03:21:25Z',\n",
       "    'published': '2019-04-08T03:21:25Z',\n",
       "    'title': 'Towards Real-Time Automatic Portrait Matting on Mobile Devices',\n",
       "    'summary': 'We tackle the problem of automatic portrait matting on mobile devices. The\\nproposed model is aimed at attaining real-time inference on mobile devices with\\nminimal degradation of model performance. Our model MMNet, based on\\nmulti-branch dilated convolution with linear bottleneck blocks, outperforms the\\nstate-of-the-art model and is orders of magnitude faster. The model can be\\naccelerated four times to attain 30 FPS on Xiaomi Mi 5 device with moderate\\nincrease in the gradient error. Under the same conditions, our model has an\\norder of magnitude less number of parameters and is faster than Mobile\\nDeepLabv3 while maintaining comparable performance. The accompanied\\nimplementation can be found at \\\\url{https://github.com/hyperconnect/MMNet}.',\n",
       "    'author': [{'name': 'Seokjun Seo'},\n",
       "     {'name': 'Seungwoo Choi'},\n",
       "     {'name': 'Martin Kersner'},\n",
       "     {'name': 'Beomjun Shin'},\n",
       "     {'name': 'Hyungsuk Yoon'},\n",
       "     {'name': 'Hyeongmin Byun'},\n",
       "     {'name': 'Sungjoo Ha'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.03816v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.03816v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.04175v1',\n",
       "    'updated': '2019-05-10T13:56:52Z',\n",
       "    'published': '2019-05-10T13:56:52Z',\n",
       "    'title': 'AI in the media and creative industries',\n",
       "    'summary': 'Thanks to the Big Data revolution and increasing computing capacities,\\nArtificial Intelligence (AI) has made an impressive revival over the past few\\nyears and is now omnipresent in both research and industry. The creative\\nsectors have always been early adopters of AI technologies and this continues\\nto be the case. As a matter of fact, recent technological developments keep\\npushing the boundaries of intelligent systems in creative applications: the\\ncritically acclaimed movie \"Sunspring\", released in 2016, was entirely written\\nby AI technology, and the first-ever Music Album, called \"Hello World\",\\nproduced using AI has been released this year. Simultaneously, the exploratory\\nnature of the creative process is raising important technical challenges for AI\\nsuch as the ability for AI-powered techniques to be accurate under limited data\\nresources, as opposed to the conventional \"Big Data\" approach, or the ability\\nto process, analyse and match data from multiple modalities (text, sound,\\nimages, etc.) at the same time. The purpose of this white paper is to\\nunderstand future technological advances in AI and their growing impact on\\ncreative industries. This paper addresses the following questions: Where does\\nAI operate in creative Industries? What is its operative role? How will AI\\ntransform creative industries in the next ten years? This white paper aims to\\nprovide a realistic perspective of the scope of AI actions in creative\\nindustries, proposes a vision of how this technology could contribute to\\nresearch and development works in such context, and identifies research and\\ndevelopment challenges.',\n",
       "    'author': [{'name': 'Giuseppe Amato',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CNR PISA'}},\n",
       "     {'name': 'Malte Behrmann',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'PANAMA'}},\n",
       "     {'name': 'Frédéric Bimbot',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'PANAMA'}},\n",
       "     {'name': 'Baptiste Caramiaux',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'LRI, EX-SITU'}},\n",
       "     {'name': 'Fabrizio Falchi',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CNR PISA'}},\n",
       "     {'name': 'Ander Garcia',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Inria'}},\n",
       "     {'name': 'Joost Geurts',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Inria'}},\n",
       "     {'name': 'Jaume Gibert',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'LinkMedia'}},\n",
       "     {'name': 'Guillaume Gravier',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'LinkMedia'}},\n",
       "     {'name': 'Hadmut Holken',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'HKU'}},\n",
       "     {'name': 'Hartmut Koenitz',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'HKU'}},\n",
       "     {'name': 'Sylvain Lefebvre',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'MFX'}},\n",
       "     {'name': 'Antoine Liutkus',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'LORIA, ZENITH'}},\n",
       "     {'name': 'Fabien Lotte',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Potioc, LaBRI'}},\n",
       "     {'name': 'Andrew Perkis',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'NTNU'}},\n",
       "     {'name': 'Rafael Redondo',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'FEP'}},\n",
       "     {'name': 'Enrico Turrin',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'FEP'}},\n",
       "     {'name': 'Thierry Vieville',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Mnemosyne'}},\n",
       "     {'name': 'Emmanuel Vincent',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'MULTISPEECH'}}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.04175v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.04175v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.01558v2',\n",
       "    'updated': '2020-10-28T15:53:55Z',\n",
       "    'published': '2019-06-04T16:21:46Z',\n",
       "    'title': 'Disentangling neural mechanisms for perceptual grouping',\n",
       "    'summary': 'Forming perceptual groups and individuating objects in visual scenes is an\\nessential step towards visual intelligence. This ability is thought to arise in\\nthe brain from computations implemented by bottom-up, horizontal, and top-down\\nconnections between neurons. However, the relative contributions of these\\nconnections to perceptual grouping are poorly understood. We address this\\nquestion by systematically evaluating neural network architectures featuring\\ncombinations bottom-up, horizontal, and top-down connections on two synthetic\\nvisual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for\\nperceptual grouping. We show that increasing the difficulty of either task\\nstrains learning for networks that rely solely on bottom-up connections.\\nHorizontal connections resolve straining on tasks with Gestalt cues by\\nsupporting incremental grouping, whereas top-down connections rescue learning\\non tasks with high-level object cues by modifying coarse predictions about the\\nposition of the target object. Our findings dissociate the computational roles\\nof bottom-up, horizontal and top-down connectivity, and demonstrate how a model\\nfeaturing all of these interactions can more flexibly learn to form perceptual\\ngroups.',\n",
       "    'author': [{'name': 'Junkyung Kim'},\n",
       "     {'name': 'Drew Linsley'},\n",
       "     {'name': 'Kalpit Thakkar'},\n",
       "     {'name': 'Thomas Serre'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in ICLR 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.01558v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.01558v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.02244v2',\n",
       "    'updated': '2019-11-21T10:48:51Z',\n",
       "    'published': '2019-10-05T10:36:47Z',\n",
       "    'title': 'Yet another but more efficient black-box adversarial attack: tiling and\\n  evolution strategies',\n",
       "    'summary': 'We introduce a new black-box attack achieving state of the art performances.\\nOur approach is based on a new objective function, borrowing ideas from\\n$\\\\ell_\\\\infty$-white box attacks, and particularly designed to fit\\nderivative-free optimization requirements. It only requires to have access to\\nthe logits of the classifier without any other information which is a more\\nrealistic scenario. Not only we introduce a new objective function, we extend\\nprevious works on black box adversarial attacks to a larger spectrum of\\nevolution strategies and other derivative-free optimization methods. We also\\nhighlight a new intriguing property that deep neural networks are not robust to\\nsingle shot tiled attacks. Our models achieve, with a budget limited to\\n$10,000$ queries, results up to $99.2\\\\%$ of success rate against InceptionV3\\nclassifier with $630$ queries to the network on average in the untargeted\\nattacks setting, which is an improvement by $90$ queries of the current state\\nof the art. In the targeted setting, we are able to reach, with a limited\\nbudget of $100,000$, $100\\\\%$ of success rate with a budget of $6,662$ queries\\non average, i.e. we need $800$ queries less than the current state of the art.',\n",
       "    'author': [{'name': 'Laurent Meunier'},\n",
       "     {'name': 'Jamal Atif'},\n",
       "     {'name': 'Olivier Teytaud'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1910.02244v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.02244v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.13351v1',\n",
       "    'updated': '2019-09-26T09:17:01Z',\n",
       "    'published': '2019-09-26T09:17:01Z',\n",
       "    'title': 'Admiring the Great Mountain: A Celebration Special Issue in Honor of\\n  Stephen Grossbergs 80th Birthday',\n",
       "    'summary': 'This editorial summarizes selected key contributions of Prof. Stephen\\nGrossberg and describes the papers in this 80th birthday special issue in his\\nhonor. His productivity, creativity, and vision would each be enough to mark a\\nscientist of the first caliber. In combination, they have resulted in\\ncontributions that have changed the entire discipline of neural networks.\\nGrossberg has been tremendously influential in engineering, dynamical systems,\\nand artificial intelligence as well. Indeed, he has been one of the most\\nimportant mentors and role models in my career, and has done so with\\nextraordinary generosity and encouragement. All authors in this special issue\\nhave taken great pleasure in hereby commemorating his extraordinary career and\\ncontributions.',\n",
       "    'author': {'name': 'Donald C. Wunsch'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Editorial for Special Issue of Neural Networks in honor of\\n  Grossberg's 80th birthday\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1910.13351v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.13351v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.13135v3',\n",
       "    'updated': '2021-04-14T18:08:35Z',\n",
       "    'published': '2019-11-29T15:02:28Z',\n",
       "    'title': 'Radon Sobolev Variational Auto-Encoders',\n",
       "    'summary': 'The quality of generative models (such as Generative adversarial networks and\\nVariational Auto-Encoders) depends heavily on the choice of a good probability\\ndistance. However some popular metrics like the Wasserstein or the Sliced\\nWasserstein distances, the Jensen-Shannon divergence, the Kullback-Leibler\\ndivergence, lack convenient properties such as (geodesic) convexity, fast\\nevaluation and so on. To address these shortcomings, we introduce a class of\\ndistances that have built-in convexity. We investigate the relationship with\\nsome known paradigms (sliced distances - a synonym for Radon distances -,\\nreproducing kernel Hilbert spaces, energy distances). The distances are shown\\nto possess fast implementations and are included in an adapted Variational\\nAuto-Encoder termed Radon Sobolev Variational Auto-Encoder (RS-VAE) which\\nproduces high quality results on standard generative datasets.\\n  Keywords: Variational Auto-Encoder; Generative model; Sobolev spaces; Radon\\nSobolev Variational Auto-Encoder;',\n",
       "    'author': {'name': 'Gabriel Turinici',\n",
       "     'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "      '#text': 'CEREMADE, Université Paris Dauphine - PSL'}},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.13135v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.13135v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2002.02210v3',\n",
       "    'updated': '2021-02-08T15:17:27Z',\n",
       "    'published': '2020-02-06T12:02:30Z',\n",
       "    'title': 'From Data to Actions in Intelligent Transportation Systems: a\\n  Prescription of Functional Requirements for Model Actionability',\n",
       "    'summary': \"Advances in Data Science permeate every field of Transportation Science and\\nEngineering, resulting in developments in the transportation sector that {are}\\ndata-driven. Nowadays, Intelligent Transportation Systems (ITS) could be\\narguably approached as a ``story'' intensively producing and consuming large\\namounts of data. A~diversity of sensing devices densely spread over the\\ninfrastructure, vehicles or the travelers' personal devices act as sources of\\ndata flows that are eventually fed {into} software running on automatic\\ndevices, actuators or control systems producing, in~turn, complex information\\nflows {among} users, traffic managers, data analysts, traffic modeling\\nscientists, etc. These~information flows provide enormous opportunities to\\nimprove model development and decision-making. This work aims to describe how\\ndata, coming from diverse ITS sources, can be used to learn and adapt\\ndata-driven models for efficiently operating ITS assets, systems and processes;\\nin~other words, for data-based models to fully become \\\\emph{actionable}.\\nGrounded in this described data modeling pipeline for ITS, we~define the\\ncharacteristics, engineering requisites and challenges intrinsic to its three\\ncompounding stages, namely, data fusion, adaptive learning and model\\nevaluation. We~deliberately generalize model learning to be adaptive, since,\\nin~the core of our paper is the firm conviction that most learners will have to\\nadapt to the ever-changing phenomenon scenario underlying the majority of ITS\\napplications. Finally, we~provide a prospect of current research lines within\\nData Science that can bring notable advances to data-based ITS modeling, which\\nwill eventually bridge the gap towards the practicality and actionability of\\nsuch models.\",\n",
       "    'author': [{'name': 'Ibai Lana'},\n",
       "     {'name': 'Javier J. Sanchez-Medina'},\n",
       "     {'name': 'Eleni I. Vlahogianni'},\n",
       "     {'name': 'Javier Del Ser'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '40 pages, 3 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2002.02210v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2002.02210v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2007.04756v1',\n",
       "    'updated': '2020-07-09T13:06:07Z',\n",
       "    'published': '2020-07-09T13:06:07Z',\n",
       "    'title': 'Learning to Prune Deep Neural Networks via Reinforcement Learning',\n",
       "    'summary': 'This paper proposes PuRL - a deep reinforcement learning (RL) based algorithm\\nfor pruning neural networks. Unlike current RL based model compression\\napproaches where feedback is given only at the end of each episode to the\\nagent, PuRL provides rewards at every pruning step. This enables PuRL to\\nachieve sparsity and accuracy comparable to current state-of-the-art methods,\\nwhile having a much shorter training cycle. PuRL achieves more than 80%\\nsparsity on the ResNet-50 model while retaining a Top-1 accuracy of 75.37% on\\nthe ImageNet dataset. Through our experiments we show that PuRL is also able to\\nsparsify already efficient architectures like MobileNet-V2. In addition to\\nperformance characterisation experiments, we also provide a discussion and\\nanalysis of the various RL design choices that went into the tuning of the\\nMarkov Decision Process underlying PuRL. Lastly, we point out that PuRL is\\nsimple to use and can be easily adapted for various architectures.',\n",
       "    'author': [{'name': 'Manas Gupta'},\n",
       "     {'name': 'Siddharth Aravindan'},\n",
       "     {'name': 'Aleksandra Kalisz'},\n",
       "     {'name': 'Vijay Chandrasekhar'},\n",
       "     {'name': 'Lin Jie'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at the ICML 2020 Workshop on Automated Machine Learning\\n  (AutoML 2020)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2007.04756v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2007.04756v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2007.05606v1',\n",
       "    'updated': '2020-07-10T20:54:42Z',\n",
       "    'published': '2020-07-10T20:54:42Z',\n",
       "    'title': 'Neuromorphic Processing and Sensing: Evolutionary Progression of AI to\\n  Spiking',\n",
       "    'summary': 'The increasing rise in machine learning and deep learning applications is\\nrequiring ever more computational resources to successfully meet the growing\\ndemands of an always-connected, automated world. Neuromorphic technologies\\nbased on Spiking Neural Network algorithms hold the promise to implement\\nadvanced artificial intelligence using a fraction of the computations and power\\nrequirements by modeling the functioning, and spiking, of the human brain. With\\nthe proliferation of tools and platforms aiding data scientists and machine\\nlearning engineers to develop the latest innovations in artificial and deep\\nneural networks, a transition to a new paradigm will require building from the\\ncurrent well-established foundations. This paper explains the theoretical\\nworkings of neuromorphic technologies based on spikes, and overviews the\\nstate-of-art in hardware processors, software platforms and neuromorphic\\nsensing devices. A progression path is paved for current machine learning\\nspecialists to update their skillset, as well as classification or predictive\\nmodels from the current generation of deep neural networks to SNNs. This can be\\nachieved by leveraging existing, specialized hardware in the form of SpiNNaker\\nand the Nengo migration toolkit. First-hand, experimental results of converting\\na VGG-16 neural network to an SNN are shared. A forward gaze into industrial,\\nmedical and commercial applications that can readily benefit from SNNs wraps up\\nthis investigation into the neuromorphic computing future.',\n",
       "    'author': [{'name': 'Philippe Reiter'},\n",
       "     {'name': 'Geet Rose Jose'},\n",
       "     {'name': 'Spyridon Bizmpikis'},\n",
       "     {'name': 'Ionela-Ancuţa Cîrjilă'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '15 pages, 13 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2007.05606v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2007.05606v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.03863v1',\n",
       "    'updated': '2020-09-08T16:59:28Z',\n",
       "    'published': '2020-09-08T16:59:28Z',\n",
       "    'title': 'TanhSoft -- a family of activation functions combining Tanh and Softplus',\n",
       "    'summary': 'Deep learning at its core, contains functions that are composition of a\\nlinear transformation with a non-linear function known as activation function.\\nIn past few years, there is an increasing interest in construction of novel\\nactivation functions resulting in better learning. In this work, we propose a\\nfamily of novel activation functions, namely TanhSoft, with four undetermined\\nhyper-parameters of the form\\ntanh({\\\\alpha}x+{\\\\beta}e^{{\\\\gamma}x})ln({\\\\delta}+e^x) and tune these\\nhyper-parameters to obtain activation functions which are shown to outperform\\nseveral well known activation functions. For instance, replacing ReLU with\\nxtanh(0.6e^x)improves top-1 classification accuracy on CIFAR-10 by 0.46% for\\nDenseNet-169 and 0.7% for Inception-v3 while with tanh(0.87x)ln(1 +e^x) top-1\\nclassification accuracy on CIFAR-100 improves by 1.24% for DenseNet-169 and\\n2.57% for SimpleNet model.',\n",
       "    'author': [{'name': 'Koushik Biswas'},\n",
       "     {'name': 'Sandeep Kumar'},\n",
       "     {'name': 'Shilpak Banerjee'},\n",
       "     {'name': 'Ashish Kumar Pandey'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2009.03863v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.03863v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.08497v1',\n",
       "    'updated': '2020-09-17T18:47:06Z',\n",
       "    'published': '2020-09-17T18:47:06Z',\n",
       "    'title': 'The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons\\n  from Infant Learning',\n",
       "    'summary': \"After a surge in popularity of supervised Deep Learning, the desire to reduce\\nthe dependence on curated, labelled data sets and to leverage the vast\\nquantities of unlabelled data available recently triggered renewed interest in\\nunsupervised learning algorithms. Despite a significantly improved performance\\ndue to approaches such as the identification of disentangled latent\\nrepresentations, contrastive learning, and clustering optimisations, the\\nperformance of unsupervised machine learning still falls short of its\\nhypothesised potential. Machine learning has previously taken inspiration from\\nneuroscience and cognitive science with great success. However, this has mostly\\nbeen based on adult learners with access to labels and a vast amount of prior\\nknowledge. In order to push unsupervised machine learning forward, we argue\\nthat developmental science of infant cognition might hold the key to unlocking\\nthe next generation of unsupervised learning approaches. Conceptually, human\\ninfant learning is the closest biological parallel to artificial unsupervised\\nlearning, as infants too must learn useful representations from unlabelled\\ndata. In contrast to machine learning, these new representations are learned\\nrapidly and from relatively few examples. Moreover, infants learn robust\\nrepresentations that can be used flexibly and efficiently in a number of\\ndifferent tasks and contexts. We identify five crucial factors enabling\\ninfants' quality and speed of learning, assess the extent to which these have\\nalready been exploited in machine learning, and propose how further adoption of\\nthese factors can give rise to previously unseen performance levels in\\nunsupervised learning.\",\n",
       "    'author': [{'name': 'Lorijn Zaadnoordijk'},\n",
       "     {'name': 'Tarek R. Besold'},\n",
       "     {'name': 'Rhodri Cusack'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2009.08497v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.08497v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.12930v2',\n",
       "    'updated': '2021-03-08T15:10:29Z',\n",
       "    'published': '2020-11-25T18:27:05Z',\n",
       "    'title': 'Unsupervised Object Keypoint Learning using Local Spatial Predictability',\n",
       "    'summary': 'We propose PermaKey, a novel approach to representation learning based on\\nobject keypoints. It leverages the predictability of local image regions from\\nspatial neighborhoods to identify salient regions that correspond to object\\nparts, which are then converted to keypoints. Unlike prior approaches, it\\nutilizes predictability to discover object keypoints, an intrinsic property of\\nobjects. This ensures that it does not overly bias keypoints to focus on\\ncharacteristics that are not unique to objects, such as movement, shape, colour\\netc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints\\ncorresponding to the most salient object parts and is robust to certain visual\\ndistractors. Further, on downstream RL tasks in the Atari domain we demonstrate\\nhow agents equipped with our keypoints outperform those using competing\\nalternatives, even on challenging environments with moving backgrounds or\\ndistractor objects.',\n",
       "    'author': [{'name': 'Anand Gopalakrishnan'},\n",
       "     {'name': 'Sjoerd van Steenkiste'},\n",
       "     {'name': 'Jürgen Schmidhuber'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICLR 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.12930v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.12930v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.00596v3',\n",
       "    'updated': '2021-06-17T00:07:48Z',\n",
       "    'published': '2020-12-01T16:03:40Z',\n",
       "    'title': 'NPAS: A Compiler-aware Framework of Unified Network Pruning and\\n  Architecture Search for Beyond Real-Time Mobile Acceleration',\n",
       "    'summary': 'With the increasing demand to efficiently deploy DNNs on mobile edge devices,\\nit becomes much more important to reduce unnecessary computation and increase\\nthe execution speed. Prior methods towards this goal, including model\\ncompression and network architecture search (NAS), are largely performed\\nindependently and do not fully consider compiler-level optimizations which is a\\nmust-do for mobile acceleration. In this work, we first propose (i) a general\\ncategory of fine-grained structured pruning applicable to various DNN layers,\\nand (ii) a comprehensive, compiler automatic code generation framework\\nsupporting different DNNs and different pruning schemes, which bridge the gap\\nof model compression and NAS. We further propose NPAS, a compiler-aware unified\\nnetwork pruning, and architecture search. To deal with large search space, we\\npropose a meta-modeling procedure based on reinforcement learning with fast\\nevaluation and Bayesian optimization, ensuring the total number of training\\nepochs comparable with representative NAS frameworks. Our framework achieves\\n6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3\\nlevel), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an\\noff-the-shelf mobile phone, consistently outperforming prior work.',\n",
       "    'author': [{'name': 'Zhengang Li'},\n",
       "     {'name': 'Geng Yuan'},\n",
       "     {'name': 'Wei Niu'},\n",
       "     {'name': 'Pu Zhao'},\n",
       "     {'name': 'Yanyu Li'},\n",
       "     {'name': 'Yuxuan Cai'},\n",
       "     {'name': 'Xuan Shen'},\n",
       "     {'name': 'Zheng Zhan'},\n",
       "     {'name': 'Zhenglun Kong'},\n",
       "     {'name': 'Qing Jin'},\n",
       "     {'name': 'Zhiyu Chen'},\n",
       "     {'name': 'Sijia Liu'},\n",
       "     {'name': 'Kaiyuan Yang'},\n",
       "     {'name': 'Bin Ren'},\n",
       "     {'name': 'Yanzhi Wang'},\n",
       "     {'name': 'Xue Lin'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted as an oral paper in the Conference on Computer Vision and\\n  Pattern Recognition (CVPR), 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.00596v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.00596v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.02911v1',\n",
       "    'updated': '2020-12-05T00:49:14Z',\n",
       "    'published': '2020-12-05T00:49:14Z',\n",
       "    'title': 'Multi-head Knowledge Distillation for Model Compression',\n",
       "    'summary': 'Several methods of knowledge distillation have been developed for neural\\nnetwork compression. While they all use the KL divergence loss to align the\\nsoft outputs of the student model more closely with that of the teacher, the\\nvarious methods differ in how the intermediate features of the student are\\nencouraged to match those of the teacher. In this paper, we propose a\\nsimple-to-implement method using auxiliary classifiers at intermediate layers\\nfor matching features, which we refer to as multi-head knowledge distillation\\n(MHKD). We add loss terms for training the student that measure the\\ndissimilarity between student and teacher outputs of the auxiliary classifiers.\\nAt the same time, the proposed method also provides a natural way to measure\\ndifferences at the intermediate layers even though the dimensions of the\\ninternal teacher and student features may be different. Through several\\nexperiments in image classification on multiple datasets we show that the\\nproposed method outperforms prior relevant approaches presented in the\\nliterature.',\n",
       "    'author': [{'name': 'Huan Wang'},\n",
       "     {'name': 'Suhas Lohit'},\n",
       "     {'name': 'Michael Jones'},\n",
       "     {'name': 'Yun Fu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Copyright: 2020 IEEE. Personal use of this material is permitted.\\n  Permission from IEEE must be obtained for all other uses, in any current or\\n  future media, including reprinting/republishing this material for advertising\\n  or promotional purposes, creating new collective works, for resale or\\n  redistribution to servers or lists, or reuse of any copyrighted component of\\n  this work in other works'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.02911v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.02911v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.09243v2',\n",
       "    'updated': '2021-04-05T19:37:45Z',\n",
       "    'published': '2020-12-16T20:16:28Z',\n",
       "    'title': 'Neural Pruning via Growing Regularization',\n",
       "    'summary': 'Regularization has long been utilized to learn sparsity in deep neural\\nnetwork pruning. However, its role is mainly explored in the small penalty\\nstrength regime. In this work, we extend its application to a new scenario\\nwhere the regularization grows large gradually to tackle two central problems\\nof pruning: pruning schedule and weight importance scoring. (1) The former\\ntopic is newly brought up in this work, which we find critical to the pruning\\nperformance while receives little research attention. Specifically, we propose\\nan L2 regularization variant with rising penalty factors and show it can bring\\nsignificant accuracy gains compared with its one-shot counterpart, even when\\nthe same weights are removed. (2) The growing penalty scheme also brings us an\\napproach to exploit the Hessian information for more accurate pruning without\\nknowing their specific values, thus not bothered by the common Hessian\\napproximation problems. Empirically, the proposed algorithms are easy to\\nimplement and scalable to large datasets and networks in both structured and\\nunstructured pruning. Their effectiveness is demonstrated with modern deep\\nneural networks on the CIFAR and ImageNet datasets, achieving competitive\\nresults compared to many state-of-the-art algorithms. Our code and trained\\nmodels are publicly available at\\nhttps://github.com/mingsuntse/regularization-pruning.',\n",
       "    'author': [{'name': 'Huan Wang'},\n",
       "     {'name': 'Can Qin'},\n",
       "     {'name': 'Yulun Zhang'},\n",
       "     {'name': 'Yun Fu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by ICLR 2021'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Learning Representations (ICLR) 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.09243v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.09243v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.13628v1',\n",
       "    'updated': '2020-12-25T20:50:15Z',\n",
       "    'published': '2020-12-25T20:50:15Z',\n",
       "    'title': 'A Simple Fine-tuning Is All You Need: Towards Robust Deep Learning Via\\n  Adversarial Fine-tuning',\n",
       "    'summary': \"Adversarial Training (AT) with Projected Gradient Descent (PGD) is an\\neffective approach for improving the robustness of the deep neural networks.\\nHowever, PGD AT has been shown to suffer from two main limitations: i) high\\ncomputational cost, and ii) extreme overfitting during training that leads to\\nreduction in model generalization. While the effect of factors such as model\\ncapacity and scale of training data on adversarial robustness have been\\nextensively studied, little attention has been paid to the effect of a very\\nimportant parameter in every network optimization on adversarial robustness:\\nthe learning rate. In particular, we hypothesize that effective learning rate\\nscheduling during adversarial training can significantly reduce the overfitting\\nissue, to a degree where one does not even need to adversarially train a model\\nfrom scratch but can instead simply adversarially fine-tune a pre-trained\\nmodel. Motivated by this hypothesis, we propose a simple yet very effective\\nadversarial fine-tuning approach based on a $\\\\textit{slow start, fast decay}$\\nlearning rate scheduling strategy which not only significantly decreases\\ncomputational cost required, but also greatly improves the accuracy and\\nrobustness of a deep neural network. Experimental results show that the\\nproposed adversarial fine-tuning approach outperforms the state-of-the-art\\nmethods on CIFAR-10, CIFAR-100 and ImageNet datasets in both test accuracy and\\nthe robustness, while reducing the computational cost by 8-10$\\\\times$.\\nFurthermore, a very important benefit of the proposed adversarial fine-tuning\\napproach is that it enables the ability to improve the robustness of any\\npre-trained deep neural network without needing to train the model from\\nscratch, which to the best of the authors' knowledge has not been previously\\ndemonstrated in research literature.\",\n",
       "    'author': [{'name': 'Ahmadreza Jeddi'},\n",
       "     {'name': 'Mohammad Javad Shafiee'},\n",
       "     {'name': 'Alexander Wong'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.13628v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.13628v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.01345v1',\n",
       "    'updated': '2021-02-02T06:45:56Z',\n",
       "    'published': '2021-02-02T06:45:56Z',\n",
       "    'title': 'Fast Exploration of Weight Sharing Opportunities for CNN Compression',\n",
       "    'summary': 'The computational workload involved in Convolutional Neural Networks (CNNs)\\nis typically out of reach for low-power embedded devices. There are a large\\nnumber of approximation techniques to address this problem. These methods have\\nhyper-parameters that need to be optimized for each CNNs using design space\\nexploration (DSE). The goal of this work is to demonstrate that the DSE phase\\ntime can easily explode for state of the art CNN. We thus propose the use of an\\noptimized exploration process to drastically reduce the exploration time\\nwithout sacrificing the quality of the output.',\n",
       "    'author': [{'name': 'Etienne Dupuis'},\n",
       "     {'name': 'David Novo'},\n",
       "     {'name': \"Ian O'Connor\"},\n",
       "     {'name': 'Alberto Bosio'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Presented at DATE Friday Workshop on System-level Design Methods for\\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.01345v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.01345v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.08634v1',\n",
       "    'updated': '2021-02-17T08:56:33Z',\n",
       "    'published': '2021-02-17T08:56:33Z',\n",
       "    'title': 'On the Post-hoc Explainability of Deep Echo State Networks for Time\\n  Series Forecasting, Image and Video Classification',\n",
       "    'summary': 'Since their inception, learning techniques under the Reservoir Computing\\nparadigm have shown a great modeling capability for recurrent systems without\\nthe computing overheads required for other approaches. Among them, different\\nflavors of echo state networks have attracted many stares through time, mainly\\ndue to the simplicity and computational efficiency of their learning algorithm.\\nHowever, these advantages do not compensate for the fact that echo state\\nnetworks remain as black-box models whose decisions cannot be easily explained\\nto the general audience. This work addresses this issue by conducting an\\nexplainability study of Echo State Networks when applied to learning tasks with\\ntime series, image and video data. Specifically, the study proposes three\\ndifferent techniques capable of eliciting understandable information about the\\nknowledge grasped by these recurrent models, namely, potential memory, temporal\\npatterns and pixel absence effect. Potential memory addresses questions related\\nto the effect of the reservoir size in the capability of the model to store\\ntemporal information, whereas temporal patterns unveils the recurrent\\nrelationships captured by the model over time. Finally, pixel absence effect\\nattempts at evaluating the effect of the absence of a given pixel when the echo\\nstate network model is used for image and video classification. We showcase the\\nbenefits of our proposed suite of techniques over three different domains of\\napplicability: time series modeling, image and, for the first time in the\\nrelated literature, video classification. Our results reveal that the proposed\\ntechniques not only allow for a informed understanding of the way these models\\nwork, but also serve as diagnostic tools capable of detecting issues inherited\\nfrom data (e.g. presence of hidden bias).',\n",
       "    'author': [{'name': 'Alejandro Barredo Arrieta'},\n",
       "     {'name': 'Sergio Gil-Lopez'},\n",
       "     {'name': 'Ibai Laña'},\n",
       "     {'name': 'Miren Nekane Bilbao'},\n",
       "     {'name': 'Javier Del Ser'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '22 pages, 9 figures, 3 tables. Currently under review'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.08634v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.08634v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.01205v3',\n",
       "    'updated': '2021-07-28T02:42:19Z',\n",
       "    'published': '2021-03-01T18:51:16Z',\n",
       "    'title': 'Statistically Significant Stopping of Neural Network Training',\n",
       "    'summary': \"The general approach taken when training deep learning classifiers is to save\\nthe parameters after every few iterations, train until either a human observer\\nor a simple metric-based heuristic decides the network isn't learning anymore,\\nand then backtrack and pick the saved parameters with the best validation\\naccuracy. Simple methods are used to determine if a neural network isn't\\nlearning anymore because, as long as it's well after the optimal values are\\nfound, the condition doesn't impact the final accuracy of the model. However\\nfrom a runtime perspective, this is of great significance to the many cases\\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\\ntuning). Motivated by this, we introduce a statistical significance test to\\ndetermine if a neural network has stopped learning. This stopping criterion\\nappears to represent a happy medium compared to other popular stopping\\ncriterions, achieving comparable accuracy to the criterions that achieve the\\nhighest final accuracies in 77% or fewer epochs, while the criterions which\\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\\nuse this as the basis of a new learning rate scheduler, removing the need to\\nmanually choose learning rate schedules and acting as a quasi-line search,\\nachieving superior or comparable empirical performance to existing methods.\",\n",
       "    'author': [{'name': 'J. K. Terry'},\n",
       "     {'name': 'Mario Jayakumar'},\n",
       "     {'name': 'Kusal De Alwis'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.01205v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.01205v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.03060v1',\n",
       "    'updated': '2021-03-04T14:37:23Z',\n",
       "    'published': '2021-03-04T14:37:23Z',\n",
       "    'title': 'BM3D vs 2-Layer ONN',\n",
       "    'summary': 'Despite their recent success on image denoising, the need for deep and\\ncomplex architectures still hinders the practical usage of CNNs. Older but\\ncomputationally more efficient methods such as BM3D remain a popular choice,\\nespecially in resource-constrained scenarios. In this study, we aim to find out\\nwhether compact neural networks can learn to produce competitive results as\\ncompared to BM3D for AWGN image denoising. To this end, we configure networks\\nwith only two hidden layers and employ different neuron models and layer widths\\nfor comparing the performance with BM3D across different AWGN noise levels. Our\\nresults conclusively show that the recently proposed self-organized variant of\\noperational neural networks based on a generative neuron model (Self-ONNs) is\\nnot only a better choice as compared to CNNs, but also provide competitive\\nresults as compared to BM3D and even significantly surpass it for high noise\\nlevels.',\n",
       "    'author': [{'name': 'Junaid Malik'},\n",
       "     {'name': 'Serkan Kiranyaz'},\n",
       "     {'name': 'Mehmet Yamac'},\n",
       "     {'name': 'Moncef Gabbouj'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted for review in ICIP 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.03060v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.03060v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.04916v3',\n",
       "    'updated': '2021-06-22T08:38:17Z',\n",
       "    'published': '2021-05-11T10:05:53Z',\n",
       "    'title': 'Pruning of Deep Spiking Neural Networks through Gradient Rewiring',\n",
       "    'summary': \"Spiking Neural Networks (SNNs) have been attached great importance due to\\ntheir biological plausibility and high energy-efficiency on neuromorphic chips.\\nAs these chips are usually resource-constrained, the compression of SNNs is\\nthus crucial along the road of practical use of SNNs. Most existing methods\\ndirectly apply pruning approaches in artificial neural networks (ANNs) to SNNs,\\nwhich ignore the difference between ANNs and SNNs, thus limiting the\\nperformance of the pruned SNNs. Besides, these methods are only suitable for\\nshallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination\\nin the neural system, we propose gradient rewiring (Grad R), a joint learning\\nalgorithm of connectivity and weight for SNNs, that enables us to seamlessly\\noptimize network structure without retraining. Our key innovation is to\\nredefine the gradient to a new synaptic parameter, allowing better exploration\\nof network structures by taking full advantage of the competition between\\npruning and regrowth of connections. The experimental results show that the\\nproposed method achieves minimal loss of SNNs' performance on MNIST and\\nCIFAR-10 dataset so far. Moreover, it reaches a $\\\\sim$3.5% accuracy loss under\\nunprecedented 0.73% connectivity, which reveals remarkable structure refining\\ncapability in SNNs. Our work suggests that there exists extremely high\\nredundancy in deep SNNs. Our codes are available at\\nhttps://github.com/Yanqi-Chen/Gradient-Rewiring.\",\n",
       "    'author': [{'name': 'Yanqi Chen'},\n",
       "     {'name': 'Zhaofei Yu'},\n",
       "     {'name': 'Wei Fang'},\n",
       "     {'name': 'Tiejun Huang'},\n",
       "     {'name': 'Yonghong Tian'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.24963/ijcai.2021/236'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.24963/ijcai.2021/236',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2105.04916v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.04916v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 7 figures, 4 tables. To appear in the 30th International\\n  Joint Conference on Artificial Intelligence (IJCAI 2021)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.05916v1',\n",
       "    'updated': '2021-05-12T19:20:09Z',\n",
       "    'published': '2021-05-12T19:20:09Z',\n",
       "    'title': 'Dynamical Isometry: The Missing Ingredient for Neural Network Pruning',\n",
       "    'summary': 'Several recent works [40, 24] observed an interesting phenomenon in neural\\nnetwork pruning: A larger finetuning learning rate can improve the final\\nperformance significantly. Unfortunately, the reason behind it remains elusive\\nup to date. This paper is meant to explain it through the lens of dynamical\\nisometry [42]. Specifically, we examine neural network pruning from an unusual\\nperspective: pruning as initialization for finetuning, and ask whether the\\ninherited weights serve as a good initialization for the finetuning? The\\ninsights from dynamical isometry suggest a negative answer. Despite its\\ncritical role, this issue has not been well-recognized by the community so far.\\nIn this paper, we will show the understanding of this problem is very important\\n-- on top of explaining the aforementioned mystery about the larger finetuning\\nrate, it also unveils the mystery about the value of pruning [5, 30]. Besides a\\nclearer theoretical understanding of pruning, resolving the problem can also\\nbring us considerable performance benefits in practice.',\n",
       "    'author': [{'name': 'Huan Wang'},\n",
       "     {'name': 'Can Qin'},\n",
       "     {'name': 'Yue Bai'},\n",
       "     {'name': 'Yun Fu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 2 figures, 7 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.05916v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.05916v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.11654v1',\n",
       "    'updated': '2021-05-25T04:15:06Z',\n",
       "    'published': '2021-05-25T04:15:06Z',\n",
       "    'title': 'Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep\\n  Spiking Neural Networks',\n",
       "    'summary': 'Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural\\nnetworks, have attracted great attentions from researchers and industry. The\\nmost efficient way to train deep SNNs is through ANN-SNN conversion. However,\\nthe conversion usually suffers from accuracy loss and long inference time,\\nwhich impede the practical application of SNN. In this paper, we theoretically\\nanalyze ANN-SNN conversion and derive sufficient conditions of the optimal\\nconversion. To better correlate ANN-SNN and get greater accuracy, we propose\\nRate Norm Layer to replace the ReLU activation function in source ANN training,\\nenabling direct conversion from a trained ANN to an SNN. Moreover, we propose\\nan optimal fit curve to quantify the fit between the activation value of source\\nANN and the actual firing rate of target SNN. We show that the inference time\\ncan be reduced by optimizing the upper bound of the fit curve in the revised\\nANN to achieve fast inference. Our theory can explain the existing work on fast\\nreasoning and get better results. The experimental results show that the\\nproposed method achieves near loss less conversion with VGG-16,\\nPreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster\\nreasoning performance under 0.265x energy consumption of the typical method.\\nThe code is available at\\nhttps://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.',\n",
       "    'author': [{'name': 'Jianhao Ding'},\n",
       "     {'name': 'Zhaofei Yu'},\n",
       "     {'name': 'Yonghong Tian'},\n",
       "     {'name': 'Tiejun Huang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 7 figures, 2 tables. To appear in the 30th International\\n  Joint Conference on Artificial Intelligence (IJCAI 2021)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.11654v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.11654v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.01862v2',\n",
       "    'updated': '2021-10-25T17:01:39Z',\n",
       "    'published': '2021-06-03T14:03:41Z',\n",
       "    'title': 'Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural\\n  Networks',\n",
       "    'summary': 'The field of neuromorphic computing promises extremely low-power and\\nlow-latency sensing and processing. Challenges in transferring learning\\nalgorithms from traditional artificial neural networks (ANNs) to spiking neural\\nnetworks (SNNs) have so far prevented their application to large-scale, complex\\nregression tasks. Furthermore, realizing a truly asynchronous and fully\\nneuromorphic pipeline that maximally attains the abovementioned benefits\\ninvolves rethinking the way in which this pipeline takes in and accumulates\\ninformation. In the case of perception, spikes would be passed as-is and\\none-by-one between an event camera and an SNN, meaning all temporal integration\\nof information must happen inside the network. In this article, we tackle these\\ntwo problems. We focus on the complex task of learning to estimate optical flow\\nfrom event-based camera inputs in a self-supervised manner, and modify the\\nstate-of-the-art ANN training pipeline to encode minimal temporal information\\nin its inputs. Moreover, we reformulate the self-supervised loss function for\\nevent-based optical flow to improve its convexity. We perform experiments with\\nvarious types of recurrent ANNs and SNNs using the proposed pipeline.\\nConcerning SNNs, we investigate the effects of elements such as parameter\\ninitialization and optimization, surrogate gradient shape, and adaptive\\nneuronal mechanisms. We find that initialization and surrogate gradient width\\nplay a crucial part in enabling learning with sparse inputs, while the\\ninclusion of adaptivity and learnable neuronal parameters can improve\\nperformance. We show that the performance of the proposed ANNs and SNNs are on\\npar with that of the current state-of-the-art ANNs trained in a self-supervised\\nmanner.',\n",
       "    'author': [{'name': 'Jesse Hagenaars'},\n",
       "     {'name': 'Federico Paredes-Vallés'},\n",
       "     {'name': 'Guido de Croon'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at NeurIPS 2021; code and additional material can be found\\n  at https://mavlab.tudelft.nl/event_flow/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.01862v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.01862v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.01920v1',\n",
       "    'updated': '2021-06-03T15:14:46Z',\n",
       "    'published': '2021-06-03T15:14:46Z',\n",
       "    'title': 'Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement\\n  Prediction',\n",
       "    'summary': 'With technological advancements and the exponential growth of data, we have\\nbeen unfolding different capabilities of neural networks in different sectors.\\nIn this paper, I have tried to use a specific type of Neural Network known as\\nConvolutional Neural Network(CNN/ConvNet) in the stock market. In other words,\\nI have tried to construct and train a convolutional neural network on past\\nstock prices data and then tried to predict the movement of stock price i.e.\\nwhether the stock price would rise or fall, in the coming time.',\n",
       "    'author': {'name': 'Kunal Bhardwaj'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '19 pages, 7 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.01920v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.01920v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.02328v1',\n",
       "    'updated': '2021-06-04T08:19:44Z',\n",
       "    'published': '2021-06-04T08:19:44Z',\n",
       "    'title': 'Temporally coherent video anonymization through GAN inpainting',\n",
       "    'summary': 'This work tackles the problem of temporally coherent face anonymization in\\nnatural video streams.We propose JaGAN, a two-stage system starting with\\ndetecting and masking out faces with black image patches in all individual\\nframes of the video. The second stage leverages a privacy-preserving Video\\nGenerative Adversarial Network designed to inpaint the missing image patches\\nwith artificially generated faces. Our initial experiments reveal that image\\nbased generative models are not capable of inpainting patches showing temporal\\ncoherent appearance across neighboring video frames. To address this issue we\\nintroduce a newly curated video collection, which is made publicly available\\nfor the research community along with this paper. We also introduce the\\nIdentity Invariance Score IdI as a means to quantify temporal coherency between\\nneighboring frames.',\n",
       "    'author': [{'name': 'Thangapavithraa Balaji'},\n",
       "     {'name': 'Patrick Blies'},\n",
       "     {'name': 'Georg Göri'},\n",
       "     {'name': 'Raphael Mitsch'},\n",
       "     {'name': 'Marcel Wasserer'},\n",
       "     {'name': 'Torsten Schön'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Preprint of our FG2021 submission'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.02328v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.02328v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.07091v1',\n",
       "    'updated': '2021-06-13T20:55:16Z',\n",
       "    'published': '2021-06-13T20:55:16Z',\n",
       "    'title': 'On-Off Center-Surround Receptive Fields for Accurate and Robust Image\\n  Classification',\n",
       "    'summary': 'Robustness to variations in lighting conditions is a key objective for any\\ndeep vision system. To this end, our paper extends the receptive field of\\nconvolutional neural networks with two residual components, ubiquitous in the\\nvisual processing system of vertebrates: On-center and off-center pathways,\\nwith excitatory center and inhibitory surround; OOCS for short. The on-center\\npathway is excited by the presence of a light stimulus in its center but not in\\nits surround, whereas the off-center one is excited by the absence of a light\\nstimulus in its center but not in its surround. We design OOCS pathways via a\\ndifference of Gaussians, with their variance computed analytically from the\\nsize of the receptive fields. OOCS pathways complement each other in their\\nresponse to light stimuli, ensuring this way a strong edge-detection\\ncapability, and as a result, an accurate and robust inference under challenging\\nlighting conditions. We provide extensive empirical evidence showing that\\nnetworks supplied with the OOCS edge representation gain accuracy and\\nillumination-robustness compared to standard deep models.',\n",
       "    'author': [{'name': 'Zahra Babaiee'},\n",
       "     {'name': 'Ramin Hasani'},\n",
       "     {'name': 'Mathias Lechner'},\n",
       "     {'name': 'Daniela Rus'},\n",
       "     {'name': 'Radu Grosu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '21 Pages. Accepted for publication in the proceedings of the 38th\\n  International Conference on Machine Learning (ICML) 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.07091v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.07091v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.09693v1',\n",
       "    'updated': '2021-06-17T17:47:01Z',\n",
       "    'published': '2021-06-17T17:47:01Z',\n",
       "    'title': 'Orthogonal-Padé Activation Functions: Trainable Activation functions\\n  for smooth and faster convergence in deep networks',\n",
       "    'summary': \"We have proposed orthogonal-Pad\\\\'e activation functions, which are trainable\\nactivation functions and show that they have faster learning capability and\\nimproves the accuracy in standard deep learning datasets and models. Based on\\nour experiments, we have found two best candidates out of six orthogonal-Pad\\\\'e\\nactivations, which we call safe Hermite-Pade (HP) activation functions, namely\\nHP-1 and HP-2. When compared to ReLU, HP-1 and HP-2 has an increment in top-1\\naccuracy by 5.06% and 4.63% respectively in PreActResNet-34, by 3.02% and 2.75%\\nrespectively in MobileNet V2 model on CIFAR100 dataset while on CIFAR10 dataset\\ntop-1 accuracy increases by 2.02% and 1.78% respectively in PreActResNet-34, by\\n2.24% and 2.06% respectively in LeNet, by 2.15% and 2.03% respectively in\\nEfficientnet B0.\",\n",
       "    'author': [{'name': 'Koushik Biswas'},\n",
       "     {'name': 'Shilpak Banerjee'},\n",
       "     {'name': 'Ashish Kumar Pandey'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.09693v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.09693v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.09857v3',\n",
       "    'updated': '2022-03-04T15:08:32Z',\n",
       "    'published': '2021-06-18T01:03:13Z',\n",
       "    'title': 'Effective Model Sparsification by Scheduled Grow-and-Prune Methods',\n",
       "    'summary': 'Deep neural networks (DNNs) are effective in solving many real-world\\nproblems. Larger DNN models usually exhibit better quality (e.g., accuracy) but\\ntheir excessive computation results in long inference time. Model\\nsparsification can reduce the computation and memory cost while maintaining\\nmodel quality. Most existing sparsification algorithms unidirectionally remove\\nweights, while others randomly or greedily explore a small subset of weights in\\neach layer for pruning. The limitations of these algorithms reduce the level of\\nachievable sparsity. In addition, many algorithms still require pre-trained\\ndense models and thus suffer from large memory footprint. In this paper, we\\npropose a novel scheduled grow-and-prune (GaP) methodology without having to\\npre-train a dense model. It addresses the shortcomings of the previous works by\\nrepeatedly growing a subset of layers to dense and then pruning them back to\\nsparse after some training. Experiments show that the models pruned using the\\nproposed methods match or beat the quality of the highly optimized dense models\\nat 80% sparsity on a variety of tasks, such as image classification, objective\\ndetection, 3D object part segmentation, and translation. They also outperform\\nother state-of-the-art (SOTA) methods for model sparsification. As an example,\\na 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1\\naccuracy on ImageNet, improving the previous SOTA results by 1.5%. Code\\navailable at: https://github.com/boone891214/GaP.',\n",
       "    'author': [{'name': 'Xiaolong Ma'},\n",
       "     {'name': 'Minghai Qin'},\n",
       "     {'name': 'Fei Sun'},\n",
       "     {'name': 'Zejiang Hou'},\n",
       "     {'name': 'Kun Yuan'},\n",
       "     {'name': 'Yi Xu'},\n",
       "     {'name': 'Yanzhi Wang'},\n",
       "     {'name': 'Yen-Kuang Chen'},\n",
       "     {'name': 'Rong Jin'},\n",
       "     {'name': 'Yuan Xie'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICLR 2022 camera ready'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.09857v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.09857v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.10944v2',\n",
       "    'updated': '2022-06-24T10:27:20Z',\n",
       "    'published': '2021-06-21T09:31:33Z',\n",
       "    'title': 'Hard hat wearing detection based on head keypoint localization',\n",
       "    'summary': 'In recent years, a lot of attention is paid to deep learning methods in the\\ncontext of vision-based construction site safety systems, especially regarding\\npersonal protective equipment. However, despite all this attention, there is\\nstill no reliable way to establish the relationship between workers and their\\nhard hats. To answer this problem a combination of deep learning, object\\ndetection and head keypoint localization, with simple rule-based reasoning is\\nproposed in this article. In tests, this solution surpassed the previous\\nmethods based on the relative bounding box position of different instances, as\\nwell as direct detection of hard hat wearers and non-wearers. The results show\\nthat the conjunction of novel deep learning methods with humanly-interpretable\\nrule-based systems can result in a solution that is both reliable and can\\nsuccessfully mimic manual, on-site supervision. This work is the next step in\\nthe development of fully autonomous construction site safety systems and shows\\nthat there is still room for improvement in this area.',\n",
       "    'author': [{'name': 'Bartosz Wójcik'},\n",
       "     {'name': 'Mateusz Żarski'},\n",
       "     {'name': 'Kamil Książek'},\n",
       "     {'name': 'Jarosław Adam Miszczak'},\n",
       "     {'name': 'Mirosław Jan Skibniewski'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '17 pages, 9 figures and 9 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.10944v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.10944v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.12499v1',\n",
       "    'updated': '2021-06-23T16:19:00Z',\n",
       "    'published': '2021-06-23T16:19:00Z',\n",
       "    'title': 'Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine\\n  MRI Synthesis',\n",
       "    'summary': 'Self-training based unsupervised domain adaptation (UDA) has shown great\\npotential to address the problem of domain shift, when applying a trained deep\\nlearning model in a source domain to unlabeled target domains. However, while\\nthe self-training UDA has demonstrated its effectiveness on discriminative\\ntasks, such as classification and segmentation, via the reliable pseudo-label\\nselection based on the softmax discrete histogram, the self-training UDA for\\ngenerative tasks, such as image synthesis, is not fully investigated. In this\\nwork, we propose a novel generative self-training (GST) UDA framework with\\ncontinuous value prediction and regression objective for cross-domain image\\nsynthesis. Specifically, we propose to filter the pseudo-label with an\\nuncertainty mask, and quantify the predictive confidence of generated images\\nwith practical variational Bayes learning. The fast test-time adaptation is\\nachieved by a round-based alternative optimization scheme. We validated our\\nframework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis\\nproblem, where datasets in the source and target domains were acquired from\\ndifferent scanners or centers. Extensive validations were carried out to verify\\nour framework against popular adversarial training UDA methods. Results show\\nthat our GST, with tagged MRI of test subjects in new target domains, improved\\nthe synthesis quality by a large margin, compared with the adversarial training\\nUDA methods.',\n",
       "    'author': [{'name': 'Xiaofeng Liu'},\n",
       "     {'name': 'Fangxu Xing'},\n",
       "     {'name': 'Maureen Stone'},\n",
       "     {'name': 'Jiachen Zhuo'},\n",
       "     {'name': 'Reese Timothy'},\n",
       "     {'name': 'Jerry L. Prince'},\n",
       "     {'name': 'Georges El Fakhri'},\n",
       "     {'name': 'Jonghye Woo'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'MICCAI 2021 (early accept <13%)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.12499v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.12499v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.06862v1',\n",
       "    'updated': '2021-06-22T17:38:34Z',\n",
       "    'published': '2021-06-22T17:38:34Z',\n",
       "    'title': 'Differentiable Programming of Reaction-Diffusion Patterns',\n",
       "    'summary': \"Reaction-Diffusion (RD) systems provide a computational framework that\\ngoverns many pattern formation processes in nature. Current RD system design\\npractices boil down to trial-and-error parameter search. We propose a\\ndifferentiable optimization method for learning the RD system parameters to\\nperform example-based texture synthesis on a 2D plane. We do this by\\nrepresenting the RD system as a variant of Neural Cellular Automata and using\\ntask-specific differentiable loss functions. RD systems generated by our method\\nexhibit robust, non-trivial 'life-like' behavior.\",\n",
       "    'author': [{'name': 'Alexander Mordvintsev'},\n",
       "     {'name': 'Ettore Randazzo'},\n",
       "     {'name': 'Eyvind Niklasson'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ALIFE 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.06862v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.06862v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.01899v2',\n",
       "    'updated': '2021-11-17T05:26:03Z',\n",
       "    'published': '2021-08-04T08:21:12Z',\n",
       "    'title': 'Generic Neural Architecture Search via Regression',\n",
       "    'summary': \"Most existing neural architecture search (NAS) algorithms are dedicated to\\nand evaluated by the downstream tasks, e.g., image classification in computer\\nvision. However, extensive experiments have shown that, prominent neural\\narchitectures, such as ResNet in computer vision and LSTM in natural language\\nprocessing, are generally good at extracting patterns from the input data and\\nperform well on different downstream tasks. In this paper, we attempt to answer\\ntwo fundamental questions related to NAS. (1) Is it necessary to use the\\nperformance of specific downstream tasks to evaluate and search for good neural\\narchitectures? (2) Can we perform NAS effectively and efficiently while being\\nagnostic to the downstream tasks? To answer these questions, we propose a novel\\nand generic NAS framework, termed Generic NAS (GenNAS). GenNAS does not use\\ntask-specific labels but instead adopts regression on a set of manually\\ndesigned synthetic signal bases for architecture evaluation. Such a\\nself-supervised regression task can effectively evaluate the intrinsic power of\\nan architecture to capture and transform the input signal patterns, and allow\\nmore sufficient usage of training samples. Extensive experiments across 13 CNN\\nsearch spaces and one NLP space demonstrate the remarkable efficiency of GenNAS\\nusing regression, in terms of both evaluating the neural architectures\\n(quantified by the ranking correlation Spearman's rho between the approximated\\nperformances and the downstream task performances) and the convergence speed\\nfor training (within a few seconds).\",\n",
       "    'author': [{'name': 'Yuhong Li'},\n",
       "     {'name': 'Cong Hao'},\n",
       "     {'name': 'Pan Li'},\n",
       "     {'name': 'Jinjun Xiong'},\n",
       "     {'name': 'Deming Chen'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear at NeurIPS 2021, 10 pages + Reference + Appendix'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.01899v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.01899v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.09598v3',\n",
       "    'updated': '2021-08-25T03:32:00Z',\n",
       "    'published': '2021-08-21T23:33:57Z',\n",
       "    'title': 'SERF: Towards better training of deep neural networks using log-Softplus\\n  ERror activation Function',\n",
       "    'summary': 'Activation functions play a pivotal role in determining the training dynamics\\nand neural network performance. The widely adopted activation function ReLU\\ndespite being simple and effective has few disadvantages including the Dying\\nReLU problem. In order to tackle such problems, we propose a novel activation\\nfunction called Serf which is self-regularized and nonmonotonic in nature. Like\\nMish, Serf also belongs to the Swish family of functions. Based on several\\nexperiments on computer vision (image classification and object detection) and\\nnatural language processing (machine translation, sentiment classification and\\nmultimodal entailment) tasks with different state-of-the-art architectures, it\\nis observed that Serf vastly outperforms ReLU (baseline) and other activation\\nfunctions including both Swish and Mish, with a markedly bigger margin on\\ndeeper architectures. Ablation studies further demonstrate that Serf based\\narchitectures perform better than those of Swish and Mish in varying scenarios,\\nvalidating the effectiveness and compatibility of Serf with varying depth,\\ncomplexity, optimizers, learning rates, batch sizes, initializers and dropout\\nrates. Finally, we investigate the mathematical relation between Swish and\\nSerf, thereby showing the impact of preconditioner function ingrained in the\\nfirst derivative of Serf which provides a regularization effect making\\ngradients smoother and optimization faster.',\n",
       "    'author': [{'name': 'Sayan Nag'}, {'name': 'Mayukh Bhattacharyya'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.09598v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.09598v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.04386v4',\n",
       "    'updated': '2022-03-24T12:46:15Z',\n",
       "    'published': '2021-09-09T16:17:38Z',\n",
       "    'title': 'ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions',\n",
       "    'summary': 'An activation function is a crucial component of a neural network that\\nintroduces non-linearity in the network. The state-of-the-art performance of a\\nneural network depends also on the perfect choice of an activation function. We\\npropose two novel non-monotonic smooth trainable activation functions, called\\nErfAct and Pserf. Experiments suggest that the proposed functions improve the\\nnetwork performance significantly compared to the widely used activations like\\nReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and\\n5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in\\nCIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet\\nV2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean\\naverage precision (mAP) on SSD300 model in Pascal VOC dataset.',\n",
       "    'author': [{'name': 'Koushik Biswas'},\n",
       "     {'name': 'Sandeep Kumar'},\n",
       "     {'name': 'Shilpak Banerjee'},\n",
       "     {'name': 'Ashish Kumar Pandey'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'AAAI 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.04386v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.04386v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.13210v1',\n",
       "    'updated': '2021-09-27T17:31:04Z',\n",
       "    'published': '2021-09-27T17:31:04Z',\n",
       "    'title': 'SAU: Smooth activation function using convolution with approximate\\n  identities',\n",
       "    'summary': 'Well-known activation functions like ReLU or Leaky ReLU are\\nnon-differentiable at the origin. Over the years, many smooth approximations of\\nReLU have been proposed using various smoothing techniques. We propose new\\nsmooth approximations of a non-differentiable activation function by convolving\\nit with approximate identities. In particular, we present smooth approximations\\nof Leaky ReLU and show that they outperform several well-known activation\\nfunctions in various datasets and models. We call this function Smooth\\nActivation Unit (SAU). Replacing ReLU by SAU, we get 5.12% improvement with\\nShuffleNet V2 (2.0x) model on CIFAR100 dataset.',\n",
       "    'author': [{'name': 'Koushik Biswas'},\n",
       "     {'name': 'Sandeep Kumar'},\n",
       "     {'name': 'Shilpak Banerjee'},\n",
       "     {'name': 'Ashish Kumar Pandey'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: text overlap with arXiv:2109.04386'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.13210v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.13210v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.13751v3',\n",
       "    'updated': '2022-11-03T12:35:43Z',\n",
       "    'published': '2021-09-28T14:11:36Z',\n",
       "    'title': 'StereoSpike: Depth Learning with a Spiking Neural Network',\n",
       "    'summary': 'Depth estimation is an important computer vision task, useful in particular\\nfor navigation in autonomous vehicles, or for object manipulation in robotics.\\nHere we solved it using an end-to-end neuromorphic approach, combining two\\nevent-based cameras and a Spiking Neural Network (SNN) with a slightly modified\\nU-Net-like encoder-decoder architecture, that we named StereoSpike. More\\nspecifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It\\nprovides a depth ground-truth, which was used to train StereoSpike in a\\nsupervised manner, using surrogate gradient descent. We propose a novel readout\\nparadigm to obtain a dense analog prediction -- the depth of each pixel -- from\\nthe spikes of the decoder. We demonstrate that this architecture generalizes\\nvery well, even better than its non-spiking counterparts, leading to\\nstate-of-the-art test accuracy. To the best of our knowledge, it is the first\\ntime that such a large-scale regression problem is solved by a fully spiking\\nnetwork. Finally, we show that low firing rates (<10%) can be obtained via\\nregularization, with a minimal cost in accuracy. This means that StereoSpike\\ncould be efficiently implemented on neuromorphic chips, opening the door for\\nlow power and real time embedded systems.',\n",
       "    'author': [{'name': 'Ulysse Rançon'},\n",
       "     {'name': 'Javier Cuadrado-Anibarro'},\n",
       "     {'name': 'Benoit R. Cottereau'},\n",
       "     {'name': 'Timothée Masquelier'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.13751v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.13751v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.14032v1',\n",
       "    'updated': '2021-10-26T21:15:17Z',\n",
       "    'published': '2021-10-26T21:15:17Z',\n",
       "    'title': 'MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the\\n  Edge',\n",
       "    'summary': 'Recently, a new trend of exploring sparsity for accelerating neural network\\ntraining has emerged, embracing the paradigm of training on the edge. This\\npaper proposes a novel Memory-Economic Sparse Training (MEST) framework\\ntargeting for accurate and fast execution on edge devices. The proposed MEST\\nframework consists of enhancements by Elastic Mutation (EM) and Soft Memory\\nBound (&S) that ensure superior accuracy at high sparsity ratios. Different\\nfrom the existing works for sparse training, this current work reveals the\\nimportance of sparsity schemes on the performance of sparse training in terms\\nof accuracy as well as training speed on real edge devices. On top of that, the\\npaper proposes to employ data efficiency for further acceleration of sparse\\ntraining. Our results suggest that unforgettable examples can be identified\\nin-situ even during the dynamic exploration of sparsity masks in the sparse\\ntraining process, and therefore can be removed for further training speedup on\\nedge devices. Comparing with state-of-the-art (SOTA) works on accuracy, our\\nMEST increases Top-1 accuracy significantly on ImageNet when using the same\\nunstructured sparsity scheme. Systematical evaluation on accuracy, training\\nspeed, and memory footprint are conducted, where the proposed MEST framework\\nconsistently outperforms representative SOTA works. A reviewer strongly against\\nour work based on his false assumptions and misunderstandings. On top of the\\nprevious submission, we employ data efficiency for further acceleration of\\nsparse training. And we explore the impact of model sparsity, sparsity schemes,\\nand sparse training algorithms on the number of removable training examples.\\nOur codes are publicly available at: https://github.com/boone891214/MEST.',\n",
       "    'author': [{'name': 'Geng Yuan'},\n",
       "     {'name': 'Xiaolong Ma'},\n",
       "     {'name': 'Wei Niu'},\n",
       "     {'name': 'Zhengang Li'},\n",
       "     {'name': 'Zhenglun Kong'},\n",
       "     {'name': 'Ning Liu'},\n",
       "     {'name': 'Yifan Gong'},\n",
       "     {'name': 'Zheng Zhan'},\n",
       "     {'name': 'Chaoyang He'},\n",
       "     {'name': 'Qing Jin'},\n",
       "     {'name': 'Siyue Wang'},\n",
       "     {'name': 'Minghai Qin'},\n",
       "     {'name': 'Bin Ren'},\n",
       "     {'name': 'Yanzhi Wang'},\n",
       "     {'name': 'Sijia Liu'},\n",
       "     {'name': 'Xue Lin'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2021 Spotlight Paper'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.14032v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.14032v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.01584v1',\n",
       "    'updated': '2021-11-02T13:20:01Z',\n",
       "    'published': '2021-11-02T13:20:01Z',\n",
       "    'title': 'Fitness Landscape Footprint: A Framework to Compare Neural Architecture\\n  Search Problems',\n",
       "    'summary': 'Neural architecture search is a promising area of research dedicated to\\nautomating the design of neural network models. This field is rapidly growing,\\nwith a surge of methodologies ranging from Bayesian optimization,neuroevoltion,\\nto differentiable search, and applications in various contexts. However,\\ndespite all great advances, few studies have presented insights on the\\ndifficulty of the problem itself, thus the success (or fail) of these\\nmethodologies remains unexplained. In this sense, the field of optimization has\\ndeveloped methods that highlight key aspects to describe optimization problems.\\nThe fitness landscape analysis stands out when it comes to characterize\\nreliably and quantitatively search algorithms. In this paper, we propose to use\\nfitness landscape analysis to study a neural architecture search problem.\\nParticularly, we introduce the fitness landscape footprint, an aggregation of\\neight (8)general-purpose metrics to synthesize the landscape of an architecture\\nsearch problem. We studied two problems, the classical image classification\\nbenchmark CIFAR-10, and the Remote-Sensing problem So2Sat LCZ42. The results\\npresent a quantitative appraisal of the problems, allowing to characterize the\\nrelative difficulty and other characteristics, such as the ruggedness or the\\npersistence, that helps to tailor a search strategy to the problem. Also, the\\nfootprint is a tool that enables the comparison of multiple problems.',\n",
       "    'author': [{'name': 'Kalifou René Traoré'},\n",
       "     {'name': 'Andrés Camero'},\n",
       "     {'name': 'Xiao Xiang Zhu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This work has been submitted to the IEEE for possible publication.\\n  Copyright may be transferred without notice, after which this version may no\\n  longer be accessible'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.01584v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.01584v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.04682v2',\n",
       "    'updated': '2022-04-11T14:28:28Z',\n",
       "    'published': '2021-11-08T17:54:08Z',\n",
       "    'title': 'SMU: smooth activation function for deep networks using smoothing\\n  maximum technique',\n",
       "    'summary': 'Deep learning researchers have a keen interest in proposing two new novel\\nactivation functions which can boost network performance. A good choice of\\nactivation function can have significant consequences in improving network\\nperformance. A handcrafted activation is the most common choice in neural\\nnetwork models. ReLU is the most common choice in the deep learning community\\ndue to its simplicity though ReLU has some serious drawbacks. In this paper, we\\nhave proposed a new novel activation function based on approximation of known\\nactivation functions like Leaky ReLU, and we call this function Smooth Maximum\\nUnit (SMU). Replacing ReLU by SMU, we have got 6.22% improvement in the\\nCIFAR100 dataset with the ShuffleNet V2 model.',\n",
       "    'author': [{'name': 'Koushik Biswas'},\n",
       "     {'name': 'Sandeep Kumar'},\n",
       "     {'name': 'Shilpak Banerjee'},\n",
       "     {'name': 'Ashish Kumar Pandey'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.04682v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.04682v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.05657v1',\n",
       "    'updated': '2021-12-10T16:43:01Z',\n",
       "    'published': '2021-12-10T16:43:01Z',\n",
       "    'title': 'Artificial Intellgence -- Application in Life Sciences and Beyond. The\\n  Upper Rhine Artificial Intelligence Symposium UR-AI 2021',\n",
       "    'summary': \"The TriRhenaTech alliance presents the accepted papers of the 'Upper-Rhine\\nArtificial Intelligence Symposium' held on October 27th 2021 in Kaiserslautern,\\nGermany. Topics of the conference are applications of Artificial Intellgence in\\nlife sciences, intelligent systems, industry 4.0, mobility and others. The\\nTriRhenaTech alliance is a network of universities in the Upper-Rhine\\nTrinational Metropolitan Region comprising of the German universities of\\napplied sciences in Furtwangen, Kaiserslautern, Karlsruhe, Offenburg and Trier,\\nthe Baden-Wuerttemberg Cooperative State University Loerrach, the French\\nuniversity network Alsace Tech (comprised of 14 'grandes \\\\'ecoles' in the\\nfields of engineering, architecture and management) and the University of\\nApplied Sciences and Arts Northwestern Switzerland. The alliance's common goal\\nis to reinforce the transfer of knowledge, research, and technology, as well as\\nthe cross-border mobility of students.\",\n",
       "    'author': [{'name': 'Karl-Herbert Schäfer',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Kaiserslautern University of Applied Sciences'}},\n",
       "     {'name': 'Franz Quint',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Karlsruhe University of Applied Sciences'}}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.05657v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.05657v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.10017v1',\n",
       "    'updated': '2021-12-18T22:37:30Z',\n",
       "    'published': '2021-12-18T22:37:30Z',\n",
       "    'title': 'Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks',\n",
       "    'summary': 'Existing research on continual learning of a sequence of tasks focused on\\ndealing with catastrophic forgetting, where the tasks are assumed to be\\ndissimilar and have little shared knowledge. Some work has also been done to\\ntransfer previously learned knowledge to the new task when the tasks are\\nsimilar and have shared knowledge. To the best of our knowledge, no technique\\nhas been proposed to learn a sequence of mixed similar and dissimilar tasks\\nthat can deal with forgetting and also transfer knowledge forward and backward.\\nThis paper proposes such a technique to learn both types of tasks in the same\\nnetwork. For dissimilar tasks, the algorithm focuses on dealing with\\nforgetting, and for similar tasks, the algorithm focuses on selectively\\ntransferring the knowledge learned from some similar previous tasks to improve\\nthe new task learning. Additionally, the algorithm automatically detects\\nwhether a new task is similar to any previous tasks. Empirical evaluation using\\nsequences of mixed tasks demonstrates the effectiveness of the proposed model.',\n",
       "    'author': [{'name': 'Zixuan Ke'},\n",
       "     {'name': 'Bing Liu'},\n",
       "     {'name': 'Xingchang Huang'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.10017v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.10017v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.12510v3',\n",
       "    'updated': '2022-12-04T17:21:14Z',\n",
       "    'published': '2021-12-22T11:25:23Z',\n",
       "    'title': 'Neuroevolution deep learning architecture search for estimation of river\\n  surface elevation from photogrammetric Digital Surface Models',\n",
       "    'summary': 'Development of the new methods of surface water observation is crucial in the\\nperspective of increasingly frequent extreme hydrological events related to\\nglobal warming and increasing demand for water. Orthophotos and digital surface\\nmodels (DSMs) obtained using UAV photogrammetry can be used to determine the\\nWater Surface Elevation (WSE) of a river. However, this task is difficult due\\nto disturbances of the water surface on DSMs caused by limitations of\\nphotogrammetric algorithms. In this study, machine learning was used to extract\\na WSE value from disturbed photogrammetric data. A brand new dataset has been\\nprepared specifically for this purpose by hydrology and photogrammetry experts.\\nThe new method is an important step toward automating water surface level\\nmeasurements with high spatial and temporal resolution. Such data can be used\\nto validate and calibrate of hydrological, hydraulic and hydrodynamic models\\nmaking hydrological forecasts more accurate, in particular predicting extreme\\nand dangerous events such as floods or droughts. For our knowledge this is the\\nfirst approach in which dataset was created for this purpose and deep learning\\nmodels were used for this task. Additionally, neuroevolution algorithm was set\\nto explore different architectures to find local optimal models and\\nnon-gradient search was performed to fine-tune the model parameters. The\\nachieved results have better accuracy compared to manual methods of determining\\nWSE from photogrammetric DSMs.',\n",
       "    'author': [{'name': 'Radosław Szostak'},\n",
       "     {'name': 'Marcin Pietroń'},\n",
       "     {'name': 'Mirosław Zimnoch'},\n",
       "     {'name': 'Przemysław Wachniew'},\n",
       "     {'name': 'Paweł Ćwiąkała'},\n",
       "     {'name': 'Edyta Puniach'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'extended version of NeurIPS 2021 Workshop paper - ML4PhysicalSciences'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.12510v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.12510v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.13121v4',\n",
       "    'updated': '2022-11-28T20:54:49Z',\n",
       "    'published': '2021-12-24T18:42:58Z',\n",
       "    'title': 'The Curse of Zero Task Diversity: On the Failure of Transfer Learning to\\n  Outperform MAML and their Empirical Equivalence',\n",
       "    'summary': 'Recently, it has been observed that a transfer learning solution might be all\\nwe need to solve many few-shot learning benchmarks -- thus raising important\\nquestions about when and how meta-learning algorithms should be deployed. In\\nthis paper, we seek to clarify these questions by proposing a novel metric --\\nthe diversity coefficient -- to measure the diversity of tasks in a few-shot\\nlearning benchmark. We hypothesize that the diversity coefficient of the\\nfew-shot learning benchmark is predictive of whether meta-learning solutions\\nwill succeed or not. Using the diversity coefficient, we show that the\\nMiniImagenet benchmark has zero diversity. This novel insight contextualizes\\nclaims that transfer learning solutions are better than meta-learned solutions.\\nSpecifically, we empirically find that a diversity coefficient of zero\\ncorrelates with a high similarity between transfer learning and Model-Agnostic\\nMeta-Learning (MAML) learned solutions in terms of meta-accuracy (at meta-test\\ntime). Therefore, we conjecture meta-learned solutions have the same meta-test\\nperformance as transfer learning when the diversity coefficient is zero. Our\\nwork provides the first test of whether diversity correlates with meta-learning\\nsuccess.',\n",
       "    'author': [{'name': 'Brando Miranda'},\n",
       "     {'name': 'Yu-Xiong Wang'},\n",
       "     {'name': 'Sanmi Koyejo'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"An updated version with updated correction is at arXiv:2208.01545 and\\n  it's acompanying neurips submission is at\\n  https://brando90.github.io/brandomiranda/publications.html\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.13121v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.13121v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.13137v1',\n",
       "    'updated': '2021-12-24T20:18:38Z',\n",
       "    'published': '2021-12-24T20:18:38Z',\n",
       "    'title': 'Does MAML Only Work via Feature Re-use? A Data Centric Perspective',\n",
       "    'summary': 'Recent work has suggested that a good embedding is all we need to solve many\\nfew-shot learning benchmarks. Furthermore, other work has strongly suggested\\nthat Model Agnostic Meta-Learning (MAML) also works via this same method - by\\nlearning a good embedding. These observations highlight our lack of\\nunderstanding of what meta-learning algorithms are doing and when they work. In\\nthis work, we provide empirical results that shed some light on how\\nmeta-learned MAML representations function. In particular, we identify three\\ninteresting properties: 1) In contrast to previous work, we show that it is\\npossible to define a family of synthetic benchmarks that result in a low degree\\nof feature re-use - suggesting that current few-shot learning benchmarks might\\nnot have the properties needed for the success of meta-learning algorithms; 2)\\nmeta-overfitting occurs when the number of classes (or concepts) are finite,\\nand this issue disappears once the task has an unbounded number of concepts\\n(e.g., online learning); 3) more adaptation at meta-test time with MAML does\\nnot necessarily result in a significant representation change or even an\\nimprovement in meta-test performance - even when training on our proposed\\nsynthetic benchmarks. Finally, we suggest that to understand meta-learning\\nalgorithms better, we must go beyond tracking only absolute performance and, in\\naddition, formally quantify the degree of meta-learning and track both metrics\\ntogether. Reporting results in future work this way will help us identify the\\nsources of meta-overfitting more accurately and help us design more flexible\\nmeta-learning algorithms that learn beyond fixed feature re-use. Finally, we\\nconjecture the core challenge of re-thinking meta-learning is in the design of\\nfew-shot learning data sets and benchmarks - rather than in the algorithms, as\\nsuggested by previous work.',\n",
       "    'author': [{'name': 'Brando Miranda'},\n",
       "     {'name': 'Yu-Xiong Wang'},\n",
       "     {'name': 'Sanmi Koyejo'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '15 pages, 12 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.13137v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.13137v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.14983v1',\n",
       "    'updated': '2021-12-30T09:35:05Z',\n",
       "    'published': '2021-12-30T09:35:05Z',\n",
       "    'title': 'Exploring the pattern of Emotion in children with ASD as an early\\n  biomarker through Recurring-Convolution Neural Network (R-CNN)',\n",
       "    'summary': 'Autism Spectrum Disorder (ASD) is found to be a major concern among various\\noccupational therapists. The foremost challenge of this neurodevelopmental\\ndisorder lies in the fact of analyzing and exploring various symptoms of the\\nchildren at their early stage of development. Such early identification could\\nprop up the therapists and clinicians to provide proper assistive support to\\nmake the children lead an independent life. Facial expressions and emotions\\nperceived by the children could contribute to such early intervention of\\nautism. In this regard, the paper implements in identifying basic facial\\nexpression and exploring their emotions upon a time variant factor. The\\nemotions are analyzed by incorporating the facial expression identified through\\nCNN using 68 landmark points plotted on the frontal face with a prediction\\nnetwork formed by RNN known as RCNN-FER system. The paper adopts R-CNN to take\\nthe advantage of increased accuracy and performance with decreased time\\ncomplexity in predicting emotion as a textual network analysis. The papers\\nproves better accuracy in identifying the emotion in autistic children when\\ncompared over simple machine learning models built for such identifications\\ncontributing to autistic society.',\n",
       "    'author': [{'name': 'Abirami S P'},\n",
       "     {'name': 'Kousalya G'},\n",
       "     {'name': 'Karthick R'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 figures and 2 tables. totally 18 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.14983v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.14983v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.05809v2',\n",
       "    'updated': '2022-01-21T17:35:42Z',\n",
       "    'published': '2022-01-15T09:34:50Z',\n",
       "    'title': 'Weighting and Pruning based Ensemble Deep Random Vector Functional Link\\n  Network for Tabular Data Classification',\n",
       "    'summary': \"In this paper, we first introduce batch normalization to the edRVFL network.\\nThis re-normalization method can help the network avoid divergence of the\\nhidden features. Then we propose novel variants of Ensemble Deep Random Vector\\nFunctional Link (edRVFL). Weighted edRVFL (WedRVFL) uses weighting methods to\\ngive training samples different weights in different layers according to how\\nthe samples were classified confidently in the previous layer thereby\\nincreasing the ensemble's diversity and accuracy. Furthermore, a pruning-based\\nedRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based\\non their importance for classification before generating the next hidden layer.\\nThrough this method, we ensure that the randomly generated inferior features\\nwill not propagate to deeper layers. Subsequently, the combination of weighting\\nand pruning, called Weighting and Pruning based Ensemble Deep Random Vector\\nFunctional Link Network (WPedRVFL), is proposed. We compare their performances\\nwith other state-of-the-art deep feedforward neural networks (FNNs) on 24\\ntabular UCI classification datasets. The experimental results illustrate the\\nsuperior performance of our proposed methods.\",\n",
       "    'author': [{'name': 'Qiushi Shi'},\n",
       "     {'name': 'Ponnuthurai Nagaratnam Suganthan'},\n",
       "     {'name': 'Rakesh Katuwal'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 tables, 8 figures, 31 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.05809v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.05809v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.06321v2',\n",
       "    'updated': '2022-01-20T15:07:49Z',\n",
       "    'published': '2022-01-17T10:14:39Z',\n",
       "    'title': 'Landscape of Neural Architecture Search across sensors: how much do they\\n  differ ?',\n",
       "    'summary': \"With the rapid rise of neural architecture search, the ability to understand\\nits complexity from the perspective of a search algorithm is desirable.\\nRecently, Traor\\\\'e et al. have proposed the framework of Fitness Landscape\\nFootprint to help describe and compare neural architecture search problems. It\\nattempts at describing why a search strategy might be successful, struggle or\\nfail on a target task. Our study leverages this methodology in the context of\\nsearching across sensors, including sensor data fusion. In particular, we apply\\nthe Fitness Landscape Footprint to the real-world image classification problem\\nof So2Sat LCZ42, in order to identify the most beneficial sensor to our neural\\nnetwork hyper-parameter optimization problem. From the perspective of\\ndistributions of fitness, our findings indicate a similar behaviour of the\\nsearch space for all sensors: the longer the training time, the larger the\\noverall fitness, and more flatness in the landscapes (less ruggedness and\\ndeviation). Regarding sensors, the better the fitness they enable (Sentinel-2),\\nthe better the search trajectories (smoother, higher persistence). Results also\\nindicate very similar search behaviour for sensors that can be decently fitted\\nby the search space (Sentinel-2 and fusion).\",\n",
       "    'author': [{'name': 'Kalifou René Traoré'},\n",
       "     {'name': 'Andrés Camero'},\n",
       "     {'name': 'Xiao Xiang Zhu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This work is under review for a conference publication'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.06321v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.06321v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.13072v1',\n",
       "    'updated': '2022-02-26T05:57:45Z',\n",
       "    'published': '2022-02-26T05:57:45Z',\n",
       "    'title': 'Adversarial Contrastive Self-Supervised Learning',\n",
       "    'summary': 'Recently, learning from vast unlabeled data, especially self-supervised\\nlearning, has been emerging and attracted widespread attention. Self-supervised\\nlearning followed by the supervised fine-tuning on a few labeled examples can\\nsignificantly improve label efficiency and outperform standard supervised\\ntraining using fully annotated data. In this work, we present a novel\\nself-supervised deep learning paradigm based on online hard negative pair\\nmining. Specifically, we design a student-teacher network to generate\\nmulti-view of the data for self-supervised learning and integrate hard negative\\npair mining into the training. Then we derive a new triplet-like loss\\nconsidering both positive sample pairs and mined hard negative sample pairs.\\nExtensive experiments demonstrate the effectiveness of the proposed method and\\nits components on ILSVRC-2012.',\n",
       "    'author': [{'name': 'Wentao Zhu'},\n",
       "     {'name': 'Hang Shang'},\n",
       "     {'name': 'Tingxun Lv'},\n",
       "     {'name': 'Chao Liao'},\n",
       "     {'name': 'Sen Yang'},\n",
       "     {'name': 'Ji Liu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 2 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.13072v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.13072v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.05508v2',\n",
       "    'updated': '2023-01-06T21:18:31Z',\n",
       "    'published': '2022-03-10T17:53:03Z',\n",
       "    'title': 'Towards Less Constrained Macro-Neural Architecture Search',\n",
       "    'summary': \"Networks found with Neural Architecture Search (NAS) achieve state-of-the-art\\nperformance in a variety of tasks, out-performing human-designed networks.\\nHowever, most NAS methods heavily rely on human-defined assumptions that\\nconstrain the search: architecture's outer-skeletons, number of layers,\\nparameter heuristics and search spaces. Additionally, common search spaces\\nconsist of repeatable modules (cells) instead of fully exploring the\\narchitecture's search space by designing entire architectures (macro-search).\\nImposing such constraints requires deep human expertise and restricts the\\nsearch to pre-defined settings. In this paper, we propose LCMNAS, a method that\\npushes NAS to less constrained search spaces by performing macro-search without\\nrelying on pre-defined heuristics or bounded search spaces. LCMNAS introduces\\nthree components for the NAS pipeline: i) a method that leverages information\\nabout well-known architectures to autonomously generate complex search spaces\\nbased on Weighted Directed Graphs with hidden properties, ii) an evolutionary\\nsearch strategy that generates complete architectures from scratch, and iii) a\\nmixed-performance estimation approach that combines information about\\narchitectures at initialization stage and lower fidelity estimates to infer\\ntheir trainability and capacity to model complex functions. We present\\nexperiments in 13 different data sets showing that LCMNAS is capable of\\ngenerating both cell and macro-based architectures with minimal GPU computation\\nand state-of-the-art results. More, we conduct extensive studies on the\\nimportance of different NAS components in both cell and macro-based settings.\\nCode for reproducibility is public at https://github.com/VascoLopes/LCMNAS.\",\n",
       "    'author': [{'name': 'Vasco Lopes'}, {'name': 'Luís A. Alexandre'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '13 pages double-column, 9 tables, 6 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.05508v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.05508v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.12853v1',\n",
       "    'updated': '2022-03-24T05:29:09Z',\n",
       "    'published': '2022-03-24T05:29:09Z',\n",
       "    'title': 'Direct evaluation of progression or regression of disease burden in\\n  brain metastatic disease with Deep Neuroevolution',\n",
       "    'summary': 'Purpose: A core component of advancing cancer treatment research is assessing\\nresponse to therapy. Doing so by hand, for example as per RECIST or RANO\\ncriteria, is tedious, time-consuming, and can miss important tumor response\\ninformation; most notably, they exclude non-target lesions. We wish to assess\\nchange in a holistic fashion that includes all lesions, obtaining simple,\\ninformative, and automated assessments of tumor progression or regression. Due\\nto often low patient enrolments in clinical trials, we wish to make response\\nassessments with small training sets. Deep neuroevolution (DNE) can produce\\nradiology artificial intelligence (AI) that performs well on small training\\nsets. Here we use DNE for function approximation that predicts progression\\nversus regression of metastatic brain disease.\\n  Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training\\nset. Half of these pairs, separated in time, qualified as disease progression,\\nwhile the other 25 images constituted regression. We trained the parameters of\\na relatively small CNN via mutations that consisted of random CNN weight\\nadjustments and mutation fitness. We then incorporated the best mutations into\\nthe next generations CNN, repeating this process for approximately 50,000\\ngenerations. We applied the CNNs to our training set, as well as a separate\\ntesting set with the same class balance of 25 progression and 25 regression\\nimages.\\n  Results: DNE achieved monotonic convergence to 100% training set accuracy.\\nDNE also converged monotonically to 100% testing set accuracy.\\n  Conclusion: DNE can accurately classify brain-metastatic disease progression\\nversus regression. Future work will extend the input from 2D image slices to\\nfull 3D volumes, and include the category of no change. We believe that an\\napproach such as our could ultimately provide a useful adjunct to RANO/RECIST\\nassessment.',\n",
       "    'author': [{'name': 'Joseph Stember'},\n",
       "     {'name': 'Robert Young'},\n",
       "     {'name': 'Hrithwik Shalu'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.12853v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.12853v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.06214v1',\n",
       "    'updated': '2022-04-13T07:35:39Z',\n",
       "    'published': '2022-04-13T07:35:39Z',\n",
       "    'title': 'Context-based Deep Learning Architecture with Optimal Integration Layer\\n  for Image Parsing',\n",
       "    'summary': 'Deep learning models have been efficient lately on image parsing tasks.\\nHowever, deep learning models are not fully capable of exploiting visual and\\ncontextual information simultaneously. The proposed three-layer context-based\\ndeep architecture is capable of integrating context explicitly with visual\\ninformation. The novel idea here is to have a visual layer to learn visual\\ncharacteristics from binary class-based learners, a contextual layer to learn\\ncontext, and then an integration layer to learn from both via genetic\\nalgorithm-based optimal fusion to produce a final decision. The experimental\\noutcomes when evaluated on benchmark datasets are promising. Further analysis\\nshows that optimized network weights can improve performance and make stable\\npredictions.',\n",
       "    'author': [{'name': 'Ranju Mandal'},\n",
       "     {'name': 'Basim Azam'},\n",
       "     {'name': 'Brijesh Verma'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICONIP2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.06214v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.06214v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.05167v2',\n",
       "    'updated': '2022-05-27T05:04:35Z',\n",
       "    'published': '2022-05-09T17:15:54Z',\n",
       "    'title': 'Robustness of Humans and Machines on Object Recognition with Extreme\\n  Image Transformations',\n",
       "    'summary': 'Recent neural network architectures have claimed to explain data from the\\nhuman visual cortex. Their demonstrated performance is however still limited by\\nthe dependence on exploiting low-level features for solving visual tasks. This\\nstrategy limits their performance in case of out-of-distribution/adversarial\\ndata. Humans, meanwhile learn abstract concepts and are mostly unaffected by\\neven extreme image distortions. Humans and networks employ strikingly different\\nstrategies to solve visual tasks. To probe this, we introduce a novel set of\\nimage transforms and evaluate humans and networks on an object recognition\\ntask. We found performance for a few common networks quickly decreases while\\nhumans are able to recognize objects with a high accuracy.',\n",
       "    'author': [{'name': 'Dakarai Crowder'}, {'name': 'Girik Malik'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at CVPR NeuroVision Workshop'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.05167v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.05167v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.05874v5',\n",
       "    'updated': '2022-08-05T18:26:34Z',\n",
       "    'published': '2022-05-12T04:37:35Z',\n",
       "    'title': 'Distinction Maximization Loss: Efficiently Improving Out-of-Distribution\\n  Detection and Uncertainty Estimation by Replacing the Loss and Calibrating',\n",
       "    'summary': 'Building robust deterministic neural networks remains a challenge. On the one\\nhand, some approaches improve out-of-distribution detection at the cost of\\nreducing classification accuracy in some situations. On the other hand, some\\nmethods simultaneously increase classification accuracy, uncertainty\\nestimation, and out-of-distribution detection at the expense of reducing the\\ninference efficiency. In this paper, we propose training deterministic neural\\nnetworks using our DisMax loss, which works as a drop-in replacement for the\\nusual SoftMax loss (i.e., the combination of the linear output layer, the\\nSoftMax activation, and the cross-entropy loss). Starting from the IsoMax+\\nloss, we create each logit based on the distances to all prototypes, rather\\nthan just the one associated with the correct class. We also introduce a\\nmechanism to combine images to construct what we call fractional probability\\nregularization. Moreover, we present a fast way to calibrate the network after\\ntraining. Finally, we propose a composite score to perform out-of-distribution\\ndetection. Our experiments show that DisMax usually outperforms current\\napproaches simultaneously in classification accuracy, uncertainty estimation,\\nand out-of-distribution detection while maintaining deterministic neural\\nnetwork inference efficiency. The code to reproduce the results is available at\\nhttps://github.com/dlmacedo/distinction-maximization-loss.',\n",
       "    'author': [{'name': 'David Macêdo'},\n",
       "     {'name': 'Cleber Zanchettin'},\n",
       "     {'name': 'Teresa Ludermir'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.05874v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.05874v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.07076v1',\n",
       "    'updated': '2022-05-14T14:47:10Z',\n",
       "    'published': '2022-05-14T14:47:10Z',\n",
       "    'title': 'Spiking Approximations of the MaxPooling Operation in Deep SNNs',\n",
       "    'summary': \"Spiking Neural Networks (SNNs) are an emerging domain of biologically\\ninspired neural networks that have shown promise for low-power AI. A number of\\nmethods exist for building deep SNNs, with Artificial Neural Network\\n(ANN)-to-SNN conversion being highly successful. MaxPooling layers in\\nConvolutional Neural Networks (CNNs) are an integral component to downsample\\nthe intermediate feature maps and introduce translational invariance, but the\\nabsence of their hardware-friendly spiking equivalents limits such CNNs'\\nconversion to deep SNNs. In this paper, we present two hardware-friendly\\nmethods to implement Max-Pooling in deep SNNs, thus facilitating easy\\nconversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute\\nSNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware\\n(with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our\\napproach.\",\n",
       "    'author': [{'name': 'Ramashish Gaurav'},\n",
       "     {'name': 'Bryan Tripp'},\n",
       "     {'name': 'Apurva Narayan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted in IJCNN-2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.07076v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.07076v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.08836v2',\n",
       "    'updated': '2022-10-21T07:37:37Z',\n",
       "    'published': '2022-05-18T10:08:28Z',\n",
       "    'title': 'Large Neural Networks Learning from Scratch with Very Few Data and\\n  without Explicit Regularization',\n",
       "    'summary': 'Recent findings have shown that highly over-parameterized Neural Networks\\ngeneralize without pretraining or explicit regularization. It is achieved with\\nzero training error, i.e., complete over-fitting by memorizing the training\\ndata. This is surprising, since it is completely against traditional machine\\nlearning wisdom. In our empirical study we fortify these findings in the domain\\nof fine-grained image classification. We show that very large Convolutional\\nNeural Networks with millions of weights do learn with only a handful of\\ntraining samples and without image augmentation, explicit regularization or\\npretraining. We train the architectures ResNet018, ResNet101 and VGG19 on\\nsubsets of the difficult benchmark datasets Caltech101, CUB_200_2011,\\nFGVCAircraft, Flowers102 and StanfordCars with 100 classes and more, perform a\\ncomprehensive comparative study and draw implications for the practical\\napplication of CNNs. Finally, we show that a randomly initialized VGG19 with\\n140 million weights learns to distinguish airplanes and motorbikes with up to\\n95% accuracy using only 20 training samples per class.',\n",
       "    'author': [{'name': 'Christoph Linse'}, {'name': 'Thomas Martinetz'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages, 3 figures, 4 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.08836v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.08836v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.10937v2',\n",
       "    'updated': '2022-05-25T12:49:04Z',\n",
       "    'published': '2022-05-22T21:54:33Z',\n",
       "    'title': 'muNet: Evolving Pretrained Deep Neural Networks into Scalable\\n  Auto-tuning Multitask Systems',\n",
       "    'summary': 'Most uses of machine learning today involve training a model from scratch for\\na particular task, or sometimes starting with a model pretrained on a related\\ntask and then fine-tuning on a downstream task. Both approaches offer limited\\nknowledge transfer between different tasks, time-consuming human-driven\\ncustomization to individual tasks and high computational costs especially when\\nstarting from randomly initialized models. We propose a method that uses the\\nlayers of a pretrained deep neural network as building blocks to construct an\\nML system that can jointly solve an arbitrary number of tasks. The resulting\\nsystem can leverage cross tasks knowledge transfer, while being immune from\\ncommon drawbacks of multitask approaches such as catastrophic forgetting,\\ngradients interference and negative transfer. We define an evolutionary\\napproach designed to jointly select the prior knowledge relevant for each task,\\nchoose the subset of the model parameters to train and dynamically auto-tune\\nits hyperparameters. Furthermore, a novel scale control method is employed to\\nachieve quality/size trade-offs that outperform common fine-tuning techniques.\\nCompared with standard fine-tuning on a benchmark of 10 diverse image\\nclassification tasks, the proposed model improves the average accuracy by 2.39%\\nwhile using 47% less parameters per task.',\n",
       "    'author': [{'name': 'Andrea Gesmundo'}, {'name': 'Jeff Dean'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.10937v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.10937v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.12013v2',\n",
       "    'updated': '2022-08-03T07:23:49Z',\n",
       "    'published': '2022-05-24T12:00:39Z',\n",
       "    'title': 'Naive Few-Shot Learning: Sequence Consistency Evaluation',\n",
       "    'summary': 'Cognitive psychologists often use the term $\\\\textit{fluid intelligence}$ to\\ndescribe the ability of humans to solve novel tasks without any prior training.\\nIn contrast to humans, deep neural networks can perform cognitive tasks only\\nafter extensive (pre-)training with a large number of relevant examples.\\nMotivated by fluid intelligence research in the cognitive sciences, we built a\\nbenchmark task which we call sequence consistency evaluation (SCE) that can be\\nused to address this gap. Solving the SCE task requires the ability to extract\\nsimple rules from sequences, a basic computation that in humans, is required\\nfor solving various intelligence tests. We tested $\\\\textit{untrained}$ (naive)\\ndeep learning models in the SCE task. Specifically, we tested two networks that\\ncan learn latent relations, Relation Networks (RN) and Contrastive Predictive\\nCoding (CPC). We found that the latter, which imposes a causal structure on the\\nlatent relations performs better. We then show that naive few-shot learning of\\nsequences can be successfully used for anomaly detection in two different\\ntasks, visual and auditory, without any prior training.',\n",
       "    'author': [{'name': 'Tomer Barak'}, {'name': 'Yonatan Loewenstein'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.12013v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.12013v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.03429v2',\n",
       "    'updated': '2022-06-09T06:24:12Z',\n",
       "    'published': '2022-06-07T16:29:51Z',\n",
       "    'title': 'Generating Long Videos of Dynamic Scenes',\n",
       "    'summary': 'We present a video generation model that accurately reproduces object motion,\\nchanges in camera viewpoint, and new content that arises over time. Existing\\nvideo generation methods often fail to produce new content as a function of\\ntime while maintaining consistencies expected in real environments, such as\\nplausible dynamics and object persistence. A common failure case is for content\\nto never change due to over-reliance on inductive biases to provide temporal\\nconsistency, such as a single latent code that dictates content for the entire\\nvideo. On the other extreme, without long-term consistency, generated videos\\nmay morph unrealistically between different scenes. To address these\\nlimitations, we prioritize the time axis by redesigning the temporal latent\\nrepresentation and learning long-term consistency from data by training on\\nlonger videos. To this end, we leverage a two-phase training strategy, where we\\nseparately train using longer videos at a low resolution and shorter videos at\\na high resolution. To evaluate the capabilities of our model, we introduce two\\nnew benchmark datasets with explicit focus on long-term temporal dynamics.',\n",
       "    'author': [{'name': 'Tim Brooks'},\n",
       "     {'name': 'Janne Hellsten'},\n",
       "     {'name': 'Miika Aittala'},\n",
       "     {'name': 'Ting-Chun Wang'},\n",
       "     {'name': 'Timo Aila'},\n",
       "     {'name': 'Jaakko Lehtinen'},\n",
       "     {'name': 'Ming-Yu Liu'},\n",
       "     {'name': 'Alexei A. Efros'},\n",
       "     {'name': 'Tero Karras'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.03429v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.03429v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.04016v1',\n",
       "    'updated': '2022-06-08T17:08:56Z',\n",
       "    'published': '2022-06-08T17:08:56Z',\n",
       "    'title': 'SYNERgy between SYNaptic consolidation and Experience Replay for general\\n  continual learning',\n",
       "    'summary': 'Continual learning (CL) in the brain is facilitated by a complex set of\\nmechanisms. This includes the interplay of multiple memory systems for\\nconsolidating information as posited by the complementary learning systems\\n(CLS) theory and synaptic consolidation for protecting the acquired knowledge\\nfrom erasure. Thus, we propose a general CL method that creates a synergy\\nbetween SYNaptic consolidation and dual memory Experience Replay (SYNERgy). Our\\nmethod maintains a semantic memory that accumulates and consolidates\\ninformation across the tasks and interacts with the episodic memory for\\neffective replay. It further employs synaptic consolidation by tracking the\\nimportance of parameters during the training trajectory and anchoring them to\\nthe consolidated parameters in the semantic memory. To the best of our\\nknowledge, our study is the first to employ dual memory experience replay in\\nconjunction with synaptic consolidation that is suitable for general CL whereby\\nthe network does not utilize task boundaries or task labels during training or\\ninference. Our evaluation on various challenging CL scenarios and\\ncharacteristics analyses demonstrate the efficacy of incorporating both\\nsynaptic consolidation and CLS theory in enabling effective CL in DNNs.',\n",
       "    'author': [{'name': 'Fahad Sarfraz'},\n",
       "     {'name': 'Elahe Arani'},\n",
       "     {'name': 'Bahram Zonooz'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at 1st Conference on Lifelong Learning Agents (CoLLAs 2022)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.04016v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.04016v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.05365v1',\n",
       "    'updated': '2022-06-10T22:26:29Z',\n",
       "    'published': '2022-06-10T22:26:29Z',\n",
       "    'title': 'Object Detection, Recognition, Deep Learning, and the Universal Law of\\n  Generalization',\n",
       "    'summary': 'Object detection and recognition are fundamental functions underlying the\\nsuccess of species. Because the appearance of an object exhibits a large\\nvariability, the brain has to group these different stimuli under the same\\nobject identity, a process of generalization. Does the process of\\ngeneralization follow some general principles or is it an ad-hoc\\n\"bag-of-tricks\"? The Universal Law of Generalization provided evidence that\\ngeneralization follows similar properties across a variety of species and\\ntasks. Here we test the hypothesis that the internal representations underlying\\ngeneralization reflect the natural properties of object detection and\\nrecognition in our environment rather than the specifics of the system solving\\nthese problems. By training a deep-neural-network with images of \"clear\" and\\n\"camouflaged\" animals, we found that with a proper choice of category\\nprototypes, the generalization functions are monotone decreasing, similar to\\nthe generalization functions of biological systems. Our findings support the\\nhypothesis of the study.',\n",
       "    'author': [{'name': 'Faris B. Rustom'},\n",
       "     {'name': 'Haluk Öğmen'},\n",
       "     {'name': 'Arash Yazdanbakhsh'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.05365v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.05365v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.08138v2',\n",
       "    'updated': '2022-07-11T08:43:12Z',\n",
       "    'published': '2022-06-15T10:27:23Z',\n",
       "    'title': 'Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone\\n  fine-tuning without episodic meta-learning dominates for few-shot learning\\n  image classification',\n",
       "    'summary': 'Although deep neural networks are capable of achieving performance superior\\nto humans on various tasks, they are notorious for requiring large amounts of\\ndata and computing resources, restricting their success to domains where such\\nresources are available. Metalearning methods can address this problem by\\ntransferring knowledge from related tasks, thus reducing the amount of data and\\ncomputing resources needed to learn new tasks. We organize the MetaDL\\ncompetition series, which provide opportunities for research groups all over\\nthe world to create and experimentally assess new meta-(deep)learning solutions\\nfor real problems. In this paper, authored collaboratively between the\\ncompetition organizers and the top-ranked participants, we describe the design\\nof the competition, the datasets, the best experimental results, as well as the\\ntop-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active\\nteams who made it to the final phase (by outperforming the baseline), making\\nover 100 code submissions during the feedback phase. The solutions of the top\\nparticipants have been open-sourced. The lessons learned include that learning\\ngood representations is essential for effective transfer learning.',\n",
       "    'author': [{'name': 'Adrian El Baz'},\n",
       "     {'name': 'Ihsan Ullah'},\n",
       "     {'name': 'Edesio Alcobaça'},\n",
       "     {'name': 'André C. P. L. F. Carvalho'},\n",
       "     {'name': 'Hong Chen'},\n",
       "     {'name': 'Fabio Ferreira'},\n",
       "     {'name': 'Henry Gouk'},\n",
       "     {'name': 'Chaoyu Guan'},\n",
       "     {'name': 'Isabelle Guyon'},\n",
       "     {'name': 'Timothy Hospedales'},\n",
       "     {'name': 'Shell Hu'},\n",
       "     {'name': 'Mike Huisman'},\n",
       "     {'name': 'Frank Hutter'},\n",
       "     {'name': 'Zhengying Liu'},\n",
       "     {'name': 'Felix Mohr'},\n",
       "     {'name': 'Ekrem Öztürk'},\n",
       "     {'name': 'Jan N. van Rijn'},\n",
       "     {'name': 'Haozhe Sun'},\n",
       "     {'name': 'Xin Wang'},\n",
       "     {'name': 'Wenwu Zhu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'version 2 is the correct version, including supplementary material at\\n  the end'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2021 Competition and Demonstration Track, Dec 2021,\\n  On-line, United States'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.08138v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.08138v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.13498v1',\n",
       "    'updated': '2022-06-27T17:59:08Z',\n",
       "    'published': '2022-06-27T17:59:08Z',\n",
       "    'title': 'Auditing Visualizations: Transparency Methods Struggle to Detect\\n  Anomalous Behavior',\n",
       "    'summary': 'Transparency methods such as model visualizations provide information that\\noutputs alone might miss, since they describe the internals of neural networks.\\nBut can we trust that model explanations reflect model behavior? For instance,\\ncan they diagnose abnormal behavior such as backdoors or shape bias? To\\nevaluate model explanations, we define a model as anomalous if it differs from\\na reference set of normal models, and we test whether transparency methods\\nassign different explanations to anomalous and normal models. We find that\\nwhile existing methods can detect stark anomalies such as shape bias or\\nadversarial training, they struggle to identify more subtle anomalies such as\\nmodels trained on incomplete data. Moreover, they generally fail to distinguish\\nthe inputs that induce anomalous behavior, e.g. images containing a backdoor\\ntrigger. These results reveal new blind spots in existing model explanations,\\npointing to the need for further method development.',\n",
       "    'author': [{'name': 'Jean-Stanislas Denain'},\n",
       "     {'name': 'Jacob Steinhardt'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.13498v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.13498v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.00787v3',\n",
       "    'updated': '2023-01-01T02:31:43Z',\n",
       "    'published': '2022-07-02T10:00:35Z',\n",
       "    'title': 'Object Representations as Fixed Points: Training Iterative Refinement\\n  Algorithms with Implicit Differentiation',\n",
       "    'summary': 'Iterative refinement -- start with a random guess, then iteratively improve\\nthe guess -- is a useful paradigm for representation learning because it offers\\na way to break symmetries among equally plausible explanations for the data.\\nThis property enables the application of such methods to infer representations\\nof sets of entities, such as objects in physical scenes, structurally\\nresembling clustering algorithms in latent space. However, most prior works\\ndifferentiate through the unrolled refinement process, which can make\\noptimization challenging. We observe that such methods can be made\\ndifferentiable by means of the implicit function theorem, and develop an\\nimplicit differentiation approach that improves the stability and tractability\\nof training by decoupling the forward and backward passes. This connection\\nenables us to apply advances in optimizing implicit layers to not only improve\\nthe optimization of the slot attention module in SLATE, a state-of-the-art\\nmethod for learning entity representations, but do so with constant space and\\ntime complexity in backpropagation and only one additional line of code.',\n",
       "    'author': [{'name': 'Michael Chang'},\n",
       "     {'name': 'Thomas L. Griffiths'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '19 pages, 13 figures, Accepted to the 36th Conference on Neural\\n  Information Processing Systems (NeurIPS 2022)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.00787v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.00787v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.05135v1',\n",
       "    'updated': '2022-06-17T11:16:28Z',\n",
       "    'published': '2022-06-17T11:16:28Z',\n",
       "    'title': 'FreeREA: Training-Free Evolution-based Architecture Search',\n",
       "    'summary': 'In the last decade, most research in Machine Learning contributed to the\\nimprovement of existing models, with the aim of increasing the performance of\\nneural networks for the solution of a variety of different tasks. However, such\\nadvancements often come at the cost of an increase of model memory and\\ncomputational requirements. This represents a significant limitation for the\\ndeployability of research output in realistic settings, where the cost, the\\nenergy consumption, and the complexity of the framework play a crucial role. To\\nsolve this issue, the designer should search for models that maximise the\\nperformance while limiting its footprint. Typical approaches to reach this goal\\nrely either on manual procedures, which cannot guarantee the optimality of the\\nfinal design, or upon Neural Architecture Search algorithms to automatise the\\nprocess, at the expenses of extremely high computational time. This paper\\nprovides a solution for the fast identification of a neural network that\\nmaximises the model accuracy while preserving size and computational\\nconstraints typical of tiny devices. Our approach, named FreeREA, is a custom\\ncell-based evolution NAS algorithm that exploits an optimised combination of\\ntraining-free metrics to rank architectures during the search, thus without\\nneed of model training. Our experiments, carried out on the common benchmarks\\nNAS-Bench-101 and NATS-Bench, demonstrate that i) FreeREA is the first method\\nable to provide very accurate models in minutes of search time; ii) it\\noutperforms State of the Art training-based and training-free techniques in all\\nthe datasets and benchmarks considered, and iii) it can easily generalise to\\nconstrained scenarios, representing a competitive solution for fast Neural\\nArchitecture Search in generic constrained applications.',\n",
       "    'author': [{'name': 'Niccolò Cavagnero'},\n",
       "     {'name': 'Luca Robbiano'},\n",
       "     {'name': 'Barbara Caputo'},\n",
       "     {'name': 'Giuseppe Averta'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '16 pages, 4 figurres'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.05135v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.05135v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.12534v2',\n",
       "    'updated': '2022-08-19T21:13:11Z',\n",
       "    'published': '2022-07-25T21:15:47Z',\n",
       "    'title': 'Trainability Preserving Neural Structured Pruning',\n",
       "    'summary': 'Several recent works empirically find finetuning learning rate is critical to\\nthe final performance in neural network structured pruning. Further researches\\nfind that the network trainability broken by pruning answers for it, thus\\ncalling for an urgent need to recover trainability before finetuning. Existing\\nattempts propose to exploit weight orthogonalization to achieve dynamical\\nisometry for improved trainability. However, they only work for linear MLP\\nnetworks. How to develop a filter pruning method that maintains or recovers\\ntrainability and is scalable to modern deep networks remains elusive. In this\\npaper, we present trainability preserving pruning (TPP), a regularization-based\\nstructured pruning method that can effectively maintain trainability during\\nsparsification. Specifically, TPP regularizes the gram matrix of convolutional\\nkernels so as to de-correlate the pruned filters from the kept filters. Beside\\nthe convolutional layers, we also propose to regularize the BN parameters for\\nbetter preserving trainability. Empirically, TPP can compete with the\\nground-truth dynamical isometry recovery method on linear MLP networks. On\\nnon-linear networks (ResNet56/VGG19, CIFAR datasets), it outperforms the other\\ncounterpart solutions by a large margin. Moreover, TPP can also work\\neffectively with modern deep networks (ResNets) on ImageNet, delivering\\nencouraging performance in comparison to many top-performing filter pruning\\nmethods. To our best knowledge, this is the first approach that effectively\\nmaintains trainability during pruning for the large-scale deep neural networks.',\n",
       "    'author': [{'name': 'Huan Wang'}, {'name': 'Yun Fu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '18 pages, 4 figures, 5 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.12534v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.12534v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.14686v1',\n",
       "    'updated': '2022-08-31T08:31:02Z',\n",
       "    'published': '2022-08-31T08:31:02Z',\n",
       "    'title': \"NeurIPS'22 Cross-Domain MetaDL competition: Design and baseline results\",\n",
       "    'summary': 'We present the design and baseline results for a new challenge in the\\nChaLearn meta-learning series, accepted at NeurIPS\\'22, focusing on\\n\"cross-domain\" meta-learning. Meta-learning aims to leverage experience gained\\nfrom previous tasks to solve new tasks efficiently (i.e., with better\\nperformance, little training data, and/or modest computational resources).\\nWhile previous challenges in the series focused on within-domain few-shot\\nlearning problems, with the aim of learning efficiently N-way k-shot tasks\\n(i.e., N class classification problems with k training examples), this\\ncompetition challenges the participants to solve \"any-way\" and \"any-shot\"\\nproblems drawn from various domains (healthcare, ecology, biology,\\nmanufacturing, and others), chosen for their humanitarian and societal impact.\\nTo that end, we created Meta-Album, a meta-dataset of 40 image classification\\ndatasets from 10 domains, from which we carve out tasks with any number of\\n\"ways\" (within the range 2-20) and any number of \"shots\" (within the range\\n1-20). The competition is with code submission, fully blind-tested on the\\nCodaLab challenge platform. The code of the winners will be open-sourced,\\nenabling the deployment of automated machine learning solutions for few-shot\\nimage classification across several domains.',\n",
       "    'author': [{'name': 'Dustin Carrión-Ojeda',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'LISN, TAU'}},\n",
       "     {'name': 'Hong Chen',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CST'}},\n",
       "     {'name': 'Adrian El Baz',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CVC'}},\n",
       "     {'name': 'Sergio Escalera',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CVC'}},\n",
       "     {'name': 'Chaoyu Guan',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CST'}},\n",
       "     {'name': 'Isabelle Guyon',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'LISN, TAU'}},\n",
       "     {'name': 'Ihsan Ullah',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'LISN, TAU'}},\n",
       "     {'name': 'Xin Wang',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CST'}},\n",
       "     {'name': 'Wenwu Zhu',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CST'}}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Meta-Knowledge Transfer/Communication in Different Systems, Sep 2022,\\n  Grenoble, France'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.14686v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.14686v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.04362v1',\n",
       "    'updated': '2022-09-09T15:51:39Z',\n",
       "    'published': '2022-09-09T15:51:39Z',\n",
       "    'title': 'EDeNN: Event Decay Neural Networks for low latency vision',\n",
       "    'summary': \"Despite the success of neural networks in computer vision tasks, digital\\n'neurons' are a very loose approximation of biological neurons. Today's\\nlearning approaches are designed to function on digital devices with digital\\ndata representations such as image frames. In contrast, biological vision\\nsystems are generally much more capable and efficient than state-of-the-art\\ndigital computer vision algorithms. Event cameras are an emerging sensor\\ntechnology which imitates biological vision with asynchronously firing pixels,\\neschewing the concept of the image frame. To leverage modern learning\\ntechniques, many event-based algorithms are forced to accumulate events back to\\nimage frames, somewhat squandering the advantages of event cameras.\\n  We follow the opposite paradigm and develop a new type of neural network\\nwhich operates closer to the original event data stream. We demonstrate\\nstate-of-the-art performance in angular velocity regression and competitive\\noptical flow estimation, while avoiding difficulties related to training SNN.\\nFurthermore, the processing latency of our proposed approached is less than\\n1/10 any other implementation, while continuous inference increases this\\nimprovement by another order of magnitude.\",\n",
       "    'author': [{'name': 'Celyn Walters'}, {'name': 'Simon Hadfield'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages, 5 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.04362v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.04362v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.06119v3',\n",
       "    'updated': '2022-09-23T17:39:14Z',\n",
       "    'published': '2022-09-10T14:26:04Z',\n",
       "    'title': \"APTx: better activation function than MISH, SWISH, and ReLU's variants\\n  used in deep learning\",\n",
       "    'summary': \"Activation Functions introduce non-linearity in the deep neural networks.\\nThis nonlinearity helps the neural networks learn faster and efficiently from\\nthe dataset. In deep learning, many activation functions are developed and used\\nbased on the type of problem statement. ReLU's variants, SWISH, and MISH are\\ngoto activation functions. MISH function is considered having similar or even\\nbetter performance than SWISH, and much better than ReLU. In this paper, we\\npropose an activation function named APTx which behaves similar to MISH, but\\nrequires lesser mathematical operations to compute. The lesser computational\\nrequirements of APTx does speed up the model training, and thus also reduces\\nthe hardware requirement for the deep learning model.\",\n",
       "    'author': {'name': 'Ravin Kumar'},\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.51483/IJAIML.2.2.2022.56-61'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.51483/IJAIML.2.2.2022.56-61',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2209.06119v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.06119v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 6 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Journal of Artificial Intelligence and Machine\\n  Learning, 2(2), 56-61 (2022)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.06399v1',\n",
       "    'updated': '2022-09-14T03:35:25Z',\n",
       "    'published': '2022-09-14T03:35:25Z',\n",
       "    'title': 'A Survey on Evolutionary Computation for Computer Vision and Image\\n  Analysis: Past, Present, and Future Trends',\n",
       "    'summary': 'Computer vision (CV) is a big and important field in artificial intelligence\\ncovering a wide range of applications. Image analysis is a major task in CV\\naiming to extract, analyse and understand the visual content of images.\\nHowever, image-related tasks are very challenging due to many factors, e.g.,\\nhigh variations across images, high dimensionality, domain expertise\\nrequirement, and image distortions. Evolutionary computation (EC) approaches\\nhave been widely used for image analysis with significant achievement. However,\\nthere is no comprehensive survey of existing EC approaches to image analysis.\\nTo fill this gap, this paper provides a comprehensive survey covering all\\nessential EC approaches to important image analysis tasks including edge\\ndetection, image segmentation, image feature analysis, image classification,\\nobject detection, and others. This survey aims to provide a better\\nunderstanding of evolutionary computer vision (ECV) by discussing the\\ncontributions of different approaches and exploring how and why EC is used for\\nCV and image analysis. The applications, challenges, issues, and trends\\nassociated to this research field are also discussed and summarised to provide\\nfurther guidelines and opportunities for future research.',\n",
       "    'author': [{'name': 'Ying Bi'},\n",
       "     {'name': 'Bing Xue'},\n",
       "     {'name': 'Pablo Mesejo'},\n",
       "     {'name': 'Stefano Cagnoni'},\n",
       "     {'name': 'Mengjie Zhang'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TEVC.2022.3220747'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TEVC.2022.3220747',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2209.06399v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.06399v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Conditionally accepted by IEEE Transactions on Evolutionary\\n  Computation'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Transactions on Evolutionary Computationm, 2022,\\n  https://ieeexplore.ieee.org/document/9943992/'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.07326v3',\n",
       "    'updated': '2022-11-06T08:45:40Z',\n",
       "    'published': '2022-09-15T14:36:17Z',\n",
       "    'title': 'A Continual Development Methodology for Large-scale Multitask Dynamic ML\\n  Systems',\n",
       "    'summary': 'The traditional Machine Learning (ML) methodology requires to fragment the\\ndevelopment and experimental process into disconnected iterations whose\\nfeedback is used to guide design or tuning choices. This methodology has\\nmultiple efficiency and scalability disadvantages, such as leading to spend\\nsignificant resources into the creation of multiple trial models that do not\\ncontribute to the final solution.The presented work is based on the intuition\\nthat defining ML models as modular and extensible artefacts allows to introduce\\na novel ML development methodology enabling the integration of multiple design\\nand evaluation iterations into the continuous enrichment of a single unbounded\\nintelligent system. We define a novel method for the generation of dynamic\\nmultitask ML models as a sequence of extensions and generalizations. We first\\nanalyze the capabilities of the proposed method by using the standard ML\\nempirical evaluation methodology. Finally, we propose a novel continuous\\ndevelopment methodology that allows to dynamically extend a pre-existing\\nmultitask large-scale ML system while analyzing the properties of the proposed\\nmethod extensions. This results in the generation of an ML model capable of\\njointly solving 124 image classification tasks achieving state of the art\\nquality with improved size and compute cost.',\n",
       "    'author': {'name': 'Andrea Gesmundo'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: text overlap with arXiv:2205.12755'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.07326v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.07326v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.13400v2',\n",
       "    'updated': '2022-12-25T06:18:19Z',\n",
       "    'published': '2022-09-26T10:43:29Z',\n",
       "    'title': 'Activation Learning by Local Competitions',\n",
       "    'summary': 'Despite its great success, backpropagation has certain limitations that\\nnecessitate the investigation of new learning methods. In this study, we\\npresent a biologically plausible local learning rule that improves upon Hebb\\'s\\nwell-known proposal and discovers unsupervised features by local competitions\\namong neurons. This simple learning rule enables the creation of a forward\\nlearning paradigm called activation learning, in which the output activation\\n(sum of the squared output) of the neural network estimates the likelihood of\\nthe input patterns, or \"learn more, activate more\" in simpler terms. For\\nclassification on a few small classical datasets, activation learning performs\\ncomparably to backpropagation using a fully connected network, and outperforms\\nbackpropagation when there are fewer training samples or unpredictable\\ndisturbances. Additionally, the same trained network can be used for a variety\\nof tasks, including image generation and completion. Activation learning also\\nachieves state-of-the-art performance on several real-world datasets for\\nanomaly detection. This new learning paradigm, which has the potential to unify\\nsupervised, unsupervised, and semi-supervised learning and is reasonably more\\nresistant to adversarial attacks, deserves in-depth investigation.',\n",
       "    'author': {'name': 'Hongchao Zhou'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Updated Equation (13) for the modification rule with feedback; Adding\\n  discussions regarding activation learning for anormaly detection'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.13400v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.13400v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.11114v1',\n",
       "    'updated': '2022-10-20T09:17:02Z',\n",
       "    'published': '2022-10-20T09:17:02Z',\n",
       "    'title': 'Pruning by Active Attention Manipulation',\n",
       "    'summary': \"Filter pruning of a CNN is typically achieved by applying discrete masks on\\nthe CNN's filter weights or activation maps, post-training. Here, we present a\\nnew filter-importance-scoring concept named pruning by active attention\\nmanipulation (PAAM), that sparsifies the CNN's set of filters through a\\nparticular attention mechanism, during-training. PAAM learns analog filter\\nscores from the filter weights by optimizing a cost function regularized by an\\nadditive term in the scores. As the filters are not independent, we use\\nattention to dynamically learn their correlations. Moreover, by training the\\npruning scores of all layers simultaneously, PAAM can account for layer\\ninter-dependencies, which is essential to finding a performant sparse\\nsub-network. PAAM can also train and generate a pruned network from scratch in\\na straightforward, one-stage training process without requiring a pre-trained\\nnetwork. Finally, PAAM does not need layer-specific hyperparameters and\\npre-defined layer budgets, since it can implicitly determine the appropriate\\nnumber of filters in each layer. Our experimental results on different network\\narchitectures suggest that PAAM outperforms state-of-the-art structured-pruning\\nmethods (SOTA). On CIFAR-10 dataset, without requiring a pre-trained baseline\\nnetwork, we obtain 1.02% and 1.19% accuracy gain and 52.3% and 54% parameters\\nreduction, on ResNet56 and ResNet110, respectively. Similarly, on the ImageNet\\ndataset, PAAM achieves 1.06% accuracy gain while pruning 51.1% of the\\nparameters on ResNet50. For Cifar-10, this is better than the SOTA with a\\nmargin of 9.5% and 6.6%, respectively, and on ImageNet with a margin of 11%.\",\n",
       "    'author': [{'name': 'Zahra Babaiee'},\n",
       "     {'name': 'Lucas Liebenwein'},\n",
       "     {'name': 'Ramin Hasani'},\n",
       "     {'name': 'Daniela Rus'},\n",
       "     {'name': 'Radu Grosu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: substantial text overlap with arXiv:2204.07412'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.11114v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.11114v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.14499v1',\n",
       "    'updated': '2022-11-26T07:03:37Z',\n",
       "    'published': '2022-11-26T07:03:37Z',\n",
       "    'title': 'Deep neuroevolution for limited, heterogeneous data: proof-of-concept\\n  application to Neuroblastoma brain metastasis using a small virtual pooled\\n  image collection',\n",
       "    'summary': 'Artificial intelligence (AI) in radiology has made great strides in recent\\nyears, but many hurdles remain. Overfitting and lack of generalizability\\nrepresent important ongoing challenges hindering accurate and dependable\\nclinical deployment. If AI algorithms can avoid overfitting and achieve true\\ngeneralizability, they can go from the research realm to the forefront of\\nclinical work. Recently, small data AI approaches such as deep neuroevolution\\n(DNE) have avoided overfitting small training sets. We seek to address both\\noverfitting and generalizability by applying DNE to a virtually pooled data set\\nconsisting of images from various institutions. Our use case is classifying\\nneuroblastoma brain metastases on MRI. Neuroblastoma is well-suited for our\\ngoals because it is a rare cancer. Hence, studying this pediatric disease\\nrequires a small data approach. As a tertiary care center, the neuroblastoma\\nimages in our local Picture Archiving and Communication System (PACS) are\\nlargely from outside institutions. These multi-institutional images provide a\\nheterogeneous data set that can simulate real world clinical deployment. As in\\nprior DNE work, we used a small training set, consisting of 30 normal and 30\\nmetastasis-containing post-contrast MRI brain scans, with 37% outside images.\\nThe testing set was enriched with 83% outside images. DNE converged to a\\ntesting set accuracy of 97%. Hence, the algorithm was able to predict image\\nclass with near-perfect accuracy on a testing set that simulates real-world\\ndata. Hence, the work described here represents a considerable contribution\\ntoward clinically feasible AI.',\n",
       "    'author': [{'name': 'Subhanik Purkayastha'},\n",
       "     {'name': 'Hrithwik Shalu'},\n",
       "     {'name': 'David Gutman'},\n",
       "     {'name': 'Shakeel Modak'},\n",
       "     {'name': 'Ellen Basu'},\n",
       "     {'name': 'Brian Kushner'},\n",
       "     {'name': 'Kim Kramer'},\n",
       "     {'name': 'Sofia Haque'},\n",
       "     {'name': 'Joseph Stember'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.14499v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.14499v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.06735v1',\n",
       "    'updated': '2022-12-13T17:14:14Z',\n",
       "    'published': '2022-12-13T17:14:14Z',\n",
       "    'title': 'POPNASv3: a Pareto-Optimal Neural Architecture Search Solution for Image\\n  and Time Series Classification',\n",
       "    'summary': 'The automated machine learning (AutoML) field has become increasingly\\nrelevant in recent years. These algorithms can develop models without the need\\nfor expert knowledge, facilitating the application of machine learning\\ntechniques in the industry. Neural Architecture Search (NAS) exploits deep\\nlearning techniques to autonomously produce neural network architectures whose\\nresults rival the state-of-the-art models hand-crafted by AI experts. However,\\nthis approach requires significant computational resources and hardware\\ninvestments, making it less appealing for real-usage applications. This article\\npresents the third version of Pareto-Optimal Progressive Neural Architecture\\nSearch (POPNASv3), a new sequential model-based optimization NAS algorithm\\ntargeting different hardware environments and multiple classification tasks.\\nOur method is able to find competitive architectures within large search\\nspaces, while keeping a flexible structure and data processing pipeline to\\nadapt to different tasks. The algorithm employs Pareto optimality to reduce the\\nnumber of architectures sampled during the search, drastically improving the\\ntime efficiency without loss in accuracy. The experiments performed on images\\nand time series classification datasets provide evidence that POPNASv3 can\\nexplore a large set of assorted operators and converge to optimal architectures\\nsuited for the type of data provided under different scenarios.',\n",
       "    'author': [{'name': 'Andrea Falanti'},\n",
       "     {'name': 'Eugenio Lomurno'},\n",
       "     {'name': 'Danilo Ardagna'},\n",
       "     {'name': 'Matteo Matteucci'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.06735v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.06735v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.00620v1',\n",
       "    'updated': '2023-01-02T12:24:24Z',\n",
       "    'published': '2023-01-02T12:24:24Z',\n",
       "    'title': 'Dynamically Modular and Sparse General Continual Learning',\n",
       "    'summary': 'Real-world applications often require learning continuously from a stream of\\ndata under ever-changing conditions. When trying to learn from such\\nnon-stationary data, deep neural networks (DNNs) undergo catastrophic\\nforgetting of previously learned information. Among the common approaches to\\navoid catastrophic forgetting, rehearsal-based methods have proven effective.\\nHowever, they are still prone to forgetting due to task-interference as all\\nparameters respond to all tasks. To counter this, we take inspiration from\\nsparse coding in the brain and introduce dynamic modularity and sparsity\\n(Dynamos) for rehearsal-based general continual learning. In this setup, the\\nDNN learns to respond to stimuli by activating relevant subsets of neurons. We\\ndemonstrate the effectiveness of Dynamos on multiple datasets under challenging\\ncontinual learning evaluation protocols. Finally, we show that our method\\nlearns representations that are modular and specialized, while maintaining\\nreusability by activating subsets of neurons with overlaps corresponding to the\\nsimilarity of stimuli.',\n",
       "    'author': [{'name': 'Arnav Varma'},\n",
       "     {'name': 'Elahe Arani'},\n",
       "     {'name': 'Bahram Zonooz'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Camera ready version - 18th International Conference on Computer\\n  Vision Theory and Applications (VISAPP 2023)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.00620v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.00620v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.02464v1',\n",
       "    'updated': '2023-01-06T11:22:59Z',\n",
       "    'published': '2023-01-06T11:22:59Z',\n",
       "    'title': 'Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for\\n  Continual Learning',\n",
       "    'summary': 'In recent years we have witnessed a renewed interest in machine learning\\nmethodologies, especially for deep representation learning, that could overcome\\nbasic i.i.d. assumptions and tackle non-stationary environments subject to\\nvarious distributional shifts or sample selection biases. Within this context,\\nseveral computational approaches based on architectural priors, regularizers\\nand replay policies have been proposed with different degrees of success\\ndepending on the specific scenario in which they were developed and assessed.\\nHowever, designing comprehensive hybrid solutions that can flexibly and\\ngenerally be applied with tunable efficiency-effectiveness trade-offs still\\nseems a distant goal. In this paper, we propose \"Architect, Regularize and\\nReplay\" (ARR), an hybrid generalization of the renowned AR1 algorithm and its\\nvariants, that can achieve state-of-the-art results in classic scenarios (e.g.\\nclass-incremental learning) but also generalize to arbitrary data streams\\ngenerated from real-world datasets such as CIFAR-100, CORe50 and ImageNet-1000.',\n",
       "    'author': [{'name': 'Vincenzo Lomonaco'},\n",
       "     {'name': 'Lorenzo Pellegrini'},\n",
       "     {'name': 'Gabriele Graffieti'},\n",
       "     {'name': 'Davide Maltoni'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Book Chapter Preprint: 15 pages, 7 figures, 2 tables. arXiv admin\\n  note: text overlap with arXiv:1912.01100'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.02464v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.02464v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.05058v1',\n",
       "    'updated': '2022-12-28T12:56:15Z',\n",
       "    'published': '2022-12-28T12:56:15Z',\n",
       "    'title': 'Sparse Coding in a Dual Memory System for Lifelong Learning',\n",
       "    'summary': 'Efficient continual learning in humans is enabled by a rich set of\\nneurophysiological mechanisms and interactions between multiple memory systems.\\nThe brain efficiently encodes information in non-overlapping sparse codes,\\nwhich facilitates the learning of new associations faster with controlled\\ninterference with previous associations. To mimic sparse coding in DNNs, we\\nenforce activation sparsity along with a dropout mechanism which encourages the\\nmodel to activate similar units for semantically similar inputs and have less\\noverlap with activation patterns of semantically dissimilar inputs. This\\nprovides us with an efficient mechanism for balancing the reusability and\\ninterference of features, depending on the similarity of classes across tasks.\\nFurthermore, we employ sparse coding in a multiple-memory replay mechanism. Our\\nmethod maintains an additional long-term semantic memory that aggregates and\\nconsolidates information encoded in the synaptic weights of the working model.\\nOur extensive evaluation and characteristics analysis show that equipped with\\nthese biologically inspired mechanisms, the model can further mitigate\\nforgetting.',\n",
       "    'author': [{'name': 'Fahad Sarfraz'},\n",
       "     {'name': 'Elahe Arani'},\n",
       "     {'name': 'Bahram Zonooz'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Camera ready version - \"Thirty-Seventh AAAI Conference on Artificial\\n  Intelligence\" (AAAI-2023)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.05058v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.05058v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/cs/9902006v1',\n",
       "    'updated': '1999-02-02T16:17:16Z',\n",
       "    'published': '1999-02-02T16:17:16Z',\n",
       "    'title': 'A Discipline of Evolutionary Programming',\n",
       "    'summary': 'Genetic fitness optimization using small populations or small population\\nupdates across generations generally suffers from randomly diverging\\nevolutions. We propose a notion of highly probable fitness optimization through\\nfeasible evolutionary computing runs on small size populations. Based on\\nrapidly mixing Markov chains, the approach pertains to most types of\\nevolutionary genetic algorithms, genetic programming and the like. We establish\\nthat for systems having associated rapidly mixing Markov chains and appropriate\\nstationary distributions the new method finds optimal programs (individuals)\\nwith probability almost 1. To make the method useful would require a structured\\ndesign methodology where the development of the program and the guarantee of\\nthe rapidly mixing property go hand in hand. We analyze a simple example to\\nshow that the method is implementable. More significant examples require\\ntheoretical advances, for example with respect to the Metropolis filter.',\n",
       "    'author': {'name': 'Paul Vitanyi'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '25 pages, LaTeX source, Theoretical Computer Science, To appear'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Theoret. Comp. Sci., 241:1-2 (2000), 3--23.'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/cs/9902006v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/cs/9902006v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.DS', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2,E.1,F.1', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.07142v2',\n",
       "    'updated': '2021-08-26T00:47:33Z',\n",
       "    'published': '2021-05-15T04:58:08Z',\n",
       "    'title': 'Move2Hear: Active Audio-Visual Source Separation',\n",
       "    'summary': \"We introduce the active audio-visual source separation problem, where an\\nagent must move intelligently in order to better isolate the sounds coming from\\nan object of interest in its environment. The agent hears multiple audio\\nsources simultaneously (e.g., a person speaking down the hall in a noisy\\nhousehold) and it must use its eyes and ears to automatically separate out the\\nsounds originating from a target object within a limited time budget. Towards\\nthis goal, we introduce a reinforcement learning approach that trains movement\\npolicies controlling the agent's camera and microphone placement over time,\\nguided by the improvement in predicted audio separation quality. We demonstrate\\nour approach in scenarios motivated by both augmented reality (system is\\nalready co-located with the target object) and mobile robotics (agent begins\\narbitrarily far from the target object). Using state-of-the-art realistic\\naudio-visual simulations in 3D environments, we demonstrate our model's ability\\nto find minimal movement sequences with maximal payoff for audio source\\nseparation. Project: http://vision.cs.utexas.edu/projects/move2hear.\",\n",
       "    'author': [{'name': 'Sagnik Majumder'},\n",
       "     {'name': 'Ziad Al-Halah'},\n",
       "     {'name': 'Kristen Grauman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICCV 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.07142v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.07142v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.04279v1',\n",
       "    'updated': '2022-01-12T03:08:03Z',\n",
       "    'published': '2022-01-12T03:08:03Z',\n",
       "    'title': 'Dynamical Audio-Visual Navigation: Catching Unheard Moving Sound Sources\\n  in Unmapped 3D Environments',\n",
       "    'summary': 'Recent work on audio-visual navigation targets a single static sound in\\nnoise-free audio environments and struggles to generalize to unheard sounds. We\\nintroduce the novel dynamic audio-visual navigation benchmark in which an\\nembodied AI agent must catch a moving sound source in an unmapped environment\\nin the presence of distractors and noisy sounds. We propose an end-to-end\\nreinforcement learning approach that relies on a multi-modal architecture that\\nfuses the spatial audio-visual information from a binaural audio signal and\\nspatial occupancy maps to encode the features needed to learn a robust\\nnavigation policy for our new complex task settings. We demonstrate that our\\napproach outperforms the current state-of-the-art with better generalization to\\nunheard sounds and better robustness to noisy scenarios on the two challenging\\n3D scanned real-world datasets Replica and Matterport3D, for the static and\\ndynamic audio-visual navigation benchmarks. Our novel benchmark will be made\\navailable at http://dav-nav.cs.uni-freiburg.de.',\n",
       "    'author': {'name': 'Abdelrahman Younes'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.04279v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.04279v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.01311v1',\n",
       "    'updated': '2020-12-02T16:31:03Z',\n",
       "    'published': '2020-12-02T16:31:03Z',\n",
       "    'title': 'Top-1 CORSMAL Challenge 2020 Submission: Filling Mass Estimation Using\\n  Multi-modal Observations of Human-robot Handovers',\n",
       "    'summary': 'Human-robot object handover is a key skill for the future of human-robot\\ncollaboration. CORSMAL 2020 Challenge focuses on the perception part of this\\nproblem: the robot needs to estimate the filling mass of a container held by a\\nhuman. Although there are powerful methods in image processing and audio\\nprocessing individually, answering such a problem requires processing data from\\nmultiple sensors together. The appearance of the container, the sound of the\\nfilling, and the depth data provide essential information. We propose a\\nmulti-modal method to predict three key indicators of the filling mass: filling\\ntype, filling level, and container capacity. These indicators are then combined\\nto estimate the filling mass of a container. Our method obtained Top-1 overall\\nperformance among all submissions to CORSMAL 2020 Challenge on both public and\\nprivate subsets while showing no evidence of overfitting. Our source code is\\npublicly available: https://github.com/v-iashin/CORSMAL',\n",
       "    'author': [{'name': 'Vladimir Iashin'},\n",
       "     {'name': 'Francesca Palermo'},\n",
       "     {'name': 'Gökhan Solak'},\n",
       "     {'name': 'Claudio Coppola'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Code: https://github.com/v-iashin/CORSMAL Docker:\\n  https://hub.docker.com/r/iashin/corsmal'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.01311v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.01311v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.11583v2',\n",
       "    'updated': '2021-04-07T01:59:26Z',\n",
       "    'published': '2020-12-21T18:59:04Z',\n",
       "    'title': 'Semantic Audio-Visual Navigation',\n",
       "    'summary': \"Recent work on audio-visual navigation assumes a constantly-sounding target\\nand restricts the role of audio to signaling the target's position. We\\nintroduce semantic audio-visual navigation, where objects in the environment\\nmake sounds consistent with their semantic meaning (e.g., toilet flushing, door\\ncreaking) and acoustic events are sporadic or short in duration. We propose a\\ntransformer-based model to tackle this new semantic AudioGoal task,\\nincorporating an inferred goal descriptor that captures both spatial and\\nsemantic properties of the target. Our model's persistent multimodal memory\\nenables it to reach the goal even long after the acoustic event stops. In\\nsupport of the new task, we also expand the SoundSpaces audio simulations to\\nprovide semantically grounded sounds for an array of objects in Matterport3D.\\nOur method strongly outperforms existing audio-visual navigation methods by\\nlearning to associate semantic, acoustic, and visual cues.\",\n",
       "    'author': [{'name': 'Changan Chen'},\n",
       "     {'name': 'Ziad Al-Halah'},\n",
       "     {'name': 'Kristen Grauman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page:\\n  http://vision.cs.utexas.edu/projects/semantic-audio-visual-navigation'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.11583v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.11583v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.00628v2',\n",
       "    'updated': '2023-01-15T02:41:34Z',\n",
       "    'published': '2022-04-04T17:59:37Z',\n",
       "    'title': 'Learning Neural Acoustic Fields',\n",
       "    'summary': \"Our environment is filled with rich and dynamic acoustic information. When we\\nwalk into a cathedral, the reverberations as much as appearance inform us of\\nthe sanctuary's wide open space. Similarly, as an object moves around us, we\\nexpect the sound emitted to also exhibit this movement. While recent advances\\nin learned implicit functions have led to increasingly higher quality\\nrepresentations of the visual world, there have not been commensurate advances\\nin learning spatial auditory representations. To address this gap, we introduce\\nNeural Acoustic Fields (NAFs), an implicit representation that captures how\\nsounds propagate in a physical scene. By modeling acoustic propagation in a\\nscene as a linear time-invariant system, NAFs learn to continuously map all\\nemitter and listener location pairs to a neural impulse response function that\\ncan then be applied to arbitrary sounds. We demonstrate that the continuous\\nnature of NAFs enables us to render spatial acoustics for a listener at an\\narbitrary location, and can predict sound propagation at novel locations. We\\nfurther show that the representation learned by NAFs can help improve visual\\nlearning with sparse views. Finally, we show that a representation informative\\nof scene structure emerges during the learning of NAFs.\",\n",
       "    'author': [{'name': 'Andrew Luo'},\n",
       "     {'name': 'Yilun Du'},\n",
       "     {'name': 'Michael J. Tarr'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Antonio Torralba'},\n",
       "     {'name': 'Chuang Gan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2022. Project page:\\n  https://www.andrew.cmu.edu/user/afluo/Neural_Acoustic_Fields/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.00628v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.00628v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.02389v1',\n",
       "    'updated': '2022-04-05T17:55:01Z',\n",
       "    'published': '2022-04-05T17:55:01Z',\n",
       "    'title': 'ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer',\n",
       "    'summary': 'Objects play a crucial role in our everyday activities. Though multisensory\\nobject-centric learning has shown great potential lately, the modeling of\\nobjects in prior work is rather unrealistic. ObjectFolder 1.0 is a recent\\ndataset that introduces 100 virtualized objects with visual, acoustic, and\\ntactile sensory data. However, the dataset is small in scale and the\\nmultisensory data is of limited quality, hampering generalization to real-world\\nscenarios. We present ObjectFolder 2.0, a large-scale, multisensory dataset of\\ncommon household objects in the form of implicit neural representations that\\nsignificantly enhances ObjectFolder 1.0 in three aspects. First, our dataset is\\n10 times larger in the amount of objects and orders of magnitude faster in\\nrendering time. Second, we significantly improve the multisensory rendering\\nquality for all three modalities. Third, we show that models learned from\\nvirtual objects in our dataset successfully transfer to their real-world\\ncounterparts in three challenging tasks: object scale estimation, contact\\nlocalization, and shape reconstruction. ObjectFolder 2.0 offers a new path and\\ntestbed for multisensory learning in computer vision and robotics. The dataset\\nis available at https://github.com/rhgao/ObjectFolder.',\n",
       "    'author': [{'name': 'Ruohan Gao'},\n",
       "     {'name': 'Zilin Si'},\n",
       "     {'name': 'Yen-Yu Chang'},\n",
       "     {'name': 'Samuel Clarke'},\n",
       "     {'name': 'Jeannette Bohg'},\n",
       "     {'name': 'Li Fei-Fei'},\n",
       "     {'name': 'Wenzhen Yuan'},\n",
       "     {'name': 'Jiajun Wu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In CVPR 2022. Gao, Si, and Chang contributed equally to this work.\\n  Project page: https://ai.stanford.edu/~rhgao/objectfolder2.0/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.02389v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.02389v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.00393v1',\n",
       "    'updated': '2022-06-01T11:00:07Z',\n",
       "    'published': '2022-06-01T11:00:07Z',\n",
       "    'title': 'Towards Generalisable Audio Representations for Audio-Visual Navigation',\n",
       "    'summary': 'In audio-visual navigation (AVN), an intelligent agent needs to navigate to a\\nconstantly sound-making object in complex 3D environments based on its audio\\nand visual perceptions. While existing methods attempt to improve the\\nnavigation performance with preciously designed path planning or intricate task\\nsettings, none has improved the model generalisation on unheard sounds with\\ntask settings unchanged. We thus propose a contrastive learning-based method to\\ntackle this challenge by regularising the audio encoder, where the\\nsound-agnostic goal-driven latent representations can be learnt from various\\naudio signals of different classes. In addition, we consider two data\\naugmentation strategies to enrich the training sounds. We demonstrate that our\\ndesigns can be easily equipped to existing AVN frameworks to obtain an\\nimmediate performance gain (13.4%$\\\\uparrow$ in SPL on Replica and\\n12.2%$\\\\uparrow$ in SPL on MP3D). Our project is available at\\nhttps://AV-GeN.github.io/.',\n",
       "    'author': [{'name': 'Shunqi Mao'},\n",
       "     {'name': 'Chaoyi Zhang'},\n",
       "     {'name': 'Heng Wang'},\n",
       "     {'name': 'Weidong Cai'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2022 Embodied AI Workshop'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.00393v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.00393v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.11684v2',\n",
       "    'updated': '2020-03-08T00:18:49Z',\n",
       "    'published': '2019-12-25T15:07:26Z',\n",
       "    'title': 'Look, Listen, and Act: Towards Audio-Visual Embodied Navigation',\n",
       "    'summary': 'A crucial ability of mobile intelligent agents is to integrate the evidence\\nfrom multiple sensory inputs in an environment and to make a sequence of\\nactions to reach their goals. In this paper, we attempt to approach the problem\\nof Audio-Visual Embodied Navigation, the task of planning the shortest path\\nfrom a random starting location in a scene to the sound source in an indoor\\nenvironment, given only raw egocentric visual and audio sensory data. To\\naccomplish this task, the agent is required to learn from various modalities,\\ni.e. relating the audio signal to the visual environment. Here we describe an\\napproach to audio-visual embodied navigation that takes advantage of both\\nvisual and audio pieces of evidence. Our solution is based on three key ideas:\\na visual perception mapper module that constructs its spatial memory of the\\nenvironment, a sound perception module that infers the relative location of the\\nsound source from the agent, and a dynamic path planner that plans a sequence\\nof actions based on the audio-visual observations and the spatial memory of the\\nenvironment to navigate toward the goal. Experimental results on a newly\\ncollected Visual-Audio-Room dataset using the simulated multi-modal environment\\ndemonstrate the effectiveness of our approach over several competitive\\nbaselines.',\n",
       "    'author': [{'name': 'Chuang Gan'},\n",
       "     {'name': 'Yiwei Zhang'},\n",
       "     {'name': 'Jiajun Wu'},\n",
       "     {'name': 'Boqing Gong'},\n",
       "     {'name': 'Joshua B. Tenenbaum'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by ICRA 2020. Project page: http://avn.csail.mit.edu'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1912.11684v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.11684v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.14843v4',\n",
       "    'updated': '2023-01-03T11:07:22Z',\n",
       "    'published': '2021-11-29T15:17:46Z',\n",
       "    'title': 'Catch Me If You Hear Me: Audio-Visual Navigation in Complex Unmapped\\n  Environments with Moving Sounds',\n",
       "    'summary': 'Audio-visual navigation combines sight and hearing to navigate to a\\nsound-emitting source in an unmapped environment. While recent approaches have\\ndemonstrated the benefits of audio input to detect and find the goal, they\\nfocus on clean and static sound sources and struggle to generalize to unheard\\nsounds. In this work, we propose the novel dynamic audio-visual navigation\\nbenchmark which requires catching a moving sound source in an environment with\\nnoisy and distracting sounds, posing a range of new challenges. We introduce a\\nreinforcement learning approach that learns a robust navigation policy for\\nthese complex settings. To achieve this, we propose an architecture that fuses\\naudio-visual information in the spatial feature space to learn correlations of\\ngeometric information inherent in both local maps and audio signals. We\\ndemonstrate that our approach consistently outperforms the current\\nstate-of-the-art by a large margin across all tasks of moving sounds, unheard\\nsounds, and noisy environments, on two challenging 3D scanned real-world\\nenvironments, namely Matterport3D and Replica. The benchmark is available at\\nhttp://dav-nav.cs.uni-freiburg.de.',\n",
       "    'author': [{'name': 'Abdelrahman Younes'},\n",
       "     {'name': 'Daniel Honerkamp'},\n",
       "     {'name': 'Tim Welschehold'},\n",
       "     {'name': 'Abhinav Valada'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This paper has been accepted for publication at IEEE ROBOTICS AND\\n  AUTOMATION LETTERS'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.14843v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.14843v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.03483v1',\n",
       "    'updated': '2022-07-07T17:59:59Z',\n",
       "    'published': '2022-07-07T17:59:59Z',\n",
       "    'title': 'Finding Fallen Objects Via Asynchronous Audio-Visual Integration',\n",
       "    'summary': 'The way an object looks and sounds provide complementary reflections of its\\nphysical properties. In many settings cues from vision and audition arrive\\nasynchronously but must be integrated, as when we hear an object dropped on the\\nfloor and then must find it. In this paper, we introduce a setting in which to\\nstudy multi-modal object localization in 3D virtual environments. An object is\\ndropped somewhere in a room. An embodied robot agent, equipped with a camera\\nand microphone, must determine what object has been dropped -- and where -- by\\ncombining audio and visual signals with knowledge of the underlying physics. To\\nstudy this problem, we have generated a large-scale dataset -- the Fallen\\nObjects dataset -- that includes 8000 instances of 30 physical object\\ncategories in 64 rooms. The dataset uses the ThreeDWorld platform which can\\nsimulate physics-based impact sounds and complex physical interactions between\\nobjects in a photorealistic setting. As a first step toward addressing this\\nchallenge, we develop a set of embodied agent baselines, based on imitation\\nlearning, reinforcement learning, and modular planning, and perform an in-depth\\nanalysis of the challenge of this new task.',\n",
       "    'author': [{'name': 'Chuang Gan'},\n",
       "     {'name': 'Yi Gu'},\n",
       "     {'name': 'Siyuan Zhou'},\n",
       "     {'name': 'Jeremy Schwartz'},\n",
       "     {'name': 'Seth Alter'},\n",
       "     {'name': 'James Traer'},\n",
       "     {'name': 'Dan Gutfreund'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Josh McDermott'},\n",
       "     {'name': 'Antonio Torralba'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2022. Project page: http://fallen-object.csail.mit.edu'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.03483v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.03483v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.00646v1',\n",
       "    'updated': '2022-12-20T21:26:05Z',\n",
       "    'published': '2022-12-20T21:26:05Z',\n",
       "    'title': 'Addressing the Selection Bias in Voice Assistance: Training Voice\\n  Assistance Model in Python with Equal Data Selection',\n",
       "    'summary': 'In recent times, voice assistants have become a part of our day-to-day lives,\\nallowing information retrieval by voice synthesis, voice recognition, and\\nnatural language processing. These voice assistants can be found in many\\nmodern-day devices such as Apple, Amazon, Google, and Samsung. This project is\\nprimarily focused on Virtual Assistance in Natural Language Processing. Natural\\nLanguage Processing is a form of AI that helps machines understand people and\\ncreate feedback loops. This project will use deep learning to create a Voice\\nRecognizer and use Commonvoice and data collected from the local community for\\nmodel training using Google Colaboratory. After recognizing a command, the AI\\nassistant will be able to perform the most suitable actions and then give a\\nresponse.\\n  The motivation for this project comes from the race and gender bias that\\nexists in many virtual assistants. The computer industry is primarily dominated\\nby the male gender, and because of this, many of the products produced do not\\nregard women. This bias has an impact on natural language processing. This\\nproject will be utilizing various open-source projects to implement machine\\nlearning algorithms and train the assistant algorithm to recognize different\\ntypes of voices, accents, and dialects. Through this project, the goal to use\\nvoice data from underrepresented groups to build a voice assistant that can\\nrecognize voices regardless of gender, race, or accent. Increasing the\\nrepresentation of women in the computer industry is important for the future of\\nthe industry. By representing women in the initial study of voice assistants,\\nit can be shown that females play a vital role in the development of this\\ntechnology. In line with related work, this project will use first-hand data\\nfrom the college population and middle-aged adults to train voice assistant to\\ncombat gender bias.',\n",
       "    'author': [{'name': 'Kashav Piya'},\n",
       "     {'name': 'Srijal Shrestha'},\n",
       "     {'name': 'Cameran Frank'},\n",
       "     {'name': 'Estephanos Jebessa'},\n",
       "     {'name': 'Tauheed Khan Mohd'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.00646v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.00646v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.AS',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.AS',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.02547v2',\n",
       "    'updated': '2019-04-02T17:48:26Z',\n",
       "    'published': '2019-03-06T18:54:55Z',\n",
       "    'title': 'Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language\\n  Navigation',\n",
       "    'summary': 'We present the Frontier Aware Search with backTracking (FAST) Navigator, a\\ngeneral framework for action decoding, that achieves state-of-the-art results\\non the Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson\\net. al. (2018). Given a natural language instruction and photo-realistic image\\nviews of a previously unseen environment, the agent was tasked with navigating\\nfrom source to target location as quickly as possible. While all current\\napproaches make local action decisions or score entire trajectories using beam\\nsearch, ours balances local and global signals when exploring an unobserved\\nenvironment. Importantly, this lets us act greedily but use global signals to\\nbacktrack when necessary. Applying FAST framework to existing state-of-the-art\\nmodels achieved a 17% relative gain, an absolute 6% gain on Success rate\\nweighted by Path Length (SPL).',\n",
       "    'author': [{'name': 'Liyiming Ke'},\n",
       "     {'name': 'Xiujun Li'},\n",
       "     {'name': 'Yonatan Bisk'},\n",
       "     {'name': 'Ari Holtzman'},\n",
       "     {'name': 'Zhe Gan'},\n",
       "     {'name': 'Jingjing Liu'},\n",
       "     {'name': 'Jianfeng Gao'},\n",
       "     {'name': 'Yejin Choi'},\n",
       "     {'name': 'Siddhartha Srinivasa'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2019 Oral, video demo: https://youtu.be/AD9TNohXoPA'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.02547v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.02547v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CL',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CL',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1907.13052v4',\n",
       "    'updated': '2020-11-23T10:31:22Z',\n",
       "    'published': '2019-07-30T16:22:39Z',\n",
       "    'title': 'GENESIS: Generative Scene Inference and Sampling with Object-Centric\\n  Latent Representations',\n",
       "    'summary': 'Generative latent-variable models are emerging as promising tools in robotics\\nand reinforcement learning. Yet, even though tasks in these domains typically\\ninvolve distinct objects, most state-of-the-art generative models do not\\nexplicitly capture the compositional nature of visual scenes. Two recent\\nexceptions, MONet and IODINE, decompose scenes into objects in an unsupervised\\nfashion. Their underlying generative processes, however, do not account for\\ncomponent interactions. Hence, neither of them allows for principled sampling\\nof novel scenes. Here we present GENESIS, the first object-centric generative\\nmodel of 3D visual scenes capable of both decomposing and generating scenes by\\ncapturing relationships between scene components. GENESIS parameterises a\\nspatial GMM over images which is decoded from a set of object-centric latent\\nvariables that are either inferred sequentially in an amortised fashion or\\nsampled from an autoregressive prior. We train GENESIS on several publicly\\navailable datasets and evaluate its performance on scene generation,\\ndecomposition, and semi-supervised learning.',\n",
       "    'author': [{'name': 'Martin Engelcke'},\n",
       "     {'name': 'Adam R. Kosiorek'},\n",
       "     {'name': 'Oiwi Parker Jones'},\n",
       "     {'name': 'Ingmar Posner'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at the International Conference on Learning Representations\\n  (ICLR) 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1907.13052v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1907.13052v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.07879v1',\n",
       "    'updated': '2020-09-16T18:18:49Z',\n",
       "    'published': '2020-09-16T18:18:49Z',\n",
       "    'title': 'Using Sensory Time-cue to enable Unsupervised Multimodal Meta-learning',\n",
       "    'summary': \"As data from IoT (Internet of Things) sensors become ubiquitous,\\nstate-of-the-art machine learning algorithms face many challenges on directly\\nusing sensor data. To overcome these challenges, methods must be designed to\\nlearn directly from sensors without manual annotations. This paper introduces\\nSensory Time-cue for Unsupervised Meta-learning (STUM). Different from\\ntraditional learning approaches that either heavily depend on labels or on\\ntime-independent feature extraction assumptions, such as Gaussian distribution\\nfeatures, the STUM system uses time relation of inputs to guide the feature\\nspace formation within and across modalities. The fact that STUM learns from a\\nvariety of small tasks may put this method in the camp of Meta-Learning.\\nDifferent from existing Meta-Learning approaches, STUM learning tasks are\\ncomposed within and across multiple modalities based on time-cue co-exist with\\nthe IoT streaming data. In an audiovisual learning example, because consecutive\\nvisual frames usually comprise the same object, this approach provides a unique\\nway to organize features from the same object together. The same method can\\nalso organize visual object features with the object's spoken-name features\\ntogether if the spoken name is presented with the object at about the same\\ntime. This cross-modality feature organization may further help the\\norganization of visual features that belong to similar objects but acquired at\\ndifferent location and time. Promising results are achieved through\\nevaluations.\",\n",
       "    'author': [{'name': 'Qiong Liu'}, {'name': 'Yanxia Zhang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2009.07879v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.07879v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.13578v1',\n",
       "    'updated': '2021-03-25T03:22:59Z',\n",
       "    'published': '2021-03-25T03:22:59Z',\n",
       "    'title': 'Test-Time Training for Deformable Multi-Scale Image Registration',\n",
       "    'summary': 'Registration is a fundamental task in medical robotics and is often a crucial\\nstep for many downstream tasks such as motion analysis, intra-operative\\ntracking and image segmentation. Popular registration methods such as ANTs and\\nNiftyReg optimize objective functions for each pair of images from scratch,\\nwhich are time-consuming for 3D and sequential images with complex\\ndeformations. Recently, deep learning-based registration approaches such as\\nVoxelMorph have been emerging and achieve competitive performance. In this\\nwork, we construct a test-time training for deep deformable image registration\\nto improve the generalization ability of conventional learning-based\\nregistration model. We design multi-scale deep networks to consecutively model\\nthe residual deformations, which is effective for high variational\\ndeformations. Extensive experiments validate the effectiveness of multi-scale\\ndeep registration with test-time training based on Dice coefficient for image\\nsegmentation and mean square error (MSE), normalized local cross-correlation\\n(NLCC) for tissue dense tracking tasks. Two videos are in\\nhttps://www.youtube.com/watch?v=NvLrCaqCiAE and\\nhttps://www.youtube.com/watch?v=pEA6ZmtTNuQ',\n",
       "    'author': [{'name': 'Wentao Zhu'},\n",
       "     {'name': 'Yufang Huang'},\n",
       "     {'name': 'Daguang Xu'},\n",
       "     {'name': 'Zhen Qian'},\n",
       "     {'name': 'Wei Fan'},\n",
       "     {'name': 'Xiaohui Xie'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2021; 8 pages, 4 figures, 2 big tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.13578v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.13578v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.IV', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.04903v2',\n",
       "    'updated': '2019-06-18T15:19:17Z',\n",
       "    'published': '2018-10-11T08:41:30Z',\n",
       "    'title': 'MOANOFS: Multi-Objective Automated Negotiation based Online Feature\\n  Selection System for Big Data Classification',\n",
       "    'summary': \"Feature Selection (FS) plays an important role in learning and classification\\ntasks. The object of FS is to select the relevant and non-redundant features.\\nConsidering the huge amount number of features in real-world applications, FS\\nmethods using batch learning technique can't resolve big data problem\\nespecially when data arrive sequentially. In this paper, we propose an online\\nfeature selection system which resolves this problem. More specifically, we\\ntreat the problem of online supervised feature selection for binary\\nclassification as a decision-making problem. A philosophical vision to this\\nproblem leads to a hybridization between two important domains: feature\\nselection using online learning technique (OFS) and automated negotiation (AN).\\nThe proposed OFS system called MOANOFS (Multi-Objective Automated Negotiation\\nbased Online Feature Selection) uses two levels of decision. In the first\\nlevel, from n learners (or OFS methods), we decide which are the k trustful\\nones (with high confidence or trust value). These elected k learners will\\nparticipate in the second level. In this level, we integrate our proposed\\nMultilateral Automated Negotiation based OFS (MANOFS) method to decide finally\\nwhich is the best solution or which are relevant features. We show that MOANOFS\\nsystem is applicable to different domains successfully and achieves high\\naccuracy with several real-world applications.\\n  Index Terms: Feature selection, online learning, multi-objective automated\\nnegotiation, trust, classification, big data.\",\n",
       "    'author': [{'name': 'Fatma BenSaid'}, {'name': 'Adel M. Alimi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '15 pages, 8 figures, journal paper'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.04903v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.04903v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.10476v1',\n",
       "    'updated': '2019-09-23T16:56:08Z',\n",
       "    'published': '2019-09-23T16:56:08Z',\n",
       "    'title': 'Research Directions in Democratizing Innovation through Design\\n  Automation, One-Click Manufacturing Services and Intelligent Machines',\n",
       "    'summary': 'The digitalization of manufacturing has created opportunities for consumers\\nto customize products that fit their individualized needs which in turn would\\ndrive demand for manufacturing services. However, this pull-based manufacturing\\nsystem production of extremely low quantity and limitless variety for products\\nis expensive to implement. New emerging technology in design automation driven\\nby data-driven computational design, manufacturing-as-a-service marketplaces\\nand digitally enabled micro-factories holds promise towards democratization of\\ninnovation. In this paper, scientific, technology and infrastructure challenges\\nare identified and if solved, the impact of these emerging technologies on\\nproduct innovation and future factory organization is discussed.',\n",
       "    'author': [{'name': 'Binil Starly'},\n",
       "     {'name': 'Atin Angrish'},\n",
       "     {'name': 'Paul Cohen'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.10476v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.10476v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CY',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CY',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.13170v3',\n",
       "    'updated': '2022-07-20T01:19:39Z',\n",
       "    'published': '2022-04-27T20:04:24Z',\n",
       "    'title': 'AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias\\n  Estimation',\n",
       "    'summary': \"In Federated Learning (FL), a number of clients or devices collaborate to\\ntrain a model without sharing their data. Models are optimized locally at each\\nclient and further communicated to a central hub for aggregation. While FL is\\nan appealing decentralized training paradigm, heterogeneity among data from\\ndifferent clients can cause the local optimization to drift away from the\\nglobal objective. In order to estimate and therefore remove this drift,\\nvariance reduction techniques have been incorporated into FL optimization\\nrecently. However, these approaches inaccurately estimate the clients' drift\\nand ultimately fail to remove it properly. In this work, we propose an adaptive\\nalgorithm that accurately estimates drift across clients. In comparison to\\nprevious works, our approach necessitates less storage and communication\\nbandwidth, as well as lower compute costs. Additionally, our proposed\\nmethodology induces stability by constraining the norm of estimates for client\\ndrift, making it more practical for large scale FL. Experimental findings\\ndemonstrate that the proposed algorithm converges significantly faster and\\nachieves higher accuracy than the baselines across various FL benchmarks.\",\n",
       "    'author': [{'name': 'Farshid Varno'},\n",
       "     {'name': 'Marzie Saghayi'},\n",
       "     {'name': 'Laya Rafiee Sevyeri'},\n",
       "     {'name': 'Sharut Gupta'},\n",
       "     {'name': 'Stan Matwin'},\n",
       "     {'name': 'Mohammad Havaei'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'AdaBest'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.13170v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.13170v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.DC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.12315v1',\n",
       "    'updated': '2022-07-25T16:30:40Z',\n",
       "    'published': '2022-07-25T16:30:40Z',\n",
       "    'title': 'Stable Parallel Training of Wasserstein Conditional Generative\\n  Adversarial Neural Networks',\n",
       "    'summary': 'We propose a stable, parallel approach to train Wasserstein Conditional\\nGenerative Adversarial Neural Networks (W-CGANs) under the constraint of a\\nfixed computational budget. Differently from previous distributed GANs training\\ntechniques, our approach avoids inter-process communications, reduces the risk\\nof mode collapse and enhances scalability by using multiple generators, each\\none of them concurrently trained on a single data label. The use of the\\nWasserstein metric also reduces the risk of cycling by stabilizing the training\\nof each generator. We illustrate the approach on the CIFAR10, CIFAR100, and\\nImageNet1k datasets, three standard benchmark image datasets, maintaining the\\noriginal resolution of the images for each dataset. Performance is assessed in\\nterms of scalability and final accuracy within a limited fixed computational\\ntime and computational resources. To measure accuracy, we use the inception\\nscore, the Frechet inception distance, and image quality. An improvement in\\ninception score and Frechet inception distance is shown in comparison to\\nprevious results obtained by performing the parallel approach on deep\\nconvolutional conditional generative adversarial neural networks (DC-CGANs) as\\nwell as an improvement of image quality of the new images created by the GANs\\napproach. Weak scaling is attained on both datasets using up to 2,000 NVIDIA\\nV100 GPUs on the OLCF supercomputer Summit.',\n",
       "    'author': [{'name': 'Massimiliano Lupo Pasini'}, {'name': 'Junqi Yin'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/s11227-022-04721-y'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/s11227-022-04721-y',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2207.12315v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.12315v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '22 pages; 9 figures'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.DC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T01, 68T10, 68M14, 65Y05, 65Y10',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.0; I.2.11; C.1.4; C.2.4',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1407.0577v1',\n",
       "    'updated': '2014-07-02T14:12:21Z',\n",
       "    'published': '2014-07-02T14:12:21Z',\n",
       "    'title': 'Systematic Derivation of Behaviour Characterisations in Evolutionary\\n  Robotics',\n",
       "    'summary': \"Evolutionary techniques driven by behavioural diversity, such as novelty\\nsearch, have shown significant potential in evolutionary robotics. These\\ntechniques rely on priorly specified behaviour characterisations to estimate\\nthe similarity between individuals. Characterisations are typically defined in\\nan ad hoc manner based on the experimenter's intuition and knowledge about the\\ntask. Alternatively, generic characterisations based on the sensor-effector\\nvalues of the agents are used. In this paper, we propose a novel approach that\\nallows for systematic derivation of behaviour characterisations for\\nevolutionary robotics, based on a formal description of the agents and their\\nenvironment. Systematically derived behaviour characterisations (SDBCs) go\\nbeyond generic characterisations in that they can contain task-specific\\nfeatures related to the internal state of the agents, environmental features,\\nand relations between them. We evaluate SDBCs with novelty search in three\\nsimulated collective robotics tasks. Our results show that SDBCs yield a\\nperformance comparable to the task-specific characterisations, in terms of both\\nsolution quality and behaviour space exploration.\",\n",
       "    'author': [{'name': 'Jorge Gomes'},\n",
       "     {'name': 'Pedro Mariano'},\n",
       "     {'name': 'Anders Lyhne Christensen'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.7551/978-0-262-32621-6-ch036'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.7551/978-0-262-32621-6-ch036',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1407.0577v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1407.0577v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in 14th International Conference on the Synthesis and\\n  Simulation of Living Systems (ALife 14)'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on the Synthesis and Simulation of Living\\n  Systems (ALife). pp. 212-219. MIT Press (2014)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.10519v3',\n",
       "    'updated': '2021-05-19T15:14:31Z',\n",
       "    'published': '2019-11-24T12:36:18Z',\n",
       "    'title': 'Three Dimensional Route Planning for Multiple Unmanned Aerial Vehicles\\n  using Salp Swarm Algorithm',\n",
       "    'summary': 'Route planning for multiple Unmanned Aerial Vehicles (UAVs) is a series of\\ntranslation and rotational steps from a given start location to the destination\\ngoal location. The goal of the route planning problem is to determine the most\\noptimal route avoiding any collisions with the obstacles present in the\\nenvironment. Route planning is an NP-hard optimization problem. In this paper,\\na newly proposed Salp Swarm Algorithm (SSA) is used, and its performance is\\ncompared with deterministic and other Nature-Inspired Algorithms (NIAs). The\\nresults illustrate that SSA outperforms all the other meta-heuristic algorithms\\nin route planning for multiple UAVs in a 3D environment. The proposed approach\\nimproves the average cost and overall time by 1.25% and 6.035% respectively\\nwhen compared to recently reported data. Route planning is involved in many\\nreal-life applications like robot navigation, self-driving car, autonomous UAV\\nfor search and rescue operations in dangerous ground-zero situations, civilian\\nsurveillance, military combat and even commercial services like package\\ndelivery by drones.',\n",
       "    'author': [{'name': 'Priyansh Saxena'},\n",
       "     {'name': 'Shivani Tayal'},\n",
       "     {'name': 'Raahat Gupta'},\n",
       "     {'name': 'Akshat Maheshwari'},\n",
       "     {'name': 'Gaurav Kaushal'},\n",
       "     {'name': 'Ritu Tiwari'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'An author was mistakenly added in the paper dur to some human error.\\n  Moreover the research is still under progress and we would like to upload the\\n  paper with the final results once the work is complete'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.10519v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.10519v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2007.04882v1',\n",
       "    'updated': '2020-07-09T15:34:22Z',\n",
       "    'published': '2020-07-09T15:34:22Z',\n",
       "    'title': 'A Neuro-inspired Theory of Joint Human-Swarm Interaction',\n",
       "    'summary': 'Human-swarm interaction (HSI) is an active research challenge in the realms\\nof swarm robotics and human-factors engineering. Here we apply a cognitive\\nsystems engineering perspective and introduce a neuro-inspired joint systems\\ntheory of HSI. The mindset defines predictions for adaptive, robust and\\nscalable HSI dynamics and therefore has the potential to inform human-swarm\\nloop design.',\n",
       "    'author': [{'name': 'Jonas D. Hasbach'}, {'name': 'Maren Bennewitz'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA Workshop on Human-Swarm Interaction 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2007.04882v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2007.04882v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.HC',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.HC',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'H.1.2; I.2.9', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.03975v1',\n",
       "    'updated': '2022-10-08T09:16:16Z',\n",
       "    'published': '2022-10-08T09:16:16Z',\n",
       "    'title': 'Self-organizing nest migration dynamics synthesis for ant colony systems',\n",
       "    'summary': \"In this study, we synthesize a novel dynamical approach for ant colonies\\nenabling them to migrate to new nest sites in a self-organizing fashion. In\\nother words, we realize ant colony migration as a self-organizing\\nphenotype-level collective behavior. For this purpose, we first segment the\\nedges of the graph of ants' pathways. Then, each segment, attributed to its own\\npheromone profile, may host an ant. So, multiple ants may occupy an edge at the\\nsame time. Thanks to this segment-wise edge formulation, ants have more\\nselection options in the course of their pathway determination, thereby\\nincreasing the diversity of their colony's emergent behaviors. In light of the\\ncontinuous pheromone dynamics of segments, each edge owns a spatio-temporal\\npiece-wise continuous pheromone profile in which both deposit and evaporation\\nprocesses are unified. The passive dynamics of the proposed migration mechanism\\nis sufficiently rich so that an ant colony can migrate to the vicinity of a new\\nnest site in a self-organizing manner without any external supervision. In\\nparticular, we perform extensive simulations to test our migration dynamics\\napplied to a colony including 500 ants traversing a pathway graph comprising\\n200 nodes and 4000 edges which are segmented based on various resolutions. The\\nobtained results exhibit the effectiveness of our strategy.\",\n",
       "    'author': {'name': 'Matin Macktoobian'},\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/s11047-022-09923-0'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/s11047-022-09923-0',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2210.03975v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.03975v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Natural Computing, 2022'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1902.03389v1',\n",
       "    'updated': '2019-02-09T07:49:42Z',\n",
       "    'published': '2019-02-09T07:49:42Z',\n",
       "    'title': 'Generative Moment Matching Network-based Random Modulation Post-filter\\n  for DNN-based Singing Voice Synthesis and Neural Double-tracking',\n",
       "    'summary': 'This paper proposes a generative moment matching network (GMMN)-based\\npost-filter that provides inter-utterance pitch variation for deep neural\\nnetwork (DNN)-based singing voice synthesis. The natural pitch variation of a\\nhuman singing voice leads to a richer musical experience and is used in\\ndouble-tracking, a recording method in which two performances of the same\\nphrase are recorded and mixed to create a richer, layered sound. However,\\nsinging voices synthesized using conventional DNN-based methods never vary\\nbecause the synthesis process is deterministic and only one waveform is\\nsynthesized from one musical score. To address this problem, we use a GMMN to\\nmodel the variation of the modulation spectrum of the pitch contour of natural\\nsinging voices and add a randomized inter-utterance variation to the pitch\\ncontour generated by conventional DNN-based singing voice synthesis.\\nExperimental evaluations suggest that 1) our approach can provide perceptible\\ninter-utterance pitch variation while preserving speech quality. We extend our\\napproach to double-tracking, and the evaluation demonstrates that 2) GMMN-based\\nneural double-tracking is perceptually closer to natural double-tracking than\\nconventional signal processing-based artificial double-tracking is.',\n",
       "    'author': [{'name': 'Hiroki Tamaru'},\n",
       "     {'name': 'Yuki Saito'},\n",
       "     {'name': 'Shinnosuke Takamichi'},\n",
       "     {'name': 'Tomoki Koriyama'},\n",
       "     {'name': 'Hiroshi Saruwatari'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '5 pages, to appear in IEEE ICASSP 2019 (Paper Code: SLP-P22.11,\\n  Session: Speech Synthesis III)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1902.03389v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1902.03389v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.11715v4',\n",
       "    'updated': '2021-09-11T21:58:38Z',\n",
       "    'published': '2020-11-23T20:40:44Z',\n",
       "    'title': 'Multi-task Language Modeling for Improving Speech Recognition of Rare\\n  Words',\n",
       "    'summary': 'End-to-end automatic speech recognition (ASR) systems are increasingly\\npopular due to their relative architectural simplicity and competitive\\nperformance. However, even though the average accuracy of these systems may be\\nhigh, the performance on rare content words often lags behind hybrid ASR\\nsystems. To address this problem, second-pass rescoring is often applied\\nleveraging upon language modeling. In this paper, we propose a second-pass\\nsystem with multi-task learning, utilizing semantic targets (such as intent and\\nslot prediction) to improve speech recognition performance. We show that our\\nrescoring model trained with these additional tasks outperforms the baseline\\nrescoring model, trained with only the language modeling task, by 1.4% on a\\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\\nrelative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR\\ndeduction compared with RNN Transducer only ASR baseline for rare words\\nrecognition.',\n",
       "    'author': [{'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Linda Liu'},\n",
       "     {'name': 'Ankur Gandhe'},\n",
       "     {'name': 'Yile Gu'},\n",
       "     {'name': 'Anirudh Raju'},\n",
       "     {'name': 'Denis Filimonov'},\n",
       "     {'name': 'Ivan Bulyko'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE Automatic Speech Recognition and Understanding\\n  (ASRU) 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.11715v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.11715v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CL',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CL',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.00173v2',\n",
       "    'updated': '2021-07-04T07:34:14Z',\n",
       "    'published': '2021-05-01T05:47:15Z',\n",
       "    'title': 'Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis\\n  Tool for Singers',\n",
       "    'summary': 'Current computational-emotion research has focused on applying acoustic\\nproperties to analyze how emotions are perceived mathematically or used in\\nnatural language processing machine learning models. While recent interest has\\nfocused on analyzing emotions from the spoken voice, little experimentation has\\nbeen performed to discover how emotions are recognized in the singing voice --\\nboth in noiseless and noisy data (i.e., data that is either inaccurate,\\ndifficult to interpret, has corrupted/distorted/nonsense information like\\nactual noise sounds in this case, or has a low ratio of usable/unusable\\ninformation). Not only does this ignore the challenges of training machine\\nlearning models on more subjective data and testing them with much noisier\\ndata, but there is also a clear disconnect in progress between advancing the\\ndevelopment of convolutional neural networks and the goal of emotionally\\ncognizant artificial intelligence. By training a new model to include this type\\nof information with a rich comprehension of psycho-acoustic properties, not\\nonly can models be trained to recognize information within extremely noisy\\ndata, but advancement can be made toward more complex biofeedback applications\\n-- including creating a model which could recognize emotions given any human\\ninformation (language, breath, voice, body, posture) and be used in any\\nperformance medium (music, speech, acting) or psychological assistance for\\npatients with disorders such as BPD, alexithymia, autism, among others. This\\npaper seeks to reflect and expand upon the findings of related research and\\npresent a stepping-stone toward this end goal.',\n",
       "    'author': {'name': 'Daniel Szelogowski'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '26 pages, 10 figures, 6 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.00173v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.00173v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2; I.5; J.3; J.5; J.7; K.3',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.02211v3',\n",
       "    'updated': '2022-12-04T08:24:02Z',\n",
       "    'published': '2022-06-05T16:18:27Z',\n",
       "    'title': 'Variable-rate hierarchical CPC leads to acoustic unit discovery in\\n  speech',\n",
       "    'summary': 'The success of deep learning comes from its ability to capture the\\nhierarchical structure of data by learning high-level representations defined\\nin terms of low-level ones. In this paper we explore self-supervised learning\\nof hierarchical representations of speech by applying multiple levels of\\nContrastive Predictive Coding (CPC). We observe that simply stacking two CPC\\nmodels does not yield significant improvements over single-level architectures.\\nInspired by the fact that speech is often described as a sequence of discrete\\nunits unevenly distributed in time, we propose a model in which the output of a\\nlow-level CPC module is non-uniformly downsampled to directly minimize the loss\\nof a high-level CPC module. The latter is designed to also enforce a prior of\\nseparability and discreteness in its representations by enforcing dissimilarity\\nof successive high-level representations through focused negative sampling, and\\nby quantization of the prediction targets. Accounting for the structure of the\\nspeech signal improves upon single-level CPC features and enhances the\\ndisentanglement of the learned representations, as measured by downstream\\nspeech recognition tasks, while resulting in a meaningful segmentation of the\\nsignal that closely resembles phone boundaries.',\n",
       "    'author': [{'name': 'Santiago Cuervo'},\n",
       "     {'name': 'Adrian Łańcucki'},\n",
       "     {'name': 'Ricard Marxer'},\n",
       "     {'name': 'Paweł Rychlikowski'},\n",
       "     {'name': 'Jan Chorowski'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to 36th Conference on Neural Information Processing Systems\\n  (NeurIPS 2022)'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Advances in Neural Information Processing Systems, 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.02211v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.02211v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.12991v3',\n",
       "    'updated': '2022-09-20T14:39:40Z',\n",
       "    'published': '2022-08-27T11:50:32Z',\n",
       "    'title': 'Sub-mW Neuromorphic SNN audio processing applications with Rockpool and\\n  Xylo',\n",
       "    'summary': 'Spiking Neural Networks (SNNs) provide an efficient computational mechanism\\nfor temporal signal processing, especially when coupled with low-power SNN\\ninference ASICs. SNNs have been historically difficult to configure, lacking a\\ngeneral method for finding solutions for arbitrary tasks. In recent years,\\ngradient-descent optimization methods have been applied to SNNs with increasing\\nease. SNNs and SNN inference processors therefore offer a good platform for\\ncommercial low-power signal processing in energy constrained environments\\nwithout cloud dependencies. However, to date these methods have not been\\naccessible to ML engineers in industry, requiring graduate-level training to\\nsuccessfully configure a single SNN application. Here we demonstrate a\\nconvenient high-level pipeline to design, train and deploy arbitrary temporal\\nsignal processing applications to sub-mW SNN inference hardware. We apply a new\\nstraightforward SNN architecture designed for temporal signal processing, using\\na pyramid of synaptic time constants to extract signal features at a range of\\ntemporal scales. We demonstrate this architecture on an ambient audio\\nclassification task, deployed to the Xylo SNN inference processor in streaming\\nmode. Our application achieves high accuracy (98%) and low latency (100ms) at\\nlow power (<100$\\\\mu$W inference power). Our approach makes training and\\ndeploying SNN applications available to ML engineers with general NN\\nbackgrounds, without requiring specific prior experience with spiking NNs. We\\nintend for our approach to make Neuromorphic hardware and SNNs an attractive\\nchoice for commercial low-power and edge signal processing applications.',\n",
       "    'author': [{'name': 'Hannah Bos'}, {'name': 'Dylan Muir'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This submission has been removed by arXiv administrators because the\\n  submitter did not have the authority to grant a license to the work at the\\n  time of submission'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.12991v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.12991v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.ET', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.12573v3',\n",
       "    'updated': '2022-11-16T09:56:26Z',\n",
       "    'published': '2022-09-26T10:38:39Z',\n",
       "    'title': 'Digital Audio Forensics: Blind Human Voice Mimicry Detection',\n",
       "    'summary': 'Audio is one of the most used way of human communication, but at the same\\ntime it can be easily misused by to trick people. With the revolution of AI,\\nthe related technologies are now accessible to almost everyone thus making it\\nsimple for the criminals to commit crimes and forgeries. In this work, we\\nintroduce a deep learning method to develop a classifier that will blindly\\nclassify an input audio as real or mimicked. The proposed model was trained on\\na set of important features extracted from a large dataset of audios to get a\\nclassifier that was tested on the same set of features from different audios.\\nTwo datasets were created for this work; an all English data set and a mixed\\ndata set (Arabic and English). These datasets have been made available through\\nGitHub for the use of the research community at\\nhttps://github.com/SaSs7/Dataset. For the purpose of comparison, the audios\\nwere also classified through human inspection with the subjects being the\\nnative speakers. The ensued results were interesting and exhibited formidable\\naccuracy.',\n",
       "    'author': [{'name': 'Sahar Al Ajmi'},\n",
       "     {'name': 'Khizar Hayat'},\n",
       "     {'name': 'Alaa M. Al Obaidi'},\n",
       "     {'name': 'Naresh Kumar'},\n",
       "     {'name': 'Munaf Najmuldeen'},\n",
       "     {'name': 'Baptiste Magnier'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages, 4 figures (6 if you count subfigures), 2 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.12573v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.12573v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T05, 68T07, 68T10',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2; I.5; I.m', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.12423v1',\n",
       "    'updated': '2022-11-22T17:30:36Z',\n",
       "    'published': '2022-11-22T17:30:36Z',\n",
       "    'title': 'On Narrative Information and the Distillation of Stories',\n",
       "    'summary': 'The act of telling stories is a fundamental part of what it means to be\\nhuman. This work introduces the concept of narrative information, which we\\ndefine to be the overlap in information space between a story and the items\\nthat compose the story. Using contrastive learning methods, we show how modern\\nartificial neural networks can be leveraged to distill stories and extract a\\nrepresentation of the narrative information. We then demonstrate how\\nevolutionary algorithms can leverage this to extract a set of narrative\\ntemplates and how these templates -- in tandem with a novel curve-fitting\\nalgorithm we introduce -- can reorder music albums to automatically induce\\nstories in them. In the process of doing so, we give strong statistical\\nevidence that these narrative information templates are present in existing\\nalbums. While we experiment only with music albums here, the premises of our\\nwork extend to any form of (largely) independent media.',\n",
       "    'author': [{'name': 'Dylan R. Ashley'},\n",
       "     {'name': 'Vincent Herrmann'},\n",
       "     {'name': 'Zachary Friggstad'},\n",
       "     {'name': 'Jürgen Schmidhuber'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'presented in the Information-Theoretic Principles in Cognitive\\n  Systems Workshop at the 36th Conference on Neural Information Processing\\n  Systems; 4 pages in main text + 2 pages of references + 8 pages of\\n  appendices, 2 figures in main text + 3 in appendices, 1 table in main text, 2\\n  algorithms in appendices; source code available at\\n  https://github.com/dylanashley/story-distiller/releases/tag/v1.0.0'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.12423v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.12423v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CL',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CL',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T07 (Primary) 68P30, 68W50, 94A15 (Secondary)',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'H.1.1; H.5.5; I.2.6; I.5.1; J.5',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.08112v1',\n",
       "    'updated': '2022-09-16T18:00:46Z',\n",
       "    'published': '2022-09-16T18:00:46Z',\n",
       "    'title': 'Optimizing Industrial HVAC Systems with Hierarchical Reinforcement\\n  Learning',\n",
       "    'summary': 'Reinforcement learning (RL) techniques have been developed to optimize\\nindustrial cooling systems, offering substantial energy savings compared to\\ntraditional heuristic policies. A major challenge in industrial control\\ninvolves learning behaviors that are feasible in the real world due to\\nmachinery constraints. For example, certain actions can only be executed every\\nfew hours while other actions can be taken more frequently. Without extensive\\nreward engineering and experimentation, an RL agent may not learn realistic\\noperation of machinery. To address this, we use hierarchical reinforcement\\nlearning with multiple agents that control subsets of actions according to\\ntheir operation time scales. Our hierarchical approach achieves energy savings\\nover existing baselines while maintaining constraints such as operating\\nchillers within safe bounds in a simulated HVAC control environment.',\n",
       "    'author': [{'name': 'William Wong'},\n",
       "     {'name': 'Praneet Dutta'},\n",
       "     {'name': 'Octavian Voicu'},\n",
       "     {'name': 'Yuri Chervonyi'},\n",
       "     {'name': 'Cosmin Paduraru'},\n",
       "     {'name': 'Jerry Luo'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages, 5 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.08112v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.08112v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.09461v1',\n",
       "    'updated': '2021-06-17T13:13:34Z',\n",
       "    'published': '2021-06-17T13:13:34Z',\n",
       "    'title': 'Modelling resource allocation in uncertain system environment through\\n  deep reinforcement learning',\n",
       "    'summary': 'Reinforcement Learning has applications in field of mechatronics, robotics,\\nand other resource-constrained control system. Problem of resource allocation\\nis primarily solved using traditional predefined techniques and modern deep\\nlearning methods. The drawback of predefined and most deep learning methods for\\nresource allocation is failing to meet the requirements in cases of uncertain\\nsystem environment. We can approach problem of resource allocation in uncertain\\nsystem environment alongside following certain criteria using deep\\nreinforcement learning. Also, reinforcement learning has ability for adapting\\nto new uncertain environment for prolonged period of time. The paper provides a\\ndetailed comparative analysis on various deep reinforcement learning methods by\\napplying different components to modify architecture of reinforcement learning\\nwith use of noisy layers, prioritized replay, bagging, duelling networks, and\\nother related combination to obtain improvement in terms of performance and\\nreduction of computational cost. The paper identifies problem of resource\\nallocation in uncertain environment could be effectively solved using Noisy\\nBagging duelling double deep Q network achieving efficiency of 97.7% by\\nmaximizing reward with significant exploration in given simulated environment\\nfor resource allocation.',\n",
       "    'author': [{'name': 'Neel Gandhi'}, {'name': 'Shakti Mishra'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Accepted at IRMAS'21\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.09461v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.09461v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.07334v1',\n",
       "    'updated': '2021-11-14T13:02:45Z',\n",
       "    'published': '2021-11-14T13:02:45Z',\n",
       "    'title': 'Relative Distributed Formation and Obstacle Avoidance with Multi-agent\\n  Reinforcement Learning',\n",
       "    'summary': \"Multi-agent formation as well as obstacle avoidance is one of the most\\nactively studied topics in the field of multi-agent systems. Although some\\nclassic controllers like model predictive control (MPC) and fuzzy control\\nachieve a certain measure of success, most of them require precise global\\ninformation which is not accessible in harsh environments. On the other hand,\\nsome reinforcement learning (RL) based approaches adopt the leader-follower\\nstructure to organize different agents' behaviors, which sacrifices the\\ncollaboration between agents thus suffering from bottlenecks in maneuverability\\nand robustness. In this paper, we propose a distributed formation and obstacle\\navoidance method based on multi-agent reinforcement learning (MARL). Agents in\\nour system only utilize local and relative information to make decisions and\\ncontrol themselves distributively. Agent in the multi-agent system will\\nreorganize themselves into a new topology quickly in case that any of them is\\ndisconnected. Our method achieves better performance regarding formation error,\\nformation convergence rate and on-par success rate of obstacle avoidance\\ncompared with baselines (both classic control methods and another RL-based\\nmethod). The feasibility of our method is verified by both simulation and\\nhardware implementation with Ackermann-steering vehicles.\",\n",
       "    'author': [{'name': 'Yuzi Yan'},\n",
       "     {'name': 'Xiaoxiang Li'},\n",
       "     {'name': 'Xinyou Qiu'},\n",
       "     {'name': 'Jiantao Qiu'},\n",
       "     {'name': 'Jian Wang'},\n",
       "     {'name': 'Yu Wang'},\n",
       "     {'name': 'Yuan Shen'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.07334v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.07334v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.SY',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.SY',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.08857v1',\n",
       "    'updated': '2021-11-17T01:36:40Z',\n",
       "    'published': '2021-11-17T01:36:40Z',\n",
       "    'title': 'SEIHAI: A Sample-efficient Hierarchical AI for the MineRL Competition',\n",
       "    'summary': 'The MineRL competition is designed for the development of reinforcement\\nlearning and imitation learning algorithms that can efficiently leverage human\\ndemonstrations to drastically reduce the number of environment interactions\\nneeded to solve the complex \\\\emph{ObtainDiamond} task with sparse rewards. To\\naddress the challenge, in this paper, we present \\\\textbf{SEIHAI}, a\\n\\\\textbf{S}ample-\\\\textbf{e}ff\\\\textbf{i}cient \\\\textbf{H}ierarchical \\\\textbf{AI},\\nthat fully takes advantage of the human demonstrations and the task structure.\\nSpecifically, we split the task into several sequentially dependent subtasks,\\nand train a suitable agent for each subtask using reinforcement learning and\\nimitation learning. We further design a scheduler to select different agents\\nfor different subtasks automatically. SEIHAI takes the first place in the\\npreliminary and final of the NeurIPS-2020 MineRL competition.',\n",
       "    'author': [{'name': 'Hangyu Mao'},\n",
       "     {'name': 'Chao Wang'},\n",
       "     {'name': 'Xiaotian Hao'},\n",
       "     {'name': 'Yihuan Mao'},\n",
       "     {'name': 'Yiming Lu'},\n",
       "     {'name': 'Chengjie Wu'},\n",
       "     {'name': 'Jianye Hao'},\n",
       "     {'name': 'Dong Li'},\n",
       "     {'name': 'Pingzhong Tang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The winner solution of NeurIPS 2020 MineRL competition\\n  (https://www.aicrowd.com/challenges/neurips-2020-minerl-competition/leaderboards).\\n  The paper has been accepted by DAI 2021 (the third International Conference\\n  on Distributed Artificial Intelligence)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.08857v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.08857v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.15030v2',\n",
       "    'updated': '2022-03-30T13:41:56Z',\n",
       "    'published': '2022-03-28T18:51:22Z',\n",
       "    'title': 'Solving Disjunctive Temporal Networks with Uncertainty under Restricted\\n  Time-Based Controllability using Tree Search and Graph Neural Networks',\n",
       "    'summary': 'Planning under uncertainty is an area of interest in artificial intelligence.\\nWe present a novel approach based on tree search and graph machine learning for\\nthe scheduling problem known as Disjunctive Temporal Networks with Uncertainty\\n(DTNU). Dynamic Controllability (DC) of DTNUs seeks a reactive scheduling\\nstrategy to satisfy temporal constraints in response to uncontrollable action\\ndurations. We introduce new semantics for reactive scheduling: Time-based\\nDynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We design\\na tree search algorithm to determine whether or not a DTNU is R-TDC. Moreover,\\nwe leverage a graph neural network as a heuristic for tree search guidance.\\nFinally, we conduct experiments on a known benchmark on which we show R-TDC to\\nretain significant completeness with regard to DC, while being faster to prove.\\nThis results in the tree search processing fifty percent more DTNU problems in\\nR-TDC than the state-of-the-art DC solver does in DC with the same time budget.\\nWe also observe that graph neural network search guidance leads to substantial\\nperformance gains on benchmarks of more complex DTNUs, with up to eleven times\\nmore problems solved than the baseline tree search.',\n",
       "    'author': [{'name': 'Kevin Osanlou'},\n",
       "     {'name': 'Jeremy Frank'},\n",
       "     {'name': 'Andrei Bursuc'},\n",
       "     {'name': 'Tristan Cazenave'},\n",
       "     {'name': 'Eric Jacopin'},\n",
       "     {'name': 'Christophe Guettier'},\n",
       "     {'name': 'J. Benton'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Thirty-Sixth AAAI Conference on Artificial Intelligence. This version\\n  includes the technical appendix. arXiv admin note: substantial text overlap\\n  with arXiv:2108.01068'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Thirty-Sixth AAAI Conference on Artificial Intelligence, 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.15030v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.15030v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1503.01820v1',\n",
       "    'updated': '2015-03-06T00:05:12Z',\n",
       "    'published': '2015-03-06T00:05:12Z',\n",
       "    'title': 'Latent Hierarchical Model for Activity Recognition',\n",
       "    'summary': 'We present a novel hierarchical model for human activity recognition. In\\ncontrast to approaches that successively recognize actions and activities, our\\napproach jointly models actions and activities in a unified framework, and\\ntheir labels are simultaneously predicted. The model is embedded with a latent\\nlayer that is able to capture a richer class of contextual information in both\\nstate-state and observation-state pairs. Although loops are present in the\\nmodel, the model has an overall linear-chain structure, where the exact\\ninference is tractable. Therefore, the model is very efficient in both\\ninference and learning. The parameters of the graphical model are learned with\\na Structured Support Vector Machine (Structured-SVM). A data-driven approach is\\nused to initialize the latent variables; therefore, no manual labeling for the\\nlatent states is required. The experimental results from using two benchmark\\ndatasets show that our model outperforms the state-of-the-art approach, and our\\nmodel is computationally more efficient.',\n",
       "    'author': [{'name': 'Ninghang Hu'},\n",
       "     {'name': 'Gwenn Englebienne'},\n",
       "     {'name': 'Zhongyu Lou'},\n",
       "     {'name': 'Ben Kröse'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1503.01820v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1503.01820v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1605.07157v4',\n",
       "    'updated': '2016-10-17T20:09:56Z',\n",
       "    'published': '2016-05-23T19:45:55Z',\n",
       "    'title': 'Unsupervised Learning for Physical Interaction through Video Prediction',\n",
       "    'summary': 'A core challenge for an agent learning to interact with the world is to\\npredict how its actions affect objects in its environment. Many existing\\nmethods for learning the dynamics of physical interactions require labeled\\nobject information. However, to scale real-world interaction learning to a\\nvariety of scenes and objects, acquiring labeled data becomes increasingly\\nimpractical. To learn about physical object motion without labels, we develop\\nan action-conditioned video prediction model that explicitly models pixel\\nmotion, by predicting a distribution over pixel motion from previous frames.\\nBecause our model explicitly predicts motion, it is partially invariant to\\nobject appearance, enabling it to generalize to previously unseen objects. To\\nexplore video prediction for real-world interactive agents, we also introduce a\\ndataset of 59,000 robot interactions involving pushing motions, including a\\ntest set with novel objects. In this dataset, accurate prediction of videos\\nconditioned on the robot\\'s future actions amounts to learning a \"visual\\nimagination\" of different futures based on different courses of action. Our\\nexperiments show that our proposed method produces more accurate video\\npredictions both quantitatively and qualitatively, when compared to prior\\nmethods.',\n",
       "    'author': [{'name': 'Chelsea Finn'},\n",
       "     {'name': 'Ian Goodfellow'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"To appear in NIPS '16; Video results, code, and data available at:\\n  http://www.sites.google.com/site/robotprediction\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1605.07157v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1605.07157v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1606.02378v3',\n",
       "    'updated': '2017-03-30T22:41:40Z',\n",
       "    'published': '2016-06-08T02:36:11Z',\n",
       "    'title': 'SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks',\n",
       "    'summary': 'We introduce SE3-Nets, which are deep neural networks designed to model and\\nlearn rigid body motion from raw point cloud data. Based only on sequences of\\ndepth images along with action vectors and point wise data associations,\\nSE3-Nets learn to segment effected object parts and predict their motion\\nresulting from the applied force. Rather than learning point wise flow vectors,\\nSE3-Nets predict SE3 transformations for different parts of the scene. Using\\nsimulated depth data of a table top scene and a robot manipulator, we show that\\nthe structure underlying SE3-Nets enables them to generate a far more\\nconsistent prediction of object motion than traditional flow based networks.\\nAdditional experiments with a depth camera observing a Baxter robot pushing\\nobjects on a table show that SE3-Nets also work well on real data.',\n",
       "    'author': [{'name': 'Arunkumar Byravan'}, {'name': 'Dieter Fox'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages. To appear at the IEEE International Conference on Robotics\\n  and Automation (ICRA), 2017. V2 Update: Final version submitted to ICRA with\\n  experiments testing the robustness of the system to noise and preliminary\\n  results on real world data'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1606.02378v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1606.02378v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1608.01127v1',\n",
       "    'updated': '2016-08-03T09:25:35Z',\n",
       "    'published': '2016-08-03T09:25:35Z',\n",
       "    'title': 'Autonomous Grounding of Visual Field Experience through Sensorimotor\\n  Prediction',\n",
       "    'summary': 'In a developmental framework, autonomous robots need to explore the world and\\nlearn how to interact with it. Without an a priori model of the system, this\\nopens the challenging problem of having robots master their interface with the\\nworld: how to perceive their environment using their sensors, and how to act in\\nit using their motors. The sensorimotor approach of perception claims that a\\nnaive agent can learn to master this interface by capturing regularities in the\\nway its actions transform its sensory inputs. In this paper, we apply such an\\napproach to the discovery and mastery of the visual field associated with a\\nvisual sensor. A computational model is formalized and applied to a simulated\\nsystem to illustrate the approach.',\n",
       "    'author': {'name': 'Alban Laflaquière'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 4 figures, ICDL-Epirob 2016'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1608.01127v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1608.01127v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1610.00696v2',\n",
       "    'updated': '2017-03-13T00:18:49Z',\n",
       "    'published': '2016-10-03T19:54:17Z',\n",
       "    'title': 'Deep Visual Foresight for Planning Robot Motion',\n",
       "    'summary': 'A key challenge in scaling up robot learning to many skills and environments\\nis removing the need for human supervision, so that robots can collect their\\nown data and improve their own performance without being limited by the cost of\\nrequesting human feedback. Model-based reinforcement learning holds the promise\\nof enabling an agent to learn to predict the effects of its actions, which\\ncould provide flexible predictive models for a wide range of tasks and\\nenvironments, without detailed human supervision. We develop a method for\\ncombining deep action-conditioned video prediction models with model-predictive\\ncontrol that uses entirely unlabeled training data. Our approach does not\\nrequire a calibrated camera, an instrumented training set-up, nor precise\\nsensing and actuation. Our results show that our method enables a real robot to\\nperform nonprehensile manipulation -- pushing objects -- and can handle novel\\nobjects not seen during training.',\n",
       "    'author': [{'name': 'Chelsea Finn'}, {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2017. Supplementary video:\\n  https://sites.google.com/site/robotforesight/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1610.00696v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1610.00696v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1707.05733v2',\n",
       "    'updated': '2019-11-19T12:43:54Z',\n",
       "    'published': '2017-07-18T16:36:56Z',\n",
       "    'title': 'Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in\\n  Changing Environments',\n",
       "    'summary': 'Object detection is an essential task for autonomous robots operating in\\ndynamic and changing environments. A robot should be able to detect objects in\\nthe presence of sensor noise that can be induced by changing lighting\\nconditions for cameras and false depth readings for range sensors, especially\\nRGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion\\napproach for object detection that learns weighting the predictions of\\ndifferent sensor modalities in an online manner. Our approach is based on a\\nmixture of convolutional neural network (CNN) experts and incorporates multiple\\nmodalities including appearance, depth and motion. We test our method in\\nextensive robot experiments, in which we detect people in a combined indoor and\\noutdoor scenario from RGB-D data, and we demonstrate that our method can adapt\\nto harsh lighting changes and severe camera motion blur. Furthermore, we\\npresent a new RGB-D dataset for people detection in mixed in- and outdoor\\nenvironments, recorded with a mobile robot. Code, pretrained models and dataset\\nare available at http://adaptivefusion.cs.uni-freiburg.de',\n",
       "    'author': [{'name': 'Oier Mees'},\n",
       "     {'name': 'Andreas Eitel'},\n",
       "     {'name': 'Wolfram Burgard'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/IROS.2016.7759048'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/IROS.2016.7759048',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1707.05733v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1707.05733v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at the 2016 IEEE/RSJ International Conference on\\n  Intelligent Robots and Systems. Added a new baseline with respect to the IROS\\n  version. Project page with code, pretrained models and our InOutDoorPeople\\n  RGB-D dataset at http://adaptivefusion.cs.uni-freiburg.de/'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1708.07969v1',\n",
       "    'updated': '2017-08-26T13:46:21Z',\n",
       "    'published': '2017-08-26T13:46:21Z',\n",
       "    'title': '3D Object Reconstruction from a Single Depth View with Adversarial\\n  Learning',\n",
       "    'summary': 'In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the\\ncomplete 3D structure of a given object from a single arbitrary depth view\\nusing generative adversarial networks. Unlike the existing work which typically\\nrequires multiple views of the same object or class labels to recover the full\\n3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of\\na depth view of the object as input, and is able to generate the complete 3D\\noccupancy grid by filling in the occluded/missing regions. The key idea is to\\ncombine the generative capabilities of autoencoders and the conditional\\nGenerative Adversarial Networks (GAN) framework, to infer accurate and\\nfine-grained 3D structures of objects in high-dimensional voxel space.\\nExtensive experiments on large synthetic datasets show that the proposed\\n3D-RecGAN significantly outperforms the state of the art in single view 3D\\nobject reconstruction, and is able to reconstruct unseen types of objects. Our\\ncode and data are available at: https://github.com/Yang7879/3D-RecGAN.',\n",
       "    'author': [{'name': 'Bo Yang'},\n",
       "     {'name': 'Hongkai Wen'},\n",
       "     {'name': 'Sen Wang'},\n",
       "     {'name': 'Ronald Clark'},\n",
       "     {'name': 'Andrew Markham'},\n",
       "     {'name': 'Niki Trigoni'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICCV Workshops 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1708.07969v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1708.07969v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.04905v1',\n",
       "    'updated': '2017-09-14T17:50:18Z',\n",
       "    'published': '2017-09-14T17:50:18Z',\n",
       "    'title': 'One-Shot Visual Imitation Learning via Meta-Learning',\n",
       "    'summary': 'In order for a robot to be a generalist that can perform a wide range of\\njobs, it must be able to acquire a wide variety of skills quickly and\\nefficiently in complex unstructured environments. High-capacity models such as\\ndeep neural networks can enable a robot to represent complex skills, but\\nlearning each skill from scratch then becomes infeasible. In this work, we\\npresent a meta-imitation learning method that enables a robot to learn how to\\nlearn more efficiently, allowing it to acquire new skills from just a single\\ndemonstration. Unlike prior methods for one-shot imitation, our method can\\nscale to raw pixel inputs and requires data from significantly fewer prior\\ntasks for effective learning of new skills. Our experiments on both simulated\\nand real robot platforms demonstrate the ability to learn new tasks,\\nend-to-end, from a single visual demonstration.',\n",
       "    'author': [{'name': 'Chelsea Finn'},\n",
       "     {'name': 'Tianhe Yu'},\n",
       "     {'name': 'Tianhao Zhang'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Conference on Robot Learning, 2017 (to appear). First two authors\\n  contributed equally. Video available at\\n  https://sites.google.com/view/one-shot-imitation'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.04905v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.04905v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.07857v2',\n",
       "    'updated': '2017-09-25T21:35:45Z',\n",
       "    'published': '2017-09-22T17:23:12Z',\n",
       "    'title': 'Using Simulation and Domain Adaptation to Improve Efficiency of Deep\\n  Robotic Grasping',\n",
       "    'summary': 'Instrumenting and collecting annotated visual grasping datasets to train\\nmodern machine learning algorithms can be extremely time-consuming and\\nexpensive. An appealing alternative is to use off-the-shelf simulators to\\nrender synthetic data for which ground-truth annotations are generated\\nautomatically. Unfortunately, models trained purely on simulated data often\\nfail to generalize to the real world. We study how randomized simulated\\nenvironments and domain adaptation methods can be extended to train a grasping\\nsystem to grasp novel objects from raw monocular RGB images. We extensively\\nevaluate our approaches with a total of more than 25,000 physical test grasps,\\nstudying a range of simulation conditions and domain adaptation methods,\\nincluding a novel extension of pixel-level domain adaptation that we term the\\nGraspGAN. We show that, by using synthetic data and domain adaptation, we are\\nable to reduce the number of real-world samples needed to achieve a given level\\nof performance by up to 50 times, using only randomly generated simulated\\nobjects. We also show that by using only unlabeled real-world data and our\\nGraspGAN methodology, we obtain real-world grasping performance without any\\nreal-world labels that is similar to that achieved with 939,777 labeled\\nreal-world samples.',\n",
       "    'author': [{'name': 'Konstantinos Bousmalis'},\n",
       "     {'name': 'Alex Irpan'},\n",
       "     {'name': 'Paul Wohlhart'},\n",
       "     {'name': 'Yunfei Bai'},\n",
       "     {'name': 'Matthew Kelcey'},\n",
       "     {'name': 'Mrinal Kalakrishnan'},\n",
       "     {'name': 'Laura Downs'},\n",
       "     {'name': 'Julian Ibarz'},\n",
       "     {'name': 'Peter Pastor'},\n",
       "     {'name': 'Kurt Konolige'},\n",
       "     {'name': 'Sergey Levine'},\n",
       "     {'name': 'Vincent Vanhoucke'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 5 figures, 3 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.07857v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.07857v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1711.06623v2',\n",
       "    'updated': '2018-03-05T14:29:23Z',\n",
       "    'published': '2017-11-17T16:54:40Z',\n",
       "    'title': 'Driven to Distraction: Self-Supervised Distractor Learning for Robust\\n  Monocular Visual Odometry in Urban Environments',\n",
       "    'summary': 'We present a self-supervised approach to ignoring \"distractors\" in camera\\nimages for the purposes of robustly estimating vehicle motion in cluttered\\nurban environments. We leverage offline multi-session mapping approaches to\\nautomatically generate a per-pixel ephemerality mask and depth map for each\\ninput image, which we use to train a deep convolutional network. At run-time we\\nuse the predicted ephemerality and depth as an input to a monocular visual\\nodometry (VO) pipeline, using either sparse features or dense photometric\\nmatching. Our approach yields metric-scale VO using only a single camera and\\ncan recover the correct egomotion even when 90% of the image is obscured by\\ndynamic, independently moving objects. We evaluate our robust VO methods on\\nmore than 400km of driving from the Oxford RobotCar Dataset and demonstrate\\nreduced odometry drift and significantly improved egomotion estimation in the\\npresence of large moving vehicles in urban traffic.',\n",
       "    'author': [{'name': 'Dan Barnes'},\n",
       "     {'name': 'Will Maddern'},\n",
       "     {'name': 'Geoffrey Pascoe'},\n",
       "     {'name': 'Ingmar Posner'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Robotics and Automation (ICRA), 2018.\\n  Video summary: http://youtu.be/ebIrBn_nc-k'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1711.06623v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1711.06623v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.07193v2',\n",
       "    'updated': '2018-06-05T15:20:52Z',\n",
       "    'published': '2018-05-18T13:17:46Z',\n",
       "    'title': 'The EuroCity Persons Dataset: A Novel Benchmark for Object Detection',\n",
       "    'summary': 'Big data has had a great share in the success of deep learning in computer\\nvision. Recent works suggest that there is significant further potential to\\nincrease object detection performance by utilizing even bigger datasets. In\\nthis paper, we introduce the EuroCity Persons dataset, which provides a large\\nnumber of highly diverse, accurate and detailed annotations of pedestrians,\\ncyclists and other riders in urban traffic scenes. The images for this dataset\\nwere collected on-board a moving vehicle in 31 cities of 12 European countries.\\nWith over 238200 person instances manually labeled in over 47300 images,\\nEuroCity Persons is nearly one order of magnitude larger than person datasets\\nused previously for benchmarking. The dataset furthermore contains a large\\nnumber of person orientation annotations (over 211200). We optimize four\\nstate-of-the-art deep learning approaches (Faster R-CNN, R-FCN, SSD and YOLOv3)\\nto serve as baselines for the new object detection benchmark. In experiments\\nwith previous datasets we analyze the generalization capabilities of these\\ndetectors when trained with the new dataset. We furthermore study the effect of\\nthe training set size, the dataset diversity (day- vs. night-time, geographical\\nregion), the dataset detail (i.e. availability of object orientation\\ninformation) and the annotation quality on the detector performance. Finally,\\nwe analyze error sources and discuss the road ahead.',\n",
       "    'author': [{'name': 'Markus Braun'},\n",
       "     {'name': 'Sebastian Krebs'},\n",
       "     {'name': 'Fabian Flohr'},\n",
       "     {'name': 'Dariu M. Gavrila'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TPAMI.2019.2897684'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TPAMI.2019.2897684',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1805.07193v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.07193v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to IEEE Trans. on Pattern Analysis and Machine Intelligence'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in IEEE Trans. on Pattern Analysis and Machine\\n  Intelligence, 2019'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.00703v1',\n",
       "    'updated': '2018-07-02T14:20:24Z',\n",
       "    'published': '2018-07-02T14:20:24Z',\n",
       "    'title': 'Introducing the Simulated Flying Shapes and Simulated Planar Manipulator\\n  Datasets',\n",
       "    'summary': 'We release two artificial datasets, Simulated Flying Shapes and Simulated\\nPlanar Manipulator that allow to test the learning ability of video processing\\nsystems. In particular, the dataset is meant as a tool which allows to easily\\nassess the sanity of deep neural network models that aim to encode, reconstruct\\nor predict video frame sequences. The datasets each consist of 90000 videos.\\nThe Simulated Flying Shapes dataset comprises scenes showing two objects of\\nequal shape (rectangle, triangle and circle) and size in which one object\\napproaches its counterpart. The Simulated Planar Manipulator shows a 3-DOF\\nplanar manipulator that executes a pick-and-place task in which it has to place\\na size-varying circle on a squared platform. Different from other widely used\\ndatasets such as moving MNIST [1], [2], the two presented datasets involve\\ngoal-oriented tasks (e.g. the manipulator grasping an object and placing it on\\na platform), rather than showing random movements. This makes our datasets more\\nsuitable for testing prediction capabilities and the learning of sophisticated\\nmotions by a machine learning model. This technical document aims at providing\\nan introduction into the usage of both datasets.',\n",
       "    'author': [{'name': 'Fabio Ferreira'},\n",
       "     {'name': 'Jonas Rothfuss'},\n",
       "     {'name': 'Eren Erdal Aksoy'},\n",
       "     {'name': 'You Zhou'},\n",
       "     {'name': 'Tamim Asfour'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'technical documentation, 2 figures, links to repositories'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.00703v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.00703v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.06757v1',\n",
       "    'updated': '2018-07-18T03:28:02Z',\n",
       "    'published': '2018-07-18T03:28:02Z',\n",
       "    'title': 'On Evaluation of Embodied Navigation Agents',\n",
       "    'summary': 'Skillful mobile operation in three-dimensional environments is a primary\\ntopic of study in Artificial Intelligence. The past two years have seen a surge\\nof creative work on navigation. This creative output has produced a plethora of\\nsometimes incompatible task definitions and evaluation protocols. To coordinate\\nongoing and future research in this area, we have convened a working group to\\nstudy empirical methodology in navigation research. The present document\\nsummarizes the consensus recommendations of this working group. We discuss\\ndifferent problem statements and the role of generalization, present evaluation\\nmeasures, and provide standard scenarios that can be used for benchmarking.',\n",
       "    'author': [{'name': 'Peter Anderson'},\n",
       "     {'name': 'Angel Chang'},\n",
       "     {'name': 'Devendra Singh Chaplot'},\n",
       "     {'name': 'Alexey Dosovitskiy'},\n",
       "     {'name': 'Saurabh Gupta'},\n",
       "     {'name': 'Vladlen Koltun'},\n",
       "     {'name': 'Jana Kosecka'},\n",
       "     {'name': 'Jitendra Malik'},\n",
       "     {'name': 'Roozbeh Mottaghi'},\n",
       "     {'name': 'Manolis Savva'},\n",
       "     {'name': 'Amir R. Zamir'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Report of a working group on empirical methodology in navigation\\n  research. Authors are listed in alphabetical order'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.06757v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.06757v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1808.00758v2',\n",
       "    'updated': '2019-08-18T06:32:40Z',\n",
       "    'published': '2018-08-02T11:09:13Z',\n",
       "    'title': 'Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D\\n  Reconstruction',\n",
       "    'summary': 'We study the problem of recovering an underlying 3D shape from a set of\\nimages. Existing learning based approaches usually resort to recurrent neural\\nnets, e.g., GRU, or intuitive pooling operations, e.g., max/mean poolings, to\\nfuse multiple deep features encoded from input images. However, GRU based\\napproaches are unable to consistently estimate 3D shapes given different\\npermutations of the same set of input images as the recurrent unit is\\npermutation variant. It is also unlikely to refine the 3D shape given more\\nimages due to the long-term memory loss of GRU. Commonly used pooling\\napproaches are limited to capturing partial information, e.g., max/mean values,\\nignoring other valuable features. In this paper, we present a new feed-forward\\nneural module, named AttSets, together with a dedicated training algorithm,\\nnamed FASet, to attentively aggregate an arbitrarily sized deep feature set for\\nmulti-view 3D reconstruction. The AttSets module is permutation invariant,\\ncomputationally efficient and flexible to implement, while the FASet algorithm\\nenables the AttSets based network to be remarkably robust and generalize to an\\narbitrary number of input images. We thoroughly evaluate FASet and the\\nproperties of AttSets on multiple large public datasets. Extensive experiments\\nshow that AttSets together with FASet algorithm significantly outperforms\\nexisting aggregation approaches.',\n",
       "    'author': [{'name': 'Bo Yang'},\n",
       "     {'name': 'Sen Wang'},\n",
       "     {'name': 'Andrew Markham'},\n",
       "     {'name': 'Niki Trigoni'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/s11263-019-01217-w'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/s11263-019-01217-w',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1808.00758v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1808.00758v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IJCV 2019. Code and data are available at\\n  https://github.com/Yang7879/AttSets'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.00912v1',\n",
       "    'updated': '2018-10-01T18:37:05Z',\n",
       "    'published': '2018-10-01T18:37:05Z',\n",
       "    'title': 'Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition',\n",
       "    'summary': 'In an open-world setting, it is inevitable that an intelligent agent (e.g., a\\nrobot) will encounter visual objects, attributes or relationships it does not\\nrecognize. In this work, we develop an agent empowered with visual curiosity,\\ni.e. the ability to ask questions to an Oracle (e.g., human) about the contents\\nin images (e.g., What is the object on the left side of the red cube?) and\\nbuild visual recognition model based on the answers received (e.g., Cylinder).\\nIn order to do this, the agent must (1) understand what it recognizes and what\\nit does not, (2) formulate a valid, unambiguous and informative language query\\n(a question) to ask the Oracle, (3) derive the parameters of visual classifiers\\nfrom the Oracle response and (4) leverage the updated visual classifiers to ask\\nmore clarified questions. Specifically, we propose a novel framework and\\nformulate the learning of visual curiosity as a reinforcement learning problem.\\nIn this framework, all components of our agent, visual recognition module (to\\nsee), question generation policy (to ask), answer digestion module (to\\nunderstand) and graph memory module (to memorize), are learned entirely\\nend-to-end to maximize the reward derived from the scene graph obtained by the\\nagent as a consequence of the dialog with the Oracle. Importantly, the question\\ngeneration policy is disentangled from the visual recognition system and\\nspecifics of the environment. Consequently, we demonstrate a sort of double\\ngeneralization. Our question generation policy generalizes to new environments\\nand a new pair of eyes, i.e., new visual system. Trained on a synthetic\\ndataset, our results show that our agent learns new visual concepts\\nsignificantly faster than several heuristic baselines, even when tested on\\nsynthetic environments with novel objects, as well as in a realistic\\nenvironment.',\n",
       "    'author': [{'name': 'Jianwei Yang'},\n",
       "     {'name': 'Jiasen Lu'},\n",
       "     {'name': 'Stefan Lee'},\n",
       "     {'name': 'Dhruv Batra'},\n",
       "     {'name': 'Devi Parikh'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '18 pages, 10 figures, Oral Presentation in Conference on Robot\\n  Learning (CoRL) 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.00912v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.00912v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.03237v1',\n",
       "    'updated': '2018-10-08T00:57:24Z',\n",
       "    'published': '2018-10-08T00:57:24Z',\n",
       "    'title': 'Task-Embedded Control Networks for Few-Shot Imitation Learning',\n",
       "    'summary': 'Much like humans, robots should have the ability to leverage knowledge from\\npreviously learned tasks in order to learn new tasks quickly in new and\\nunfamiliar environments. Despite this, most robot learning approaches have\\nfocused on learning a single task, from scratch, with a limited notion of\\ngeneralisation, and no way of leveraging the knowledge to learn other tasks\\nmore efficiently. One possible solution is meta-learning, but many of the\\nrelated approaches are limited in their ability to scale to a large number of\\ntasks and to learn further tasks without forgetting previously learned ones.\\nWith this in mind, we introduce Task-Embedded Control Networks, which employ\\nideas from metric learning in order to create a task embedding that can be used\\nby a robot to learn new tasks from one or more demonstrations. In the area of\\nvisually-guided manipulation, we present simulation results in which we surpass\\nthe performance of a state-of-the-art method when using only visual information\\nfrom each demonstration. Additionally, we demonstrate that our approach can\\nalso be used in conjunction with domain randomisation to train our few-shot\\nlearning ability in simulation and then deploy in the real world without any\\nadditional training. Once deployed, the robot can learn new tasks from a single\\nreal-world demonstration.',\n",
       "    'author': [{'name': 'Stephen James'},\n",
       "     {'name': 'Michael Bloesch'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at the Conference on Robot Learning (CoRL) 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.03237v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.03237v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1812.00971v2',\n",
       "    'updated': '2019-03-26T23:55:19Z',\n",
       "    'published': '2018-12-03T18:46:02Z',\n",
       "    'title': 'Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using\\n  Meta-Learning',\n",
       "    'summary': 'Learning is an inherently continuous phenomenon. When humans learn a new task\\nthere is no explicit distinction between training and inference. As we learn a\\ntask, we keep learning about it while performing the task. What we learn and\\nhow we learn it varies during different stages of learning. Learning how to\\nlearn and adapt is a key property that enables us to generalize effortlessly to\\nnew settings. This is in contrast with conventional settings in machine\\nlearning where a trained model is frozen during inference. In this paper we\\nstudy the problem of learning to learn at both training and test time in the\\ncontext of visual navigation. A fundamental challenge in navigation is\\ngeneralization to unseen scenes. In this paper we propose a self-adaptive\\nvisual navigation method (SAVN) which learns to adapt to new environments\\nwithout any explicit supervision. Our solution is a meta-reinforcement learning\\napproach where an agent learns a self-supervised interaction loss that\\nencourages effective navigation. Our experiments, performed in the AI2-THOR\\nframework, show major improvements in both success rate and SPL for visual\\nnavigation in novel scenes. Our code and data are available at:\\nhttps://github.com/allenai/savn .',\n",
       "    'author': [{'name': 'Mitchell Wortsman'},\n",
       "     {'name': 'Kiana Ehsani'},\n",
       "     {'name': 'Mohammad Rastegari'},\n",
       "     {'name': 'Ali Farhadi'},\n",
       "     {'name': 'Roozbeh Mottaghi'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1812.00971v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1812.00971v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1901.05105v2',\n",
       "    'updated': '2019-03-06T03:31:12Z',\n",
       "    'published': '2019-01-16T01:46:57Z',\n",
       "    'title': 'Uncertainty-Aware Driver Trajectory Prediction at Urban Intersections',\n",
       "    'summary': \"Predicting the motion of a driver's vehicle is crucial for advanced driving\\nsystems, enabling detection of potential risks towards shared control between\\nthe driver and automation systems. In this paper, we propose a variational\\nneural network approach that predicts future driver trajectory distributions\\nfor the vehicle based on multiple sensors. Our predictor generates both a\\nconditional variational distribution of future trajectories, as well as a\\nconfidence estimate for different time horizons. Our approach allows us to\\nhandle inherently uncertain situations, and reason about information gain from\\neach input, as well as combine our model with additional predictors, creating a\\nmixture of experts. We show how to augment the variational predictor with a\\nphysics-based predictor, and based on their confidence estimations, improve\\noverall system performance. The resulting combined model is aware of the\\nuncertainty associated with its predictions, which can help the vehicle\\nautonomy to make decisions with more confidence. The model is validated on\\nreal-world urban driving data collected in multiple locations. This validation\\ndemonstrates that our approach improves the prediction error of a physics-based\\nmodel by 25% while successfully identifying the uncertain cases with 82%\\naccuracy.\",\n",
       "    'author': [{'name': 'Xin Huang'},\n",
       "     {'name': 'Stephen McGill'},\n",
       "     {'name': 'Brian C. Williams'},\n",
       "     {'name': 'Luke Fletcher'},\n",
       "     {'name': 'Guy Rosman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Accepted at ICRA'19. 8 pages, 9 figures, 1 table. Video at\\n  https://youtu.be/clR08hRdtlM\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1901.05105v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1901.05105v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1901.10915v2',\n",
       "    'updated': '2019-03-28T11:58:29Z',\n",
       "    'published': '2019-01-30T15:50:54Z',\n",
       "    'title': 'Benchmarking Classic and Learned Navigation in Complex 3D Environments',\n",
       "    'summary': 'Navigation research is attracting renewed interest with the advent of\\nlearning-based methods. However, this new line of work is largely disconnected\\nfrom well-established classic navigation approaches. In this paper, we take a\\nstep towards coordinating these two directions of research. We set up classic\\nand learning-based navigation systems in common simulated environments and\\nthoroughly evaluate them in indoor spaces of varying complexity, with access to\\ndifferent sensory modalities. Additionally, we measure human performance in the\\nsame environments. We find that a classic pipeline, when properly tuned, can\\nperform very well in complex cluttered environments. On the other hand, learned\\nsystems can operate more robustly with a limited sensor suite. Overall, both\\napproaches are still far from human-level performance.',\n",
       "    'author': [{'name': 'Dmytro Mishkin'},\n",
       "     {'name': 'Alexey Dosovitskiy'},\n",
       "     {'name': 'Vladlen Koltun'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Added CNN-Monodepth and OpenCV Stereo agents'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1901.10915v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1901.10915v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1901.10968v1',\n",
       "    'updated': '2019-01-30T17:35:42Z',\n",
       "    'published': '2019-01-30T17:35:42Z',\n",
       "    'title': 'Bootstrapping Robotic Ecological Perception from a Limited Set of\\n  Hypotheses Through Interactive Perception',\n",
       "    'summary': 'To solve its task, a robot needs to have the ability to interpret its\\nperceptions. In vision, this interpretation is particularly difficult and\\nrelies on the understanding of the structure of the scene, at least to the\\nextent of its task and sensorimotor abilities. A robot with the ability to\\nbuild and adapt this interpretation process according to its own tasks and\\ncapabilities would push away the limits of what robots can achieve in a non\\ncontrolled environment. A solution is to provide the robot with processes to\\nbuild such representations that are not specific to an environment or a\\nsituation. A lot of works focus on objects segmentation, recognition and\\nmanipulation. Defining an object solely on the basis of its visual appearance\\nis challenging given the wide range of possible objects and environments.\\nTherefore, current works make simplifying assumptions about the structure of a\\nscene. Such assumptions reduce the adaptivity of the object extraction process\\nto the environments in which the assumption holds. To limit such assumptions,\\nwe introduce an exploration method aimed at identifying moveable elements in a\\nscene without considering the concept of object. By using the interactive\\nperception framework, we aim at bootstrapping the acquisition process of a\\nrepresentation of the environment with a minimum of context specific\\nassumptions. The robotic system builds a perceptual map called relevance map\\nwhich indicates the moveable parts of the current scene. A classifier is\\ntrained online to predict the category of each region (moveable or\\nnon-moveable). It is also used to select a region with which to interact, with\\nthe goal of minimizing the uncertainty of the classification. A specific\\nclassifier is introduced to fit these needs: the collaborative mixture models\\nclassifier. The method is tested on a set of scenarios of increasing\\ncomplexity, using both simulations and a PR2 robot.',\n",
       "    'author': [{'name': 'Léni K. Le Goff'},\n",
       "     {'name': 'Ghanim Mukhtar'},\n",
       "     {'name': 'Alexandre Coninx'},\n",
       "     {'name': 'Stéphane Doncieux'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '21 pages, 21 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1901.10968v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1901.10968v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1902.05947v1',\n",
       "    'updated': '2019-02-18T18:57:05Z',\n",
       "    'published': '2019-02-18T18:57:05Z',\n",
       "    'title': 'DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching',\n",
       "    'summary': 'Robots should understand both semantics and physics to be functional in the\\nreal world. While robot platforms provide means for interacting with the\\nphysical world they cannot autonomously acquire object-level semantics without\\nneeding human. In this paper, we investigate how to minimize human effort and\\nintervention to teach robots perform real world tasks that incorporate\\nsemantics. We study this question in the context of visual servoing of mobile\\nrobots and propose DIViS, a Domain Invariant policy learning approach for\\ncollision free Visual Servoing. DIViS incorporates high level semantics from\\npreviously collected static human-labeled datasets and learns collision free\\nservoing entirely in simulation and without any real robot data. However, DIViS\\ncan directly be deployed on a real robot and is capable of servoing to the\\nuser-specified object categories while avoiding collisions in the real world.\\nDIViS is not constrained to be queried by the final view of goal but rather is\\nrobust to servo to image goals taken from initial robot view with high\\nocclusions without this impairing its ability to maintain a collision free\\npath. We show the generalization capability of DIViS on real mobile robots in\\nmore than 90 real world test scenarios with various unseen object goals in\\nunstructured environments. DIViS is compared to prior approaches via real world\\nexperiments and rigorous tests in simulation. For supplementary videos, see:\\n\\\\href{https://fsadeghi.github.io/DIViS}{https://fsadeghi.github.io/DIViS}',\n",
       "    'author': {'name': 'Fereshteh Sadeghi'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Supplementary videos: https://fsadeghi.github.io/DIViS'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1902.05947v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1902.05947v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.01534v1',\n",
       "    'updated': '2019-03-04T20:51:37Z',\n",
       "    'published': '2019-03-04T20:51:37Z',\n",
       "    'title': 'Selective Sensor Fusion for Neural Visual-Inertial Odometry',\n",
       "    'summary': 'Deep learning approaches for Visual-Inertial Odometry (VIO) have proven\\nsuccessful, but they rarely focus on incorporating robust fusion strategies for\\ndealing with imperfect input sensory data. We propose a novel end-to-end\\nselective sensor fusion framework for monocular VIO, which fuses monocular\\nimages and inertial measurements in order to estimate the trajectory whilst\\nimproving robustness to real-life issues, such as missing and corrupted data or\\nbad sensor synchronization. In particular, we propose two fusion modalities\\nbased on different masking strategies: deterministic soft fusion and stochastic\\nhard fusion, and we compare with previously proposed direct fusion baselines.\\nDuring testing, the network is able to selectively process the features of the\\navailable sensor modalities and produce a trajectory at scale. We present a\\nthorough investigation on the performances on three public autonomous driving,\\nMicro Aerial Vehicle (MAV) and hand-held VIO datasets. The results demonstrate\\nthe effectiveness of the fusion strategies, which offer better performances\\ncompared to direct fusion, particularly in presence of corrupted data. In\\naddition, we study the interpretability of the fusion networks by visualising\\nthe masking layers in different scenarios and with varying data corruption,\\nrevealing interesting correlations between the fusion networks and imperfect\\nsensory input data.',\n",
       "    'author': [{'name': 'Changhao Chen'},\n",
       "     {'name': 'Stefano Rosa'},\n",
       "     {'name': 'Yishu Miao'},\n",
       "     {'name': 'Chris Xiaoxuan Lu'},\n",
       "     {'name': 'Wei Wu'},\n",
       "     {'name': 'Andrew Markham'},\n",
       "     {'name': 'Niki Trigoni'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted by CVPR 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.01534v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.01534v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.04128v1',\n",
       "    'updated': '2019-03-11T05:14:34Z',\n",
       "    'published': '2019-03-11T05:14:34Z',\n",
       "    'title': 'Manipulation by Feel: Touch-Based Control with Deep Predictive Models',\n",
       "    'summary': 'Touch sensing is widely acknowledged to be important for dexterous robotic\\nmanipulation, but exploiting tactile sensing for continuous, non-prehensile\\nmanipulation is challenging. General purpose control techniques that are able\\nto effectively leverage tactile sensing as well as accurate physics models of\\ncontacts and forces remain largely elusive, and it is unclear how to even\\nspecify a desired behavior in terms of tactile percepts. In this paper, we take\\na step towards addressing these issues by combining high-resolution tactile\\nsensing with data-driven modeling using deep neural network dynamics models. We\\npropose deep tactile MPC, a framework for learning to perform tactile servoing\\nfrom raw tactile sensor inputs, without manual supervision. We show that this\\nmethod enables a robot equipped with a GelSight-style tactile sensor to\\nmanipulate a ball, analog stick, and 20-sided die, learning from unsupervised\\nautonomous interaction and then using the learned tactile predictive model to\\nreposition each object to user-specified configurations, indicated by a goal\\ntactile reading. Videos, visualizations and the code are available here:\\nhttps://sites.google.com/view/deeptactilempc',\n",
       "    'author': [{'name': 'Stephen Tian'},\n",
       "     {'name': 'Frederik Ebert'},\n",
       "     {'name': 'Dinesh Jayaraman'},\n",
       "     {'name': 'Mayur Mudigonda'},\n",
       "     {'name': 'Chelsea Finn'},\n",
       "     {'name': 'Roberto Calandra'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICRA 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.04128v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.04128v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.08960v2',\n",
       "    'updated': '2019-07-26T18:31:06Z',\n",
       "    'published': '2019-03-21T12:49:31Z',\n",
       "    'title': 'Short-Term Prediction and Multi-Camera Fusion on Semantic Grids',\n",
       "    'summary': \"An environment representation (ER) is a substantial part of every autonomous\\nsystem. It introduces a common interface between perception and other system\\ncomponents, such as decision making, and allows downstream algorithms to deal\\nwith abstracted data without knowledge of the used sensor. In this work, we\\npropose and evaluate a novel architecture that generates an egocentric,\\ngrid-based, predictive, and semantically-interpretable ER. In particular, we\\nprovide a proof of concept for the spatio-temporal fusion of multiple camera\\nsequences and short-term prediction in such an ER. Our design utilizes a strong\\nsemantic segmentation network together with depth and egomotion estimates to\\nfirst extract semantic information from multiple camera streams and then\\ntransform these separately into egocentric temporally-aligned bird's-eye view\\ngrids. A deep encoder-decoder network is trained to fuse a stack of these grids\\ninto a unified semantic grid representation and to predict the dynamics of its\\nsurrounding. We evaluate this representation on real-world sequences of the\\nCityscapes dataset and show that our architecture can make accurate predictions\\nin complex sensor fusion scenarios and significantly outperforms a model-driven\\nbaseline in a category-based evaluation.\",\n",
       "    'author': [{'name': 'Lukas Hoyer'},\n",
       "     {'name': 'Patrick Kesper'},\n",
       "     {'name': 'Anna Khoreva'},\n",
       "     {'name': 'Volker Fischer'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.08960v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.08960v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.02266v3',\n",
       "    'updated': '2019-08-23T23:21:18Z',\n",
       "    'published': '2019-04-03T23:25:01Z',\n",
       "    'title': 'Continuous Direct Sparse Visual Odometry from RGB-D Images',\n",
       "    'summary': 'This paper reports on a novel formulation and evaluation of visual odometry\\nfrom RGB-D images. Assuming a static scene, the developed theoretical framework\\ngeneralizes the widely used direct energy formulation (photometric error\\nminimization) technique for obtaining a rigid body transformation that aligns\\ntwo overlapping RGB-D images to a continuous formulation. The continuity is\\nachieved through functional treatment of the problem and representing the\\nprocess models over RGB-D images in a reproducing kernel Hilbert space;\\nconsequently, the registration is not limited to the specific image resolution\\nand the framework is fully analytical with a closed-form derivation of the\\ngradient. We solve the problem by maximizing the inner product between two\\nfunctions defined over RGB-D images, while the continuous action of the rigid\\nbody motion Lie group is captured through the integration of the flow in the\\ncorresponding Lie algebra. Energy-based approaches have been extremely\\nsuccessful and the developed framework in this paper shares many of their\\ndesired properties such as the parallel structure on both CPUs and GPUs,\\nsparsity, semi-dense tracking, avoiding explicit data association which is\\ncomputationally expensive, and possible extensions to the simultaneous\\nlocalization and mapping frameworks. The evaluations on experimental data and\\ncomparison with the equivalent energy-based formulation of the problem confirm\\nthe effectiveness of the proposed technique, especially, when the lack of\\nstructure and texture in the environment is evident.',\n",
       "    'author': [{'name': 'Maani Ghaffari'},\n",
       "     {'name': 'William Clark'},\n",
       "     {'name': 'Anthony Bloch'},\n",
       "     {'name': 'Ryan M. Eustice'},\n",
       "     {'name': 'Jessy W. Grizzle'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.02266v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.02266v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '53B21, 46C05, 46C07, 68T40, 68T45, 93C85',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.10; I.4.10; G.1.6; G.4',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.08405v3',\n",
       "    'updated': '2020-08-08T10:55:56Z',\n",
       "    'published': '2019-04-17T17:59:34Z',\n",
       "    'title': 'Event-based Vision: A Survey',\n",
       "    'summary': 'Event cameras are bio-inspired sensors that differ from conventional frame\\ncameras: Instead of capturing images at a fixed rate, they asynchronously\\nmeasure per-pixel brightness changes, and output a stream of events that encode\\nthe time, location and sign of the brightness changes. Event cameras offer\\nattractive properties compared to traditional cameras: high temporal resolution\\n(in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low\\npower consumption, and high pixel bandwidth (on the order of kHz) resulting in\\nreduced motion blur. Hence, event cameras have a large potential for robotics\\nand computer vision in challenging scenarios for traditional cameras, such as\\nlow-latency, high speed, and high dynamic range. However, novel methods are\\nrequired to process the unconventional output of these sensors in order to\\nunlock their potential. This paper provides a comprehensive overview of the\\nemerging field of event-based vision, with a focus on the applications and the\\nalgorithms developed to unlock the outstanding properties of event cameras. We\\npresent event cameras from their working principle, the actual sensors that are\\navailable and the tasks that they have been used for, from low-level vision\\n(feature detection and tracking, optic flow, etc.) to high-level vision\\n(reconstruction, segmentation, recognition). We also discuss the techniques\\ndeveloped to process events, including learning-based techniques, as well as\\nspecialized processors for these novel sensors, such as spiking neural\\nnetworks. Additionally, we highlight the challenges that remain to be tackled\\nand the opportunities that lie ahead in the search for a more efficient,\\nbio-inspired way for machines to perceive and interact with the world.',\n",
       "    'author': [{'name': 'Guillermo Gallego'},\n",
       "     {'name': 'Tobi Delbruck'},\n",
       "     {'name': 'Garrick Orchard'},\n",
       "     {'name': 'Chiara Bartolozzi'},\n",
       "     {'name': 'Brian Taba'},\n",
       "     {'name': 'Andrea Censi'},\n",
       "     {'name': 'Stefan Leutenegger'},\n",
       "     {'name': 'Andrew Davison'},\n",
       "     {'name': 'Joerg Conradt'},\n",
       "     {'name': 'Kostas Daniilidis'},\n",
       "     {'name': 'Davide Scaramuzza'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TPAMI.2020.3008413'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TPAMI.2020.3008413',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1904.08405v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.08405v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Transactions on Pattern Analysis and Machine Intelligence,\\n  2020'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.09503v2',\n",
       "    'updated': '2019-10-21T21:11:58Z',\n",
       "    'published': '2019-04-20T22:02:45Z',\n",
       "    'title': 'Model-free Deep Reinforcement Learning for Urban Autonomous Driving',\n",
       "    'summary': 'Urban autonomous driving decision making is challenging due to complex road\\ngeometry and multi-agent interactions. Current decision making methods are\\nmostly manually designing the driving policy, which might result in sub-optimal\\nsolutions and is expensive to develop, generalize and maintain at scale. On the\\nother hand, with reinforcement learning (RL), a policy can be learned and\\nimproved automatically without any manual designs. However, current RL methods\\ngenerally do not work well on complex urban scenarios. In this paper, we\\npropose a framework to enable model-free deep reinforcement learning in\\nchallenging urban autonomous driving scenarios. We design a specific input\\nrepresentation and use visual encoding to capture the low-dimensional latent\\nstates. Several state-of-the-art model-free deep RL algorithms are implemented\\ninto our framework, with several tricks to improve their performance. We\\nevaluate our method in a challenging roundabout task with dense surrounding\\nvehicles in a high-definition driving simulator. The result shows that our\\nmethod can solve the task well and is significantly better than the baseline.',\n",
       "    'author': [{'name': 'Jianyu Chen'},\n",
       "     {'name': 'Bodi Yuan'},\n",
       "     {'name': 'Masayoshi Tomizuka'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 6 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.09503v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.09503v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.02843v1',\n",
       "    'updated': '2019-05-07T23:26:03Z',\n",
       "    'published': '2019-05-07T23:26:03Z',\n",
       "    'title': 'FANTrack: 3D Multi-Object Tracking with Feature Association Network',\n",
       "    'summary': 'We propose a data-driven approach to online multi-object tracking (MOT) that\\nuses a convolutional neural network (CNN) for data association in a\\ntracking-by-detection framework. The problem of multi-target tracking aims to\\nassign noisy detections to a-priori unknown and time-varying number of tracked\\nobjects across a sequence of frames. A majority of the existing solutions focus\\non either tediously designing cost functions or formulating the task of data\\nassociation as a complex optimization problem that can be solved effectively.\\nInstead, we exploit the power of deep learning to formulate the data\\nassociation problem as inference in a CNN. To this end, we propose to learn a\\nsimilarity function that combines cues from both image and spatial features of\\nobjects. Our solution learns to perform global assignments in 3D purely from\\ndata, handles noisy detections and a varying number of targets, and is easy to\\ntrain. We evaluate our approach on the challenging KITTI dataset and show\\ncompetitive results. Our code is available at\\nhttps://git.uwaterloo.ca/wise-lab/fantrack.',\n",
       "    'author': [{'name': 'Erkan Baser'},\n",
       "     {'name': 'Venkateshwaran Balasubramanian'},\n",
       "     {'name': 'Prarthana Bhattacharyya'},\n",
       "     {'name': 'Krzysztof Czarnecki'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 10 figures, IEEE Intelligent Vehicles Symposium (IV 19)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.02843v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.02843v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.12887v2',\n",
       "    'updated': '2019-10-22T06:33:45Z',\n",
       "    'published': '2019-05-30T07:18:33Z',\n",
       "    'title': 'Does computer vision matter for action?',\n",
       "    'summary': 'Computer vision produces representations of scene content. Much computer\\nvision research is predicated on the assumption that these intermediate\\nrepresentations are useful for action. Recent work at the intersection of\\nmachine learning and robotics calls this assumption into question by training\\nsensorimotor systems directly for the task at hand, from pixels to actions,\\nwith no explicit intermediate representations. Thus the central question of our\\nwork: Does computer vision matter for action? We probe this question and its\\noffshoots via immersive simulation, which allows us to conduct controlled\\nreproducible experiments at scale. We instrument immersive three-dimensional\\nenvironments to simulate challenges such as urban driving, off-road trail\\ntraversal, and battle. Our main finding is that computer vision does matter.\\nModels equipped with intermediate representations train faster, achieve higher\\ntask performance, and generalize better to previously unseen environments. A\\nvideo that summarizes the work and illustrates the results can be found at\\nhttps://youtu.be/4MfWa2yZ0Jc',\n",
       "    'author': [{'name': 'Brady Zhou'},\n",
       "     {'name': 'Philipp Krähenbühl'},\n",
       "     {'name': 'Vladlen Koltun'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1126/scirobotics.aaw6661'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1126/scirobotics.aaw6661',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1905.12887v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.12887v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in Science Robotics, 4(30), May 2019'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Science Robotics 22 May 2019: Vol. 4, Issue 30, eaaw6661'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.00208v2',\n",
       "    'updated': '2019-07-17T16:28:00Z',\n",
       "    'published': '2019-06-01T11:57:38Z',\n",
       "    'title': 'RGB and LiDAR fusion based 3D Semantic Segmentation for Autonomous\\n  Driving',\n",
       "    'summary': 'LiDAR has become a standard sensor for autonomous driving applications as\\nthey provide highly precise 3D point clouds. LiDAR is also robust for low-light\\nscenarios at night-time or due to shadows where the performance of cameras is\\ndegraded. LiDAR perception is gradually becoming mature for algorithms\\nincluding object detection and SLAM. However, semantic segmentation algorithm\\nremains to be relatively less explored. Motivated by the fact that semantic\\nsegmentation is a mature algorithm on image data, we explore sensor fusion\\nbased 3D segmentation. Our main contribution is to convert the RGB image to a\\npolar-grid mapping representation used for LiDAR and design early and mid-level\\nfusion architectures. Additionally, we design a hybrid fusion architecture that\\ncombines both fusion algorithms. We evaluate our algorithm on KITTI dataset\\nwhich provides segmentation annotation for cars, pedestrians and cyclists. We\\nevaluate two state-of-the-art architectures namely SqueezeSeg and PointSeg and\\nimprove the mIoU score by 10 % in both cases relative to the LiDAR only\\nbaseline.',\n",
       "    'author': [{'name': 'Khaled El Madawy'},\n",
       "     {'name': 'Hazem Rashed'},\n",
       "     {'name': 'Ahmad El Sallab'},\n",
       "     {'name': 'Omar Nasr'},\n",
       "     {'name': 'Hanan Kamel'},\n",
       "     {'name': 'Senthil Yogamani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for Oral Presentation at IEEE Intelligent Transportation\\n  Systems Conference (ITSC) 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.00208v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.00208v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.03173v2',\n",
       "    'updated': '2019-08-04T21:21:33Z',\n",
       "    'published': '2019-06-07T15:39:21Z',\n",
       "    'title': 'Ego-Pose Estimation and Forecasting as Real-Time PD Control',\n",
       "    'summary': 'We propose the use of a proportional-derivative (PD) control based policy\\nlearned via reinforcement learning (RL) to estimate and forecast 3D human pose\\nfrom egocentric videos. The method learns directly from unsegmented egocentric\\nvideos and motion capture data consisting of various complex human motions\\n(e.g., crouching, hopping, bending, and motion transitions). We propose a\\nvideo-conditioned recurrent control technique to forecast physically-valid and\\nstable future motions of arbitrary length. We also introduce a value function\\nbased fail-safe mechanism which enables our method to run as a single pass\\nalgorithm over the video data. Experiments with both controlled and in-the-wild\\ndata show that our approach outperforms previous art in both quantitative\\nmetrics and visual quality of the motions, and is also robust enough to\\ntransfer directly to real-world scenarios. Additionally, our time analysis\\nshows that the combined use of our pose estimation and forecasting can run at\\n30 FPS, making it suitable for real-time applications.',\n",
       "    'author': [{'name': 'Ye Yuan'}, {'name': 'Kris Kitani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICCV 2019; Webpage: https://www.ye-yuan.com/ego-pose; Video:\\n  https://youtu.be/968IIDZeWE0'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.03173v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.03173v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.08236v1',\n",
       "    'updated': '2019-06-19T17:35:43Z',\n",
       "    'published': '2019-06-19T17:35:43Z',\n",
       "    'title': 'PyRobot: An Open-source Robotics Framework for Research and Benchmarking',\n",
       "    'summary': 'This paper introduces PyRobot, an open-source robotics framework for research\\nand benchmarking. PyRobot is a light-weight, high-level interface on top of ROS\\nthat provides a consistent set of hardware independent mid-level APIs to\\ncontrol different robots. PyRobot abstracts away details about low-level\\ncontrollers and inter-process communication, and allows non-robotics\\nresearchers (ML, CV researchers) to focus on building high-level AI\\napplications. PyRobot aims to provide a research ecosystem with convenient\\naccess to robotics datasets, algorithm implementations and models that can be\\nused to quickly create a state-of-the-art baseline. We believe PyRobot, when\\npaired up with low-cost robot platforms such as LoCoBot, will reduce the entry\\nbarrier into robotics, and democratize robotics. PyRobot is open-source, and\\ncan be accessed via https://pyrobot.org.',\n",
       "    'author': [{'name': 'Adithyavairavan Murali'},\n",
       "     {'name': 'Tao Chen'},\n",
       "     {'name': 'Kalyan Vasudev Alwala'},\n",
       "     {'name': 'Dhiraj Gandhi'},\n",
       "     {'name': 'Lerrel Pinto'},\n",
       "     {'name': 'Saurabh Gupta'},\n",
       "     {'name': 'Abhinav Gupta'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.08236v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.08236v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.06988v1',\n",
       "    'updated': '2019-10-15T18:17:58Z',\n",
       "    'published': '2019-10-15T18:17:58Z',\n",
       "    'title': 'Autonomous Aerial Cinematography In Unstructured Environments With\\n  Learned Artistic Decision-Making',\n",
       "    'summary': 'Aerial cinematography is revolutionizing industries that require live and\\ndynamic camera viewpoints such as entertainment, sports, and security. However,\\nsafely piloting a drone while filming a moving target in the presence of\\nobstacles is immensely taxing, often requiring multiple expert human operators.\\nHence, there is demand for an autonomous cinematographer that can reason about\\nboth geometry and scene context in real-time. Existing approaches do not\\naddress all aspects of this problem; they either require high-precision\\nmotion-capture systems or GPS tags to localize targets, rely on prior maps of\\nthe environment, plan for short time horizons, or only follow artistic\\nguidelines specified before flight.\\n  In this work, we address the problem in its entirety and propose a complete\\nsystem for real-time aerial cinematography that for the first time combines:\\n(1) vision-based target estimation; (2) 3D signed-distance mapping for\\nocclusion estimation; (3) efficient trajectory optimization for long\\ntime-horizon camera motion; and (4) learning-based artistic shot selection. We\\nextensively evaluate our system both in simulation and in field experiments by\\nfilming dynamic targets moving through unstructured environments. Our results\\nindicate that our system can operate reliably in the real world without\\nrestrictive assumptions. We also provide in-depth analysis and discussions for\\neach module, with the hope that our design tradeoffs can generalize to other\\nrelated applications. Videos of the complete system can be found at:\\nhttps://youtu.be/ookhHnqmlaU.',\n",
       "    'author': [{'name': 'Rogerio Bonatti'},\n",
       "     {'name': 'Wenshan Wang'},\n",
       "     {'name': 'Cherie Ho'},\n",
       "     {'name': 'Aayush Ahuja'},\n",
       "     {'name': 'Mirko Gschwindt'},\n",
       "     {'name': 'Efe Camci'},\n",
       "     {'name': 'Erdal Kayacan'},\n",
       "     {'name': 'Sanjiban Choudhury'},\n",
       "     {'name': 'Sebastian Scherer'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1910.06988v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.06988v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.14442v3',\n",
       "    'updated': '2021-08-09T22:58:22Z',\n",
       "    'published': '2019-10-30T01:04:37Z',\n",
       "    'title': 'Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive\\n  Navigation in Cluttered Environments',\n",
       "    'summary': 'We present Interactive Gibson Benchmark, the first comprehensive benchmark\\nfor training and evaluating Interactive Navigation: robot navigation strategies\\nwhere physical interaction with objects is allowed and even encouraged to\\naccomplish a task. For example, the robot can move objects if needed in order\\nto clear a path leading to the goal location. Our benchmark comprises two novel\\nelements: 1) a new experimental setup, the Interactive Gibson Environment\\n(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high\\nfidelity physical dynamics of the robot and common objects found in these\\nscenes; 2) a set of Interactive Navigation metrics which allows one to study\\nthe interplay between navigation and physical interaction. We present and\\nevaluate multiple learning-based baselines in Interactive Gibson, and provide\\ninsights into regimes of navigation with different trade-offs between\\nnavigation path efficiency and disturbance of surrounding objects. We make our\\nbenchmark publicly\\navailable(https://sites.google.com/view/interactivegibsonenv) and encourage\\nresearchers from all disciplines in robotics (e.g. planning, learning, control)\\nto propose, evaluate, and compare their Interactive Navigation solutions in\\nInteractive Gibson.',\n",
       "    'author': [{'name': 'Fei Xia'},\n",
       "     {'name': 'William B. Shen'},\n",
       "     {'name': 'Chengshu Li'},\n",
       "     {'name': 'Priya Kasimbeg'},\n",
       "     {'name': 'Micael Tchapmi'},\n",
       "     {'name': 'Alexander Toshev'},\n",
       "     {'name': 'Li Fei-Fei'},\n",
       "     {'name': 'Roberto Martín-Martín'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2020.2965078'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2020.2965078',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1910.14442v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.14442v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 8 figures. Consider citing a newer version\\n  (https://arxiv.org/abs/2012.02924) if you are using iGibson'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters, Vol. 5, No. 2, April 2020'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.12294v1',\n",
       "    'updated': '2019-12-27T18:59:04Z',\n",
       "    'published': '2019-12-27T18:59:04Z',\n",
       "    'title': 'Learning by Cheating',\n",
       "    'summary': 'Vision-based urban driving is hard. The autonomous system needs to learn to\\nperceive the world and act in it. We show that this challenging learning\\nproblem can be simplified by decomposing it into two stages. We first train an\\nagent that has access to privileged information. This privileged agent cheats\\nby observing the ground-truth layout of the environment and the positions of\\nall traffic participants. In the second stage, the privileged agent acts as a\\nteacher that trains a purely vision-based sensorimotor agent. The resulting\\nsensorimotor agent does not have access to any privileged information and does\\nnot cheat. This two-stage training procedure is counter-intuitive at first, but\\nhas a number of important advantages that we analyze and empirically\\ndemonstrate. We use the presented approach to train a vision-based autonomous\\ndriving system that substantially outperforms the state of the art on the CARLA\\nbenchmark and the recent NoCrash benchmark. Our approach achieves, for the\\nfirst time, 100% success rate on all tasks in the original CARLA benchmark,\\nsets a new record on the NoCrash benchmark, and reduces the frequency of\\ninfractions by an order of magnitude compared to the prior state of the art.\\nFor the video that summarizes this work, see https://youtu.be/u9ZCxxD-UUw',\n",
       "    'author': [{'name': 'Dian Chen'},\n",
       "     {'name': 'Brady Zhou'},\n",
       "     {'name': 'Vladlen Koltun'},\n",
       "     {'name': 'Philipp Krähenbühl'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Paper published in CoRL2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1912.12294v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.12294v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.06082v4',\n",
       "    'updated': '2020-11-11T18:39:43Z',\n",
       "    'published': '2020-03-13T02:03:05Z',\n",
       "    'title': 'An Adversarial Objective for Scalable Exploration',\n",
       "    'summary': 'Model-based curiosity combines active learning approaches to optimal sampling\\nwith the information gain based incentives for exploration presented in the\\ncuriosity literature. Existing model-based curiosity methods look to\\napproximate prediction uncertainty with approaches which struggle to scale to\\nmany prediction-planning pipelines used in robotics tasks. We address these\\nscalability issues with an adversarial curiosity method minimizing a score\\ngiven by a discriminator network. This discriminator is optimized jointly with\\na prediction model and enables our active learning approach to sample sequences\\nof observations and actions which result in predictions considered the least\\nrealistic by the discriminator. We demonstrate progressively increasing\\nadvantages as compute is restricted of our adversarial curiosity approach over\\nleading model-based exploration strategies in simulated environments. We\\nfurther demonstrate the ability of our adversarial curiosity method to scale to\\na robotic manipulation prediction-planning pipeline where we improve sample\\nefficiency and prediction performance for a domain transfer problem.',\n",
       "    'author': [{'name': 'Bernadette Bucher'},\n",
       "     {'name': 'Karl Schmeckpeper'},\n",
       "     {'name': 'Nikolai Matni'},\n",
       "     {'name': 'Kostas Daniilidis'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Additional visualizations of our results are available on our website\\n  at https://sites.google.com/view/action-for-better-prediction . Bernadette\\n  Bucher and Karl Schmeckpeper contributed equally'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.06082v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.06082v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.04851v1',\n",
       "    'updated': '2020-04-09T23:20:30Z',\n",
       "    'published': '2020-04-09T23:20:30Z',\n",
       "    'title': 'Spatial Priming for Detecting Human-Object Interactions',\n",
       "    'summary': 'The relative spatial layout of a human and an object is an important cue for\\ndetermining how they interact. However, until now, spatial layout has been used\\njust as side-information for detecting human-object interactions (HOIs). In\\nthis paper, we present a method for exploiting this spatial layout information\\nfor detecting HOIs in images. The proposed method consists of a layout module\\nwhich primes a visual module to predict the type of interaction between a human\\nand an object. The visual and layout modules share information through lateral\\nconnections at several stages. The model uses predictions from the layout\\nmodule as a prior to the visual module and the prediction from the visual\\nmodule is given as the final output. It also incorporates semantic information\\nabout the object using word2vec vectors. The proposed model reaches an mAP of\\n24.79% for HICO-Det dataset which is about 2.8% absolute points higher than the\\ncurrent state-of-the-art.',\n",
       "    'author': [{'name': 'Ankan Bansal'},\n",
       "     {'name': 'Sai Saketh Rambhatla'},\n",
       "     {'name': 'Abhinav Shrivastava'},\n",
       "     {'name': 'Rama Chellappa'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2004.04851v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.04851v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.09141v2',\n",
       "    'updated': '2020-06-04T10:56:49Z',\n",
       "    'published': '2020-04-20T09:06:10Z',\n",
       "    'title': 'Spatial Action Maps for Mobile Manipulation',\n",
       "    'summary': 'Typical end-to-end formulations for learning robotic navigation involve\\npredicting a small set of steering command actions (e.g., step forward, turn\\nleft, turn right, etc.) from images of the current state (e.g., a bird\\'s-eye\\nview of a SLAM reconstruction). Instead, we show that it can be advantageous to\\nlearn with dense action representations defined in the same domain as the\\nstate. In this work, we present \"spatial action maps,\" in which the set of\\npossible actions is represented by a pixel map (aligned with the input image of\\nthe current state), where each pixel represents a local navigational endpoint\\nat the corresponding scene location. Using ConvNets to infer spatial action\\nmaps from state images, action predictions are thereby spatially anchored on\\nlocal visual features in the scene, enabling significantly faster learning of\\ncomplex behaviors for mobile manipulation tasks with reinforcement learning. In\\nour experiments, we task a robot with pushing objects to a goal location, and\\nfind that policies learned with spatial action maps achieve much better\\nperformance than traditional alternatives.',\n",
       "    'author': [{'name': 'Jimmy Wu'},\n",
       "     {'name': 'Xingyuan Sun'},\n",
       "     {'name': 'Andy Zeng'},\n",
       "     {'name': 'Shuran Song'},\n",
       "     {'name': 'Johnny Lee'},\n",
       "     {'name': 'Szymon Rusinkiewicz'},\n",
       "     {'name': 'Thomas Funkhouser'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.15607/RSS.2020.XVI.035'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.15607/RSS.2020.XVI.035',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2004.09141v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.09141v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear at Robotics: Science and Systems (RSS), 2020. Project page:\\n  https://spatial-action-maps.cs.princeton.edu'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.03301v1',\n",
       "    'updated': '2020-09-04T20:14:59Z',\n",
       "    'published': '2020-09-04T20:14:59Z',\n",
       "    'title': 'Sensors, Safety Models and A System-Level Approach to Safe and Scalable\\n  Automated Vehicles',\n",
       "    'summary': \"When considering the accuracy of sensors in an automated vehicle (AV), it is\\nnot sufficient to evaluate the performance of any given sensor in isolation.\\nRather, the performance of any individual sensor must be considered in the\\ncontext of the overall system design. Techniques like redundancy and different\\nsensing modalities can reduce the chances of a sensing failure. Additionally,\\nthe use of safety models is essential to understanding whether any particular\\nsensing failure is relevant. Only when the entire system design is taken into\\naccount can one properly understand the meaning of safety-relevant sensing\\nfailures in an AV. In this paper, we will consider what should actually\\nconstitute a sensing failure, how safety models play an important role in\\nmitigating potential failures, how a system-level approach to safety will\\ndeliver a safe and scalable AV, and what an acceptable sensing failure rate\\nshould be considering the full picture of an AV's architecture.\",\n",
       "    'author': {'name': 'Jack Weast'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2009.03301v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.03301v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.09093v1',\n",
       "    'updated': '2020-09-18T21:29:06Z',\n",
       "    'published': '2020-09-18T21:29:06Z',\n",
       "    'title': 'Holistic Grid Fusion Based Stop Line Estimation',\n",
       "    'summary': 'Intersection scenarios provide the most complex traffic situations in\\nAutonomous Driving and Driving Assistance Systems. Knowing where to stop in\\nadvance in an intersection is an essential parameter in controlling the\\nlongitudinal velocity of the vehicle. Most of the existing methods in\\nliterature solely use cameras to detect stop lines, which is typically not\\nsufficient in terms of detection range. To address this issue, we propose a\\nmethod that takes advantage of fused multi-sensory data including stereo camera\\nand lidar as input and utilizes a carefully designed convolutional neural\\nnetwork architecture to detect stop lines. Our experiments show that the\\nproposed approach can improve detection range compared to camera data alone,\\nworks under heavy occlusion without observing the ground markings explicitly,\\nis able to predict stop lines for all lanes and allows detection at a distance\\nup to 50 meters.',\n",
       "    'author': [{'name': 'Runsheng Xu'},\n",
       "     {'name': 'Faezeh Tafazzoli'},\n",
       "     {'name': 'Li Zhang'},\n",
       "     {'name': 'Timo Rehfeld'},\n",
       "     {'name': 'Gunther Krehl'},\n",
       "     {'name': 'Arunava Seal'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICPR48806.2021.9413070'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICPR48806.2021.9413070',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2009.09093v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.09093v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICPR2020'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2020 25th International Conference on Pattern Recognition (ICPR),\\n  Milan, Italy, 2021 pp. 8400-8407'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.09273v1',\n",
       "    'updated': '2020-10-19T07:35:51Z',\n",
       "    'published': '2020-10-19T07:35:51Z',\n",
       "    'title': 'DeepReflecs: Deep Learning for Automotive Object Classification with\\n  Radar Reflections',\n",
       "    'summary': 'This paper presents an novel object type classification method for automotive\\napplications which uses deep learning with radar reflections. The method\\nprovides object class information such as pedestrian, cyclist, car, or\\nnon-obstacle. The method is both powerful and efficient, by using a\\nlight-weight deep learning approach on reflection level radar data. It fills\\nthe gap between low-performant methods of handcrafted features and\\nhigh-performant methods with convolutional neural networks. The proposed\\nnetwork exploits the specific characteristics of radar reflection data: It\\nhandles unordered lists of arbitrary length as input and it combines both\\nextraction of local and global features. In experiments with real data the\\nproposed network outperforms existing methods of handcrafted or learned\\nfeatures. An ablation study analyzes the impact of the proposed global context\\nlayer.',\n",
       "    'author': [{'name': 'Michael Ulrich'},\n",
       "     {'name': 'Claudius Gläser'},\n",
       "     {'name': 'Fabian Timm'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'preprint, under review'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.09273v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.09273v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.09662v3',\n",
       "    'updated': '2021-09-10T19:02:03Z',\n",
       "    'published': '2020-10-19T16:57:24Z',\n",
       "    'title': 'Attention Augmented ConvLSTM for Environment Prediction',\n",
       "    'summary': \"Safe and proactive planning in robotic systems generally requires accurate\\npredictions of the environment. Prior work on environment prediction applied\\nvideo frame prediction techniques to bird's-eye view environment\\nrepresentations, such as occupancy grids. ConvLSTM-based frameworks used\\npreviously often result in significant blurring and vanishing of moving\\nobjects, thus hindering their applicability for use in safety-critical\\napplications. In this work, we propose two extensions to the ConvLSTM to\\naddress these issues. We present the Temporal Attention Augmented ConvLSTM\\n(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks\\nfor spatiotemporal occupancy prediction, and demonstrate improved performance\\nover baseline architectures on the real-world KITTI and Waymo datasets.\",\n",
       "    'author': [{'name': 'Bernard Lange'},\n",
       "     {'name': 'Masha Itkina'},\n",
       "     {'name': 'Mykel J. Kochenderfer'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to be published on 2021 International Conference on\\n  Intelligent Robots and Systems (IROS)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.09662v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.09662v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.10', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.01975v1',\n",
       "    'updated': '2020-11-03T19:42:32Z',\n",
       "    'published': '2020-11-03T19:42:32Z',\n",
       "    'title': 'Rearrangement: A Challenge for Embodied AI',\n",
       "    'summary': 'We describe a framework for research and evaluation in Embodied AI. Our\\nproposal is based on a canonical task: Rearrangement. A standard task can focus\\nthe development of new techniques and serve as a source of trained models that\\ncan be transferred to other settings. In the rearrangement task, the goal is to\\nbring a given physical environment into a specified state. The goal state can\\nbe specified by object poses, by images, by a description in language, or by\\nletting the agent experience the environment in the goal state. We characterize\\nrearrangement scenarios along different axes and describe metrics for\\nbenchmarking rearrangement performance. To facilitate research and exploration,\\nwe present experimental testbeds of rearrangement scenarios in four different\\nsimulation environments. We anticipate that other datasets will be released and\\nnew simulation platforms will be built to support training of rearrangement\\nagents and their deployment on physical systems.',\n",
       "    'author': [{'name': 'Dhruv Batra'},\n",
       "     {'name': 'Angel X. Chang'},\n",
       "     {'name': 'Sonia Chernova'},\n",
       "     {'name': 'Andrew J. Davison'},\n",
       "     {'name': 'Jia Deng'},\n",
       "     {'name': 'Vladlen Koltun'},\n",
       "     {'name': 'Sergey Levine'},\n",
       "     {'name': 'Jitendra Malik'},\n",
       "     {'name': 'Igor Mordatch'},\n",
       "     {'name': 'Roozbeh Mottaghi'},\n",
       "     {'name': 'Manolis Savva'},\n",
       "     {'name': 'Hao Su'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Authors are listed in alphabetical order'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.01975v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.01975v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.07613v1',\n",
       "    'updated': '2020-11-15T19:37:24Z',\n",
       "    'published': '2020-11-15T19:37:24Z',\n",
       "    'title': \"BirdSLAM: Monocular Multibody SLAM in Bird's-Eye View\",\n",
       "    'summary': \"In this paper, we present BirdSLAM, a novel simultaneous localization and\\nmapping (SLAM) system for the challenging scenario of autonomous driving\\nplatforms equipped with only a monocular camera. BirdSLAM tackles challenges\\nfaced by other monocular SLAM systems (such as scale ambiguity in monocular\\nreconstruction, dynamic object localization, and uncertainty in feature\\nrepresentation) by using an orthographic (bird's-eye) view as the configuration\\nspace in which localization and mapping are performed. By assuming only the\\nheight of the ego-camera above the ground, BirdSLAM leverages single-view\\nmetrology cues to accurately localize the ego-vehicle and all other traffic\\nparticipants in bird's-eye view. We demonstrate that our system outperforms\\nprior work that uses strictly greater information, and highlight the relevance\\nof each design decision via an ablation analysis.\",\n",
       "    'author': [{'name': 'Swapnil Daga'},\n",
       "     {'name': 'Gokul B. Nair'},\n",
       "     {'name': 'Anirudha Ramesh'},\n",
       "     {'name': 'Rahul Sajnani'},\n",
       "     {'name': 'Junaid Ahmed Ansari'},\n",
       "     {'name': 'K. Madhava Krishna'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted in VISIGRAPP (VISAPP) 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.07613v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.07613v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.12149v2',\n",
       "    'updated': '2021-04-09T16:42:19Z',\n",
       "    'published': '2020-11-24T15:00:56Z',\n",
       "    'title': 'SpinNet: Learning a General Surface Descriptor for 3D Point Cloud\\n  Registration',\n",
       "    'summary': 'Extracting robust and general 3D local features is key to downstream tasks\\nsuch as point cloud registration and reconstruction. Existing learning-based\\nlocal descriptors are either sensitive to rotation transformations, or rely on\\nclassical handcrafted features which are neither general nor representative. In\\nthis paper, we introduce a new, yet conceptually simple, neural architecture,\\ntermed SpinNet, to extract local features which are rotationally invariant\\nwhilst sufficiently informative to enable accurate registration. A Spatial\\nPoint Transformer is first introduced to map the input local surface into a\\ncarefully designed cylindrical space, enabling end-to-end optimization with\\nSO(2) equivariant representation. A Neural Feature Extractor which leverages\\nthe powerful point-based and 3D cylindrical convolutional neural layers is then\\nutilized to derive a compact and representative descriptor for matching.\\nExtensive experiments on both indoor and outdoor datasets demonstrate that\\nSpinNet outperforms existing state-of-the-art techniques by a large margin.\\nMore critically, it has the best generalization ability across unseen scenarios\\nwith different sensor modalities. The code is available at\\nhttps://github.com/QingyongHu/SpinNet.',\n",
       "    'author': [{'name': 'Sheng Ao'},\n",
       "     {'name': 'Qingyong Hu'},\n",
       "     {'name': 'Bo Yang'},\n",
       "     {'name': 'Andrew Markham'},\n",
       "     {'name': 'Yulan Guo'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.12149v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.12149v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.06117v1',\n",
       "    'updated': '2020-12-11T04:28:48Z',\n",
       "    'published': '2020-12-11T04:28:48Z',\n",
       "    'title': 'How to Train PointGoal Navigation Agents on a (Sample and Compute)\\n  Budget',\n",
       "    'summary': \"PointGoal navigation has seen significant recent interest and progress,\\nspurred on by the Habitat platform and associated challenge. In this paper, we\\nstudy PointGoal navigation under both a sample budget (75 million frames) and a\\ncompute budget (1 GPU for 1 day). We conduct an extensive set of experiments,\\ncumulatively totaling over 50,000 GPU-hours, that let us identify and discuss a\\nnumber of ostensibly minor but significant design choices -- the advantage\\nestimation procedure (a key component in training), visual encoder\\narchitecture, and a seemingly minor hyper-parameter change. Overall, these\\ndesign choices to lead considerable and consistent improvements over the\\nbaselines present in Savva et al. Under a sample budget, performance for RGB-D\\nagents improves 8 SPL on Gibson (14% relative improvement) and 20 SPL on\\nMatterport3D (38% relative improvement). Under a compute budget, performance\\nfor RGB-D agents improves by 19 SPL on Gibson (32% relative improvement) and 35\\nSPL on Matterport3D (220% relative improvement). We hope our findings and\\nrecommendations will make serve to make the community's experiments more\\nefficient.\",\n",
       "    'author': [{'name': 'Erik Wijmans'},\n",
       "     {'name': 'Irfan Essa'},\n",
       "     {'name': 'Dhruv Batra'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.06117v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.06117v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.14359v1',\n",
       "    'updated': '2020-12-28T16:55:19Z',\n",
       "    'published': '2020-12-28T16:55:19Z',\n",
       "    'title': 'Commonsense Visual Sensemaking for Autonomous Driving: On Generalised\\n  Neurosymbolic Online Abduction Integrating Vision and Semantics',\n",
       "    'summary': 'We demonstrate the need and potential of systematically integrated vision and\\nsemantics solutions for visual sensemaking in the backdrop of autonomous\\ndriving. A general neurosymbolic method for online visual sensemaking using\\nanswer set programming (ASP) is systematically formalised and fully\\nimplemented. The method integrates state of the art in visual computing, and is\\ndeveloped as a modular framework that is generally usable within hybrid\\narchitectures for realtime perception and control. We evaluate and demonstrate\\nwith community established benchmarks KITTIMOD, MOT-2017, and MOT-2020. As\\nuse-case, we focus on the significance of human-centred visual sensemaking --\\ne.g., involving semantic representation and explainability, question-answering,\\ncommonsense interpolation -- in safety-critical autonomous driving situations.\\nThe developed neurosymbolic framework is domain-independent, with the case of\\nautonomous driving designed to serve as an exemplar for online visual\\nsensemaking in diverse cognitive interaction settings in the backdrop of select\\nhuman-centred AI technology design considerations.\\n  Keywords: Cognitive Vision, Deep Semantics, Declarative Spatial Reasoning,\\nKnowledge Representation and Reasoning, Commonsense Reasoning, Visual\\nAbduction, Answer Set Programming, Autonomous Driving, Human-Centred Computing\\nand Design, Standardisation in Driving Technology, Spatial Cognition and AI.',\n",
       "    'author': [{'name': 'Jakob Suchan'},\n",
       "     {'name': 'Mehul Bhatt'},\n",
       "     {'name': 'Srikrishna Varadarajan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This is a preprint / review version of an accepted contribution to be\\n  published as part of the Artificial Intelligence Journal (AIJ).? The article\\n  is an extended version of an IJCAI 2019 publication [74, arXiv:1906.00107]'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.14359v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.14359v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.15373v1',\n",
       "    'updated': '2020-12-30T23:59:09Z',\n",
       "    'published': '2020-12-30T23:59:09Z',\n",
       "    'title': 'Model-Based Visual Planning with Self-Supervised Functional Distances',\n",
       "    'summary': 'A generalist robot must be able to complete a variety of tasks in its\\nenvironment. One appealing way to specify each task is in terms of a goal\\nobservation. However, learning goal-reaching policies with reinforcement\\nlearning remains a challenging problem, particularly when hand-engineered\\nreward functions are not available. Learned dynamics models are a promising\\napproach for learning about the environment without rewards or task-directed\\ndata, but planning to reach goals with such a model requires a notion of\\nfunctional similarity between observations and goal states. We present a\\nself-supervised method for model-based visual goal reaching, which uses both a\\nvisual dynamics model as well as a dynamical distance function learned using\\nmodel-free reinforcement learning. Our approach learns entirely using offline,\\nunlabeled data, making it practical to scale to large and diverse datasets. In\\nour experiments, we find that our method can successfully learn models that\\nperform a variety of tasks at test-time, moving objects amid distractors with a\\nsimulated robotic arm and even learning to open and close a drawer using a\\nreal-world robot. In comparisons, we find that this approach substantially\\noutperforms both model-free and model-based prior methods. Videos and\\nvisualizations are available here: http://sites.google.com/berkeley.edu/mbold.',\n",
       "    'author': [{'name': 'Stephen Tian'},\n",
       "     {'name': 'Suraj Nair'},\n",
       "     {'name': 'Frederik Ebert'},\n",
       "     {'name': 'Sudeep Dasari'},\n",
       "     {'name': 'Benjamin Eysenbach'},\n",
       "     {'name': 'Chelsea Finn'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.15373v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.15373v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.02722v1',\n",
       "    'updated': '2021-01-07T19:03:34Z',\n",
       "    'published': '2021-01-07T19:03:34Z',\n",
       "    'title': 'The Distracting Control Suite -- A Challenging Benchmark for\\n  Reinforcement Learning from Pixels',\n",
       "    'summary': 'Robots have to face challenging perceptual settings, including changes in\\nviewpoint, lighting, and background. Current simulated reinforcement learning\\n(RL) benchmarks such as DM Control provide visual input without such\\ncomplexity, which limits the transfer of well-performing methods to the real\\nworld. In this paper, we extend DM Control with three kinds of visual\\ndistractions (variations in background, color, and camera pose) to produce a\\nnew challenging benchmark for vision-based control, and we analyze state of the\\nart RL algorithms in these settings. Our experiments show that current RL\\nmethods for vision-based control perform poorly under distractions, and that\\ntheir performance decreases with increasing distraction complexity, showing\\nthat new methods are needed to cope with the visual complexities of the real\\nworld. We also find that combinations of multiple distraction types are more\\ndifficult than a mere combination of their individual effects.',\n",
       "    'author': [{'name': 'Austin Stone'},\n",
       "     {'name': 'Oscar Ramirez'},\n",
       "     {'name': 'Kurt Konolige'},\n",
       "     {'name': 'Rico Jonschkowski'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Code available at\\n  https://github.com/google-research/google-research/tree/master/distracting_control'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.02722v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.02722v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.04882v1',\n",
       "    'updated': '2021-01-13T05:20:20Z',\n",
       "    'published': '2021-01-13T05:20:20Z',\n",
       "    'title': 'Asymmetric self-play for automatic goal discovery in robotic\\n  manipulation',\n",
       "    'summary': \"We train a single, goal-conditioned policy that can solve many robotic\\nmanipulation tasks, including tasks with previously unseen goals and objects.\\nWe rely on asymmetric self-play for goal discovery, where two agents, Alice and\\nBob, play a game. Alice is asked to propose challenging goals and Bob aims to\\nsolve them. We show that this method can discover highly diverse and complex\\ngoals without any human priors. Bob can be trained with only sparse rewards,\\nbecause the interaction between Alice and Bob results in a natural curriculum\\nand Bob can learn from Alice's trajectory when relabeled as a goal-conditioned\\ndemonstration. Finally, our method scales, resulting in a single policy that\\ncan generalize to many unseen tasks such as setting a table, stacking blocks,\\nand solving simple puzzles. Videos of a learned policy is available at\\nhttps://robotics-self-play.github.io.\",\n",
       "    'author': [{'name': 'OpenAI OpenAI'},\n",
       "     {'name': 'Matthias Plappert'},\n",
       "     {'name': 'Raul Sampedro'},\n",
       "     {'name': 'Tao Xu'},\n",
       "     {'name': 'Ilge Akkaya'},\n",
       "     {'name': 'Vineet Kosaraju'},\n",
       "     {'name': 'Peter Welinder'},\n",
       "     {'name': \"Ruben D'Sa\"},\n",
       "     {'name': 'Arthur Petron'},\n",
       "     {'name': 'Henrique P. d. O. Pinto'},\n",
       "     {'name': 'Alex Paino'},\n",
       "     {'name': 'Hyeonwoo Noh'},\n",
       "     {'name': 'Lilian Weng'},\n",
       "     {'name': 'Qiming Yuan'},\n",
       "     {'name': 'Casey Chu'},\n",
       "     {'name': 'Wojciech Zaremba'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Videos are shown at https://robotics-self-play.github.io'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.04882v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.04882v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.06806v1',\n",
       "    'updated': '2021-01-18T00:09:30Z',\n",
       "    'published': '2021-01-18T00:09:30Z',\n",
       "    'title': 'MP3: A Unified Model to Map, Perceive, Predict and Plan',\n",
       "    'summary': 'High-definition maps (HD maps) are a key component of most modern\\nself-driving systems due to their valuable semantic and geometric information.\\nUnfortunately, building HD maps has proven hard to scale due to their cost as\\nwell as the requirements they impose in the localization system that has to\\nwork everywhere with centimeter-level accuracy. Being able to drive without an\\nHD map would be very beneficial to scale self-driving solutions as well as to\\nincrease the failure tolerance of existing ones (e.g., if localization fails or\\nthe map is not up-to-date). Towards this goal, we propose MP3, an end-to-end\\napproach to mapless driving where the input is raw sensor data and a high-level\\ncommand (e.g., turn left at the intersection). MP3 predicts intermediate\\nrepresentations in the form of an online map and the current and future state\\nof dynamic agents, and exploits them in a novel neural motion planner to make\\ninterpretable decisions taking into account uncertainty. We show that our\\napproach is significantly safer, more comfortable, and can follow commands\\nbetter than the baselines in challenging long-term closed-loop simulations, as\\nwell as when compared to an expert driver in a large-scale real-world dataset.',\n",
       "    'author': [{'name': 'Sergio Casas'},\n",
       "     {'name': 'Abbas Sadat'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.06806v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.06806v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.06832v2',\n",
       "    'updated': '2021-04-29T06:26:56Z',\n",
       "    'published': '2021-01-18T01:43:36Z',\n",
       "    'title': 'Deep Structured Reactive Planning',\n",
       "    'summary': 'An intelligent agent operating in the real-world must balance achieving its\\ngoal with maintaining the safety and comfort of not only itself, but also other\\nparticipants within the surrounding scene. This requires jointly reasoning\\nabout the behavior of other actors while deciding its own actions as these two\\nprocesses are inherently intertwined - a vehicle will yield to us if we decide\\nto proceed first at the intersection but will proceed first if we decide to\\nyield. However, this is not captured in most self-driving pipelines, where\\nplanning follows prediction. In this paper we propose a novel data-driven,\\nreactive planning objective which allows a self-driving vehicle to jointly\\nreason about its own plans as well as how other actors will react to them. We\\nformulate the problem as an energy-based deep structured model that is learned\\nfrom observational data and encodes both the planning and prediction problems.\\nThrough simulations based on both real-world driving and synthetically\\ngenerated dense traffic, we demonstrate that our reactive model outperforms a\\nnon-reactive variant in successfully completing highly complex maneuvers (lane\\nmerges/turns in traffic) faster, without trading off collision rate.',\n",
       "    'author': [{'name': 'Jerry Liu'},\n",
       "     {'name': 'Wenyuan Zeng'},\n",
       "     {'name': 'Raquel Urtasun'},\n",
       "     {'name': 'Ersin Yumer'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.06832v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.06832v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.04353v5',\n",
       "    'updated': '2021-10-02T01:56:30Z',\n",
       "    'published': '2021-02-08T17:00:26Z',\n",
       "    'title': 'Unlocking Pixels for Reinforcement Learning via Implicit Attention',\n",
       "    'summary': 'There has recently been significant interest in training reinforcement\\nlearning (RL) agents in vision-based environments. This poses many challenges,\\nsuch as high dimensionality and the potential for observational overfitting\\nthrough spurious correlations. A promising approach to solve both of these\\nproblems is an attention bottleneck, which provides a simple and effective\\nframework for learning high performing policies, even in the presence of\\ndistractions. However, due to poor scalability of attention architectures,\\nthese methods cannot be applied beyond low resolution visual inputs, using\\nlarge patches (thus small attention matrices). In this paper we make use of new\\nefficient attention algorithms, recently shown to be highly effective for\\nTransformers, and demonstrate that these techniques can be successfully adopted\\nfor the RL setting. This allows our attention-based controllers to scale to\\nlarger visual inputs, and facilitate the use of smaller patches, even\\nindividual pixels, improving generalization. We show this on a range of tasks\\nfrom the Distracting Control Suite to vision-based quadruped robots locomotion.\\nWe provide rigorous theoretical analysis of the proposed algorithm.',\n",
       "    'author': [{'name': 'Krzysztof Marcin Choromanski'},\n",
       "     {'name': 'Deepali Jain'},\n",
       "     {'name': 'Wenhao Yu'},\n",
       "     {'name': 'Xingyou Song'},\n",
       "     {'name': 'Jack Parker-Holder'},\n",
       "     {'name': 'Tingnan Zhang'},\n",
       "     {'name': 'Valerii Likhosherstov'},\n",
       "     {'name': 'Aldo Pacchiano'},\n",
       "     {'name': 'Anirban Santara'},\n",
       "     {'name': 'Yunhao Tang'},\n",
       "     {'name': 'Jie Tan'},\n",
       "     {'name': 'Adrian Weller'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.04353v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.04353v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.08094v1',\n",
       "    'updated': '2021-02-16T11:29:09Z',\n",
       "    'published': '2021-02-16T11:29:09Z',\n",
       "    'title': 'Composing Pick-and-Place Tasks By Grounding Language',\n",
       "    'summary': 'Controlling robots to perform tasks via natural language is one of the most\\nchallenging topics in human-robot interaction. In this work, we present a robot\\nsystem that follows unconstrained language instructions to pick and place\\narbitrary objects and effectively resolves ambiguities through dialogues. Our\\napproach infers objects and their relationships from input images and language\\nexpressions and can place objects in accordance with the spatial relations\\nexpressed by the user. Unlike previous approaches, we consider grounding not\\nonly for the picking but also for the placement of everyday objects from\\nlanguage. Specifically, by grounding objects and their spatial relations, we\\nallow specification of complex placement instructions, e.g. \"place it behind\\nthe middle red bowl\". Our results obtained using a real-world PR2 robot\\ndemonstrate the effectiveness of our method in understanding pick-and-place\\nlanguage instructions and sequentially composing them to solve tabletop\\nmanipulation tasks. Videos are available at\\nhttp://speechrobot.cs.uni-freiburg.de',\n",
       "    'author': [{'name': 'Oier Mees'}, {'name': 'Wolfram Burgard'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at the International Symposium on Experimental Robotics\\n  (ISER) 2020. Videos at http://speechrobot.cs.uni-freiburg.de'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.08094v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.08094v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.14231v1',\n",
       "    'updated': '2021-03-26T02:42:33Z',\n",
       "    'published': '2021-03-26T02:42:33Z',\n",
       "    'title': 'Congestion-aware Multi-agent Trajectory Prediction for Collision\\n  Avoidance',\n",
       "    'summary': 'Predicting agents\\' future trajectories plays a crucial role in modern AI\\nsystems, yet it is challenging due to intricate interactions exhibited in\\nmulti-agent systems, especially when it comes to collision avoidance. To\\naddress this challenge, we propose to learn congestion patterns as contextual\\ncues explicitly and devise a novel \"Sense--Learn--Reason--Predict\" framework by\\nexploiting advantages of three different doctrines of thought, which yields the\\nfollowing desirable benefits: (i) Representing congestion as contextual cues\\nvia latent factors subsumes the concept of social force commonly used in\\nphysics-based approaches and implicitly encodes the distance as a cost, similar\\nto the way a planning-based method models the environment. (ii) By decomposing\\nthe learning phases into two stages, a \"student\" can learn contextual cues from\\na \"teacher\" while generating collision-free trajectories. To make the framework\\ncomputationally tractable, we formulate it as an optimization problem and\\nderive an upper bound by leveraging the variational parametrization. In\\nexperiments, we demonstrate that the proposed model is able to generate\\ncollision-free trajectory predictions in a synthetic dataset designed for\\ncollision avoidance evaluation and remains competitive on the commonly used\\nNGSIM US-101 highway dataset.',\n",
       "    'author': [{'name': 'Xu Xie'},\n",
       "     {'name': 'Chi Zhang'},\n",
       "     {'name': 'Yixin Zhu'},\n",
       "     {'name': 'Ying Nian Wu'},\n",
       "     {'name': 'Song-Chun Zhu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2021 paper. Project:\\n  https://xuxie1031.github.io/projects/GTA/GTAProj.html'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.14231v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.14231v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.02646v1',\n",
       "    'updated': '2021-04-06T16:32:01Z',\n",
       "    'published': '2021-04-06T16:32:01Z',\n",
       "    'title': 'gradSim: Differentiable simulation for system identification and\\n  visuomotor control',\n",
       "    'summary': \"We consider the problem of estimating an object's physical properties such as\\nmass, friction, and elasticity directly from video sequences. Such a system\\nidentification problem is fundamentally ill-posed due to the loss of\\ninformation during image formation. Current solutions require precise 3D labels\\nwhich are labor-intensive to gather, and infeasible to create for many systems\\nsuch as deformable solids or cloth. We present gradSim, a framework that\\novercomes the dependence on 3D supervision by leveraging differentiable\\nmultiphysics simulation and differentiable rendering to jointly model the\\nevolution of scene dynamics and image formation. This novel combination enables\\nbackpropagation from pixels in a video sequence through to the underlying\\nphysical attributes that generated them. Moreover, our unified computation\\ngraph -- spanning from the dynamics and through the rendering process --\\nenables learning in challenging visuomotor control tasks, without relying on\\nstate-based (3D) supervision, while obtaining performance competitive to or\\nbetter than techniques that rely on precise 3D labels.\",\n",
       "    'author': [{'name': 'Krishna Murthy Jatavallabhula'},\n",
       "     {'name': 'Miles Macklin'},\n",
       "     {'name': 'Florian Golemo'},\n",
       "     {'name': 'Vikram Voleti'},\n",
       "     {'name': 'Linda Petrini'},\n",
       "     {'name': 'Martin Weiss'},\n",
       "     {'name': 'Breandan Considine'},\n",
       "     {'name': 'Jerome Parent-Levesque'},\n",
       "     {'name': 'Kevin Xie'},\n",
       "     {'name': 'Kenny Erleben'},\n",
       "     {'name': 'Liam Paull'},\n",
       "     {'name': 'Florian Shkurti'},\n",
       "     {'name': 'Derek Nowrouzezahrai'},\n",
       "     {'name': 'Sanja Fidler'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICLR 2021. Project page (and a dynamic web version of the article):\\n  https://gradsim.github.io'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.02646v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.02646v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.11213v1',\n",
       "    'updated': '2021-04-22T17:49:04Z',\n",
       "    'published': '2021-04-22T17:49:04Z',\n",
       "    'title': 'ManipulaTHOR: A Framework for Visual Object Manipulation',\n",
       "    'summary': 'The domain of Embodied AI has recently witnessed substantial progress,\\nparticularly in navigating agents within their environments. These early\\nsuccesses have laid the building blocks for the community to tackle tasks that\\nrequire agents to actively interact with objects in their environment. Object\\nmanipulation is an established research domain within the robotics community\\nand poses several challenges including manipulator motion, grasping and\\nlong-horizon planning, particularly when dealing with oft-overlooked practical\\nsetups involving visually rich and complex scenes, manipulation using mobile\\nagents (as opposed to tabletop manipulation), and generalization to unseen\\nenvironments and objects. We propose a framework for object manipulation built\\nupon the physics-enabled, visually rich AI2-THOR framework and present a new\\nchallenge to the Embodied AI community known as ArmPointNav. This task extends\\nthe popular point navigation task to object manipulation and offers new\\nchallenges including 3D obstacle avoidance, manipulating objects in the\\npresence of occlusion, and multi-object manipulation that necessitates long\\nterm planning. Popular learning paradigms that are successful on PointNav\\nchallenges show promise, but leave a large room for improvement.',\n",
       "    'author': [{'name': 'Kiana Ehsani'},\n",
       "     {'name': 'Winson Han'},\n",
       "     {'name': 'Alvaro Herrasti'},\n",
       "     {'name': 'Eli VanderBilt'},\n",
       "     {'name': 'Luca Weihs'},\n",
       "     {'name': 'Eric Kolve'},\n",
       "     {'name': 'Aniruddha Kembhavi'},\n",
       "     {'name': 'Roozbeh Mottaghi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2021 -- (Oral presentation)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.11213v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.11213v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.09371v2',\n",
       "    'updated': '2021-10-08T14:30:36Z',\n",
       "    'published': '2021-05-19T19:25:23Z',\n",
       "    'title': 'VOILA: Visual-Observation-Only Imitation Learning for Autonomous\\n  Navigation',\n",
       "    'summary': 'While imitation learning for vision based autonomous mobile robot navigation\\nhas recently received a great deal of attention in the research community,\\nexisting approaches typically require state action demonstrations that were\\ngathered using the deployment platform. However, what if one cannot easily\\noutfit their platform to record these demonstration signals or worse yet the\\ndemonstrator does not have access to the platform at all? Is imitation learning\\nfor vision based autonomous navigation even possible in such scenarios? In this\\nwork, we hypothesize that the answer is yes and that recent ideas from the\\nImitation from Observation (IfO) literature can be brought to bear such that a\\nrobot can learn to navigate using only ego centric video collected by a\\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\\nintroduce a new algorithm, Visual Observation only Imitation Learning for\\nAutonomous navigation (VOILA), that can successfully learn navigation policies\\nfrom a single video demonstration collected from a physically different agent.\\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\\nnot only successfully imitates the expert, but that it also learns navigation\\npolicies that can generalize to novel environments. Further, we demonstrate the\\neffectiveness of VOILA in a real world setting by showing that it allows a\\nwheeled Jackal robot to successfully imitate a human walking in an environment\\nusing a video recorded using a mobile phone camera.',\n",
       "    'author': [{'name': 'Haresh Karnan'},\n",
       "     {'name': 'Garrett Warnell'},\n",
       "     {'name': 'Xuesu Xiao'},\n",
       "     {'name': 'Peter Stone'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Under Submission to ICRA+RAL 2022'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.09371v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.09371v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.02930v1',\n",
       "    'updated': '2021-06-05T16:51:54Z',\n",
       "    'published': '2021-06-05T16:51:54Z',\n",
       "    'title': 'Spectral Temporal Graph Neural Network for Trajectory Prediction',\n",
       "    'summary': 'An effective understanding of the contextual environment and accurate motion\\nforecasting of surrounding agents is crucial for the development of autonomous\\nvehicles and social mobile robots. This task is challenging since the behavior\\nof an autonomous agent is not only affected by its own intention, but also by\\nthe static environment and surrounding dynamically interacting agents. Previous\\nworks focused on utilizing the spatial and temporal information in time domain\\nwhile not sufficiently taking advantage of the cues in frequency domain. To\\nthis end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which\\ncan capture inter-agent correlations and temporal dependency simultaneously in\\nfrequency domain in addition to time domain. SpecTGNN operates on both an agent\\ngraph with dynamic state information and an environment graph with the features\\nextracted from context images in two streams. The model integrates graph\\nFourier transform, spectral graph convolution and temporal gated convolution to\\nencode history information and forecast future trajectories. Moreover, we\\nincorporate a multi-head spatio-temporal attention mechanism to mitigate the\\neffect of error propagation in a long time horizon. We demonstrate the\\nperformance of SpecTGNN on two public trajectory prediction benchmark datasets,\\nwhich achieves state-of-the-art performance in terms of prediction accuracy.',\n",
       "    'author': [{'name': 'Defu Cao'},\n",
       "     {'name': 'Jiachen Li'},\n",
       "     {'name': 'Hengbo Ma'},\n",
       "     {'name': 'Masayoshi Tomizuka'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.02930v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.02930v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.03911v3',\n",
       "    'updated': '2021-12-13T12:40:16Z',\n",
       "    'published': '2021-06-07T18:45:07Z',\n",
       "    'title': 'XIRL: Cross-embodiment Inverse Reinforcement Learning',\n",
       "    'summary': 'We investigate the visual cross-embodiment imitation setting, in which agents\\nlearn policies from videos of other agents (such as humans) demonstrating the\\nsame task, but with stark differences in their embodiments -- shape, actions,\\nend-effector dynamics, etc. In this work, we demonstrate that it is possible to\\nautomatically discover and learn vision-based reward functions from\\ncross-embodiment demonstration videos that are robust to these differences.\\nSpecifically, we present a self-supervised method for Cross-embodiment Inverse\\nReinforcement Learning (XIRL) that leverages temporal cycle-consistency\\nconstraints to learn deep visual embeddings that capture task progression from\\noffline videos of demonstrations across multiple expert agents, each performing\\nthe same task differently due to embodiment differences. Prior to our work,\\nproducing rewards from self-supervised embeddings typically required alignment\\nwith a reference trajectory, which may be difficult to acquire under stark\\nembodiment differences. We show empirically that if the embeddings are aware of\\ntask progress, simply taking the negative distance between the current state\\nand goal state in the learned embedding space is useful as a reward for\\ntraining policies with reinforcement learning. We find our learned reward\\nfunction not only works for embodiments seen during training, but also\\ngeneralizes to entirely new embodiments. Additionally, when transferring\\nreal-world human demonstrations to a simulated robot, we find that XIRL is more\\nsample efficient than current best methods. Qualitative results, code, and\\ndatasets are available at https://x-irl.github.io',\n",
       "    'author': [{'name': 'Kevin Zakka'},\n",
       "     {'name': 'Andy Zeng'},\n",
       "     {'name': 'Pete Florence'},\n",
       "     {'name': 'Jonathan Tompson'},\n",
       "     {'name': 'Jeannette Bohg'},\n",
       "     {'name': 'Debidatta Dwibedi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Oral Accept, CoRL '21\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.03911v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.03911v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.05969v3',\n",
       "    'updated': '2022-10-29T03:02:39Z',\n",
       "    'published': '2021-06-10T17:59:50Z',\n",
       "    'title': 'Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation',\n",
       "    'summary': 'We propose a method for object-aware 3D egocentric pose estimation that\\ntightly integrates kinematics modeling, dynamics modeling, and scene object\\ninformation. Unlike prior kinematics or dynamics-based approaches where the two\\ncomponents are used disjointly, we synergize the two approaches via\\ndynamics-regulated training. At each timestep, a kinematic model is used to\\nprovide a target pose using video evidence and simulation state. Then, a\\nprelearned dynamics model attempts to mimic the kinematic pose in a physics\\nsimulator. By comparing the pose instructed by the kinematic model against the\\npose generated by the dynamics model, we can use their misalignment to further\\nimprove the kinematic model. By factoring in the 6DoF pose of objects (e.g.,\\nchairs, boxes) in the scene, we demonstrate for the first time, the ability to\\nestimate physically-plausible 3D human-object interactions using a single\\nwearable camera. We evaluate our egocentric pose estimation method in both\\ncontrolled laboratory settings and real-world scenarios.',\n",
       "    'author': [{'name': 'Zhengyi Luo'},\n",
       "     {'name': 'Ryo Hachiuma'},\n",
       "     {'name': 'Ye Yuan'},\n",
       "     {'name': 'Kris Kitani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2021. Project page:\\n  https://zhengyiluo.github.io/projects/kin_poly/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.05969v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.05969v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.09678v1',\n",
       "    'updated': '2021-06-17T17:28:18Z',\n",
       "    'published': '2021-06-17T17:28:18Z',\n",
       "    'title': 'SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual\\n  Policies',\n",
       "    'summary': 'Generalization has been a long-standing challenge for reinforcement learning\\n(RL). Visual RL, in particular, can be easily distracted by irrelevant factors\\nin high-dimensional observation space. In this work, we consider robust policy\\nlearning which targets zero-shot generalization to unseen visual environments\\nwith large distributional shift. We propose SECANT, a novel self-expert cloning\\ntechnique that leverages image augmentation in two stages to decouple robust\\nrepresentation learning from policy optimization. Specifically, an expert\\npolicy is first trained by RL from scratch with weak augmentations. A student\\nnetwork then learns to mimic the expert policy by supervised learning with\\nstrong augmentations, making its representation more robust against visual\\nvariations compared to the expert. Extensive experiments demonstrate that\\nSECANT significantly advances the state of the art in zero-shot generalization\\nacross 4 challenging domains. Our average reward improvements over prior SOTAs\\nare: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based\\nautonomous driving (+47.7%), and indoor object navigation (+15.8%). Code\\nrelease and video are available at https://linxifan.github.io/secant-site/.',\n",
       "    'author': [{'name': 'Linxi Fan'},\n",
       "     {'name': 'Guanzhi Wang'},\n",
       "     {'name': 'De-An Huang'},\n",
       "     {'name': 'Zhiding Yu'},\n",
       "     {'name': 'Li Fei-Fei'},\n",
       "     {'name': 'Yuke Zhu'},\n",
       "     {'name': 'Anima Anandkumar'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2021. Website: https://linxifan.github.io/secant-site/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.09678v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.09678v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.13415v1',\n",
       "    'updated': '2021-06-25T04:10:58Z',\n",
       "    'published': '2021-06-25T04:10:58Z',\n",
       "    'title': 'Building Intelligent Autonomous Navigation Agents',\n",
       "    'summary': \"Breakthroughs in machine learning in the last decade have led to `digital\\nintelligence', i.e. machine learning models capable of learning from vast\\namounts of labeled data to perform several digital tasks such as speech\\nrecognition, face recognition, machine translation and so on. The goal of this\\nthesis is to make progress towards designing algorithms capable of `physical\\nintelligence', i.e. building intelligent autonomous navigation agents capable\\nof learning to perform complex navigation tasks in the physical world involving\\nvisual perception, natural language understanding, reasoning, planning, and\\nsequential decision making. Despite several advances in classical navigation\\nmethods in the last few decades, current navigation agents struggle at\\nlong-term semantic navigation tasks. In the first part of the thesis, we\\ndiscuss our work on short-term navigation using end-to-end reinforcement\\nlearning to tackle challenges such as obstacle avoidance, semantic perception,\\nlanguage grounding, and reasoning. In the second part, we present a new class\\nof navigation methods based on modular learning and structured explicit map\\nrepresentations, which leverage the strengths of both classical and end-to-end\\nlearning methods, to tackle long-term navigation tasks. We show that these\\nmethods are able to effectively tackle challenges such as localization,\\nmapping, long-term planning, exploration and learning semantic priors. These\\nmodular learning methods are capable of long-term spatial and semantic\\nunderstanding and achieve state-of-the-art results on various navigation tasks.\",\n",
       "    'author': {'name': 'Devendra Singh Chaplot'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CMU Ph.D. Thesis, March 2021. For more details see\\n  http://devendrachaplot.github.io/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.13415v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.13415v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.02308v1',\n",
       "    'updated': '2021-07-05T22:43:27Z',\n",
       "    'published': '2021-07-05T22:43:27Z',\n",
       "    'title': 'A visual introduction to Gaussian Belief Propagation',\n",
       "    'summary': 'In this article, we present a visual introduction to Gaussian Belief\\nPropagation (GBP), an approximate probabilistic inference algorithm that\\noperates by passing messages between the nodes of arbitrarily structured factor\\ngraphs. A special case of loopy belief propagation, GBP updates rely only on\\nlocal information and will converge independently of the message schedule. Our\\nkey argument is that, given recent trends in computing hardware, GBP has the\\nright computational properties to act as a scalable distributed probabilistic\\ninference framework for future machine learning systems.',\n",
       "    'author': [{'name': 'Joseph Ortiz'},\n",
       "     {'name': 'Talfan Evans'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'See online version of this article: https://gaussianbp.github.io/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.02308v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.02308v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.08142v3',\n",
       "    'updated': '2021-08-09T16:51:06Z',\n",
       "    'published': '2021-07-16T23:20:26Z',\n",
       "    'title': 'Autonomy 2.0: Why is self-driving always 5 years away?',\n",
       "    'summary': 'Despite the numerous successes of machine learning over the past decade\\n(image recognition, decision-making, NLP, image synthesis), self-driving\\ntechnology has not yet followed the same trend. In this paper, we study the\\nhistory, composition, and development bottlenecks of the modern self-driving\\nstack. We argue that the slow progress is caused by approaches that require too\\nmuch hand-engineering, an over-reliance on road testing, and high fleet\\ndeployment costs. We observe that the classical stack has several bottlenecks\\nthat preclude the necessary scale needed to capture the long tail of rare\\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\\nan ML-first approach to self-driving, as a viable alternative to the currently\\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\\nreactive simulation, and (iii) large-scale, low-cost data collections as\\ncritical solutions towards scalability issues. We outline the general\\narchitecture, survey promising works in this direction and propose key\\nchallenges to be addressed by the community in the future.',\n",
       "    'author': [{'name': 'Ashesh Jain'},\n",
       "     {'name': 'Luca Del Pero'},\n",
       "     {'name': 'Hugo Grimmett'},\n",
       "     {'name': 'Peter Ondruska'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.08142v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.08142v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.09046v1',\n",
       "    'updated': '2021-07-19T17:54:48Z',\n",
       "    'published': '2021-07-19T17:54:48Z',\n",
       "    'title': 'Playful Interactions for Representation Learning',\n",
       "    'summary': 'One of the key challenges in visual imitation learning is collecting large\\namounts of expert demonstrations for a given task. While methods for collecting\\nhuman demonstrations are becoming easier with teleoperation methods and the use\\nof low-cost assistive tools, we often still require 100-1000 demonstrations for\\nevery task to learn a visual representation and policy. To address this, we\\nturn to an alternate form of data that does not require task-specific\\ndemonstrations -- play. Playing is a fundamental method children use to learn a\\nset of skills and behaviors and visual representations in early learning.\\nImportantly, play data is diverse, task-agnostic, and relatively cheap to\\nobtain. In this work, we propose to use playful interactions in a\\nself-supervised manner to learn visual representations for downstream tasks. We\\ncollect 2 hours of playful data in 19 diverse environments and use\\nself-predictive learning to extract visual representations. Given these\\nrepresentations, we train policies using imitation learning for two downstream\\ntasks: Pushing and Stacking. We demonstrate that our visual representations\\ngeneralize better than standard behavior cloning and can achieve similar\\nperformance with only half the number of required demonstrations. Our\\nrepresentations, which are trained from scratch, compare favorably against\\nImageNet pretrained representations. Finally, we provide an experimental\\nanalysis on the effects of different pretraining modes on downstream task\\nlearning.',\n",
       "    'author': [{'name': 'Sarah Young'},\n",
       "     {'name': 'Jyothish Pari'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Lerrel Pinto'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.09046v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.09046v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.13865v2',\n",
       "    'updated': '2022-01-28T17:06:06Z',\n",
       "    'published': '2021-08-31T14:18:40Z',\n",
       "    'title': 'InSeGAN: A Generative Approach to Segmenting Identical Instances in\\n  Depth Images',\n",
       "    'summary': 'In this paper, we present InSeGAN, an unsupervised 3D generative adversarial\\nnetwork (GAN) for segmenting (nearly) identical instances of rigid objects in\\ndepth images. Using an analysis-by-synthesis approach, we design a novel GAN\\narchitecture to synthesize a multiple-instance depth image with independent\\ncontrol over each instance. InSeGAN takes in a set of code vectors (e.g.,\\nrandom noise vectors), each encoding the 3D pose of an object that is\\nrepresented by a learned implicit object template. The generator has two\\ndistinct modules. The first module, the instance feature generator, uses each\\nencoded pose to transform the implicit template into a feature map\\nrepresentation of each object instance. The second module, the depth image\\nrenderer, aggregates all of the single-instance feature maps output by the\\nfirst module and generates a multiple-instance depth image. A discriminator\\ndistinguishes the generated multiple-instance depth images from the\\ndistribution of true depth images. To use our model for instance segmentation,\\nwe propose an instance pose encoder that learns to take in a generated depth\\nimage and reproduce the pose code vectors for all of the object instances. To\\nevaluate our approach, we introduce a new synthetic dataset, \"Insta-10\",\\nconsisting of 100,000 depth images, each with 5 instances of an object from one\\nof 10 classes. Our experiments on Insta-10, as well as on real-world noisy\\ndepth images, show that InSeGAN achieves state-of-the-art performance, often\\noutperforming prior methods by large margins.',\n",
       "    'author': [{'name': 'Anoop Cherian'},\n",
       "     {'name': 'Goncalo Dias Pais'},\n",
       "     {'name': 'Siddarth Jain'},\n",
       "     {'name': 'Tim K. Marks'},\n",
       "     {'name': 'Alan Sullivan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at ICCV 2021. Code & data @\\n  https://www.merl.com/research/license/InSeGAN'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.13865v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.13865v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.03615v1',\n",
       "    'updated': '2021-09-08T13:03:08Z',\n",
       "    'published': '2021-09-08T13:03:08Z',\n",
       "    'title': 'Tactile Image-to-Image Disentanglement of Contact Geometry from\\n  Motion-Induced Shear',\n",
       "    'summary': 'Robotic touch, particularly when using soft optical tactile sensors, suffers\\nfrom distortion caused by motion-dependent shear. The manner in which the\\nsensor contacts a stimulus is entangled with the tactile information about the\\ngeometry of the stimulus. In this work, we propose a supervised convolutional\\ndeep neural network model that learns to disentangle, in the latent space, the\\ncomponents of sensor deformations caused by contact geometry from those due to\\nsliding-induced shear. The approach is validated by reconstructing unsheared\\ntactile images from sheared images and showing they match unsheared tactile\\nimages collected with no sliding motion. In addition, the unsheared tactile\\nimages give a faithful reconstruction of the contact geometry that is not\\npossible from the sheared data, and robust estimation of the contact pose that\\ncan be used for servo control sliding around various 2D shapes. Finally, the\\ncontact geometry reconstruction in conjunction with servo control sliding were\\nused for faithful full object reconstruction of various 2D shapes. The methods\\nhave broad applicability to deep learning models for robots with a\\nshear-sensitive sense of touch.',\n",
       "    'author': [{'name': 'Anupam K. Gupta'},\n",
       "     {'name': 'Laurence Aitchison'},\n",
       "     {'name': 'Nathan F. Lepora'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '15 pages, 6 figure, under review CORL 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.03615v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.03615v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.04378v1',\n",
       "    'updated': '2021-09-09T16:04:14Z',\n",
       "    'published': '2021-09-09T16:04:14Z',\n",
       "    'title': 'Dynamic Modeling of Hand-Object Interactions via Tactile Sensing',\n",
       "    'summary': 'Tactile sensing is critical for humans to perform everyday tasks. While\\nsignificant progress has been made in analyzing object grasping from vision, it\\nremains unclear how we can utilize tactile sensing to reason about and model\\nthe dynamics of hand-object interactions. In this work, we employ a\\nhigh-resolution tactile glove to perform four different interactive activities\\non a diversified set of objects. We build our model on a cross-modal learning\\nframework and generate the labels using a visual processing pipeline to\\nsupervise the tactile model, which can then be used on its own during the test\\ntime. The tactile model aims to predict the 3d locations of both the hand and\\nthe object purely from the touch data by combining a predictive model and a\\ncontrastive learning module. This framework can reason about the interaction\\npatterns from the tactile data, hallucinate the changes in the environment,\\nestimate the uncertainty of the prediction, and generalize to unseen objects.\\nWe also provide detailed ablation studies regarding different system designs as\\nwell as visualizations of the predicted trajectories. This work takes a step on\\ndynamics modeling in hand-object interactions from dense tactile sensing, which\\nopens the door for future applications in activity learning, human-computer\\ninteractions, and imitation learning for robotics.',\n",
       "    'author': [{'name': 'Qiang Zhang'},\n",
       "     {'name': 'Yunzhu Li'},\n",
       "     {'name': 'Yiyue Luo'},\n",
       "     {'name': 'Wan Shou'},\n",
       "     {'name': 'Michael Foshey'},\n",
       "     {'name': 'Junchi Yan'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Wojciech Matusik'},\n",
       "     {'name': 'Antonio Torralba'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IROS 2021. First two authors contributed equally. Project page:\\n  http://phystouch.csail.mit.edu/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.04378v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.04378v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.04468v3',\n",
       "    'updated': '2022-02-14T17:16:53Z',\n",
       "    'published': '2021-09-09T17:59:52Z',\n",
       "    'title': 'Leveraging Local Domains for Image-to-Image Translation',\n",
       "    'summary': \"Image-to-image (i2i) networks struggle to capture local changes because they\\ndo not affect the global scene structure. For example, translating from highway\\nscenes to offroad, i2i networks easily focus on global color features but\\nignore obvious traits for humans like the absence of lane markings. In this\\npaper, we leverage human knowledge about spatial domain characteristics which\\nwe refer to as 'local domains' and demonstrate its benefit for image-to-image\\ntranslation. Relying on a simple geometrical guidance, we train a patch-based\\nGAN on few source data and hallucinate a new unseen domain which subsequently\\neases transfer learning to target. We experiment on three tasks ranging from\\nunstructured environments to adverse weather. Our comprehensive evaluation\\nsetting shows we are able to generate realistic translations, with minimal\\npriors, and training only on a few images. Furthermore, when trained on our\\ntranslations images we show that all tested proxy tasks are significantly\\nimproved, without ever seeing target domain at training.\",\n",
       "    'author': [{'name': \"Anthony Dell'Eva\"},\n",
       "     {'name': 'Fabio Pizzati'},\n",
       "     {'name': 'Massimo Bertozzi'},\n",
       "     {'name': 'Raoul de Charette'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'VISAPP 2022 Best Paper Award'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.04468v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.04468v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.08817v2',\n",
       "    'updated': '2021-11-17T18:28:57Z',\n",
       "    'published': '2021-09-18T03:07:06Z',\n",
       "    'title': 'Learning to Regrasp by Learning to Place',\n",
       "    'summary': \"In this paper, we explore whether a robot can learn to regrasp a diverse set\\nof objects to achieve various desired grasp poses. Regrasping is needed\\nwhenever a robot's current grasp pose fails to perform desired manipulation\\ntasks. Endowing robots with such an ability has applications in many domains\\nsuch as manufacturing or domestic services. Yet, it is a challenging task due\\nto the large diversity of geometry in everyday objects and the high\\ndimensionality of the state and action space. In this paper, we propose a\\nsystem for robots to take partial point clouds of an object and the supporting\\nenvironment as inputs and output a sequence of pick-and-place operations to\\ntransform an initial object grasp pose to the desired object grasp poses. The\\nkey technique includes a neural stable placement predictor and a regrasp\\ngraph-based solution through leveraging and changing the surrounding\\nenvironment. We introduce a new and challenging synthetic dataset for learning\\nand evaluating the proposed approach. We demonstrate the effectiveness of our\\nproposed system with both simulator and real-world experiments. More videos and\\nvisualization examples are available on our project webpage.\",\n",
       "    'author': [{'name': 'Shuo Cheng'},\n",
       "     {'name': 'Kaichun Mo'},\n",
       "     {'name': 'Lin Shao'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to Conference on Robot Learning (CoRL) 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.08817v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.08817v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.09690v2',\n",
       "    'updated': '2021-09-21T15:27:37Z',\n",
       "    'published': '2021-09-20T17:00:07Z',\n",
       "    'title': 'Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks\\n  with Sparse Gaussian Processes',\n",
       "    'summary': 'This paper presents a probabilistic framework to obtain both reliable and\\nfast uncertainty estimates for predictions with Deep Neural Networks (DNNs).\\nOur main contribution is a practical and principled combination of DNNs with\\nsparse Gaussian Processes (GPs). We prove theoretically that DNNs can be seen\\nas a special case of sparse GPs, namely mixtures of GP experts (MoE-GP), and we\\ndevise a learning algorithm that brings the derived theory into practice. In\\nexperiments from two different robotic tasks -- inverse dynamics of a\\nmanipulator and object detection on a micro-aerial vehicle (MAV) -- we show the\\neffectiveness of our approach in terms of predictive uncertainty, improved\\nscalability, and run-time efficiency on a Jetson TX2. We thus argue that our\\napproach can pave the way towards reliable and fast robot learning systems with\\nuncertainty awareness.',\n",
       "    'author': [{'name': 'Jongseok Lee'},\n",
       "     {'name': 'Jianxiang Feng'},\n",
       "     {'name': 'Matthias Humt'},\n",
       "     {'name': 'Marcus G. Müller'},\n",
       "     {'name': 'Rudolph Triebel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 6 figures and 1 table. Accepted at the 5th Conference on\\n  Robot Learning (CORL 2021), London, UK'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.09690v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.09690v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.01641v1',\n",
       "    'updated': '2021-10-04T18:03:18Z',\n",
       "    'published': '2021-10-04T18:03:18Z',\n",
       "    'title': 'Investigating Fairness of Ocular Biometrics Among Young, Middle-Aged,\\n  and Older Adults',\n",
       "    'summary': 'A number of studies suggest bias of the face biometrics, i.e., face\\nrecognition and soft-biometric estimation methods, across gender, race, and age\\ngroups. There is a recent urge to investigate the bias of different biometric\\nmodalities toward the deployment of fair and trustworthy biometric solutions.\\nOcular biometrics has obtained increased attention from academia and industry\\ndue to its high accuracy, security, privacy, and ease of use in mobile devices.\\nA recent study in $2020$ also suggested the fairness of ocular-based user\\nrecognition across males and females. This paper aims to evaluate the fairness\\nof ocular biometrics in the visible spectrum among age groups; young, middle,\\nand older adults. Thanks to the availability of the latest large-scale 2020\\nUFPR ocular biometric dataset, with subjects acquired in the age range 18 - 79\\nyears, to facilitate this study. Experimental results suggest the overall\\nequivalent performance of ocular biometrics across gender and age groups in\\nuser verification and gender classification. Performance difference for older\\nadults at lower false match rate and young adults was noted at user\\nverification and age classification, respectively. This could be attributed to\\ninherent characteristics of the biometric data from these age groups impacting\\nspecific applications, which suggest a need for advancement in sensor\\ntechnology and software solutions.',\n",
       "    'author': [{'name': 'Anoop Krishnan'},\n",
       "     {'name': 'Ali Almadan'},\n",
       "     {'name': 'Ajita Rattani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 Pages, Fairness of Ocular Biometrics'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.01641v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.01641v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.10563v1',\n",
       "    'updated': '2021-10-20T13:46:15Z',\n",
       "    'published': '2021-10-20T13:46:15Z',\n",
       "    'title': 'Robust Monocular Localization in Sparse HD Maps Leveraging Multi-Task\\n  Uncertainty Estimation',\n",
       "    'summary': 'Robust localization in dense urban scenarios using a low-cost sensor setup\\nand sparse HD maps is highly relevant for the current advances in autonomous\\ndriving, but remains a challenging topic in research. We present a novel\\nmonocular localization approach based on a sliding-window pose graph that\\nleverages predicted uncertainties for increased precision and robustness\\nagainst challenging scenarios and per frame failures. To this end, we propose\\nan efficient multi-task uncertainty-aware perception module, which covers\\nsemantic segmentation, as well as bounding box detection, to enable the\\nlocalization of vehicles in sparse maps, containing only lane borders and\\ntraffic lights. Further, we design differentiable cost maps that are directly\\ngenerated from the estimated uncertainties. This opens up the possibility to\\nminimize the reprojection loss of amorphous map elements in an association free\\nand uncertainty-aware manner. Extensive evaluation on the Lyft 5 dataset shows\\nthat, despite the sparsity of the map, our approach enables robust and accurate\\n6D localization in challenging urban scenarios',\n",
       "    'author': [{'name': 'Kürsat Petek'},\n",
       "     {'name': 'Kshitij Sirohi'},\n",
       "     {'name': 'Daniel Büscher'},\n",
       "     {'name': 'Wolfram Burgard'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.10563v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.10563v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.00966v1',\n",
       "    'updated': '2021-11-01T14:17:09Z',\n",
       "    'published': '2021-11-01T14:17:09Z',\n",
       "    'title': 'VPFNet: Voxel-Pixel Fusion Network for Multi-class 3D Object Detection',\n",
       "    'summary': 'Many LiDAR-based methods for detecting large objects, single-class object\\ndetection, or under easy situations were claimed to perform quite well.\\nHowever, their performances of detecting small objects or under hard situations\\ndid not surpass those of the fusion-based ones due to failure to leverage the\\nimage semantics. In order to elevate the detection performance in a complicated\\nenvironment, this paper proposes a deep learning (DL)-embedded fusion-based\\nmulti-class 3D object detection network which admits both LiDAR and camera\\nsensor data streams, named Voxel-Pixel Fusion Network (VPFNet). Inside this\\nnetwork, a key novel component is called Voxel-Pixel Fusion (VPF) layer, which\\ntakes advantage of the geometric relation of a voxel-pixel pair and fuses the\\nvoxel features and the pixel features with proper mechanisms. Moreover, several\\nparameters are particularly designed to guide and enhance the fusion effect\\nafter considering the characteristics of a voxel-pixel pair. Finally, the\\nproposed method is evaluated on the KITTI benchmark for multi-class 3D object\\ndetection task under multilevel difficulty, and is shown to outperform all\\nstate-of-the-art methods in mean average precision (mAP). It is also noteworthy\\nthat our approach here ranks the first on the KITTI leaderboard for the\\nchallenging pedestrian class.',\n",
       "    'author': [{'name': 'Chia-Hung Wang'},\n",
       "     {'name': 'Hsueh-Wei Chen'},\n",
       "     {'name': 'Li-Chen Fu'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.00966v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.00966v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.01674v1',\n",
       "    'updated': '2021-10-25T17:59:58Z',\n",
       "    'published': '2021-10-25T17:59:58Z',\n",
       "    'title': 'Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged\\n  Robots',\n",
       "    'summary': 'Legged locomotion is commonly studied and expressed as a discrete set of gait\\npatterns, like walk, trot, gallop, which are usually treated as given and\\npre-programmed in legged robots for efficient locomotion at different speeds.\\nHowever, fixing a set of pre-programmed gaits limits the generality of\\nlocomotion. Recent animal motor studies show that these conventional gaits are\\nonly prevalent in ideal flat terrain conditions while real-world locomotion is\\nunstructured and more like bouts of intermittent steps. What principles could\\nlead to both structured and unstructured patterns across mammals and how to\\nsynthesize them in robots? In this work, we take an analysis-by-synthesis\\napproach and learn to move by minimizing mechanical energy. We demonstrate that\\nlearning to minimize energy consumption plays a key role in the emergence of\\nnatural locomotion gaits at different speeds in real quadruped robots. The\\nemergent gaits are structured in ideal terrains and look similar to that of\\nhorses and sheep. The same approach leads to unstructured gaits in rough\\nterrains which is consistent with the findings in animal motor control. We\\nvalidate our hypothesis in both simulation and real hardware across natural\\nterrains. Videos at https://energy-locomotion.github.io',\n",
       "    'author': [{'name': 'Zipeng Fu'},\n",
       "     {'name': 'Ashish Kumar'},\n",
       "     {'name': 'Jitendra Malik'},\n",
       "     {'name': 'Deepak Pathak'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2021. Website at https://energy-locomotion.github.io'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.01674v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.01674v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.03987v1',\n",
       "    'updated': '2021-11-07T02:31:09Z',\n",
       "    'published': '2021-11-07T02:31:09Z',\n",
       "    'title': 'V-MAO: Generative Modeling for Multi-Arm Manipulation of Articulated\\n  Objects',\n",
       "    'summary': 'Manipulating articulated objects requires multiple robot arms in general. It\\nis challenging to enable multiple robot arms to collaboratively complete\\nmanipulation tasks on articulated objects. In this paper, we present\\n$\\\\textbf{V-MAO}$, a framework for learning multi-arm manipulation of\\narticulated objects. Our framework includes a variational generative model that\\nlearns contact point distribution over object rigid parts for each robot arm.\\nThe training signal is obtained from interaction with the simulation\\nenvironment which is enabled by planning and a novel formulation of\\nobject-centric control for articulated objects. We deploy our framework in a\\ncustomized MuJoCo simulation environment and demonstrate that our framework\\nachieves a high success rate on six different objects and two different robots.\\nWe also show that generative modeling can effectively learn the contact point\\ndistribution on articulated objects.',\n",
       "    'author': [{'name': 'Xingyu Liu'}, {'name': 'Kris M. Kitani'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.03987v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.03987v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.13489v2',\n",
       "    'updated': '2022-04-04T07:38:44Z',\n",
       "    'published': '2021-11-26T13:39:38Z',\n",
       "    'title': 'SurfEmb: Dense and Continuous Correspondence Distributions for Object\\n  Pose Estimation with Learnt Surface Embeddings',\n",
       "    'summary': 'We present an approach to learn dense, continuous 2D-3D correspondence\\ndistributions over the surface of objects from data with no prior knowledge of\\nvisual ambiguities like symmetry. We also present a new method for 6D pose\\nestimation of rigid objects using the learnt distributions to sample, score and\\nrefine pose hypotheses. The correspondence distributions are learnt with a\\ncontrastive loss, represented in object-specific latent spaces by an\\nencoder-decoder query model and a small fully connected key model. Our method\\nis unsupervised with respect to visual ambiguities, yet we show that the query-\\nand key models learn to represent accurate multi-modal surface distributions.\\nOur pose estimation method improves the state-of-the-art significantly on the\\ncomprehensive BOP Challenge, trained purely on synthetic data, even compared\\nwith methods trained on real data. The project site is at\\nhttps://surfemb.github.io/ .',\n",
       "    'author': [{'name': 'Rasmus Laurvig Haugaard'},\n",
       "     {'name': 'Anders Glent Buch'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.13489v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.13489v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.14820v4',\n",
       "    'updated': '2022-04-05T17:22:09Z',\n",
       "    'published': '2021-11-29T18:59:09Z',\n",
       "    'title': 'Towards Robust and Adaptive Motion Forecasting: A Causal Representation\\n  Perspective',\n",
       "    'summary': 'Learning behavioral patterns from observational data has been a de-facto\\napproach to motion forecasting. Yet, the current paradigm suffers from two\\nshortcomings: brittle under distribution shifts and inefficient for knowledge\\ntransfer. In this work, we propose to address these challenges from a causal\\nrepresentation perspective. We first introduce a causal formalism of motion\\nforecasting, which casts the problem as a dynamic process with three groups of\\nlatent variables, namely invariant variables, style confounders, and spurious\\nfeatures. We then introduce a learning framework that treats each group\\nseparately: (i) unlike the common practice mixing datasets collected from\\ndifferent locations, we exploit their subtle distinctions by means of an\\ninvariance loss encouraging the model to suppress spurious correlations; (ii)\\nwe devise a modular architecture that factorizes the representations of\\ninvariant mechanisms and style confounders to approximate a sparse causal\\ngraph; (iii) we introduce a style contrastive loss that not only enforces the\\nstructure of style representations but also serves as a self-supervisory signal\\nfor test-time refinement on the fly. Experiments on synthetic and real datasets\\nshow that our proposed method improves the robustness and reusability of\\nlearned motion representations, significantly outperforming prior\\nstate-of-the-art motion forecasting models for out-of-distribution\\ngeneralization and low-shot transfer.',\n",
       "    'author': [{'name': 'Yuejiang Liu'},\n",
       "     {'name': 'Riccardo Cadei'},\n",
       "     {'name': 'Jonas Schweizer'},\n",
       "     {'name': 'Sherwin Bahmani'},\n",
       "     {'name': 'Alexandre Alahi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2022. Code is available at\\n  https://github.com/vita-epfl/causalmotion. v4: fixed typo'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.14820v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.14820v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.09120v2',\n",
       "    'updated': '2022-04-08T16:29:09Z',\n",
       "    'published': '2021-12-16T18:58:03Z',\n",
       "    'title': 'Human Hands as Probes for Interactive Object Understanding',\n",
       "    'summary': 'Interactive object understanding, or what we can do to objects and how is a\\nlong-standing goal of computer vision. In this paper, we tackle this problem\\nthrough observation of human hands in in-the-wild egocentric videos. We\\ndemonstrate that observation of what human hands interact with and how can\\nprovide both the relevant data and the necessary supervision. Attending to\\nhands, readily localizes and stabilizes active objects for learning and reveals\\nplaces where interactions with objects occur. Analyzing the hands shows what we\\ncan do to objects and how. We apply these basic principles on the EPIC-KITCHENS\\ndataset, and successfully learn state-sensitive features, and object\\naffordances (regions of interaction and afforded grasps), purely by observing\\nhands in egocentric videos.',\n",
       "    'author': [{'name': 'Mohit Goyal'},\n",
       "     {'name': 'Sahil Modi'},\n",
       "     {'name': 'Rishabh Goyal'},\n",
       "     {'name': 'Saurabh Gupta'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To Appear at CVPR 2022. Project website at\\n  https://s-gupta.github.io/hands-as-probes/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.09120v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.09120v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.12107v1',\n",
       "    'updated': '2022-01-28T13:31:42Z',\n",
       "    'published': '2022-01-28T13:31:42Z',\n",
       "    'title': 'Feature Visualization within an Automated Design Assessment leveraging\\n  Explainable Artificial Intelligence Methods',\n",
       "    'summary': \"Not only automation of manufacturing processes but also automation of\\nautomation procedures itself become increasingly relevant to automation\\nresearch. In this context, automated capability assessment, mainly leveraged by\\ndeep learning systems driven from 3D CAD data, have been presented. Current\\nassessment systems may be able to assess CAD data with regards to abstract\\nfeatures, e.g. the ability to automatically separate components from bulk\\ngoods, or the presence of gripping surfaces. Nevertheless, they suffer from the\\nfactor of black box systems, where an assessment can be learned and generated\\neasily, but without any geometrical indicator about the reasons of the system's\\ndecision. By utilizing explainable AI (xAI) methods, we attempt to open up the\\nblack box. Explainable AI methods have been used in order to assess whether a\\nneural network has successfully learned a given task or to analyze which\\nfeatures of an input might lead to an adversarial attack. These methods aim to\\nderive additional insights into a neural network, by analyzing patterns from a\\ngiven input and its impact to the network output. Within the NeuroCAD Project,\\nxAI methods are used to identify geometrical features which are associated with\\na certain abstract feature. Within this work, a sensitivity analysis (SA), the\\nlayer-wise relevance propagation (LRP), the Gradient-weighted Class Activation\\nMapping (Grad-CAM) method as well as the Local Interpretable Model-Agnostic\\nExplanations (LIME) have been implemented in the NeuroCAD environment, allowing\\nnot only to assess CAD models but also to identify features which have been\\nrelevant for the network decision. In the medium run, this might enable to\\nidentify regions of interest supporting product designers to optimize their\\nmodels with regards to assembly processes.\",\n",
       "    'author': [{'name': 'Raoul Schönhof'},\n",
       "     {'name': 'Artem Werner'},\n",
       "     {'name': 'Jannes Elstner'},\n",
       "     {'name': 'Boldizsar Zopcsak'},\n",
       "     {'name': 'Ramez Awad'},\n",
       "     {'name': 'Marco Huber'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.procir.2021.05.075'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.procir.2021.05.075',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2201.12107v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.12107v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CIRP Design 2021, 10.1016/j.procir.2021.05.075'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2021, Procedia CIRP 100(7):331-336'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.03059v1',\n",
       "    'updated': '2022-02-07T10:51:23Z',\n",
       "    'published': '2022-02-07T10:51:23Z',\n",
       "    'title': 'Evaluation of Runtime Monitoring for UAV Emergency Landing',\n",
       "    'summary': 'To certify UAV operations in populated areas, risk mitigation strategies --\\nsuch as Emergency Landing (EL) -- must be in place to account for potential\\nfailures. EL aims at reducing ground risk by finding safe landing areas using\\non-board sensors. The first contribution of this paper is to present a new EL\\napproach, in line with safety requirements introduced in recent research. In\\nparticular, the proposed EL pipeline includes mechanisms to monitor learning\\nbased components during execution. This way, another contribution is to study\\nthe behavior of Machine Learning Runtime Monitoring (MLRM) approaches within\\nthe context of a real-world critical system. A new evaluation methodology is\\nintroduced, and applied to assess the practical safety benefits of three MLRM\\nmechanisms. The proposed approach is compared to a default mitigation strategy\\n(open a parachute when a failure is detected), and appears to be much safer.',\n",
       "    'author': [{'name': 'Joris Guerin'},\n",
       "     {'name': 'Kevin Delmas'},\n",
       "     {'name': 'Jérémie Guiochet'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 4 figures, 1 table. To appear in the proceedings of 2022\\n  IEEE International Conference on Robotics and Automation (ICRA)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.03059v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.03059v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.03957v1',\n",
       "    'updated': '2022-02-08T16:09:02Z',\n",
       "    'published': '2022-02-08T16:09:02Z',\n",
       "    'title': 'Bingham Policy Parameterization for 3D Rotations in Reinforcement\\n  Learning',\n",
       "    'summary': 'We propose a new policy parameterization for representing 3D rotations during\\nreinforcement learning. Today in the continuous control reinforcement learning\\nliterature, many stochastic policy parameterizations are Gaussian. We argue\\nthat universally applying a Gaussian policy parameterization is not always\\ndesirable for all environments. One such case in particular where this is true\\nare tasks that involve predicting a 3D rotation output, either in isolation, or\\ncoupled with translation as part of a full 6D pose output. Our proposed Bingham\\nPolicy Parameterization (BPP) models the Bingham distribution and allows for\\nbetter rotation (quaternion) prediction over a Gaussian policy parameterization\\nin a range of reinforcement learning tasks. We evaluate BPP on the rotation\\nWahba problem task, as well as a set of vision-based next-best pose robot\\nmanipulation tasks from RLBench. We hope that this paper encourages more\\nresearch into developing other policy parameterization that are more suited for\\nparticular environments, rather than always assuming Gaussian.',\n",
       "    'author': [{'name': 'Stephen James'}, {'name': 'Pieter Abbeel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page and code: https://sites.google.com/view/rl-bpp'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.03957v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.03957v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.10448v2',\n",
       "    'updated': '2022-07-24T06:08:35Z',\n",
       "    'published': '2022-02-21T18:59:59Z',\n",
       "    'title': 'Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans\\n  on Youtube',\n",
       "    'summary': 'We build a system that enables any human to control a robot hand and arm,\\nsimply by demonstrating motions with their own hand. The robot observes the\\nhuman operator via a single RGB camera and imitates their actions in real-time.\\nHuman hands and robot hands differ in shape, size, and joint structure, and\\nperforming this translation from a single uncalibrated camera is a highly\\nunderconstrained problem. Moreover, the retargeted trajectories must\\neffectively execute tasks on a physical robot, which requires them to be\\ntemporally smooth and free of self-collisions. Our key insight is that while\\npaired human-robot correspondence data is expensive to collect, the internet\\ncontains a massive corpus of rich and diverse human hand videos. We leverage\\nthis data to train a system that understands human hands and retargets a human\\nvideo stream into a robot hand-arm trajectory that is smooth, swift, safe, and\\nsemantically similar to the guiding demonstration. We demonstrate that it\\nenables previously untrained people to teleoperate a robot on various dexterous\\nmanipulation tasks. Our low-cost, glove-free, marker-free remote teleoperation\\nsystem makes robot teaching more accessible and we hope that it can aid robots\\nin learning to act autonomously in the real world. Videos at\\nhttps://robotic-telekinesis.github.io/',\n",
       "    'author': [{'name': 'Aravind Sivakumar'},\n",
       "     {'name': 'Kenneth Shaw'},\n",
       "     {'name': 'Deepak Pathak'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'RSS 2022 final version. Website and demos at\\n  https://robotic-telekinesis.github.io/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.10448v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.10448v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.13427v1',\n",
       "    'updated': '2022-02-27T19:09:21Z',\n",
       "    'published': '2022-02-27T19:09:21Z',\n",
       "    'title': 'Meta-path Analysis on Spatio-Temporal Graphs for Pedestrian Trajectory\\n  Prediction',\n",
       "    'summary': 'Spatio-temporal graphs (ST-graphs) have been used to model time series tasks\\nsuch as traffic forecasting, human motion modeling, and action recognition. The\\nhigh-level structure and corresponding features from ST-graphs have led to\\nimproved performance over traditional architectures. However, current methods\\ntend to be limited by simple features, despite the rich information provided by\\nthe full graph structure, which leads to inefficiencies and suboptimal\\nperformance in downstream tasks. We propose the use of features derived from\\nmeta-paths, walks across different types of edges, in ST-graphs to improve the\\nperformance of Structural Recurrent Neural Network. In this paper, we present\\nthe Meta-path Enhanced Structural Recurrent Neural Network (MESRNN), a generic\\nframework that can be applied to any spatio-temporal task in a simple and\\nscalable manner. We employ MESRNN for pedestrian trajectory prediction,\\nutilizing these meta-path based features to capture the relationships between\\nthe trajectories of pedestrians at different points in time and space. We\\ncompare our MESRNN against state-of-the-art ST-graph methods on standard\\ndatasets to show the performance boost provided by meta-path information. The\\nproposed model consistently outperforms the baselines in trajectory prediction\\nover long time horizons by over 32\\\\%, and produces more socially compliant\\ntrajectories in dense crowds. For more information please refer to the project\\nwebsite at https://sites.google.com/illinois.edu/mesrnn/home.',\n",
       "    'author': [{'name': 'Aamir Hasan'},\n",
       "     {'name': 'Pranav Sriram'},\n",
       "     {'name': 'Katherine Driggs-Campbell'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.13427v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.13427v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.00156v2',\n",
       "    'updated': '2022-04-16T21:07:54Z',\n",
       "    'published': '2022-03-01T00:21:39Z',\n",
       "    'title': 'Preemptive Motion Planning for Human-to-Robot Indirect Placement\\n  Handovers',\n",
       "    'summary': \"As technology advances, the need for safe, efficient, and collaborative\\nhuman-robot-teams has become increasingly important. One of the most\\nfundamental collaborative tasks in any setting is the object handover.\\nHuman-to-robot handovers can take either of two approaches: (1) direct\\nhand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach\\nensures minimal contact between the human and robot but can also result in\\nincreased idle time due to having to wait for the object to first be placed\\ndown on a surface. To minimize such idle time, the robot must preemptively\\npredict the human intent of where the object will be placed. Furthermore, for\\nthe robot to preemptively act in any sort of productive manner, predictions and\\nmotion planning must occur in real-time. We introduce a novel\\nprediction-planning pipeline that allows the robot to preemptively move towards\\nthe human agent's intended placement location using gaze and gestures as model\\ninputs. In this paper, we investigate the performance and drawbacks of our\\nearly intent predictor-planner as well as the practical benefits of using such\\na pipeline through a human-robot case study.\",\n",
       "    'author': [{'name': 'Andrew Choi'},\n",
       "     {'name': 'Mohammad Khalid Jawed'},\n",
       "     {'name': 'Jungseock Joo'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 6 figures, to appear in ICRA 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.00156v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.00156v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.02489v1',\n",
       "    'updated': '2022-03-04T18:39:31Z',\n",
       "    'published': '2022-03-04T18:39:31Z',\n",
       "    'title': 'Pedestrian Stop and Go Forecasting with Hybrid Feature Fusion',\n",
       "    'summary': \"Forecasting pedestrians' future motions is essential for autonomous driving\\nsystems to safely navigate in urban areas. However, existing prediction\\nalgorithms often overly rely on past observed trajectories and tend to fail\\naround abrupt dynamic changes, such as when pedestrians suddenly start or stop\\nwalking. We suggest that predicting these highly non-linear transitions should\\nform a core component to improve the robustness of motion prediction\\nalgorithms. In this paper, we introduce the new task of pedestrian stop and go\\nforecasting. Considering the lack of suitable existing datasets for it, we\\nrelease TRANS, a benchmark for explicitly studying the stop and go behaviors of\\npedestrians in urban traffic. We build it from several existing datasets\\nannotated with pedestrians' walking motions, in order to have various scenarios\\nand behaviors. We also propose a novel hybrid model that leverages\\npedestrian-specific and scene features from several modalities, both video\\nsequences and high-level attributes, and gradually fuses them to integrate\\nmultiple levels of context. We evaluate our model and several baselines on\\nTRANS, and set a new benchmark for the community to work on pedestrian stop and\\ngo forecasting.\",\n",
       "    'author': [{'name': 'Dongxu Guo'},\n",
       "     {'name': 'Taylor Mordan'},\n",
       "     {'name': 'Alexandre Alahi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE International Conference on Robotics and Automation\\n  (ICRA) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.02489v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.02489v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.07662v4',\n",
       "    'updated': '2022-08-01T01:27:54Z',\n",
       "    'published': '2022-03-15T05:51:40Z',\n",
       "    'title': \"What's in the Black Box? The False Negative Mechanisms Inside Object\\n  Detectors\",\n",
       "    'summary': \"In object detection, false negatives arise when a detector fails to detect a\\ntarget object. To understand why object detectors produce false negatives, we\\nidentify five 'false negative mechanisms', where each mechanism describes how a\\nspecific component inside the detector architecture failed. Focusing on\\ntwo-stage and one-stage anchor-box object detector architectures, we introduce\\na framework for quantifying these false negative mechanisms. Using this\\nframework, we investigate why Faster R-CNN and RetinaNet fail to detect objects\\nin benchmark vision datasets and robotics datasets. We show that a detector's\\nfalse negative mechanisms differ significantly between computer vision\\nbenchmark datasets and robotics deployment scenarios. This has implications for\\nthe translation of object detectors developed for benchmark datasets to\\nrobotics applications. Code is publicly available at\\nhttps://github.com/csiro-robotics/fn_mechanisms\",\n",
       "    'author': [{'name': 'Dimity Miller'},\n",
       "     {'name': 'Peyman Moghadam'},\n",
       "     {'name': 'Mark Cox'},\n",
       "     {'name': 'Matt Wildie'},\n",
       "     {'name': 'Raja Jurdak'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2022.3187831'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2022.3187831',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2203.07662v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.07662v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 5 figures. Contact emails: d24.miller@qut.edu.au,\\n  peyman.moghadam@data61.csiro.au, mark.cox@data61.csiro.au,\\n  matt.wildie@data61.csiro.au, r.jurdak@qut.edu.au'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters (July 2022), Volume 7, Issue\\n  3, pages 8510-8517'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.10073v1',\n",
       "    'updated': '2022-03-18T17:38:52Z',\n",
       "    'published': '2022-03-18T17:38:52Z',\n",
       "    'title': 'Lunar Rover Localization Using Craters as Landmarks',\n",
       "    'summary': \"Onboard localization capabilities for planetary rovers to date have used\\nrelative navigation, by integrating combinations of wheel odometry, visual\\nodometry, and inertial measurements during each drive to track position\\nrelative to the start of each drive. At the end of each drive, a\\nground-in-the-loop (GITL) interaction is used to get a position update from\\nhuman operators in a more global reference frame, by matching images or local\\nmaps from onboard the rover to orbital reconnaissance images or maps of a large\\nregion around the rover's current position. Autonomous rover drives are limited\\nin distance so that accumulated relative navigation error does not risk the\\npossibility of the rover driving into hazards known from orbital images.\\nHowever, several rover mission concepts have recently been studied that require\\nmuch longer drives between GITL cycles, particularly for the Moon. These\\nconcepts require greater autonomy to minimize GITL cycles to enable such large\\nrange; onboard global localization is a key element of such autonomy. Multiple\\ntechniques have been studied in the past for onboard rover global localization,\\nbut a satisfactory solution has not yet emerged. For the Moon, the ubiquitous\\ncraters offer a new possibility, which involves mapping craters from orbit,\\nthen recognizing crater landmarks with cameras and-or a lidar onboard the\\nrover. This approach is applicable everywhere on the Moon, does not require\\nhigh resolution stereo imaging from orbit as some other approaches do, and has\\npotential to enable position knowledge with order of 5 to 10 m accuracy at all\\ntimes. This paper describes our technical approach to crater-based lunar rover\\nlocalization and presents initial results on crater detection using 3D point\\ncloud data from onboard lidar or stereo cameras, as well as using shading cues\\nin monocular onboard imagery.\",\n",
       "    'author': [{'name': 'Larry Matthies'},\n",
       "     {'name': 'Shreyansh Daftry'},\n",
       "     {'name': 'Scott Tepsuporn'},\n",
       "     {'name': 'Yang Cheng'},\n",
       "     {'name': 'Deegan Atha'},\n",
       "     {'name': 'R. Michael Swan'},\n",
       "     {'name': 'Sanjna Ravichandar'},\n",
       "     {'name': 'Masahiro Ono'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Aerospace Conference, 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.10073v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.10073v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.17251v1',\n",
       "    'updated': '2022-03-31T17:55:33Z',\n",
       "    'published': '2022-03-31T17:55:33Z',\n",
       "    'title': 'Continuous Scene Representations for Embodied AI',\n",
       "    'summary': 'We propose Continuous Scene Representations (CSR), a scene representation\\nconstructed by an embodied agent navigating within a space, where objects and\\ntheir relationships are modeled by continuous valued embeddings. Our method\\ncaptures feature relationships between objects, composes them into a graph\\nstructure on-the-fly, and situates an embodied agent within the representation.\\nOur key insight is to embed pair-wise relationships between objects in a latent\\nspace. This allows for a richer representation compared to discrete relations\\n(e.g., [support], [next-to]) commonly used for building scene representations.\\nCSR can track objects as the agent moves in a scene, update the representation\\naccordingly, and detect changes in room configurations. Using CSR, we\\noutperform state-of-the-art approaches for the challenging downstream task of\\nvisual room rearrangement, without any task specific training. Moreover, we\\nshow the learned embeddings capture salient spatial details of the scene and\\nshow applicability to real world data. A summery video and code is available at\\nhttps://prior.allenai.org/projects/csr.',\n",
       "    'author': [{'name': 'Samir Yitzhak Gadre'},\n",
       "     {'name': 'Kiana Ehsani'},\n",
       "     {'name': 'Shuran Song'},\n",
       "     {'name': 'Roozbeh Mottaghi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.17251v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.17251v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.01571v1',\n",
       "    'updated': '2022-04-04T15:23:14Z',\n",
       "    'published': '2022-04-04T15:23:14Z',\n",
       "    'title': 'Coarse-to-Fine Q-attention with Learned Path Ranking',\n",
       "    'summary': 'We propose Learned Path Ranking (LPR), a method that accepts an end-effector\\ngoal pose, and learns to rank a set of goal-reaching paths generated from an\\narray of path generating methods, including: path planning, Bezier curve\\nsampling, and a learned policy. The core idea being that each of the path\\ngeneration modules will be useful in different tasks, or at different stages in\\na task. When LPR is added as an extension to C2F-ARM, our new system,\\nC2F-ARM+LPR, retains the sample efficiency of its predecessor, while also being\\nable to accomplish a larger set of tasks; in particular, tasks that require\\nvery specific motions (e.g. opening toilet seat) that need to be inferred from\\nboth demonstrations and exploration data. In addition to benchmarking our\\napproach across 16 RLBench tasks, we also learn real-world tasks, tabula rasa,\\nin 10-15 minutes, with only 3 demonstrations.',\n",
       "    'author': [{'name': 'Stephen James'}, {'name': 'Pieter Abbeel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page and code: https://sites.google.com/view/q-attention-lpr'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.01571v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.01571v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.07049v2',\n",
       "    'updated': '2022-07-21T14:33:51Z',\n",
       "    'published': '2022-04-14T15:54:01Z',\n",
       "    'title': 'Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for\\n  Robotic Bin Picking',\n",
       "    'summary': 'In this paper, we propose an iterative self-training framework for\\nsim-to-real 6D object pose estimation to facilitate cost-effective robotic\\ngrasping. Given a bin-picking scenario, we establish a photo-realistic\\nsimulator to synthesize abundant virtual data, and use this to train an initial\\npose estimation network. This network then takes the role of a teacher model,\\nwhich generates pose predictions for unlabeled real data. With these\\npredictions, we further design a comprehensive adaptive selection scheme to\\ndistinguish reliable results, and leverage them as pseudo labels to update a\\nstudent model for pose estimation on real data. To continuously improve the\\nquality of pseudo labels, we iterate the above steps by taking the trained\\nstudent model as a new teacher and re-label real data using the refined teacher\\nmodel. We evaluate our method on a public benchmark and our newly-released\\ndataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.\\nOur method is also able to improve robotic bin-picking success by 19.54%,\\ndemonstrating the potential of iterative sim-to-real solutions for robotic\\napplications.',\n",
       "    'author': [{'name': 'Kai Chen'},\n",
       "     {'name': 'Rui Cao'},\n",
       "     {'name': 'Stephen James'},\n",
       "     {'name': 'Yichuan Li'},\n",
       "     {'name': 'Yun-Hui Liu'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Qi Dou'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ECCV 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.07049v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.07049v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.09616v2',\n",
       "    'updated': '2022-10-25T14:17:27Z',\n",
       "    'published': '2022-04-20T16:51:07Z',\n",
       "    'title': 'Assembly Planning from Observations under Physical Constraints',\n",
       "    'summary': 'This paper addresses the problem of copying an unknown assembly of primitives\\nwith known shape and appearance using information extracted from a single\\nphotograph by an off-the-shelf procedure for object detection and pose\\nestimation. The proposed algorithm uses a simple combination of physical\\nstability constraints, convex optimization and Monte Carlo tree search to plan\\nassemblies as sequences of pick-and-place operations represented by STRIPS\\noperators. It is efficient and, most importantly, robust to the errors in\\nobject detection and pose estimation unavoidable in any real robotic system.\\nThe proposed approach is demonstrated with thorough experiments on a UR5\\nmanipulator.',\n",
       "    'author': [{'name': 'Thomas Chabal'},\n",
       "     {'name': 'Robin Strudel'},\n",
       "     {'name': 'Etienne Arlaud'},\n",
       "     {'name': 'Jean Ponce'},\n",
       "     {'name': 'Cordelia Schmid'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IROS 2022. See the project webpage at\\n  https://www.di.ens.fr/willow/research/assembly-planning/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.09616v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.09616v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.13590v1',\n",
       "    'updated': '2022-04-28T16:00:39Z',\n",
       "    'published': '2022-04-28T16:00:39Z',\n",
       "    'title': 'Computer Vision for Road Imaging and Pothole Detection: A\\n  State-of-the-Art Review of Systems and Algorithms',\n",
       "    'summary': 'Computer vision algorithms have been prevalently utilized for 3-D road\\nimaging and pothole detection for over two decades. Nonetheless, there is a\\nlack of systematic survey articles on state-of-the-art (SoTA) computer vision\\ntechniques, especially deep learning models, developed to tackle these\\nproblems. This article first introduces the sensing systems employed for 2-D\\nand 3-D road data acquisition, including camera(s), laser scanners, and\\nMicrosoft Kinect. Afterward, it thoroughly and comprehensively reviews the SoTA\\ncomputer vision algorithms, including (1) classical 2-D image processing, (2)\\n3-D point cloud modeling and segmentation, and (3) machine/deep learning,\\ndeveloped for road pothole detection. This article also discusses the existing\\nchallenges and future development trends of computer vision-based road pothole\\ndetection approaches: classical 2-D image processing-based and 3-D point cloud\\nmodeling and segmentation-based approaches have already become history; and\\nConvolutional neural networks (CNNs) have demonstrated compelling road pothole\\ndetection results and are promising to break the bottleneck with the future\\nadvances in self/un-supervised learning for multi-modal semantic segmentation.\\nWe believe that this survey can serve as practical guidance for developing the\\nnext-generation road condition assessment systems.',\n",
       "    'author': [{'name': 'Nachuan Ma'},\n",
       "     {'name': 'Jiahe Fan'},\n",
       "     {'name': 'Wenshuo Wang'},\n",
       "     {'name': 'Jin Wu'},\n",
       "     {'name': 'Yu Jiang'},\n",
       "     {'name': 'Lihua Xie'},\n",
       "     {'name': 'Rui Fan'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1093/tse/tdac026'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1093/tse/tdac026',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2204.13590v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.13590v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'accepted to Transportation Safety and Environment'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.13678v1',\n",
       "    'updated': '2022-04-28T17:40:44Z',\n",
       "    'published': '2022-04-28T17:40:44Z',\n",
       "    'title': 'Unified Simulation, Perception, and Generation of Human Behavior',\n",
       "    'summary': 'Understanding and modeling human behavior is fundamental to almost any\\ncomputer vision and robotics applications that involve humans. In this thesis,\\nwe take a holistic approach to human behavior modeling and tackle its three\\nessential aspects -- simulation, perception, and generation. Throughout the\\nthesis, we show how the three aspects are deeply connected and how utilizing\\nand improving one aspect can greatly benefit the other aspects. We also discuss\\nthe lessons learned and our vision for what is next for human behavior\\nmodeling.',\n",
       "    'author': {'name': 'Ye Yuan'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'PhD thesis'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.13678v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.13678v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.01089v1',\n",
       "    'updated': '2022-05-02T17:59:13Z',\n",
       "    'published': '2022-05-02T17:59:13Z',\n",
       "    'title': 'ComPhy: Compositional Physical Reasoning of Objects and Events from\\n  Videos',\n",
       "    'summary': \"Objects' motions in nature are governed by complex interactions and their\\nproperties. While some properties, such as shape and material, can be\\nidentified via the object's visual appearances, others like mass and electric\\ncharge are not directly visible. The compositionality between the visible and\\nhidden properties poses unique challenges for AI models to reason from the\\nphysical world, whereas humans can effortlessly infer them with limited\\nobservations. Existing studies on video reasoning mainly focus on visually\\nobservable elements such as object appearance, movement, and contact\\ninteraction. In this paper, we take an initial step to highlight the importance\\nof inferring the hidden physical properties not directly observable from visual\\nappearances, by introducing the Compositional Physical Reasoning (ComPhy)\\ndataset. For a given set of objects, ComPhy includes few videos of them moving\\nand interacting under different initial conditions. The model is evaluated\\nbased on its capability to unravel the compositional hidden properties, such as\\nmass and charge, and use this knowledge to answer a set of questions posted on\\none of the videos. Evaluation results of several state-of-the-art video\\nreasoning models on ComPhy show unsatisfactory performance as they fail to\\ncapture these hidden properties. We further propose an oracle neural-symbolic\\nframework named Compositional Physics Learner (CPL), combining visual\\nperception, physical property learning, dynamic prediction, and symbolic\\nexecution into a unified framework. CPL can effectively identify objects'\\nphysical properties from their interactions and predict their dynamics to\\nanswer questions.\",\n",
       "    'author': [{'name': 'Zhenfang Chen'},\n",
       "     {'name': 'Kexin Yi'},\n",
       "     {'name': 'Yunzhu Li'},\n",
       "     {'name': 'Mingyu Ding'},\n",
       "     {'name': 'Antonio Torralba'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Chuang Gan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICLR 2022. Project page: https://comphyreasoning.github.io/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.01089v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.01089v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.08129v1',\n",
       "    'updated': '2022-05-17T06:58:17Z',\n",
       "    'published': '2022-05-17T06:58:17Z',\n",
       "    'title': 'Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in\\n  Latent Space',\n",
       "    'summary': 'General-purpose robots require diverse repertoires of behaviors to complete\\nchallenging tasks in real-world unstructured environments. To address this\\nissue, goal-conditioned reinforcement learning aims to acquire policies that\\ncan reach configurable goals for a wide range of tasks on command. However,\\nsuch goal-conditioned policies are notoriously difficult and time-consuming to\\ntrain from scratch. In this paper, we propose Planning to Practice (PTP), a\\nmethod that makes it practical to train goal-conditioned policies for\\nlong-horizon tasks that require multiple distinct types of interactions to\\nsolve. Our approach is based on two key ideas. First, we decompose the\\ngoal-reaching problem hierarchically, with a high-level planner that sets\\nintermediate subgoals using conditional subgoal generators in the latent space\\nfor a low-level model-free policy. Second, we propose a hybrid approach which\\nfirst pre-trains both the conditional subgoal generator and the policy on\\npreviously collected data through offline reinforcement learning, and then\\nfine-tunes the policy via online exploration. This fine-tuning process is\\nitself facilitated by the planned subgoals, which breaks down the original\\ntarget task into short-horizon goal-reaching tasks that are significantly\\neasier to learn. We conduct experiments in both the simulation and real world,\\nin which the policy is pre-trained on demonstrations of short primitive\\nbehaviors and fine-tuned for temporally extended tasks that are unseen in the\\noffline data. Our experimental results show that PTP can generate feasible\\nsequences of subgoals that enable the policy to efficiently solve the target\\ntasks.',\n",
       "    'author': [{'name': 'Kuan Fang'},\n",
       "     {'name': 'Patrick Yin'},\n",
       "     {'name': 'Ashvin Nair'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.08129v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.08129v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.03271v1',\n",
       "    'updated': '2022-06-07T13:24:00Z',\n",
       "    'published': '2022-06-07T13:24:00Z',\n",
       "    'title': 'On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning',\n",
       "    'summary': 'Intelligent agents should have the ability to leverage knowledge from\\npreviously learned tasks in order to learn new ones quickly and efficiently.\\nMeta-learning approaches have emerged as a popular solution to achieve this.\\nHowever, meta-reinforcement learning (meta-RL) algorithms have thus far been\\nrestricted to simple environments with narrow task distributions. Moreover, the\\nparadigm of pretraining followed by fine-tuning to adapt to new tasks has\\nemerged as a simple yet effective solution in supervised and self-supervised\\nlearning. This calls into question the benefits of meta-learning approaches\\nalso in reinforcement learning, which typically come at the cost of high\\ncomplexity. We hence investigate meta-RL approaches in a variety of\\nvision-based benchmarks, including Procgen, RLBench, and Atari, where\\nevaluations are made on completely novel tasks. Our findings show that when\\nmeta-learning approaches are evaluated on different tasks (rather than\\ndifferent variations of the same task), multi-task pretraining with fine-tuning\\non new tasks performs equally as well, or better, than meta-pretraining with\\nmeta test-time adaptation. This is encouraging for future research, as\\nmulti-task pretraining tends to be simpler and computationally cheaper than\\nmeta-RL. From these findings, we advocate for evaluating future meta-RL methods\\non more challenging tasks and including multi-task pretraining with fine-tuning\\nas a simple, yet strong baseline.',\n",
       "    'author': [{'name': 'Zhao Mandi'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Stephen James'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.03271v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.03271v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.13499v1',\n",
       "    'updated': '2022-06-27T17:59:17Z',\n",
       "    'published': '2022-06-27T17:59:17Z',\n",
       "    'title': 'Prompting Decision Transformer for Few-Shot Policy Generalization',\n",
       "    'summary': 'Humans can leverage prior experience and learn novel tasks from a handful of\\ndemonstrations. In contrast to offline meta-reinforcement learning, which aims\\nto achieve quick adaptation through better algorithm design, we investigate the\\neffect of architecture inductive bias on the few-shot learning capability. We\\npropose a Prompt-based Decision Transformer (Prompt-DT), which leverages the\\nsequential modeling ability of the Transformer architecture and the prompt\\nframework to achieve few-shot adaptation in offline RL. We design the\\ntrajectory prompt, which contains segments of the few-shot demonstrations, and\\nencodes task-specific information to guide policy generation. Our experiments\\nin five MuJoCo control benchmarks show that Prompt-DT is a strong few-shot\\nlearner without any extra finetuning on unseen target tasks. Prompt-DT\\noutperforms its variants and strong meta offline RL baselines by a large margin\\nwith a trajectory prompt containing only a few timesteps. Prompt-DT is also\\nrobust to prompt length changes and can generalize to out-of-distribution (OOD)\\nenvironments.',\n",
       "    'author': [{'name': 'Mengdi Xu'},\n",
       "     {'name': 'Yikang Shen'},\n",
       "     {'name': 'Shun Zhang'},\n",
       "     {'name': 'Yuchen Lu'},\n",
       "     {'name': 'Ding Zhao'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Chuang Gan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2022. Project page: https://mxu34.github.io/PromptDT/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.13499v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.13499v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.14244v2',\n",
       "    'updated': '2022-11-15T05:13:34Z',\n",
       "    'published': '2022-06-28T18:42:27Z',\n",
       "    'title': 'Masked World Models for Visual Control',\n",
       "    'summary': 'Visual model-based reinforcement learning (RL) has the potential to enable\\nsample-efficient robot learning from visual observations. Yet the current\\napproaches typically train a single model end-to-end for learning both visual\\nrepresentations and dynamics, making it difficult to accurately model the\\ninteraction between robots and small objects. In this work, we introduce a\\nvisual model-based RL framework that decouples visual representation learning\\nand dynamics learning. Specifically, we train an autoencoder with convolutional\\nlayers and vision transformers (ViT) to reconstruct pixels given masked\\nconvolutional features, and learn a latent dynamics model that operates on the\\nrepresentations from the autoencoder. Moreover, to encode task-relevant\\ninformation, we introduce an auxiliary reward prediction objective for the\\nautoencoder. We continually update both autoencoder and dynamics model using\\nonline samples collected from environment interaction. We demonstrate that our\\ndecoupling approach achieves state-of-the-art performance on a variety of\\nvisual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7%\\nsuccess rate on 50 visual robotic manipulation tasks from Meta-world, while the\\nbaseline achieves 67.9%. Code is available on the project website:\\nhttps://sites.google.com/view/mwm-rl.',\n",
       "    'author': [{'name': 'Younggyo Seo'},\n",
       "     {'name': 'Danijar Hafner'},\n",
       "     {'name': 'Hao Liu'},\n",
       "     {'name': 'Fangchen Liu'},\n",
       "     {'name': 'Stephen James'},\n",
       "     {'name': 'Kimin Lee'},\n",
       "     {'name': 'Pieter Abbeel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project website: https://sites.google.com/view/mwm-rl. Accepted to\\n  CoRL 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.14244v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.14244v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.15469v1',\n",
       "    'updated': '2022-06-30T17:58:18Z',\n",
       "    'published': '2022-06-30T17:58:18Z',\n",
       "    'title': 'Watch and Match: Supercharging Imitation with Regularized Optimal\\n  Transport',\n",
       "    'summary': 'Imitation learning holds tremendous promise in learning policies efficiently\\nfor complex decision making problems. Current state-of-the-art algorithms often\\nuse inverse reinforcement learning (IRL), where given a set of expert\\ndemonstrations, an agent alternatively infers a reward function and the\\nassociated optimal policy. However, such IRL approaches often require\\nsubstantial online interactions for complex control problems. In this work, we\\npresent Regularized Optimal Transport (ROT), a new imitation learning algorithm\\nthat builds on recent advances in optimal transport based trajectory-matching.\\nOur key technical insight is that adaptively combining trajectory-matching\\nrewards with behavior cloning can significantly accelerate imitation even with\\nonly a few demonstrations. Our experiments on 20 visual control tasks across\\nthe DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World\\nBenchmark demonstrate an average of 7.8X faster imitation to reach 90% of\\nexpert performance compared to prior state-of-the-art methods. On real-world\\nrobotic manipulation, with just one demonstration and an hour of online\\ntraining, ROT achieves an average success rate of 90.1% across 14 tasks.',\n",
       "    'author': [{'name': 'Siddhant Haldar'},\n",
       "     {'name': 'Vaibhav Mathur'},\n",
       "     {'name': 'Denis Yarats'},\n",
       "     {'name': 'Lerrel Pinto'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Robot videos are best viewed on https://rot-robot.github.io/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.15469v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.15469v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.07418v1',\n",
       "    'updated': '2022-07-15T11:57:14Z',\n",
       "    'published': '2022-07-15T11:57:14Z',\n",
       "    'title': 'LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds\\n  Representing Laparoscopic Scenes',\n",
       "    'summary': 'The semantic segmentation of surgical scenes is a prerequisite for task\\nautomation in robot assisted interventions. We propose LapSeg3D, a novel\\nDNN-based approach for the voxel-wise annotation of point clouds representing\\nsurgical scenes. As the manual annotation of training data is highly time\\nconsuming, we introduce a semi-autonomous clustering-based pipeline for the\\nannotation of the gallbladder, which is used to generate segmented labels for\\nthe DNN. When evaluated against manually annotated data, LapSeg3D achieves an\\nF1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo\\nporcine livers. We show LapSeg3D to generalize accurately across different\\ngallbladders and datasets recorded with different RGB-D camera systems.',\n",
       "    'author': [{'name': 'Benjamin Alt'},\n",
       "     {'name': 'Christian Kunz'},\n",
       "     {'name': 'Darko Katic'},\n",
       "     {'name': 'Rayan Younis'},\n",
       "     {'name': 'Rainer Jäkel'},\n",
       "     {'name': 'Beat Peter Müller-Stich'},\n",
       "     {'name': 'Martin Wagner'},\n",
       "     {'name': 'Franziska Mathis-Ullrich'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 5 figures, accepted at the 2022 IEEE/RSJ International\\n  Conference on Intelligent Robots and Systems (IROS 2022), Kyoto, Japan'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.07418v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.07418v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T42 (Primary), 68T40 (Secondary)',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2; I.4; J.3', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.10082v1',\n",
       "    'updated': '2022-07-20T07:26:36Z',\n",
       "    'published': '2022-07-20T07:26:36Z',\n",
       "    'title': 'Model Compression for Resource-Constrained Mobile Robots',\n",
       "    'summary': \"The number of mobile robots with constrained computing resources that need to\\nexecute complex machine learning models has been increasing during the past\\ndecade. Commonly, these robots rely on edge infrastructure accessible over\\nwireless communication to execute heavy computational complex tasks. However,\\nthe edge might become unavailable and, consequently, oblige the execution of\\nthe tasks on the robot. This work focuses on making it possible to execute the\\ntasks on the robots by reducing the complexity and the total number of\\nparameters of pre-trained computer vision models. This is achieved by using\\nmodel compression techniques such as Pruning and Knowledge Distillation. These\\ncompression techniques have strong theoretical and practical foundations, but\\ntheir combined usage has not been widely explored in the literature. Therefore,\\nthis work especially focuses on investigating the effects of combining these\\ntwo compression techniques. The results of this work reveal that up to 90% of\\nthe total number of parameters of a computer vision model can be removed\\nwithout any considerable reduction in the model's accuracy.\",\n",
       "    'author': [{'name': 'Timotheos Souroulla',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Ericsson Research AI'}},\n",
       "     {'name': 'Alberto Hata',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Ericsson Research AI'}},\n",
       "     {'name': 'Ahmad Terra',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Ericsson Research AI'}},\n",
       "     {'name': 'Özer Özkahraman',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'KTH, Royal Institute of Technology'}},\n",
       "     {'name': 'Rafia Inam',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Ericsson Research AI'}}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.4204/EPTCS.362.7'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.4204/EPTCS.362.7',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2207.10082v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.10082v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In Proceedings AREA 2022, arXiv:2207.09058'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'EPTCS 362, 2022, pp. 54-64'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.00113v1',\n",
       "    'updated': '2022-07-30T01:48:23Z',\n",
       "    'published': '2022-07-30T01:48:23Z',\n",
       "    'title': 'Neural Correspondence Field for Object Pose Estimation',\n",
       "    'summary': 'We propose a method for estimating the 6DoF pose of a rigid object with an\\navailable 3D model from a single RGB image. Unlike classical\\ncorrespondence-based methods which predict 3D object coordinates at pixels of\\nthe input image, the proposed method predicts 3D object coordinates at 3D query\\npoints sampled in the camera frustum. The move from pixels to 3D points, which\\nis inspired by recent PIFu-style methods for 3D reconstruction, enables\\nreasoning about the whole object, including its (self-)occluded parts. For a 3D\\nquery point associated with a pixel-aligned image feature, we train a\\nfully-connected neural network to predict: (i) the corresponding 3D object\\ncoordinates, and (ii) the signed distance to the object surface, with the first\\ndefined only for query points in the surface vicinity. We call the mapping\\nrealized by this network as Neural Correspondence Field. The object pose is\\nthen robustly estimated from the predicted 3D-3D correspondences by the\\nKabsch-RANSAC algorithm. The proposed method achieves state-of-the-art results\\non three BOP datasets and is shown superior especially in challenging cases\\nwith occlusion. The project website is at: linhuang17.github.io/NCF.',\n",
       "    'author': [{'name': 'Lin Huang'},\n",
       "     {'name': 'Tomas Hodan'},\n",
       "     {'name': 'Lingni Ma'},\n",
       "     {'name': 'Linguang Zhang'},\n",
       "     {'name': 'Luan Tran'},\n",
       "     {'name': 'Christopher Twigg'},\n",
       "     {'name': 'Po-Chen Wu'},\n",
       "     {'name': 'Junsong Yuan'},\n",
       "     {'name': 'Cem Keskin'},\n",
       "     {'name': 'Robert Wang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ECCV 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.00113v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.00113v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.11079v2',\n",
       "    'updated': '2022-08-24T00:52:09Z',\n",
       "    'published': '2022-08-23T16:56:54Z',\n",
       "    'title': 'Robot Active Neural Sensing and Planning in Unknown Cluttered\\n  Environments',\n",
       "    'summary': 'Active sensing and planning in unknown, cluttered environments is an open\\nchallenge for robots intending to provide home service, search and rescue,\\nnarrow-passage inspection, and medical assistance. Although many active sensing\\nmethods exist, they often consider open spaces, assume known settings, or\\nmostly do not generalize to real-world scenarios. We present the active neural\\nsensing approach that generates the kinematically feasible viewpoint sequences\\nfor the robot manipulator with an in-hand camera to gather the minimum number\\nof observations needed to reconstruct the underlying environment. Our framework\\nactively collects the visual RGBD observations, aggregates them into scene\\nrepresentation, and performs object shape inference to avoid unnecessary robot\\ninteractions with the environment. We train our approach on synthetic data with\\ndomain randomization and demonstrate its successful execution via sim-to-real\\ntransfer in reconstructing narrow, covered, real-world cabinet environments\\ncluttered with unknown objects. The natural cabinet scenarios impose\\nsignificant challenges for robot motion and scene reconstruction due to\\nsurrounding obstacles and low ambient lighting conditions. However, despite\\nunfavorable settings, our method exhibits high performance compared to its\\nbaselines in terms of various environment reconstruction metrics, including\\nplanning speed, the number of viewpoints, and overall scene coverage.',\n",
       "    'author': [{'name': 'Hanwen Ren'}, {'name': 'Ahmed H. Qureshi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'https://sites.google.com/view/active-neural-sensing/home'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.11079v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.11079v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.09097v1',\n",
       "    'updated': '2022-09-16T12:53:49Z',\n",
       "    'published': '2022-09-16T12:53:49Z',\n",
       "    'title': 'Disentangling Shape and Pose for Object-Centric Deep Active Inference\\n  Models',\n",
       "    'summary': \"Active inference is a first principles approach for understanding the brain\\nin particular, and sentient agents in general, with the single imperative of\\nminimizing free energy. As such, it provides a computational account for\\nmodelling artificial intelligent agents, by defining the agent's generative\\nmodel and inferring the model parameters, actions and hidden state beliefs.\\nHowever, the exact specification of the generative model and the hidden state\\nspace structure is left to the experimenter, whose design choices influence the\\nresulting behaviour of the agent. Recently, deep learning methods have been\\nproposed to learn a hidden state space structure purely from data, alleviating\\nthe experimenter from this tedious design task, but resulting in an entangled,\\nnon-interpreteable state space. In this paper, we hypothesize that such a\\nlearnt, entangled state space does not necessarily yield the best model in\\nterms of free energy, and that enforcing different factors in the state space\\ncan yield a lower model complexity. In particular, we consider the problem of\\n3D object representation, and focus on different instances of the ShapeNet\\ndataset. We propose a model that factorizes object shape, pose and category,\\nwhile still learning a representation for each factor using a deep neural\\nnetwork. We show that models, with best disentanglement properties, perform\\nbest when adopted by an active agent in reaching preferred observations.\",\n",
       "    'author': [{'name': 'Stefano Ferraro'},\n",
       "     {'name': 'Toon Van de Maele'},\n",
       "     {'name': 'Pietro Mazzaglia'},\n",
       "     {'name': 'Tim Verbelen'},\n",
       "     {'name': 'Bart Dhoedt'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.09097v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.09097v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.09659v1',\n",
       "    'updated': '2022-09-20T11:59:05Z',\n",
       "    'published': '2022-09-20T11:59:05Z',\n",
       "    'title': 'Ki-Pode: Keypoint-based Implicit Pose Distribution Estimation of Rigid\\n  Objects',\n",
       "    'summary': 'The estimation of 6D poses of rigid objects is a fundamental problem in\\ncomputer vision. Traditionally pose estimation is concerned with the\\ndetermination of a single best estimate. However, a single estimate is unable\\nto express visual ambiguity, which in many cases is unavoidable due to object\\nsymmetries or occlusion of identifying features. Inability to account for\\nambiguities in pose can lead to failure in subsequent methods, which is\\nunacceptable when the cost of failure is high. Estimates of full pose\\ndistributions are, contrary to single estimates, well suited for expressing\\nuncertainty on pose. Motivated by this, we propose a novel pose distribution\\nestimation method. An implicit formulation of the probability distribution over\\nobject pose is derived from an intermediary representation of an object as a\\nset of keypoints. This ensures that the pose distribution estimates have a high\\nlevel of interpretability. Furthermore, our method is based on conservative\\napproximations, which leads to reliable estimates. The method has been\\nevaluated on the task of rotation distribution estimation on the YCB-V and\\nT-LESS datasets and performs reliably on all objects.',\n",
       "    'author': [{'name': 'Thorbjørn Mosekjær Iversen'},\n",
       "     {'name': 'Rasmus Laurvig Haugaard'},\n",
       "     {'name': 'Anders Glent Buch'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages, 2 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.09659v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.09659v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.11693v1',\n",
       "    'updated': '2022-09-19T15:01:09Z',\n",
       "    'published': '2022-09-19T15:01:09Z',\n",
       "    'title': 'T3VIP: Transformation-based 3D Video Prediction',\n",
       "    'summary': 'For autonomous skill acquisition, robots have to learn about the physical\\nrules governing the 3D world dynamics from their own past experience to predict\\nand reason about plausible future outcomes. To this end, we propose a\\ntransformation-based 3D video prediction (T3VIP) approach that explicitly\\nmodels the 3D motion by decomposing a scene into its object parts and\\npredicting their corresponding rigid transformations. Our model is fully\\nunsupervised, captures the stochastic nature of the real world, and the\\nobservational cues in image and point cloud domains constitute its learning\\nsignals. To fully leverage all the 2D and 3D observational signals, we equip\\nour model with automatic hyperparameter optimization (HPO) to interpret the\\nbest way of learning from them. To the best of our knowledge, our model is the\\nfirst generative model that provides an RGB-D video prediction of the future\\nfor a static camera. Our extensive evaluation with simulated and real-world\\ndatasets demonstrates that our formulation leads to interpretable 3D models\\nthat predict future depth videos while achieving on-par performance with 2D\\nmodels on RGB video prediction. Moreover, we demonstrate that our model\\noutperforms 2D baselines on visuomotor control. Videos, code, dataset, and\\npre-trained models are available at http://t3vip.cs.uni-freiburg.de.',\n",
       "    'author': [{'name': 'Iman Nematollahi'},\n",
       "     {'name': 'Erick Rosete-Beas'},\n",
       "     {'name': 'Seyed Mahdi B. Azad'},\n",
       "     {'name': 'Raghu Rajan'},\n",
       "     {'name': 'Frank Hutter'},\n",
       "     {'name': 'Wolfram Burgard'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at the 2022 IEEE/RSJ International Conference on Intelligent\\n  Robots and Systems (IROS)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.11693v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.11693v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.12729v2',\n",
       "    'updated': '2022-09-27T09:11:34Z',\n",
       "    'published': '2022-09-26T14:33:30Z',\n",
       "    'title': 'DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras\\n  and Radars',\n",
       "    'summary': \"We propose DeepFusion, a modular multi-modal architecture to fuse lidars,\\ncameras and radars in different combinations for 3D object detection.\\nSpecialized feature extractors take advantage of each modality and can be\\nexchanged easily, making the approach simple and flexible. Extracted features\\nare transformed into bird's-eye-view as a common representation for fusion.\\nSpatial and semantic alignment is performed prior to fusing modalities in the\\nfeature space. Finally, a detection head exploits rich multi-modal features for\\nimproved 3D detection performance. Experimental results for lidar-camera,\\nlidar-camera-radar and camera-radar fusion show the flexibility and\\neffectiveness of our fusion approach. In the process, we study the largely\\nunexplored task of faraway car detection up to 225 meters, showing the benefits\\nof our lidar-camera fusion. Furthermore, we investigate the required density of\\nlidar points for 3D object detection and illustrate implications at the example\\nof robustness against adverse weather conditions. Moreover, ablation studies on\\nour camera-radar fusion highlight the importance of accurate depth estimation.\",\n",
       "    'author': [{'name': 'Florian Drews'},\n",
       "     {'name': 'Di Feng'},\n",
       "     {'name': 'Florian Faion'},\n",
       "     {'name': 'Lars Rosenbaum'},\n",
       "     {'name': 'Michael Ulrich'},\n",
       "     {'name': 'Claudius Gläser'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.12729v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.12729v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.02443v1',\n",
       "    'updated': '2022-10-05T17:59:51Z',\n",
       "    'published': '2022-10-05T17:59:51Z',\n",
       "    'title': 'Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D\\n  Object Detection',\n",
       "    'summary': \"While recent camera-only 3D detection methods leverage multiple timesteps,\\nthe limited history they use significantly hampers the extent to which temporal\\nfusion can improve object perception. Observing that existing works' fusion of\\nmulti-frame images are instances of temporal stereo matching, we find that\\nperformance is hindered by the interplay between 1) the low granularity of\\nmatching resolution and 2) the sub-optimal multi-view setup produced by limited\\nhistory usage. Our theoretical and empirical analysis demonstrates that the\\noptimal temporal difference between views varies significantly for different\\npixels and depths, making it necessary to fuse many timesteps over long-term\\nhistory. Building on our investigation, we propose to generate a cost volume\\nfrom a long history of image observations, compensating for the coarse but\\nefficient matching resolution with a more optimal multi-view matching setup.\\nFurther, we augment the per-frame monocular depth predictions used for\\nlong-term, coarse matching with short-term, fine-grained matching and find that\\nlong and short term temporal fusion are highly complementary. While maintaining\\nhigh efficiency, our framework sets new state-of-the-art on nuScenes, achieving\\nfirst place on the test set and outperforming previous best art by 5.2% mAP and\\n3.7% NDS on the validation set. Code will be released\\n$\\\\href{https://github.com/Divadi/SOLOFusion}{here.}$\",\n",
       "    'author': [{'name': 'Jinhyung Park'},\n",
       "     {'name': 'Chenfeng Xu'},\n",
       "     {'name': 'Shijia Yang'},\n",
       "     {'name': 'Kurt Keutzer'},\n",
       "     {'name': 'Kris Kitani'},\n",
       "     {'name': 'Masayoshi Tomizuka'},\n",
       "     {'name': 'Wei Zhan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Code will be released at https://github.com/Divadi/SOLOFusion'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.02443v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.02443v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.05064v1',\n",
       "    'updated': '2022-10-11T00:27:02Z',\n",
       "    'published': '2022-10-11T00:27:02Z',\n",
       "    'title': 'VER: Scaling On-Policy RL Leads to the Emergence of Navigation in\\n  Embodied Rearrangement',\n",
       "    'summary': 'We present Variable Experience Rollout (VER), a technique for efficiently\\nscaling batched on-policy reinforcement learning in heterogenous environments\\n(where different environments take vastly different times to generate rollouts)\\nto many GPUs residing on, potentially, many machines. VER combines the\\nstrengths of and blurs the line between synchronous and asynchronous on-policy\\nRL methods (SyncOnRL and AsyncOnRL, respectively). VER learns from on-policy\\nexperience (like SyncOnRL) and has no synchronization points (like AsyncOnRL).\\n  VER leads to significant and consistent speed-ups across a broad range of\\nembodied navigation and mobile manipulation tasks in photorealistic 3D\\nsimulation environments. Specifically, for PointGoal navigation and ObjectGoal\\nnavigation in Habitat 1.0, VER is 60-100% faster (1.6-2x speedup) than DD-PPO,\\nthe current state of art distributed SyncOnRL, with similar sample efficiency.\\nFor mobile manipulation tasks (open fridge/cabinet, pick/place objects) in\\nHabitat 2.0 VER is 150% faster (2.5x speedup) on 1 GPU and 170% faster (2.7x\\nspeedup) on 8 GPUs than DD-PPO. Compared to SampleFactory (the current\\nstate-of-the-art AsyncOnRL), VER matches its speed on 1 GPU, and is 70% faster\\n(1.7x speedup) on 8 GPUs with better sample efficiency.\\n  We leverage these speed-ups to train chained skills for GeometricGoal\\nrearrangement tasks in the Home Assistant Benchmark (HAB). We find a surprising\\nemergence of navigation in skills that do not ostensible require any\\nnavigation. Specifically, the Pick skill involves a robot picking an object\\nfrom a table. During training the robot was always spawned close to the table\\nand never needed to navigate. However, we find that if base movement is part of\\nthe action space, the robot learns to navigate then pick an object in new\\nenvironments with 50% success, demonstrating surprisingly high\\nout-of-distribution generalization.',\n",
       "    'author': [{'name': 'Erik Wijmans'},\n",
       "     {'name': 'Irfan Essa'},\n",
       "     {'name': 'Dhruv Batra'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at NeurIPS 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.05064v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.05064v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.09846v1',\n",
       "    'updated': '2022-10-15T11:00:54Z',\n",
       "    'published': '2022-10-15T11:00:54Z',\n",
       "    'title': 'Analyzing the Robustness of PECNet',\n",
       "    'summary': 'Comprehensive robustness analysis of PECNet, a pedestrian trajectory\\nprediction system for autonomous vehicles. A novel metric is introduced for\\ndataset analysis and classification. Synthetic data augmentation techniques\\nranging from Newtonian mechanics to Deep Reinforcement Learning based\\nsimulations are used to improve and test the system. An improvement of 9.5%\\nover state-of-the-art results is seen on the FDE while compromising ADE. We\\nintroduce novel architectural changes using SIRENs for higher precision results\\nto validate our robustness hypotheses. Additionally, we diagrammatically\\npropose a novel multi-modal system for the same task.',\n",
       "    'author': [{'name': 'Aryan Garg'}, {'name': 'Renu M. Rameshan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '13 pages, 17 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.09846v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.09846v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.10947v1',\n",
       "    'updated': '2022-10-20T01:32:41Z',\n",
       "    'published': '2022-10-20T01:32:41Z',\n",
       "    'title': 'Does Decentralized Learning with Non-IID Unlabeled Data Benefit from\\n  Self Supervision?',\n",
       "    'summary': 'Decentralized learning has been advocated and widely deployed to make\\nefficient use of distributed datasets, with an extensive focus on supervised\\nlearning (SL) problems. Unfortunately, the majority of real-world data are\\nunlabeled and can be highly heterogeneous across sources. In this work, we\\ncarefully study decentralized learning with unlabeled data through the lens of\\nself-supervised learning (SSL), specifically contrastive visual representation\\nlearning. We study the effectiveness of a range of contrastive learning\\nalgorithms under decentralized learning settings, on relatively large-scale\\ndatasets including ImageNet-100, MS-COCO, and a new real-world robotic\\nwarehouse dataset. Our experiments show that the decentralized SSL (Dec-SSL)\\napproach is robust to the heterogeneity of decentralized datasets, and learns\\nuseful representation for object classification, detection, and segmentation\\ntasks. This robustness makes it possible to significantly reduce communication\\nand reduce the participation ratio of data sources with only minimal drops in\\nperformance. Interestingly, using the same amount of data, the representation\\nlearned by Dec-SSL can not only perform on par with that learned by centralized\\nSSL which requires communication and excessive data storage costs, but also\\nsometimes outperform representations extracted from decentralized SL which\\nrequires extra knowledge about the data labels. Finally, we provide theoretical\\ninsights into understanding why data heterogeneity is less of a concern for\\nDec-SSL objectives, and introduce feature alignment and clustering techniques\\nto develop a new Dec-SSL algorithm that further improves the performance, in\\nthe face of highly non-IID data. Our study presents positive evidence to\\nembrace unlabeled data in decentralized learning, and we hope to provide new\\ninsights into whether and why decentralized SSL is effective.',\n",
       "    'author': [{'name': 'Lirui Wang'},\n",
       "     {'name': 'Kaiqing Zhang'},\n",
       "     {'name': 'Yunzhu Li'},\n",
       "     {'name': 'Yonglong Tian'},\n",
       "     {'name': 'Russ Tedrake'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.10947v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.10947v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.14222v1',\n",
       "    'updated': '2022-10-25T17:59:46Z',\n",
       "    'published': '2022-10-25T17:59:46Z',\n",
       "    'title': 'PlanT: Explainable Planning Transformers via Object-Level\\n  Representations',\n",
       "    'summary': 'Planning an optimal route in a complex environment requires efficient\\nreasoning about the surrounding scene. While human drivers prioritize important\\nobjects and ignore details not relevant to the decision, learning-based\\nplanners typically extract features from dense, high-dimensional grid\\nrepresentations containing all vehicle and road context information. In this\\npaper, we propose PlanT, a novel approach for planning in the context of\\nself-driving that uses a standard transformer architecture. PlanT is based on\\nimitation learning with a compact object-level input representation. On the\\nLongest6 benchmark for CARLA, PlanT outperforms all prior methods (matching the\\ndriving score of the expert) while being 5.3x faster than equivalent\\npixel-based planning baselines during inference. Combining PlanT with an\\noff-the-shelf perception module provides a sensor-based driving system that is\\nmore than 10 points better in terms of driving score than the existing state of\\nthe art. Furthermore, we propose an evaluation protocol to quantify the ability\\nof planners to identify relevant objects, providing insights regarding their\\ndecision-making. Our results indicate that PlanT can focus on the most relevant\\nobject in the scene, even when this object is geometrically distant.',\n",
       "    'author': [{'name': 'Katrin Renz'},\n",
       "     {'name': 'Kashyap Chitta'},\n",
       "     {'name': 'Otniel-Bogdan Mercea'},\n",
       "     {'name': 'A. Sophia Koepke'},\n",
       "     {'name': 'Zeynep Akata'},\n",
       "     {'name': 'Andreas Geiger'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2022. Project Page: https://www.katrinrenz.de/plant/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.14222v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.14222v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.15185v1',\n",
       "    'updated': '2022-10-27T05:30:43Z',\n",
       "    'published': '2022-10-27T05:30:43Z',\n",
       "    'title': 'SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via\\n  Differentiable Physics-Based Simulation and Rendering',\n",
       "    'summary': 'Model-based reinforcement learning (MBRL) is recognized with the potential to\\nbe significantly more sample efficient than model-free RL. How an accurate\\nmodel can be developed automatically and efficiently from raw sensory inputs\\n(such as images), especially for complex environments and tasks, is a\\nchallenging problem that hinders the broad application of MBRL in the real\\nworld. In this work, we propose a sensing-aware model-based reinforcement\\nlearning system called SAM-RL. Leveraging the differentiable physics-based\\nsimulation and rendering, SAM-RL automatically updates the model by comparing\\nrendered images with real raw images and produces the policy efficiently. With\\nthe sensing-aware learning pipeline, SAM-RL allows a robot to select an\\ninformative viewpoint to monitor the task process. We apply our framework to\\nreal-world experiments for accomplishing three manipulation tasks: robotic\\nassembly, tool manipulation, and deformable object manipulation. We demonstrate\\nthe effectiveness of SAM-RL via extensive experiments. Supplemental materials\\nand videos are available on our project webpage at\\nhttps://sites.google.com/view/sam-rl.',\n",
       "    'author': [{'name': 'Jun Lv'},\n",
       "     {'name': 'Yunhai Feng'},\n",
       "     {'name': 'Cheng Zhang'},\n",
       "     {'name': 'Shuang Zhao'},\n",
       "     {'name': 'Lin Shao'},\n",
       "     {'name': 'Cewu Lu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to IEEE International Conference on Robotics and Automation\\n  (ICRA) 2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.15185v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.15185v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.16567v1',\n",
       "    'updated': '2022-10-29T10:58:43Z',\n",
       "    'published': '2022-10-29T10:58:43Z',\n",
       "    'title': 'DeFIX: Detecting and Fixing Failure Scenarios with Reinforcement\\n  Learning in Imitation Learning Based Autonomous Driving',\n",
       "    'summary': 'Safely navigating through an urban environment without violating any traffic\\nrules is a crucial performance target for reliable autonomous driving. In this\\npaper, we present a Reinforcement Learning (RL) based methodology to DEtect and\\nFIX (DeFIX) failures of an Imitation Learning (IL) agent by extracting\\ninfraction spots and re-constructing mini-scenarios on these infraction areas\\nto train an RL agent for fixing the shortcomings of the IL approach. DeFIX is a\\ncontinuous learning framework, where extraction of failure scenarios and\\ntraining of RL agents are executed in an infinite loop. After each new policy\\nis trained and added to the library of policies, a policy classifier method\\neffectively decides on which policy to activate at each step during the\\nevaluation. It is demonstrated that even with only one RL agent trained on\\nfailure scenario of an IL agent, DeFIX method is either competitive or does\\noutperform state-of-the-art IL and RL based autonomous urban driving\\nbenchmarks. We trained and validated our approach on the most challenging map\\n(Town05) of CARLA simulator which involves complex, realistic, and adversarial\\ndriving scenarios. The source code is publicly available at\\nhttps://github.com/data-and-decision-lab/DeFIX',\n",
       "    'author': [{'name': 'Resul Dagdanov'},\n",
       "     {'name': 'Feyza Eksen'},\n",
       "     {'name': 'Halil Durmus'},\n",
       "     {'name': 'Ferhat Yurdakul'},\n",
       "     {'name': 'Nazim Kemal Ure'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ITSC55140.2022.9922209'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ITSC55140.2022.9922209',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2210.16567v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.16567v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 4 figures, 2 tables, published in IEEE International\\n  Conference on Intelligent Transportation Systems (ITSC), October 12, 2022,\\n  Macau, China'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2022 IEEE 25th International Conference on Intelligent\\n  Transportation Systems (ITSC), 2022, pp. 4215-4220'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.00164v1',\n",
       "    'updated': '2022-10-31T22:12:48Z',\n",
       "    'published': '2022-10-31T22:12:48Z',\n",
       "    'title': 'Agent-Controller Representations: Principled Offline RL with Rich\\n  Exogenous Information',\n",
       "    'summary': 'Learning to control an agent from data collected offline in a rich\\npixel-based visual observation space is vital for real-world applications of\\nreinforcement learning (RL). A major challenge in this setting is the presence\\nof input information that is hard to model and irrelevant to controlling the\\nagent. This problem has been approached by the theoretical RL community through\\nthe lens of exogenous information, i.e, any control-irrelevant information\\ncontained in observations. For example, a robot navigating in busy streets\\nneeds to ignore irrelevant information, such as other people walking in the\\nbackground, textures of objects, or birds in the sky. In this paper, we focus\\non the setting with visually detailed exogenous information, and introduce new\\noffline RL benchmarks offering the ability to study this problem. We find that\\ncontemporary representation learning techniques can fail on datasets where the\\nnoise is a complex and time dependent process, which is prevalent in practical\\napplications. To address these, we propose to use multi-step inverse models,\\nwhich have seen a great deal of interest in the RL theory community, to learn\\nAgent-Controller Representations for Offline-RL (ACRO). Despite being simple\\nand requiring no reward, we show theoretically and empirically that the\\nrepresentation created by this objective greatly outperforms baselines.',\n",
       "    'author': [{'name': 'Riashat Islam'},\n",
       "     {'name': 'Manan Tomar'},\n",
       "     {'name': 'Alex Lamb'},\n",
       "     {'name': 'Yonathan Efroni'},\n",
       "     {'name': 'Hongyu Zang'},\n",
       "     {'name': 'Aniket Didolkar'},\n",
       "     {'name': 'Dipendra Misra'},\n",
       "     {'name': 'Xin Li'},\n",
       "     {'name': 'Harm van Seijen'},\n",
       "     {'name': 'Remi Tachet des Combes'},\n",
       "     {'name': 'John Langford'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.00164v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.00164v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.01644v1',\n",
       "    'updated': '2022-11-03T08:36:09Z',\n",
       "    'published': '2022-11-03T08:36:09Z',\n",
       "    'title': 'StereoPose: Category-Level 6D Transparent Object Pose Estimation from\\n  Stereo Images via Back-View NOCS',\n",
       "    'summary': 'Most existing methods for category-level pose estimation rely on object point\\nclouds. However, when considering transparent objects, depth cameras are\\nusually not able to capture meaningful data, resulting in point clouds with\\nsevere artifacts. Without a high-quality point cloud, existing methods are not\\napplicable to challenging transparent objects. To tackle this problem, we\\npresent StereoPose, a novel stereo image framework for category-level object\\npose estimation, ideally suited for transparent objects. For a robust\\nestimation from pure stereo images, we develop a pipeline that decouples\\ncategory-level pose estimation into object size estimation, initial pose\\nestimation, and pose refinement. StereoPose then estimates object pose based on\\nrepresentation in the normalized object coordinate space~(NOCS). To address the\\nissue of image content aliasing, we further define a back-view NOCS map for the\\ntransparent object. The back-view NOCS aims to reduce the network learning\\nambiguity caused by content aliasing, and leverage informative cues on the back\\nof the transparent object for more accurate pose estimation. To further improve\\nthe performance of the stereo framework, StereoPose is equipped with a parallax\\nattention module for stereo feature fusion and an epipolar loss for improving\\nthe stereo-view consistency of network predictions. Extensive experiments on\\nthe public TOD dataset demonstrate the superiority of the proposed StereoPose\\nframework for category-level 6D transparent object pose estimation.',\n",
       "    'author': [{'name': 'Kai Chen'},\n",
       "     {'name': 'Stephen James'},\n",
       "     {'name': 'Congying Sui'},\n",
       "     {'name': 'Yun-Hui Liu'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Qi Dou'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 6 figures, Project homepage:\\n  https://appsrv.cse.cuhk.edu.hk/~kaichen/stereopose.html'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.01644v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.01644v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.06134v1',\n",
       "    'updated': '2022-11-11T11:24:55Z',\n",
       "    'published': '2022-11-11T11:24:55Z',\n",
       "    'title': 'Active Task Randomization: Learning Visuomotor Skills for Sequential\\n  Manipulation by Proposing Feasible and Novel Tasks',\n",
       "    'summary': 'Solving real-world sequential manipulation tasks requires robots to have a\\nrepertoire of skills applicable to a wide range of circumstances. To acquire\\nsuch skills using data-driven approaches, we need massive and diverse training\\ndata which is often labor-intensive and non-trivial to collect and curate. In\\nthis work, we introduce Active Task Randomization (ATR), an approach that\\nlearns visuomotor skills for sequential manipulation by automatically creating\\nfeasible and novel tasks in simulation. During training, our approach\\nprocedurally generates tasks using a graph-based task parameterization. To\\nadaptively estimate the feasibility and novelty of sampled tasks, we develop a\\nrelational neural network that maps each task parameter into a compact\\nembedding. We demonstrate that our approach can automatically create suitable\\ntasks for efficiently training the skill policies to handle diverse scenarios\\nwith a variety of objects. We evaluate our method on simulated and real-world\\nsequential manipulation tasks by composing the learned skills using a task\\nplanner. Compared to baseline methods, the skills learned using our approach\\nconsistently achieve better success rates.',\n",
       "    'author': [{'name': 'Kuan Fang'},\n",
       "     {'name': 'Toki Migimatsu'},\n",
       "     {'name': 'Ajay Mandlekar'},\n",
       "     {'name': 'Li Fei-Fei'},\n",
       "     {'name': 'Jeannette Bohg'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 5 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.06134v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.06134v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.11679v1',\n",
       "    'updated': '2022-11-21T17:47:48Z',\n",
       "    'published': '2022-11-21T17:47:48Z',\n",
       "    'title': 'Mean Shift Mask Transformer for Unseen Object Instance Segmentation',\n",
       "    'summary': 'Segmenting unseen objects is a critical task in many different domains. For\\nexample, a robot may need to grasp an unseen object, which means it needs to\\nvisually separate this object from the background and/or other objects. Mean\\nshift clustering is a common method in object segmentation tasks. However, the\\ntraditional mean shift clustering algorithm is not easily integrated into an\\nend-to-end neural network training pipeline. In this work, we propose the Mean\\nShift Mask Transformer (MSMFormer), a new transformer architecture that\\nsimulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing\\nfor the joint training and inference of both the feature extractor and the\\nclustering. Its central component is a hypersphere attention mechanism, which\\nupdates object queries on a hypersphere. To illustrate the effectiveness of our\\nmethod, we apply MSMFormer to Unseen Object Instance Segmentation, which yields\\na new state-of-the-art of 87.3 Boundary F-meansure on the real-world Object\\nClutter Indoor Dataset (OCID). Code is available at\\nhttps://github.com/YoungSean/UnseenObjectsWithMeanShift',\n",
       "    'author': [{'name': 'Yangxiao Lu'},\n",
       "     {'name': 'Yuqiao Chen'},\n",
       "     {'name': 'Nicholas Ruozzi'},\n",
       "     {'name': 'Yu Xiang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.11679v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.11679v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.11746v1',\n",
       "    'updated': '2022-11-21T18:59:58Z',\n",
       "    'published': '2022-11-21T18:59:58Z',\n",
       "    'title': 'Last-Mile Embodied Visual Navigation',\n",
       "    'summary': 'Realistic long-horizon tasks like image-goal navigation involve exploratory\\nand exploitative phases. Assigned with an image of the goal, an embodied agent\\nmust explore to discover the goal, i.e., search efficiently using learned\\npriors. Once the goal is discovered, the agent must accurately calibrate the\\nlast-mile of navigation to the goal. As with any robust system, switches\\nbetween exploratory goal discovery and exploitative last-mile navigation enable\\nbetter recovery from errors. Following these intuitive guide rails, we propose\\nSLING to improve the performance of existing image-goal navigation systems.\\nEntirely complementing prior methods, we focus on last-mile navigation and\\nleverage the underlying geometric structure of the problem with neural\\ndescriptors. With simple but effective switches, we can easily connect SLING\\nwith heuristic, reinforcement learning, and neural modular policies. On a\\nstandardized image-goal navigation benchmark (Hahn et al. 2021), we improve\\nperformance across policies, scenes, and episode complexity, raising the\\nstate-of-the-art from 45% to 55% success rate. Beyond photorealistic\\nsimulation, we conduct real-robot experiments in three physical scenes and find\\nthese improvements to transfer well to real environments.',\n",
       "    'author': [{'name': 'Justin Wasserman'},\n",
       "     {'name': 'Karmesh Yadav'},\n",
       "     {'name': 'Girish Chowdhary'},\n",
       "     {'name': 'Abhinav Gupta'},\n",
       "     {'name': 'Unnat Jain'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at CoRL 2022. Code and results available at\\n  https://jbwasse2.github.io/portfolio/SLING'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.11746v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.11746v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.02503v1',\n",
       "    'updated': '2022-11-24T13:04:08Z',\n",
       "    'published': '2022-11-24T13:04:08Z',\n",
       "    'title': 'Relation-based Motion Prediction using Traffic Scene Graphs',\n",
       "    'summary': 'Representing relevant information of a traffic scene and understanding its\\nenvironment is crucial for the success of autonomous driving. Modeling the\\nsurrounding of an autonomous car using semantic relations, i.e., how different\\ntraffic participants relate in the context of traffic rule based behaviors, is\\nhardly been considered in previous work. This stems from the fact that these\\nrelations are hard to extract from real-world traffic scenes. In this work, we\\nmodel traffic scenes in a form of spatial semantic scene graphs for various\\ndifferent predictions about the traffic participants, e.g., acceleration and\\ndeceleration. Our learning and inference approach uses Graph Neural Networks\\n(GNNs) and shows that incorporating explicit information about the spatial\\nsemantic relations between traffic participants improves the predicdtion\\nresults. Specifically, the acceleration prediction of traffic participants is\\nimproved by up to 12% compared to the baselines, which do not exploit this\\nexplicit information. Furthermore, by including additional information about\\nprevious scenes, we achieve 73% improvements.',\n",
       "    'author': [{'name': 'Maximilian Zipfl'},\n",
       "     {'name': 'Felix Hertlein'},\n",
       "     {'name': 'Achim Rettinger'},\n",
       "     {'name': 'Steffen Thoma'},\n",
       "     {'name': 'Lavdim Halilaj'},\n",
       "     {'name': 'Juergen Luettin'},\n",
       "     {'name': 'Stefan Schmid'},\n",
       "     {'name': 'Cory Henson'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ITSC55140.2022.9922155'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ITSC55140.2022.9922155',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2212.02503v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.02503v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.07398v2',\n",
       "    'updated': '2023-01-20T00:47:23Z',\n",
       "    'published': '2022-12-14T18:31:47Z',\n",
       "    'title': 'Self-Play and Self-Describe: Policy Adaptation with Vision-Language\\n  Foundation Models',\n",
       "    'summary': 'Recent progress on vision-language foundation models have brought significant\\nadvancement to building general-purpose robots. By using the pre-trained models\\nto encode the scene and instructions as inputs for decision making, the\\ninstruction-conditioned policy can generalize across different objects and\\ntasks. While this is encouraging, the policy still fails in most cases given an\\nunseen task or environment. To adapt the policy to unseen tasks and\\nenvironments, we explore a new paradigm on leveraging the pre-trained\\nfoundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the\\ntrained policy to a new task or a new environment, we first let the policy\\nself-play with randomly generated instructions to record the demonstrations.\\nWhile the execution could be wrong, we can use the pre-trained foundation\\nmodels to accurately self-describe (i.e., re-label or classify) the\\ndemonstrations. This automatically provides new pairs of\\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\\non a broad range of experiments with the focus on generalization on unseen\\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\\nSPLAYD improves baselines by a large margin in all cases. Our project page is\\navailable at https://geyuying.github.io/SPLAYD/',\n",
       "    'author': [{'name': 'Yuying Ge'},\n",
       "     {'name': 'Annabella Macaluso'},\n",
       "     {'name': 'Li Erran Li'},\n",
       "     {'name': 'Ping Luo'},\n",
       "     {'name': 'Xiaolong Wang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page: https://geyuying.github.io/SPLAYD/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.07398v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.07398v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.00493v1',\n",
       "    'updated': '2023-01-02T00:36:22Z',\n",
       "    'published': '2023-01-02T00:36:22Z',\n",
       "    'title': 'Argoverse 2: Next Generation Datasets for Self-Driving Perception and\\n  Forecasting',\n",
       "    'summary': 'We introduce Argoverse 2 (AV2) - a collection of three datasets for\\nperception and forecasting research in the self-driving domain. The annotated\\nSensor Dataset contains 1,000 sequences of multimodal data, encompassing\\nhigh-resolution imagery from seven ring cameras, and two stereo cameras in\\naddition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain\\n3D cuboid annotations for 26 object categories, all of which are\\nsufficiently-sampled to support training and evaluation of 3D perception\\nmodels. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point\\nclouds and map-aligned pose. This dataset is the largest ever collection of\\nlidar sensor data and supports self-supervised learning and the emerging task\\nof point cloud forecasting. Finally, the Motion Forecasting Dataset contains\\n250,000 scenarios mined for interesting and challenging interactions between\\nthe autonomous vehicle and other actors in each local scene. Models are tasked\\nwith the prediction of future motion for \"scored actors\" in each scenario and\\nare provided with track histories that capture object location, heading,\\nvelocity, and category. In all three datasets, each scenario contains its own\\nHD Map with 3D lane and crosswalk geometry - sourced from data captured in six\\ndistinct cities. We believe these datasets will support new and existing\\nmachine learning research problems in ways that existing datasets do not. All\\ndatasets are released under the CC BY-NC-SA 4.0 license.',\n",
       "    'author': [{'name': 'Benjamin Wilson'},\n",
       "     {'name': 'William Qi'},\n",
       "     {'name': 'Tanmay Agarwal'},\n",
       "     {'name': 'John Lambert'},\n",
       "     {'name': 'Jagjeet Singh'},\n",
       "     {'name': 'Siddhesh Khandelwal'},\n",
       "     {'name': 'Bowen Pan'},\n",
       "     {'name': 'Ratnesh Kumar'},\n",
       "     {'name': 'Andrew Hartnett'},\n",
       "     {'name': 'Jhony Kaesemodel Pontes'},\n",
       "     {'name': 'Deva Ramanan'},\n",
       "     {'name': 'Peter Carr'},\n",
       "     {'name': 'James Hays'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the Neural Information Processing Systems Track on\\n  Datasets and Benchmarks'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.00493v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.00493v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1509.07831v2',\n",
       "    'updated': '2017-05-17T15:12:33Z',\n",
       "    'published': '2015-09-25T18:55:45Z',\n",
       "    'title': 'Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds,\\n  Language and Trajectories',\n",
       "    'summary': 'A robot operating in a real-world environment needs to perform reasoning over\\na variety of sensor modalities such as vision, language and motion\\ntrajectories. However, it is extremely challenging to manually design features\\nrelating such disparate modalities. In this work, we introduce an algorithm\\nthat learns to embed point-cloud, natural language, and manipulation trajectory\\ndata into a shared embedding space with a deep neural network. To learn\\nsemantically meaningful spaces throughout our network, we use a loss-based\\nmargin to bring embeddings of relevant pairs closer together while driving\\nless-relevant cases from different modalities further apart. We use this both\\nto pre-train its lower layers and fine-tune our final embedding space, leading\\nto a more robust representation. We test our algorithm on the task of\\nmanipulating novel objects and appliances based on prior experience with other\\nobjects. On a large dataset, we achieve significant improvements in both\\naccuracy and inference time over the previous state of the art. We also perform\\nend-to-end experiments on a PR2 robot utilizing our learned embedding space.',\n",
       "    'author': [{'name': 'Jaeyong Sung'},\n",
       "     {'name': 'Ian Lenz'},\n",
       "     {'name': 'Ashutosh Saxena'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE International Conference on Robotics and Automation (ICRA), 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1509.07831v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1509.07831v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1603.02199v4',\n",
       "    'updated': '2016-08-28T23:32:37Z',\n",
       "    'published': '2016-03-07T18:53:00Z',\n",
       "    'title': 'Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning\\n  and Large-Scale Data Collection',\n",
       "    'summary': 'We describe a learning-based approach to hand-eye coordination for robotic\\ngrasping from monocular images. To learn hand-eye coordination for grasping, we\\ntrained a large convolutional neural network to predict the probability that\\ntask-space motion of the gripper will result in successful grasps, using only\\nmonocular camera images and independently of camera calibration or the current\\nrobot pose. This requires the network to observe the spatial relationship\\nbetween the gripper and objects in the scene, thus learning hand-eye\\ncoordination. We then use this network to servo the gripper in real time to\\nachieve successful grasps. To train our network, we collected over 800,000\\ngrasp attempts over the course of two months, using between 6 and 14 robotic\\nmanipulators at any given time, with differences in camera placement and\\nhardware. Our experimental evaluation demonstrates that our method achieves\\neffective real-time control, can successfully grasp novel objects, and corrects\\nmistakes by continuous servoing.',\n",
       "    'author': [{'name': 'Sergey Levine'},\n",
       "     {'name': 'Peter Pastor'},\n",
       "     {'name': 'Alex Krizhevsky'},\n",
       "     {'name': 'Deirdre Quillen'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This is an extended version of \"Learning Hand-Eye Coordination for\\n  Robotic Grasping with Large-Scale Data Collection,\" ISER 2016. Draft modified\\n  to correct typo in Algorithm 1 and add a link to the publicly available\\n  dataset'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1603.02199v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1603.02199v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1604.01802v2',\n",
       "    'updated': '2016-08-16T02:34:48Z',\n",
       "    'published': '2016-04-06T20:39:34Z',\n",
       "    'title': 'Learning to Track at 100 FPS with Deep Regression Networks',\n",
       "    'summary': \"Machine learning techniques are often used in computer vision due to their\\nability to leverage large amounts of training data to improve performance.\\nUnfortunately, most generic object trackers are still trained from scratch\\nonline and do not benefit from the large number of videos that are readily\\navailable for offline training. We propose a method for offline training of\\nneural networks that can track novel objects at test-time at 100 fps. Our\\ntracker is significantly faster than previous methods that use neural networks\\nfor tracking, which are typically very slow to run and not practical for\\nreal-time applications. Our tracker uses a simple feed-forward network with no\\nonline training required. The tracker learns a generic relationship between\\nobject motion and appearance and can be used to track novel objects that do not\\nappear in the training set. We test our network on a standard tracking\\nbenchmark to demonstrate our tracker's state-of-the-art performance. Further,\\nour performance improves as we add more videos to our offline training set. To\\nthe best of our knowledge, our tracker is the first neural-network tracker that\\nlearns to track generic objects at 100 fps.\",\n",
       "    'author': [{'name': 'David Held'},\n",
       "     {'name': 'Sebastian Thrun'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in European Conference on Computer Vision (ECCV) 2016'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1604.01802v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1604.01802v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1604.03692v2',\n",
       "    'updated': '2016-04-20T21:02:02Z',\n",
       "    'published': '2016-04-13T08:40:06Z',\n",
       "    'title': 'Learning Social Affordance for Human-Robot Interaction',\n",
       "    'summary': 'In this paper, we present an approach for robot learning of social affordance\\nfrom human activity videos. We consider the problem in the context of\\nhuman-robot interaction: Our approach learns structural representations of\\nhuman-human (and human-object-human) interactions, describing how body-parts of\\neach agent move with respect to each other and what spatial relations they\\nshould maintain to complete each sub-event (i.e., sub-goal). This enables the\\nrobot to infer its own movement in reaction to the human body motion, allowing\\nit to naturally replicate such interactions.\\n  We introduce the representation of social affordance and propose a generative\\nmodel for its weakly supervised learning from human demonstration videos. Our\\napproach discovers critical steps (i.e., latent sub-events) in an interaction\\nand the typical motion associated with them, learning what body-parts should be\\ninvolved and how. The experimental results demonstrate that our Markov Chain\\nMonte Carlo (MCMC) based learning algorithm automatically discovers\\nsemantically meaningful interactive affordance from RGB-D videos, which allows\\nus to generate appropriate full body motion for an agent.',\n",
       "    'author': [{'name': 'Tianmin Shu'},\n",
       "     {'name': 'M. S. Ryoo'},\n",
       "     {'name': 'Song-Chun Zhu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Joint Conference on Artificial Intelligence (IJCAI),\\n  2016'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1604.03692v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1604.03692v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1605.00164v2',\n",
       "    'updated': '2016-08-05T22:15:48Z',\n",
       "    'published': '2016-04-30T20:39:16Z',\n",
       "    'title': 'Look-ahead before you leap: end-to-end active recognition by forecasting\\n  the effect of motion',\n",
       "    'summary': 'Visual recognition systems mounted on autonomous moving agents face the\\nchallenge of unconstrained data, but simultaneously have the opportunity to\\nimprove their performance by moving to acquire new views of test data. In this\\nwork, we first show how a recurrent neural network-based system may be trained\\nto perform end-to-end learning of motion policies suited for this \"active\\nrecognition\" setting. Further, we hypothesize that active vision requires an\\nagent to have the capacity to reason about the effects of its motions on its\\nview of the world. To verify this hypothesis, we attempt to induce this\\ncapacity in our active recognition pipeline, by simultaneously learning to\\nforecast the effects of the agent\\'s motions on its internal representation of\\nthe environment conditional on all past views. Results across two challenging\\ndatasets confirm both that our end-to-end system successfully learns meaningful\\npolicies for active category recognition, and that \"learning to look ahead\"\\nfurther boosts recognition performance.',\n",
       "    'author': [{'name': 'Dinesh Jayaraman'}, {'name': 'Kristen Grauman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'A preliminary version of the material in this document was filed as\\n  University of Texas technical report no. UT AI15-06, December, 2015, at:\\n  http://apps.cs.utexas.edu/tech_reports/reports/ai/AI-2214.pdf, ECCV 2016'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1605.00164v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1605.00164v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1609.09365v3',\n",
       "    'updated': '2017-04-19T14:31:32Z',\n",
       "    'published': '2016-09-29T14:39:10Z',\n",
       "    'title': 'Deep Tracking on the Move: Learning to Track the World from a Moving\\n  Vehicle using Recurrent Neural Networks',\n",
       "    'summary': 'This paper presents an end-to-end approach for tracking static and dynamic\\nobjects for an autonomous vehicle driving through crowded urban environments.\\nUnlike traditional approaches to tracking, this method is learned end-to-end,\\nand is able to directly predict a full unoccluded occupancy grid map from raw\\nlaser input data. Inspired by the recently presented DeepTracking approach\\n[Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the\\ntemporal evolution of the state of the environment, and propose to use Spatial\\nTransformer modules to exploit estimates of the egomotion of the vehicle. Our\\nresults demonstrate the ability to track a range of objects, including cars,\\nbuses, pedestrians, and cyclists through occlusion, from both moving and\\nstationary platforms, using a single learned model. Experimental results\\ndemonstrate that the model can also predict the future states of objects from\\ncurrent inputs, with greater accuracy than previous work.',\n",
       "    'author': [{'name': 'Julie Dequaire'},\n",
       "     {'name': 'Dushyant Rao'},\n",
       "     {'name': 'Peter Ondruska'},\n",
       "     {'name': 'Dominic Wang'},\n",
       "     {'name': 'Ingmar Posner'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1609.09365v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1609.09365v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1610.01238v3',\n",
       "    'updated': '2017-11-17T16:54:44Z',\n",
       "    'published': '2016-10-05T00:44:49Z',\n",
       "    'title': 'Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for\\n  Urban Autonomy',\n",
       "    'summary': 'We present a weakly-supervised approach to segmenting proposed drivable paths\\nin images with the goal of autonomous driving in complex urban environments.\\nUsing recorded routes from a data collection vehicle, our proposed method\\ngenerates vast quantities of labelled images containing proposed paths and\\nobstacles without requiring manual annotation, which we then use to train a\\ndeep semantic segmentation network. With the trained network we can segment\\nproposed paths and obstacles at run-time using a vehicle equipped with only a\\nmonocular camera without relying on explicit modelling of road or lane\\nmarkings. We evaluate our method on the large-scale KITTI and Oxford RobotCar\\ndatasets and demonstrate reliable path proposal and obstacle segmentation in a\\nwide variety of environments under a range of lighting, weather and traffic\\nconditions. We illustrate how the method can generalise to multiple path\\nproposals at intersections and outline plans to incorporate the system into a\\nframework for autonomous urban driving.',\n",
       "    'author': [{'name': 'Dan Barnes'},\n",
       "     {'name': 'Will Maddern'},\n",
       "     {'name': 'Ingmar Posner'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Robotics and Automation (ICRA), 2017.\\n  Video summary: http://youtu.be/rbZ8ck_1nZk'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1610.01238v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1610.01238v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1610.03677v2',\n",
       "    'updated': '2017-09-18T01:03:55Z',\n",
       "    'published': '2016-10-12T11:40:24Z',\n",
       "    'title': 'Deep Fruit Detection in Orchards',\n",
       "    'summary': 'An accurate and reliable image based fruit detection system is critical for\\nsupporting higher level agriculture tasks such as yield mapping and robotic\\nharvesting. This paper presents the use of a state-of-the-art object detection\\nframework, Faster R-CNN, in the context of fruit detection in orchards,\\nincluding mangoes, almonds and apples. Ablation studies are presented to better\\nunderstand the practical deployment of the detection network, including how\\nmuch training data is required to capture variability in the dataset. Data\\naugmentation techniques are shown to yield significant performance gains,\\nresulting in a greater than two-fold reduction in the number of training images\\nrequired. In contrast, transferring knowledge between orchards contributed to\\nnegligible performance gain over initialising the Deep Convolutional Neural\\nNetwork directly from ImageNet features. Finally, to operate over orchard data\\ncontaining between 100-1000 fruit per image, a tiling approach is introduced\\nfor the Faster R-CNN framework. The study has resulted in the best yet\\ndetection performance for these orchards relative to previous works, with an\\nF1-score of >0.9 achieved for apples and mangoes.',\n",
       "    'author': [{'name': 'Suchet Bargoti'}, {'name': 'James Underwood'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to the IEEE International Conference on Robotics and\\n  Automation 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1610.03677v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1610.03677v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1611.03673v3',\n",
       "    'updated': '2017-01-13T11:15:22Z',\n",
       "    'published': '2016-11-11T12:14:45Z',\n",
       "    'title': 'Learning to Navigate in Complex Environments',\n",
       "    'summary': 'Learning to navigate in complex environments with dynamic elements is an\\nimportant milestone in developing AI agents. In this work we formulate the\\nnavigation question as a reinforcement learning problem and show that data\\nefficiency and task performance can be dramatically improved by relying on\\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\\nwe consider jointly learning the goal-driven reinforcement learning problem\\nwith auxiliary depth prediction and loop closure classification tasks. This\\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\\napproaching human-level performance even under conditions where the goal\\nlocation changes frequently. We provide detailed analysis of the agent\\nbehaviour, its ability to localise, and its network activity dynamics, showing\\nthat the agent implicitly learns key navigation abilities.',\n",
       "    'author': [{'name': 'Piotr Mirowski'},\n",
       "     {'name': 'Razvan Pascanu'},\n",
       "     {'name': 'Fabio Viola'},\n",
       "     {'name': 'Hubert Soyer'},\n",
       "     {'name': 'Andrew J. Ballard'},\n",
       "     {'name': 'Andrea Banino'},\n",
       "     {'name': 'Misha Denil'},\n",
       "     {'name': 'Ross Goroshin'},\n",
       "     {'name': 'Laurent Sifre'},\n",
       "     {'name': 'Koray Kavukcuoglu'},\n",
       "     {'name': 'Dharshan Kumaran'},\n",
       "     {'name': 'Raia Hadsell'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages, 5 appendix pages, 11 figures, 3 tables, under review as a\\n  conference paper at ICLR 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1611.03673v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1611.03673v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1702.03920v3',\n",
       "    'updated': '2019-02-07T18:54:58Z',\n",
       "    'published': '2017-02-13T18:52:04Z',\n",
       "    'title': 'Cognitive Mapping and Planning for Visual Navigation',\n",
       "    'summary': \"We introduce a neural architecture for navigation in novel environments. Our\\nproposed architecture learns to map from first-person views and plans a\\nsequence of actions towards goals in the environment. The Cognitive Mapper and\\nPlanner (CMP) is based on two key ideas: a) a unified joint architecture for\\nmapping and planning, such that the mapping is driven by the needs of the task,\\nand b) a spatial memory with the ability to plan given an incomplete set of\\nobservations about the world. CMP constructs a top-down belief map of the world\\nand applies a differentiable neural net planner to produce the next action at\\neach time step. The accumulated belief of the world enables the agent to track\\nvisited regions of the environment. We train and test CMP on navigation\\nproblems in simulation environments derived from scans of real world buildings.\\nOur experiments demonstrate that CMP outperforms alternate learning-based\\narchitectures, as well as, classical mapping and path planning approaches in\\nmany cases. Furthermore, it naturally extends to semantically specified goals,\\nsuch as 'going to a chair'. We also deploy CMP on physical robots in indoor\\nenvironments, where it achieves reasonable performance, even though it is\\ntrained entirely in simulation.\",\n",
       "    'author': [{'name': 'Saurabh Gupta'},\n",
       "     {'name': 'Varun Tolani'},\n",
       "     {'name': 'James Davidson'},\n",
       "     {'name': 'Sergey Levine'},\n",
       "     {'name': 'Rahul Sukthankar'},\n",
       "     {'name': 'Jitendra Malik'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Extended IJCV Version of the original paper at CVPR17. Project\\n  website with code, models, simulation environment and videos:\\n  https://sites.google.com/view/cognitive-mapping-and-planning/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1702.03920v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1702.03920v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1703.01040v2',\n",
       "    'updated': '2017-07-24T08:02:11Z',\n",
       "    'published': '2017-03-03T05:27:50Z',\n",
       "    'title': 'Learning Robot Activities from First-Person Human Videos Using\\n  Convolutional Future Regression',\n",
       "    'summary': \"We design a new approach that allows robot learning of new activities from\\nunlabeled human example videos. Given videos of humans executing the same\\nactivity from a human's viewpoint (i.e., first-person videos), our objective is\\nto make the robot learn the temporal structure of the activity as its future\\nregression network, and learn to transfer such model for its own motor\\nexecution. We present a new deep learning model: We extend the state-of-the-art\\nconvolutional object detection network for the representation/estimation of\\nhuman hands in training videos, and newly introduce the concept of using a\\nfully convolutional network to regress (i.e., predict) the intermediate scene\\nrepresentation corresponding to the future frame (e.g., 1-2 seconds later).\\nCombining these allows direct prediction of future locations of human hands and\\nobjects, which enables the robot to infer the motor control plan using our\\nmanipulation network. We experimentally confirm that our approach makes\\nlearning of robot activities from unlabeled human interaction videos possible,\\nand demonstrate that our robot is able to execute the learned collaborative\\nactivities in real-time directly based on its camera input.\",\n",
       "    'author': [{'name': 'Jangwon Lee'}, {'name': 'Michael S. Ryoo'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1703.01040v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1703.01040v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1705.03550v1',\n",
       "    'updated': '2017-05-09T21:32:19Z',\n",
       "    'published': '2017-05-09T21:32:19Z',\n",
       "    'title': 'CORe50: a New Dataset and Benchmark for Continuous Object Recognition',\n",
       "    'summary': 'Continuous/Lifelong learning of high-dimensional data streams is a\\nchallenging research problem. In fact, fully retraining models each time new\\ndata become available is infeasible, due to computational and storage issues,\\nwhile na\\\\\"ive incremental strategies have been shown to suffer from\\ncatastrophic forgetting. In the context of real-world object recognition\\napplications (e.g., robotic vision), where continuous learning is crucial, very\\nfew datasets and benchmarks are available to evaluate and compare emerging\\ntechniques. In this work we propose a new dataset and benchmark CORe50,\\nspecifically designed for continuous object recognition, and introduce baseline\\napproaches for different continuous learning scenarios.',\n",
       "    'author': [{'name': 'Vincenzo Lomonaco'}, {'name': 'Davide Maltoni'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1705.03550v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1705.03550v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1707.03167v1',\n",
       "    'updated': '2017-07-11T08:21:58Z',\n",
       "    'published': '2017-07-11T08:21:58Z',\n",
       "    'title': 'RegNet: Multimodal Sensor Registration Using Deep Neural Networks',\n",
       "    'summary': 'In this paper, we present RegNet, the first deep convolutional neural network\\n(CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between\\nmultimodal sensors, exemplified using a scanning LiDAR and a monocular camera.\\nCompared to existing approaches, RegNet casts all three conventional\\ncalibration steps (feature extraction, feature matching and global regression)\\ninto a single real-time capable CNN. Our method does not require any human\\ninteraction and bridges the gap between classical offline and target-less\\nonline calibration approaches as it provides both a stable initial estimation\\nas well as a continuous online correction of the extrinsic parameters. During\\ntraining we randomly decalibrate our system in order to train RegNet to infer\\nthe correspondence between projected depth measurements and RGB image and\\nfinally regress the extrinsic calibration. Additionally, with an iterative\\nexecution of multiple CNNs, that are trained on different magnitudes of\\ndecalibration, our approach compares favorably to state-of-the-art methods in\\nterms of a mean calibration error of 0.28 degrees for the rotational and 6 cm\\nfor the translation components even for large decalibrations up to 1.5 m and 20\\ndegrees.',\n",
       "    'author': [{'name': 'Nick Schneider'},\n",
       "     {'name': 'Florian Piewak'},\n",
       "     {'name': 'Christoph Stiller'},\n",
       "     {'name': 'Uwe Franke'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'published in IEEE Intelligent Vehicles Symposium, 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1707.03167v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1707.03167v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1708.07303v4',\n",
       "    'updated': '2018-06-15T03:40:53Z',\n",
       "    'published': '2017-08-24T08:09:04Z',\n",
       "    'title': 'Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D\\n  Representations',\n",
       "    'summary': 'This paper focuses on the problem of learning 6-DOF grasping with a parallel\\njaw gripper in simulation. We propose the notion of a geometry-aware\\nrepresentation in grasping based on the assumption that knowledge of 3D\\ngeometry is at the heart of interaction. Our key idea is constraining and\\nregularizing grasping interaction learning through 3D geometry prediction.\\nSpecifically, we formulate the learning of deep geometry-aware grasping model\\nin two steps: First, we learn to build mental geometry-aware representation by\\nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via\\ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with\\nits internal geometry-aware representation. The learned outcome prediction\\nmodel is used to sequentially propose grasping solutions via\\nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best\\nof our knowledge, we are presenting for the first time a method to learn a\\n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from\\ndemonstrations in virtual reality with rich sensory and interaction\\nannotations. This dataset includes 101 everyday objects spread across 7\\ncategories, additionally, we propose a data augmentation strategy for effective\\nlearning; (3) We demonstrate that the learned geometry-aware representation\\nleads to about 10 percent relative performance improvement over the baseline\\nCNN on grasping objects from our dataset. (4) We further demonstrate that the\\nmodel generalizes to novel viewpoints and object instances.',\n",
       "    'author': [{'name': 'Xinchen Yan'},\n",
       "     {'name': 'Jasmine Hsu'},\n",
       "     {'name': 'Mohi Khansari'},\n",
       "     {'name': 'Yunfei Bai'},\n",
       "     {'name': 'Arkanath Pathak'},\n",
       "     {'name': 'Abhinav Gupta'},\n",
       "     {'name': 'James Davidson'},\n",
       "     {'name': 'Honglak Lee'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at ICRA 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1708.07303v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1708.07303v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.02249v2',\n",
       "    'updated': '2017-09-08T06:11:36Z',\n",
       "    'published': '2017-09-03T18:57:54Z',\n",
       "    'title': 'Uncertainty-Aware Learning from Demonstration using Mixture Density\\n  Networks with Sampling-Free Variance Modeling',\n",
       "    'summary': 'In this paper, we propose an uncertainty-aware learning from demonstration\\nmethod by presenting a novel uncertainty estimation method utilizing a mixture\\ndensity network appropriate for modeling complex and noisy human behaviors. The\\nproposed uncertainty acquisition can be done with a single forward path without\\nMonte Carlo sampling and is suitable for real-time robotics applications. The\\nproperties of the proposed uncertainty measure are analyzed through three\\ndifferent synthetic examples, absence of data, heavy measurement noise, and\\ncomposition of functions scenarios. We show that each case can be distinguished\\nusing the proposed uncertainty measure and presented an uncertainty-aware\\nlearn- ing from demonstration method of an autonomous driving using this\\nproperty. The proposed uncertainty-aware learning from demonstration method\\noutperforms other compared methods in terms of safety using a complex\\nreal-world driving dataset.',\n",
       "    'author': [{'name': 'Sungjoon Choi'},\n",
       "     {'name': 'Kyungjae Lee'},\n",
       "     {'name': 'Sungbin Lim'},\n",
       "     {'name': 'Songhwai Oh'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.02249v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.02249v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.09882v2',\n",
       "    'updated': '2019-01-03T14:16:09Z',\n",
       "    'published': '2017-09-28T10:16:52Z',\n",
       "    'title': \"Are we done with object recognition? The iCub robot's perspective\",\n",
       "    'summary': 'We report on an extensive study of the benefits and limitations of current\\ndeep learning approaches to object recognition in robot vision scenarios,\\nintroducing a novel dataset used for our investigation. To avoid the biases in\\ncurrently available datasets, we consider a natural human-robot interaction\\nsetting to design a data-acquisition protocol for visual object recognition on\\nthe iCub humanoid robot. Analyzing the performance of off-the-shelf models\\ntrained off-line on large-scale image retrieval datasets, we show the necessity\\nfor knowledge transfer. We evaluate different ways in which this last step can\\nbe done, and identify the major bottlenecks affecting robotic scenarios. By\\nstudying both object categorization and identification problems, we highlight\\nkey differences between object recognition in robotics applications and in\\nimage retrieval tasks, for which the considered deep learning approaches have\\nbeen originally designed. In a nutshell, our results confirm the remarkable\\nimprovements yield by deep learning in this setting, while pointing to specific\\nopen challenges that need be addressed for seamless deployment in robotics.',\n",
       "    'author': [{'name': 'Giulia Pasquale'},\n",
       "     {'name': 'Carlo Ciliberto'},\n",
       "     {'name': 'Francesca Odone'},\n",
       "     {'name': 'Lorenzo Rosasco'},\n",
       "     {'name': 'Lorenzo Natale'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.robot.2018.11.001'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.robot.2018.11.001',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1709.09882v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.09882v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '21 pages + supplementary material'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Robotics and Autonomous Systems, Volume 112, February 2019, Pages\\n  260-281'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.10; I.2.11; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2;\\n  I.5.4; I.5.5',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1710.05268v1',\n",
       "    'updated': '2017-10-15T02:58:20Z',\n",
       "    'published': '2017-10-15T02:58:20Z',\n",
       "    'title': 'Self-Supervised Visual Planning with Temporal Skip Connections',\n",
       "    'summary': 'In order to autonomously learn wide repertoires of complex skills, robots\\nmust be able to learn from their own autonomously collected data, without human\\nsupervision. One learning signal that is always available for autonomously\\ncollected data is prediction: if a robot can learn to predict the future, it\\ncan use this predictive model to take actions to produce desired outcomes, such\\nas moving an object to a particular location. However, in complex open-world\\nscenarios, designing a representation for prediction is difficult. In this\\nwork, we instead aim to enable self-supervised robotic learning through direct\\nvideo prediction: instead of attempting to design a good representation, we\\ndirectly predict what the robot will see next, and then use this model to\\nachieve desired goals. A key challenge in video prediction for robotic\\nmanipulation is handling complex spatial arrangements such as occlusions. To\\nthat end, we introduce a video prediction model that can keep track of objects\\nthrough occlusion by incorporating temporal skip-connections. Together with a\\nnovel planning criterion and action space formulation, we demonstrate that this\\nmodel substantially outperforms prior work on video prediction-based control.\\nOur results show manipulation of objects not seen during training, handling\\nmultiple objects, and pushing objects around obstructions. These results\\nrepresent a significant advance in the range and complexity of skills that can\\nbe performed entirely with self-supervised robotic learning.',\n",
       "    'author': [{'name': 'Frederik Ebert'},\n",
       "     {'name': 'Chelsea Finn'},\n",
       "     {'name': 'Alex X. Lee'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'accepted at the Conference on Robot Learning (CoRL) 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1710.05268v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1710.05268v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1710.06422v2',\n",
       "    'updated': '2018-03-04T04:08:58Z',\n",
       "    'published': '2017-10-17T17:54:50Z',\n",
       "    'title': 'Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from\\n  Simulation',\n",
       "    'summary': 'Learning-based approaches to robotic manipulation are limited by the\\nscalability of data collection and accessibility of labels. In this paper, we\\npresent a multi-task domain adaptation framework for instance grasping in\\ncluttered scenes by utilizing simulated robot experiments. Our neural network\\ntakes monocular RGB images and the instance segmentation mask of a specified\\ntarget object as inputs, and predicts the probability of successfully grasping\\nthe specified object for each candidate motor command. The proposed transfer\\nlearning framework trains a model for instance grasping in simulation and uses\\na domain-adversarial loss to transfer the trained model to real robots using\\nindiscriminate grasping data, which is available both in simulation and the\\nreal world. We evaluate our model in real-world robot experiments, comparing it\\nwith alternative model architectures as well as an indiscriminate grasping\\nbaseline.',\n",
       "    'author': [{'name': 'Kuan Fang'},\n",
       "     {'name': 'Yunfei Bai'},\n",
       "     {'name': 'Stefan Hinterstoisser'},\n",
       "     {'name': 'Silvio Savarese'},\n",
       "     {'name': 'Mrinal Kalakrishnan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1710.06422v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1710.06422v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1711.00267v2',\n",
       "    'updated': '2017-11-22T11:38:17Z',\n",
       "    'published': '2017-11-01T10:04:29Z',\n",
       "    'title': 'Acquiring Target Stacking Skills by Goal-Parameterized Deep\\n  Reinforcement Learning',\n",
       "    'summary': 'Understanding physical phenomena is a key component of human intelligence and\\nenables physical interaction with previously unseen environments. In this\\npaper, we study how an artificial agent can autonomously acquire this intuition\\nthrough interaction with the environment. We created a synthetic block stacking\\nenvironment with physics simulation in which the agent can learn a policy\\nend-to-end through trial and error. Thereby, we bypass to explicitly model\\nphysical knowledge within the policy. We are specifically interested in tasks\\nthat require the agent to reach a given goal state that may be different for\\nevery new trial. To this end, we propose a deep reinforcement learning\\nframework that learns policies which are parametrized by a goal. We validated\\nthe model on a toy example navigating in a grid world with different target\\npositions and in a block stacking task with different target structures of the\\nfinal tower. In contrast to prior work, our policies show better generalization\\nacross different goals.',\n",
       "    'author': [{'name': 'Wenbin Li'},\n",
       "     {'name': 'Jeannette Bohg'},\n",
       "     {'name': 'Mario Fritz'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1711.00267v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1711.00267v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1711.03938v1',\n",
       "    'updated': '2017-11-10T17:54:40Z',\n",
       "    'published': '2017-11-10T17:54:40Z',\n",
       "    'title': 'CARLA: An Open Urban Driving Simulator',\n",
       "    'summary': \"We introduce CARLA, an open-source simulator for autonomous driving research.\\nCARLA has been developed from the ground up to support development, training,\\nand validation of autonomous urban driving systems. In addition to open-source\\ncode and protocols, CARLA provides open digital assets (urban layouts,\\nbuildings, vehicles) that were created for this purpose and can be used freely.\\nThe simulation platform supports flexible specification of sensor suites and\\nenvironmental conditions. We use CARLA to study the performance of three\\napproaches to autonomous driving: a classic modular pipeline, an end-to-end\\nmodel trained via imitation learning, and an end-to-end model trained via\\nreinforcement learning. The approaches are evaluated in controlled scenarios of\\nincreasing difficulty, and their performance is examined via metrics provided\\nby CARLA, illustrating the platform's utility for autonomous driving research.\\nThe supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E\",\n",
       "    'author': [{'name': 'Alexey Dosovitskiy'},\n",
       "     {'name': 'German Ros'},\n",
       "     {'name': 'Felipe Codevilla'},\n",
       "     {'name': 'Antonio Lopez'},\n",
       "     {'name': 'Vladlen Koltun'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at the 1st Conference on Robot Learning (CoRL)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1711.03938v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1711.03938v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1802.00411v2',\n",
       "    'updated': '2018-08-23T23:54:05Z',\n",
       "    'published': '2018-02-01T17:39:15Z',\n",
       "    'title': 'Dense 3D Object Reconstruction from a Single Depth View',\n",
       "    'summary': 'In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs\\nthe complete 3D structure of a given object from a single arbitrary depth view\\nusing generative adversarial networks. Unlike existing work which typically\\nrequires multiple views of the same object or class labels to recover the full\\n3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation\\nof a depth view of the object as input, and is able to generate the complete 3D\\noccupancy grid with a high resolution of 256^3 by recovering the\\noccluded/missing regions. The key idea is to combine the generative\\ncapabilities of autoencoders and the conditional Generative Adversarial\\nNetworks (GAN) framework, to infer accurate and fine-grained 3D structures of\\nobjects in high-dimensional voxel space. Extensive experiments on large\\nsynthetic datasets and real-world Kinect datasets show that the proposed\\n3D-RecGAN++ significantly outperforms the state of the art in single view 3D\\nobject reconstruction, and is able to reconstruct unseen types of objects.',\n",
       "    'author': [{'name': 'Bo Yang'},\n",
       "     {'name': 'Stefano Rosa'},\n",
       "     {'name': 'Andrew Markham'},\n",
       "     {'name': 'Niki Trigoni'},\n",
       "     {'name': 'Hongkai Wen'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TPAMI.2018.2868195'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TPAMI.2018.2868195',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1802.00411v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1802.00411v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'TPAMI 2018. Code and data are available at:\\n  https://github.com/Yang7879/3D-RecGAN-extended. This article extends from\\n  arXiv:1708.07969'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1802.01557v1',\n",
       "    'updated': '2018-02-05T18:36:19Z',\n",
       "    'published': '2018-02-05T18:36:19Z',\n",
       "    'title': 'One-Shot Imitation from Observing Humans via Domain-Adaptive\\n  Meta-Learning',\n",
       "    'summary': 'Humans and animals are capable of learning a new behavior by observing others\\nperform the skill just once. We consider the problem of allowing a robot to do\\nthe same -- learning from a raw video pixels of a human, even when there is\\nsubstantial domain shift in the perspective, environment, and embodiment\\nbetween the robot and the observed human. Prior approaches to this problem have\\nhand-specified how human and robot actions correspond and often relied on\\nexplicit human pose detection systems. In this work, we present an approach for\\none-shot learning from a video of a human by using human and robot\\ndemonstration data from a variety of previous tasks to build up prior knowledge\\nthrough meta-learning. Then, combining this prior knowledge and only a single\\nvideo demonstration from a human, the robot can perform the task that the human\\ndemonstrated. We show experiments on both a PR2 arm and a Sawyer arm,\\ndemonstrating that after meta-learning, the robot can learn to place, push, and\\npick-and-place new objects using just one video of a human performing the\\nmanipulation.',\n",
       "    'author': [{'name': 'Tianhe Yu'},\n",
       "     {'name': 'Chelsea Finn'},\n",
       "     {'name': 'Annie Xie'},\n",
       "     {'name': 'Sudeep Dasari'},\n",
       "     {'name': 'Tianhao Zhang'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'First two authors contributed equally. Video available at\\n  https://sites.google.com/view/daml'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1802.01557v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1802.01557v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1803.00653v1',\n",
       "    'updated': '2018-03-01T22:50:35Z',\n",
       "    'published': '2018-03-01T22:50:35Z',\n",
       "    'title': 'Semi-parametric Topological Memory for Navigation',\n",
       "    'summary': 'We introduce a new memory architecture for navigation in previously unseen\\nenvironments, inspired by landmark-based navigation in animals. The proposed\\nsemi-parametric topological memory (SPTM) consists of a (non-parametric) graph\\nwith nodes corresponding to locations in the environment and a (parametric)\\ndeep network capable of retrieving nodes from the graph based on observations.\\nThe graph stores no metric information, only connectivity of locations\\ncorresponding to the nodes. We use SPTM as a planning module in a navigation\\nsystem. Given only 5 minutes of footage of a previously unseen maze, an\\nSPTM-based navigation agent can build a topological map of the environment and\\nuse it to confidently navigate towards goals. The average success rate of the\\nSPTM agent in goal-directed navigation across test environments is higher than\\nthe best-performing baseline by a factor of three. A video of the agent is\\navailable at https://youtu.be/vRF7f4lhswo',\n",
       "    'author': [{'name': 'Nikolay Savinov'},\n",
       "     {'name': 'Alexey Dosovitskiy'},\n",
       "     {'name': 'Vladlen Koltun'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at International Conference on Learning Representations\\n  (ICLR) 2018. Project website at https://sites.google.com/view/SPTM'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1803.00653v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1803.00653v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1803.08740v2',\n",
       "    'updated': '2018-08-27T15:19:38Z',\n",
       "    'published': '2018-03-23T11:13:29Z',\n",
       "    'title': 'Speeding-up Object Detection Training for Robotics with FALKON',\n",
       "    'summary': 'Latest deep learning methods for object detection provide remarkable\\nperformance, but have limits when used in robotic applications. One of the most\\nrelevant issues is the long training time, which is due to the large size and\\nimbalance of the associated training sets, characterized by few positive and a\\nlarge number of negative examples (i.e. background). Proposed approaches are\\nbased on end-to-end learning by back-propagation [22] or kernel methods trained\\nwith Hard Negatives Mining on top of deep features [8]. These solutions are\\neffective, but prohibitively slow for on-line applications. In this paper we\\npropose a novel pipeline for object detection that overcomes this problem and\\nprovides comparable performance, with a 60x training speedup. Our pipeline\\ncombines (i) the Region Proposal Network and the deep feature extractor from\\n[22] to efficiently select candidate RoIs and encode them into powerful\\nrepresentations, with (ii) the FALKON [23] algorithm, a novel kernel-based\\nmethod that allows fast training on large scale problems (millions of points).\\nWe address the size and imbalance of training data by exploiting the stochastic\\nsubsampling intrinsic into the method and a novel, fast, bootstrapping\\napproach. We assess the effectiveness of the approach on a standard Computer\\nVision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a\\nreal robotic scenario with the iCubWorld Transformations [18] dataset.',\n",
       "    'author': [{'name': 'Elisa Maiettini'},\n",
       "     {'name': 'Giulia Pasquale'},\n",
       "     {'name': 'Lorenzo Rosasco'},\n",
       "     {'name': 'Lorenzo Natale'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/IROS.2018.8593990'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/IROS.2018.8593990',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1803.08740v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1803.08740v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE/RSJ International Conference on Intelligent Robots and\\n  Systems (IROS), 2018'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1804.01523v1',\n",
       "    'updated': '2018-04-04T17:55:40Z',\n",
       "    'published': '2018-04-04T17:55:40Z',\n",
       "    'title': 'Stochastic Adversarial Video Prediction',\n",
       "    'summary': 'Being able to predict what may happen in the future requires an in-depth\\nunderstanding of the physical and causal rules that govern the world. A model\\nthat is able to do so has a number of appealing applications, from robotic\\nplanning to representation learning. However, learning to predict raw future\\nobservations, such as frames in a video, is exceedingly challenging -- the\\nambiguous nature of the problem can cause a naively designed model to average\\ntogether possible futures into a single, blurry prediction. Recently, this has\\nbeen addressed by two distinct approaches: (a) latent variational variable\\nmodels that explicitly model underlying stochasticity and (b)\\nadversarially-trained models that aim to produce naturalistic images. However,\\na standard latent variable model can struggle to produce realistic results, and\\na standard adversarially-trained model underutilizes latent variables and fails\\nto produce diverse predictions. We show that these distinct methods are in fact\\ncomplementary. Combining the two produces predictions that look more realistic\\nto human raters and better cover the range of possible futures. Our method\\noutperforms prior and concurrent work in these aspects.',\n",
       "    'author': [{'name': 'Alex X. Lee'},\n",
       "     {'name': 'Richard Zhang'},\n",
       "     {'name': 'Frederik Ebert'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Chelsea Finn'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Website: https://alexlee-gk.github.io/video_prediction/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1804.01523v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1804.01523v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.00275v2',\n",
       "    'updated': '2018-07-03T00:47:09Z',\n",
       "    'published': '2018-07-01T06:02:48Z',\n",
       "    'title': 'Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from\\n  LiDAR and Monocular Camera',\n",
       "    'summary': 'Depth completion, the technique of estimating a dense depth image from sparse\\ndepth measurements, has a variety of applications in robotics and autonomous\\ndriving. However, depth completion faces 3 main challenges: the irregularly\\nspaced pattern in the sparse depth input, the difficulty in handling multiple\\nsensor modalities (when color images are available), as well as the lack of\\ndense, pixel-level ground truth depth labels. In this work, we address all\\nthese challenges. Specifically, we develop a deep regression model to learn a\\ndirect mapping from sparse depth (and color images) to dense depth. We also\\npropose a self-supervised training framework that requires only sequences of\\ncolor and sparse depth images, without the need for dense depth labels. Our\\nexperiments demonstrate that our network, when trained with semi-dense\\nannotations, attains state-of-the- art accuracy and is the winning approach on\\nthe KITTI depth completion benchmark at the time of submission. Furthermore,\\nthe self-supervised framework outperforms a number of existing solutions\\ntrained with semi- dense annotations.',\n",
       "    'author': [{'name': 'Fangchang Ma'},\n",
       "     {'name': 'Guilherme Venturelli Cavalheiro'},\n",
       "     {'name': 'Sertac Karaman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Software:\\n  https://github.com/fangchangma/self-supervised-depth-completion . Video:\\n  https://youtu.be/bGXfvF261pc . 12 pages, 6 figures, 3 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.00275v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.00275v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.03480v2',\n",
       "    'updated': '2019-03-06T21:56:52Z',\n",
       "    'published': '2018-07-10T04:55:45Z',\n",
       "    'title': 'Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video\\n  Demonstration',\n",
       "    'summary': 'Our goal is to generate a policy to complete an unseen task given just a\\nsingle video demonstration of the task in a given domain. We hypothesize that\\nto successfully generalize to unseen complex tasks from a single video\\ndemonstration, it is necessary to explicitly incorporate the compositional\\nstructure of the tasks into the model. To this end, we propose Neural Task\\nGraph (NTG) Networks, which use conjugate task graph as the intermediate\\nrepresentation to modularize both the video demonstration and the derived\\npolicy. We empirically show NTG achieves inter-task generalization on two\\ncomplex tasks: Block Stacking in BulletPhysics and Object Collection in\\nAI2-THOR. NTG improves data efficiency with visual input as well as achieve\\nstrong generalization without the need for dense hierarchical supervision. We\\nfurther show that similar performance trends hold when applied to real-world\\ndata. We show that NTG can effectively predict task structure on the JIGSAWS\\nsurgical dataset and generalize to unseen tasks.',\n",
       "    'author': [{'name': 'De-An Huang'},\n",
       "     {'name': 'Suraj Nair'},\n",
       "     {'name': 'Danfei Xu'},\n",
       "     {'name': 'Yuke Zhu'},\n",
       "     {'name': 'Animesh Garg'},\n",
       "     {'name': 'Li Fei-Fei'},\n",
       "     {'name': 'Silvio Savarese'},\n",
       "     {'name': 'Juan Carlos Niebles'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.03480v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.03480v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.07049v1',\n",
       "    'updated': '2018-07-18T17:25:28Z',\n",
       "    'published': '2018-07-18T17:25:28Z',\n",
       "    'title': 'Robot Learning in Homes: Improving Generalization and Reducing Dataset\\n  Bias',\n",
       "    'summary': \"Data-driven approaches to solving robotic tasks have gained a lot of traction\\nin recent years. However, most existing policies are trained on large-scale\\ndatasets collected in curated lab settings. If we aim to deploy these models in\\nunstructured visual environments like people's homes, they will be unable to\\ncope with the mismatch in data distribution. In such light, we present the\\nfirst systematic effort in collecting a large dataset for robotic grasping in\\nhomes. First, to scale and parallelize data collection, we built a low cost\\nmobile manipulator assembled for under 3K USD. Second, data collected using low\\ncost robots suffer from noisy labels due to imperfect execution and calibration\\nerrors. To handle this, we develop a framework which factors out the noise as a\\nlatent variable. Our model is trained on 28K grasps collected in several houses\\nunder an array of different environmental conditions. We evaluate our models by\\nphysically executing grasps on a collection of novel objects in multiple unseen\\nhomes. The models trained with our home dataset showed a marked improvement of\\n43.7% over a baseline model trained with data collected in lab. Our\\narchitecture which explicitly models the latent noise in the dataset also\\nperformed 10% better than one that did not factor out the noise. We hope this\\neffort inspires the robotics community to look outside the lab and embrace\\nlearning based approaches to handle inaccurate cheap robots.\",\n",
       "    'author': [{'name': 'Abhinav Gupta'},\n",
       "     {'name': 'Adithyavairavan Murali'},\n",
       "     {'name': 'Dhiraj Gandhi'},\n",
       "     {'name': 'Lerrel Pinto'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.07049v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.07049v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.08894v2',\n",
       "    'updated': '2018-09-19T05:23:11Z',\n",
       "    'published': '2018-07-24T03:42:53Z',\n",
       "    'title': 'ClusterNet: 3D Instance Segmentation in RGB-D Images',\n",
       "    'summary': \"We propose a method for instance-level segmentation that uses RGB-D data as\\ninput and provides detailed information about the location, geometry and number\\nof individual objects in the scene. This level of understanding is fundamental\\nfor autonomous robots. It enables safe and robust decision-making under the\\nlarge uncertainty of the real-world. In our model, we propose to use the first\\nand second order moments of the object occupancy function to represent an\\nobject instance. We train an hourglass Deep Neural Network (DNN) where each\\npixel in the output votes for the 3D position of the corresponding object\\ncenter and for the object's size and pose. The final instance segmentation is\\nachieved through clustering in the space of moments. The object-centric\\ntraining loss is defined on the output of the clustering. Our method\\noutperforms the state-of-the-art instance segmentation method on our\\nsynthesized dataset. We show that our method generalizes well on real-world\\ndata achieving visually better segmentation results.\",\n",
       "    'author': [{'name': 'Lin Shao'},\n",
       "     {'name': 'Ye Tian'},\n",
       "     {'name': 'Jeannette Bohg'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.08894v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.08894v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1809.00716v1',\n",
       "    'updated': '2018-09-03T20:42:27Z',\n",
       "    'published': '2018-09-03T20:42:27Z',\n",
       "    'title': 'InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes\\n  Dataset',\n",
       "    'summary': 'Datasets have gained an enormous amount of popularity in the computer vision\\ncommunity, from training and evaluation of Deep Learning-based methods to\\nbenchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt,\\nsynthetic imagery bears a vast potential due to scalability in terms of amounts\\nof data obtainable without tedious manual ground truth annotations or\\nmeasurements. Here, we present a dataset with the aim of providing a higher\\ndegree of photo-realism, larger scale, more variability as well as serving a\\nwider range of purposes compared to existing datasets. Our dataset leverages\\nthe availability of millions of professional interior designs and millions of\\nproduction-level furniture and object assets -- all coming with fine geometric\\ndetails and high-resolution texture. We render high-resolution and high\\nframe-rate video sequences following realistic trajectories while supporting\\nvarious camera types as well as providing inertial measurements. Together with\\nthe release of the dataset, we will make executable program of our interactive\\nsimulator software as well as our renderer available at\\nhttps://interiornetdataset.github.io. To showcase the usability and uniqueness\\nof our dataset, we show benchmarking results of both sparse and dense SLAM\\nalgorithms.',\n",
       "    'author': [{'name': 'Wenbin Li',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Department of Computing, Imperial College London, London UK, SW7 2AZ'}},\n",
       "     {'name': 'Sajad Saeedi',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Department of Computing, Imperial College London, London UK, SW7 2AZ'}},\n",
       "     {'name': 'John McCormac',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Department of Computing, Imperial College London, London UK, SW7 2AZ'}},\n",
       "     {'name': 'Ronald Clark',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Department of Computing, Imperial College London, London UK, SW7 2AZ'}},\n",
       "     {'name': 'Dimos Tzoumanikas',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Department of Computing, Imperial College London, London UK, SW7 2AZ'}},\n",
       "     {'name': 'Qing Ye',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'KooLab, Kujiale.com, Hangzhou China'}},\n",
       "     {'name': 'Yuzhong Huang',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'KooLab, Kujiale.com, Hangzhou China'}},\n",
       "     {'name': 'Rui Tang',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'KooLab, Kujiale.com, Hangzhou China'}},\n",
       "     {'name': 'Stefan Leutenegger',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Department of Computing, Imperial College London, London UK, SW7 2AZ'}}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'British Machine Vision Conference (BMVC) 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1809.00716v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1809.00716v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1809.08626v3',\n",
       "    'updated': '2020-02-24T19:37:38Z',\n",
       "    'published': '2018-09-23T16:29:29Z',\n",
       "    'title': 'Domain Adaptation for Robot Predictive Maintenance Systems',\n",
       "    'summary': 'Industrial robots play an increasingly important role in a growing number of\\nfields. For example, robotics is used to increase productivity while reducing\\ncosts in various aspects of manufacturing. Since robots are often set up in\\nproduction lines, the breakdown of a single robot has a negative impact on the\\nentire process, in the worst case bringing the whole line to a halt until the\\nissue is resolved, leading to substantial financial losses due to the\\nunforeseen downtime. Therefore, predictive maintenance systems based on the\\ninternal signals of robots have gained attention as an essential component of\\nrobotics service offerings. The main shortcoming of existing predictive\\nmaintenance algorithms is that the extracted features typically differ\\nsignificantly from the learnt model when the operation of the robot changes,\\nincurring false alarms. In order to mitigate this problem, predictive\\nmaintenance algorithms require the model to be retrained with normal data of\\nthe new operation. In this paper, we propose a novel solution based on transfer\\nlearning to pass the knowledge of the trained model from one operation to\\nanother in order to prevent the need for retraining and to eliminate such false\\nalarms. The deployment of the proposed unsupervised transfer learning algorithm\\non real-world datasets demonstrates that the algorithm can not only distinguish\\nbetween operation and mechanical condition change, it further yields a sharper\\ndeviation from the trained model in case of a mechanical condition change and\\nthus detects mechanical issues with higher confidence.',\n",
       "    'author': [{'name': 'Arash Golibagh Mahyari'}, {'name': 'Thomas Locker'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1809.08626v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1809.08626v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.01849v1',\n",
       "    'updated': '2018-10-03T17:24:06Z',\n",
       "    'published': '2018-10-03T17:24:06Z',\n",
       "    'title': 'SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation',\n",
       "    'summary': 'Recent techniques in self-supervised monocular depth estimation are\\napproaching the performance of supervised methods, but operate in low\\nresolution only. We show that high resolution is key towards high-fidelity\\nself-supervised monocular depth prediction. Inspired by recent deep learning\\nmethods for Single-Image Super-Resolution, we propose a sub-pixel convolutional\\nlayer extension for depth super-resolution that accurately synthesizes\\nhigh-resolution disparities from their corresponding low-resolution\\nconvolutional features. In addition, we introduce a differentiable\\nflip-augmentation layer that accurately fuses predictions from the image and\\nits horizontally flipped version, reducing the effect of left and right shadow\\nregions generated in the disparity map due to occlusions. Both contributions\\nprovide significant performance gains over the state-of-the-art in\\nself-supervised depth and pose estimation on the public KITTI benchmark. A\\nvideo of our approach can be found at https://youtu.be/jKNgBeBMx0I.',\n",
       "    'author': [{'name': 'Sudeep Pillai'},\n",
       "     {'name': 'Rares Ambrus'},\n",
       "     {'name': 'Adrien Gaidon'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 5 figures, 2 tables, ICRA 2019 Submission'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.01849v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.01849v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.04871v1',\n",
       "    'updated': '2018-10-11T07:22:54Z',\n",
       "    'published': '2018-10-11T07:22:54Z',\n",
       "    'title': 'A Data-Efficient Framework for Training and Sim-to-Real Transfer of\\n  Navigation Policies',\n",
       "    'summary': 'Learning effective visuomotor policies for robots purely from data is\\nchallenging, but also appealing since a learning-based system should not\\nrequire manual tuning or calibration. In the case of a robot operating in a\\nreal environment the training process can be costly, time-consuming, and even\\ndangerous since failures are common at the start of training. For this reason,\\nit is desirable to be able to leverage \\\\textit{simulation} and\\n\\\\textit{off-policy} data to the extent possible to train the robot. In this\\nwork, we introduce a robust framework that plans in simulation and transfers\\nwell to the real environment. Our model incorporates a gradient-descent based\\nplanning module, which, given the initial image and goal image, encodes the\\nimages to a lower dimensional latent state and plans a trajectory to reach the\\ngoal. The model, consisting of the encoder and planner modules, is trained\\nthrough a meta-learning strategy in simulation first. We subsequently perform\\nadversarial domain transfer on the encoder by using a bank of unlabelled but\\nrandom images from the simulation and real environments to enable the encoder\\nto map images from the real and simulated environments to a similarly\\ndistributed latent representation. By fine tuning the entire model (encoder +\\nplanner) with far fewer real world expert demonstrations, we show successful\\nplanning performances in different navigation tasks.',\n",
       "    'author': [{'name': 'Homanga Bharadhwaj'},\n",
       "     {'name': 'Zihan Wang'},\n",
       "     {'name': 'Yoshua Bengio'},\n",
       "     {'name': 'Liam Paull'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Under review in ICRA 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.04871v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.04871v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.05017v1',\n",
       "    'updated': '2018-10-11T13:46:18Z',\n",
       "    'published': '2018-10-11T13:46:18Z',\n",
       "    'title': 'One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL',\n",
       "    'summary': 'Humans are experts at high-fidelity imitation -- closely mimicking a\\ndemonstration, often in one attempt. Humans use this ability to quickly solve a\\ntask instance, and to bootstrap learning of new tasks. Achieving these\\nabilities in autonomous agents is an open problem. In this paper, we introduce\\nan off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn\\nboth (i) policies for high-fidelity one-shot imitation of diverse novel skills,\\nand (ii) policies that enable the agent to solve tasks more efficiently than\\nthe demonstrators. MetaMimic relies on the principle of storing all experiences\\nin a memory and replaying these to learn massive deep neural network policies\\nby off-policy RL. This paper introduces, to the best of our knowledge, the\\nlargest existing neural networks for deep RL and shows that larger networks\\nwith normalization are needed to achieve one-shot high-fidelity imitation on a\\nchallenging manipulation task. The results also show that both types of policy\\ncan be learned from vision, in spite of the task rewards being sparse, and\\nwithout access to demonstrator actions.',\n",
       "    'author': [{'name': 'Tom Le Paine'},\n",
       "     {'name': 'Sergio Gómez Colmenarejo'},\n",
       "     {'name': 'Ziyu Wang'},\n",
       "     {'name': 'Scott Reed'},\n",
       "     {'name': 'Yusuf Aytar'},\n",
       "     {'name': 'Tobias Pfaff'},\n",
       "     {'name': 'Matt W. Hoffman'},\n",
       "     {'name': 'Gabriel Barth-Maron'},\n",
       "     {'name': 'Serkan Cabi'},\n",
       "     {'name': 'David Budden'},\n",
       "     {'name': 'Nando de Freitas'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.05017v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.05017v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1812.00568v1',\n",
       "    'updated': '2018-12-03T06:06:25Z',\n",
       "    'published': '2018-12-03T06:06:25Z',\n",
       "    'title': 'Visual Foresight: Model-Based Deep Reinforcement Learning for\\n  Vision-Based Robotic Control',\n",
       "    'summary': 'Deep reinforcement learning (RL) algorithms can learn complex robotic skills\\nfrom raw sensory inputs, but have yet to achieve the kind of broad\\ngeneralization and applicability demonstrated by deep learning methods in\\nsupervised domains. We present a deep RL method that is practical for\\nreal-world robotics tasks, such as robotic manipulation, and generalizes\\neffectively to never-before-seen tasks and objects. In these settings, ground\\ntruth reward signals are typically unavailable, and we therefore propose a\\nself-supervised model-based approach, where a predictive model learns to\\ndirectly predict the future from raw sensory readings, such as camera images.\\nAt test time, we explore three distinct goal specification methods: designated\\npixels, where a user specifies desired object manipulation tasks by selecting\\nparticular pixels in an image and corresponding goal positions, goal images,\\nwhere the desired goal state is specified with an image, and image classifiers,\\nwhich define spaces of goal states. Our deep predictive models are trained\\nusing data collected autonomously and continuously by a robot interacting with\\nhundreds of objects, without human supervision. We demonstrate that visual MPC\\ncan generalize to never-before-seen objects---both rigid and deformable---and\\nsolve a range of user-defined object manipulation tasks using the same model.',\n",
       "    'author': [{'name': 'Frederik Ebert'},\n",
       "     {'name': 'Chelsea Finn'},\n",
       "     {'name': 'Sudeep Dasari'},\n",
       "     {'name': 'Annie Xie'},\n",
       "     {'name': 'Alex Lee'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1812.00568v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1812.00568v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1902.10194v1',\n",
       "    'updated': '2019-02-26T20:13:14Z',\n",
       "    'published': '2019-02-26T20:13:14Z',\n",
       "    'title': 'Learning to See the Wood for the Trees: Deep Laser Localization in Urban\\n  and Natural Environments on a CPU',\n",
       "    'summary': 'Localization in challenging, natural environments such as forests or\\nwoodlands is an important capability for many applications from guiding a robot\\nnavigating along a forest trail to monitoring vegetation growth with handheld\\nsensors. In this work we explore laser-based localization in both urban and\\nnatural environments, which is suitable for online applications. We propose a\\ndeep learning approach capable of learning meaningful descriptors directly from\\n3D point clouds by comparing triplets (anchor, positive and negative examples).\\nThe approach learns a feature space representation for a set of segmented point\\nclouds that are matched between a current and previous observations. Our\\nlearning method is tailored towards loop closure detection resulting in a small\\nmodel which can be deployed using only a CPU. The proposed learning method\\nwould allow the full pipeline to run on robots with limited computational\\npayload such as drones, quadrupeds or UGVs.',\n",
       "    'author': [{'name': 'Georgi Tinchev'},\n",
       "     {'name': 'Adrian Penate-Sanchez'},\n",
       "     {'name': 'Maurice Fallon'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2019.2895264'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2019.2895264',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1902.10194v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1902.10194v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for publication at RA-L/ICRA 2019. More info:\\n  https://ori.ox.ac.uk/esm-localization'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.00445v1',\n",
       "    'updated': '2019-03-01T18:16:03Z',\n",
       "    'published': '2019-03-01T18:16:03Z',\n",
       "    'title': 'A Behavioral Approach to Visual Navigation with Graph Localization\\n  Networks',\n",
       "    'summary': 'Inspired by research in psychology, we introduce a behavioral approach for\\nvisual navigation using topological maps. Our goal is to enable a robot to\\nnavigate from one location to another, relying only on its visual input and the\\ntopological map of the environment. We propose using graph neural networks for\\nlocalizing the agent in the map, and decompose the action space into primitive\\nbehaviors implemented as convolutional or recurrent neural networks. Using the\\nGibson simulator, we verify that our approach outperforms relevant baselines\\nand is able to navigate in both seen and unseen environments.',\n",
       "    'author': [{'name': 'Kevin Chen'},\n",
       "     {'name': 'Juan Pablo de Vicente'},\n",
       "     {'name': 'Gabriel Sepulveda'},\n",
       "     {'name': 'Fei Xia'},\n",
       "     {'name': 'Alvaro Soto'},\n",
       "     {'name': 'Marynel Vazquez'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Video: https://youtu.be/nN3B1F90CFM'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.00445v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.00445v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.11174v1',\n",
       "    'updated': '2019-03-26T22:05:05Z',\n",
       "    'published': '2019-03-26T22:05:05Z',\n",
       "    'title': 'Improved Generalization of Heading Direction Estimation for Aerial\\n  Filming Using Semi-supervised Regression',\n",
       "    'summary': 'In the task of Autonomous aerial filming of a moving actor (e.g. a person or\\na vehicle), it is crucial to have a good heading direction estimation for the\\nactor from the visual input. However, the models obtained in other similar\\ntasks, such as pedestrian collision risk analysis and human-robot interaction,\\nare very difficult to generalize to the aerial filming task, because of the\\ndifference in data distributions. Towards improving generalization with less\\namount of labeled data, this paper presents a semi-supervised algorithm for\\nheading direction estimation problem. We utilize temporal continuity as the\\nunsupervised signal to regularize the model and achieve better generalization\\nability. This semi-supervised algorithm is applied to both training and testing\\nphases, which increases the testing performance by a large margin. We show that\\nby leveraging unlabeled sequences, the amount of labeled data required can be\\nsignificantly reduced. We also discuss several important details on improving\\nthe performance by balancing labeled and unlabeled loss, and making good\\ncombinations. Experimental results show that our approach robustly outputs the\\nheading direction for different types of actor. The aesthetic value of the\\nvideo is also improved in the aerial filming task.',\n",
       "    'author': [{'name': 'Wenshan Wang'},\n",
       "     {'name': 'Aayush Ahuja'},\n",
       "     {'name': 'Yanfu Zhang'},\n",
       "     {'name': 'Rogerio Bonatti'},\n",
       "     {'name': 'Sebastian Scherer'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.11174v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.11174v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.04281v1',\n",
       "    'updated': '2019-04-08T18:17:36Z',\n",
       "    'published': '2019-04-08T18:17:36Z',\n",
       "    'title': '3D Local Features for Direct Pairwise Registration',\n",
       "    'summary': 'We present a novel, data driven approach for solving the problem of\\nregistration of two point cloud scans. Our approach is direct in the sense that\\na single pair of corresponding local patches already provides the necessary\\ntransformation cue for the global registration. To achieve that, we first endow\\nthe state of the art PPF-FoldNet auto-encoder (AE) with a pose-variant sibling,\\nwhere the discrepancy between the two leads to pose-specific descriptors. Based\\nupon this, we introduce RelativeNet, a relative pose estimation network to\\nassign correspondence-specific orientations to the keypoints, eliminating any\\nlocal reference frame computations. Finally, we devise a simple yet effective\\nhypothesize-and-verify algorithm to quickly use the predictions and align two\\npoint sets. Our extensive quantitative and qualitative experiments suggests\\nthat our approach outperforms the state of the art in challenging real datasets\\nof pairwise registration and that augmenting the keypoints with local pose\\ninformation leads to better generalization and a dramatic speed-up.',\n",
       "    'author': [{'name': 'Haowen Deng'},\n",
       "     {'name': 'Tolga Birdal'},\n",
       "     {'name': 'Slobodan Ilic'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in CVPR 2019. 16 pages, identical to the camera ready\\n  submission'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.04281v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.04281v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.04404v1',\n",
       "    'updated': '2019-04-09T00:33:17Z',\n",
       "    'published': '2019-04-09T00:33:17Z',\n",
       "    'title': 'Embodied Visual Recognition',\n",
       "    'summary': 'Passive visual systems typically fail to recognize objects in the amodal\\nsetting where they are heavily occluded. In contrast, humans and other embodied\\nagents have the ability to move in the environment, and actively control the\\nviewing angle to better understand object shapes and semantics. In this work,\\nwe introduce the task of Embodied Visual Recognition (EVR): An agent is\\ninstantiated in a 3D environment close to an occluded target object, and is\\nfree to move in the environment to perform object classification, amodal object\\nlocalization, and amodal object segmentation. To address this, we develop a new\\nmodel called Embodied Mask R-CNN, for agents to learn to move strategically to\\nimprove their visual recognition abilities. We conduct experiments using the\\nHouse3D environment. Experimental results show that: 1) agents with embodiment\\n(movement) achieve better visual recognition performance than passive ones; 2)\\nin order to improve visual recognition abilities, agents can learn strategical\\nmoving paths that are different from shortest paths.',\n",
       "    'author': [{'name': 'Jianwei Yang'},\n",
       "     {'name': 'Zhile Ren'},\n",
       "     {'name': 'Mingze Xu'},\n",
       "     {'name': 'Xinlei Chen'},\n",
       "     {'name': 'David Crandall'},\n",
       "     {'name': 'Devi Parikh'},\n",
       "     {'name': 'Dhruv Batra'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages, 13 figures, technical report'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.04404v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.04404v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.08159v2',\n",
       "    'updated': '2019-05-22T22:46:23Z',\n",
       "    'published': '2019-04-17T09:51:12Z',\n",
       "    'title': '3D Object Recognition with Ensemble Learning --- A Study of Point\\n  Cloud-Based Deep Learning Models',\n",
       "    'summary': 'In this study, we present an analysis of model-based ensemble learning for 3D\\npoint-cloud object classification and detection. An ensemble of multiple model\\ninstances is known to outperform a single model instance, but there is little\\nstudy of the topic of ensemble learning for 3D point clouds. First, an ensemble\\nof multiple model instances trained on the same part of the\\n$\\\\textit{ModelNet40}$ dataset was tested for seven deep learning, point\\ncloud-based classification algorithms: $\\\\textit{PointNet}$,\\n$\\\\textit{PointNet++}$, $\\\\textit{SO-Net}$, $\\\\textit{KCNet}$,\\n$\\\\textit{DeepSets}$, $\\\\textit{DGCNN}$, and $\\\\textit{PointCNN}$. Second, the\\nensemble of different architectures was tested. Results of our experiments show\\nthat the tested ensemble learning methods improve over state-of-the-art on the\\n$\\\\textit{ModelNet40}$ dataset, from $92.65\\\\%$ to $93.64\\\\%$ for the ensemble of\\nsingle architecture instances, $94.03\\\\%$ for two different architectures, and\\n$94.15\\\\%$ for five different architectures. We show that the ensemble of two\\nmodels with different architectures can be as effective as the ensemble of 10\\nmodels with the same architecture. Third, a study on classic bagging i.e. with\\ndifferent subsets used for training multiple model instances) was tested and\\nsources of ensemble accuracy growth were investigated for best-performing\\narchitecture, i.e. $\\\\textit{SO-Net}$. We also investigate the ensemble learning\\nof $\\\\textit{Frustum PointNet}$ approach in the task of 3D object detection,\\nincreasing the average precision of 3D box detection on the $\\\\textit{KITTI}$\\ndataset from $63.1\\\\%$ to $66.5\\\\%$ using only three model instances. We measure\\nthe inference time of all 3D classification architectures on a $\\\\textit{Nvidia\\nJetson TX2}$, a common embedded computer for mobile robots, to allude to the\\nuse of these models in real-life applications.',\n",
       "    'author': [{'name': 'Daniel Koguciuk'},\n",
       "     {'name': 'Łukasz Chechliński'},\n",
       "     {'name': 'Tarek El-Gaaly'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.08159v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.08159v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.12612v2',\n",
       "    'updated': '2019-10-15T08:10:25Z',\n",
       "    'published': '2019-05-29T17:50:19Z',\n",
       "    'title': 'Learning Navigation Subroutines from Egocentric Videos',\n",
       "    'summary': 'Planning at a higher level of abstraction instead of low level torques\\nimproves the sample efficiency in reinforcement learning, and computational\\nefficiency in classical planning. We propose a method to learn such\\nhierarchical abstractions, or subroutines from egocentric video data of experts\\nperforming tasks. We learn a self-supervised inverse model on small amounts of\\nrandom interaction data to pseudo-label the expert egocentric videos with agent\\nactions. Visuomotor subroutines are acquired from these pseudo-labeled videos\\nby learning a latent intent-conditioned policy that predicts the inferred\\npseudo-actions from the corresponding image observations. We demonstrate our\\nproposed approach in context of navigation, and show that we can successfully\\nlearn consistent and diverse visuomotor subroutines from passive egocentric\\nvideos. We demonstrate the utility of our acquired visuomotor subroutines by\\nusing them as is for exploration, and as sub-policies in a hierarchical RL\\nframework for reaching point goals and semantic goals. We also demonstrate\\nbehavior of our subroutines in the real world, by deploying them on a real\\nrobotic platform. Project website:\\nhttps://ashishkumar1993.github.io/subroutines/.',\n",
       "    'author': [{'name': 'Ashish Kumar'},\n",
       "     {'name': 'Saurabh Gupta'},\n",
       "     {'name': 'Jitendra Malik'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.12612v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.12612v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.01140v2',\n",
       "    'updated': '2019-09-05T06:47:05Z',\n",
       "    'published': '2019-06-04T00:33:56Z',\n",
       "    'title': 'Learning Object Bounding Boxes for 3D Instance Segmentation on Point\\n  Clouds',\n",
       "    'summary': 'We propose a novel, conceptually simple and general framework for instance\\nsegmentation on 3D point clouds. Our method, called 3D-BoNet, follows the\\nsimple design philosophy of per-point multilayer perceptrons (MLPs). The\\nframework directly regresses 3D bounding boxes for all instances in a point\\ncloud, while simultaneously predicting a point-level mask for each instance. It\\nconsists of a backbone network followed by two parallel network branches for 1)\\nbounding box regression and 2) point mask prediction. 3D-BoNet is single-stage,\\nanchor-free and end-to-end trainable. Moreover, it is remarkably\\ncomputationally efficient as, unlike existing approaches, it does not require\\nany post-processing steps such as non-maximum suppression, feature sampling,\\nclustering or voting. Extensive experiments show that our approach surpasses\\nexisting work on both ScanNet and S3DIS datasets while being approximately 10x\\nmore computationally efficient. Comprehensive ablation studies demonstrate the\\neffectiveness of our design.',\n",
       "    'author': [{'name': 'Bo Yang'},\n",
       "     {'name': 'Jianan Wang'},\n",
       "     {'name': 'Ronald Clark'},\n",
       "     {'name': 'Qingyong Hu'},\n",
       "     {'name': 'Sen Wang'},\n",
       "     {'name': 'Andrew Markham'},\n",
       "     {'name': 'Niki Trigoni'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2019 Spotlight. Code and data are available at\\n  https://github.com/Yang7879/3D-BoNet'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.01140v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.01140v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.03853v2',\n",
       "    'updated': '2019-06-11T18:10:45Z',\n",
       "    'published': '2019-06-10T09:13:02Z',\n",
       "    'title': 'DensePhysNet: Learning Dense Physical Object Representations via\\n  Multi-step Dynamic Interactions',\n",
       "    'summary': \"We study the problem of learning physical object representations for robot\\nmanipulation. Understanding object physics is critical for successful object\\nmanipulation, but also challenging because physical object properties can\\nrarely be inferred from the object's static appearance. In this paper, we\\npropose DensePhysNet, a system that actively executes a sequence of dynamic\\ninteractions (e.g., sliding and colliding), and uses a deep predictive model\\nover its visual observations to learn dense, pixel-wise representations that\\nreflect the physical properties of observed objects. Our experiments in both\\nsimulation and real settings demonstrate that the learned representations carry\\nrich physical information, and can directly be used to decode physical object\\nproperties such as friction and mass. The use of dense representation enables\\nDensePhysNet to generalize well to novel scenes with more objects than in\\ntraining. With knowledge of object physics, the learned representation also\\nleads to more accurate and efficient manipulation in downstream tasks than the\\nstate-of-the-art.\",\n",
       "    'author': [{'name': 'Zhenjia Xu'},\n",
       "     {'name': 'Jiajun Wu'},\n",
       "     {'name': 'Andy Zeng'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Shuran Song'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'RSS 2019. Project page: http://zhenjiaxu.com/DensePhysNet'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.03853v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.03853v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1907.05553v1',\n",
       "    'updated': '2019-07-12T02:40:37Z',\n",
       "    'published': '2019-07-12T02:40:37Z',\n",
       "    'title': 'MLR (Memory, Learning and Recognition): A General Cognitive Model --\\n  applied to Intelligent Robots and Systems Control',\n",
       "    'summary': 'This paper introduces a new perspective of intelligent robots and systems\\ncontrol. The presented and proposed cognitive model: Memory, Learning and\\nRecognition (MLR), is an effort to bridge the gap between Robotics, AI,\\nCognitive Science, and Neuroscience. The currently existing gap prevents us\\nfrom integrating the current advancement and achievements of these four\\nresearch fields which are actively trying to define intelligence in either\\napplication-based way or in generic way. This cognitive model defines\\nintelligence more specifically, parametrically and detailed. The proposed MLR\\nmodel helps us create a general control model for robots and systems\\nindependent of their application domains and platforms since it is mainly based\\non the dataset provided for robots and systems controls. This paper is mainly\\nproposing and introducing this concept and trying to prove this concept in a\\nsmall scale, firstly through experimentation. The proposed concept is also\\napplicable to other different platforms in real-time as well as in simulation.',\n",
       "    'author': {'name': 'Aras R. Dargazany'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1907.05553v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1907.05553v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.09295v2',\n",
       "    'updated': '2020-09-25T08:52:13Z',\n",
       "    'published': '2019-09-20T02:17:20Z',\n",
       "    'title': 'Learning Your Way Without Map or Compass: Panoramic Target Driven Visual\\n  Navigation',\n",
       "    'summary': 'We present a robot navigation system that uses an imitation learning\\nframework to successfully navigate in complex environments. Our framework takes\\na pre-built 3D scan of a real environment and trains an agent from\\npre-generated expert trajectories to navigate to any position given a panoramic\\nview of the goal and the current visual input without relying on map, compass,\\nodometry, or relative position of the target at runtime. Our end-to-end trained\\nagent uses RGB and depth (RGBD) information and can handle large environments\\n(up to $1031m^2$) across multiple rooms (up to $40$) and generalizes to unseen\\ntargets. We show that when compared to several baselines our method (1)\\nrequires fewer training examples and less training time, (2) reaches the goal\\nlocation with higher accuracy, and (3) produces better solutions with shorter\\npaths for long-range navigation tasks.',\n",
       "    'author': [{'name': 'David Watkins-Valls'},\n",
       "     {'name': 'Jingxi Xu'},\n",
       "     {'name': 'Nicholas Waytowich'},\n",
       "     {'name': 'Peter Allen'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.09295v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.09295v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.10304v2',\n",
       "    'updated': '2019-11-28T10:38:02Z',\n",
       "    'published': '2019-09-23T11:50:46Z',\n",
       "    'title': 'Where to Look Next: Unsupervised Active Visual Exploration on 360°\\n  Input',\n",
       "    'summary': \"We address the problem of active visual exploration of large 360{\\\\deg}\\ninputs. In our setting an active agent with a limited camera bandwidth explores\\nits 360{\\\\deg} environment by changing its viewing direction at limited discrete\\ntime steps. As such, it observes the world as a sequence of narrow\\nfield-of-view 'glimpses', deciding for itself where to look next. Our proposed\\nmethod exceeds previous works' performance by a significant margin without the\\nneed for deep reinforcement learning or training separate networks as\\nsidekicks. A key component of our system are the spatial memory maps that make\\nthe system aware of the glimpses' orientations (locations in the 360{\\\\deg}\\nimage). Further, we stress the advantages of retina-like glimpses when the\\nagent's sensor bandwidth and time-steps are limited. Finally, we use our\\ntrained model to do classification of the whole scene using only the\\ninformation observed in the glimpses.\",\n",
       "    'author': [{'name': 'Soroush Seifi'}, {'name': 'Tinne Tuytelaars'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Oral Presentation and best Paper Award at 360 Perception and\\n  Interaction Workshop at ICCV 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.10304v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.10304v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.10312v2',\n",
       "    'updated': '2019-11-28T10:38:42Z',\n",
       "    'published': '2019-09-23T12:12:17Z',\n",
       "    'title': 'How to improve CNN-based 6-DoF camera pose estimation',\n",
       "    'summary': \"Convolutional neural networks (CNNs) and transfer learning have recently been\\nused for 6 degrees of freedom (6-DoF) camera pose estimation. While they do not\\nreach the same accuracy as visual SLAM-based approaches and are restricted to a\\nspecific environment, they excel in robustness and can be applied even to a\\nsingle image. In this paper, we study PoseNet [1] and investigate modifications\\nbased on datasets' characteristics to improve the accuracy of the pose\\nestimates. In particular, we emphasize the importance of field-of-view over\\nimage resolution; we present a data augmentation scheme to reduce overfitting;\\nwe study the effect of Long-Short-Term-Memory (LSTM) cells. Lastly, we combine\\nthese modifications and improve PoseNet's performance for monocular CNN based\\ncamera pose regression.\",\n",
       "    'author': [{'name': 'Soroush Seifi'}, {'name': 'Tinne Tuytelaars'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at Deep Learning for Visual SLAM workshop at ICCV 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.10312v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.10312v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.11730v4',\n",
       "    'updated': '2020-08-15T18:10:40Z',\n",
       "    'published': '2019-09-25T19:50:36Z',\n",
       "    'title': '\"Good Robot!\": Efficient Reinforcement Learning for Multi-Step Visual\\n  Tasks with Sim to Real Transfer',\n",
       "    'summary': 'Current Reinforcement Learning (RL) algorithms struggle with long-horizon\\ntasks where time can be wasted exploring dead ends and task progress may be\\neasily reversed. We develop the SPOT framework, which explores within action\\nsafety zones, learns about unsafe regions without exploring them, and\\nprioritizes experiences that reverse earlier progress to learn with remarkable\\nefficiency.\\n  The SPOT framework successfully completes simulated trials of a variety of\\ntasks, improving a baseline trial success rate from 13% to 100% when stacking 4\\ncubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when\\nclearing toys arranged in adversarial patterns. Efficiency with respect to\\nactions per trial typically improves by 30% or more, while training takes just\\n1-20k actions, depending on the task.\\n  Furthermore, we demonstrate direct sim to real transfer. We are able to\\ncreate real stacks in 100% of trials with 61% efficiency and real rows in 100%\\nof trials with 59% efficiency by directly loading the simulation-trained model\\non the real robot with no additional real-world fine-tuning. To our knowledge,\\nthis is the first instance of reinforcement learning with successful sim to\\nreal transfer applied to long term multi-step tasks such as block-stacking and\\nrow-making with consideration of progress reversal. Code is available at\\nhttps://github.com/jhu-lcsr/good_robot .',\n",
       "    'author': [{'name': 'Andrew Hundt'},\n",
       "     {'name': 'Benjamin Killeen'},\n",
       "     {'name': 'Nicholas Greene'},\n",
       "     {'name': 'Hongtao Wu'},\n",
       "     {'name': 'Heeyeon Kwon'},\n",
       "     {'name': 'Chris Paxton'},\n",
       "     {'name': 'Gregory D. Hager'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2020.3015448'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2020.3015448',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1909.11730v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.11730v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to the journal IEEE Robotics and Automation Letters (RA-L)\\n  and to be presented at IROS 2020. This is a minor update to v3. 8 pages, 6\\n  figures, 3 tables, 1 algorithm. Code is available at\\n  https://github.com/jhu-lcsr/good_robot and a video overview is at\\n  https://youtu.be/MbCuEZadkIw'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.12271v1',\n",
       "    'updated': '2019-09-26T17:26:18Z',\n",
       "    'published': '2019-09-26T17:26:18Z',\n",
       "    'title': 'RLBench: The Robot Learning Benchmark & Learning Environment',\n",
       "    'summary': \"We present a challenging new benchmark and learning-environment for robot\\nlearning: RLBench. The benchmark features 100 completely unique, hand-designed\\ntasks ranging in difficulty, from simple target reaching and door opening, to\\nlonger multi-stage tasks, such as opening an oven and placing a tray in it. We\\nprovide an array of both proprioceptive observations and visual observations,\\nwhich include rgb, depth, and segmentation masks from an over-the-shoulder\\nstereo camera and an eye-in-hand monocular camera. Uniquely, each task comes\\nwith an infinite supply of demos through the use of motion planners operating\\non a series of waypoints given during task creation time; enabling an exciting\\nflurry of demonstration-based learning. RLBench has been designed with\\nscalability in mind; new tasks, along with their motion-planned demos, can be\\neasily created and then verified by a series of tools, allowing users to submit\\ntheir own tasks to the RLBench task repository. This large-scale benchmark aims\\nto accelerate progress in a number of vision-guided manipulation research\\nareas, including: reinforcement learning, imitation learning, multi-task\\nlearning, geometric computer vision, and in particular, few-shot learning. With\\nthe benchmark's breadth of tasks and demonstrations, we propose the first\\nlarge-scale few-shot challenge in robotics. We hope that the scale and\\ndiversity of RLBench offers unparalleled research opportunities in the robot\\nlearning community and beyond.\",\n",
       "    'author': [{'name': 'Stephen James'},\n",
       "     {'name': 'Zicong Ma'},\n",
       "     {'name': 'David Rovick Arrojo'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Videos and code: https://sites.google.com/view/rlbench'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.12271v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.12271v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.13072v1',\n",
       "    'updated': '2019-09-28T11:30:24Z',\n",
       "    'published': '2019-09-28T11:30:24Z',\n",
       "    'title': 'Regression Planning Networks',\n",
       "    'summary': 'Recent learning-to-plan methods have shown promising results on planning\\ndirectly from observation space. Yet, their ability to plan for long-horizon\\ntasks is limited by the accuracy of the prediction model. On the other hand,\\nclassical symbolic planners show remarkable capabilities in solving\\nlong-horizon tasks, but they require predefined symbolic rules and symbolic\\nstates, restricting their real-world applicability. In this work, we combine\\nthe benefits of these two paradigms and propose a learning-to-plan method that\\ncan directly generate a long-term symbolic plan conditioned on high-dimensional\\nobservations. We borrow the idea of regression (backward) planning from\\nclassical planning literature and introduce Regression Planning Networks (RPN),\\na neural network architecture that plans backward starting at a task goal and\\ngenerates a sequence of intermediate goals that reaches the current\\nobservation. We show that our model not only inherits many favorable traits\\nfrom symbolic planning, e.g., the ability to solve previously unseen tasks but\\nalso can learn from visual inputs in an end-to-end manner. We evaluate the\\ncapabilities of RPN in a grid world environment and a simulated 3D kitchen\\nenvironment featuring complex visual scenes and long task horizons, and show\\nthat it achieves near-optimal performance in completely new task instances.',\n",
       "    'author': [{'name': 'Danfei Xu'},\n",
       "     {'name': 'Roberto Martín-Martín'},\n",
       "     {'name': 'De-An Huang'},\n",
       "     {'name': 'Yuke Zhu'},\n",
       "     {'name': 'Silvio Savarese'},\n",
       "     {'name': 'Li Fei-Fei'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at NeurIPS 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.13072v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.13072v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.03997v2',\n",
       "    'updated': '2020-06-25T07:42:58Z',\n",
       "    'published': '2019-10-09T14:04:59Z',\n",
       "    'title': 'Semantic Understanding of Foggy Scenes with Purely Synthetic Data',\n",
       "    'summary': 'This work addresses the problem of semantic scene understanding under foggy\\nroad conditions. Although marked progress has been made in semantic scene\\nunderstanding over the recent years, it is mainly concentrated on clear weather\\noutdoor scenes. Extending semantic segmentation methods to adverse weather\\nconditions like fog is crucially important for outdoor applications such as\\nself-driving cars. In this paper, we propose a novel method, which uses purely\\nsynthetic data to improve the performance on unseen real-world foggy scenes\\ncaptured in the streets of Zurich and its surroundings. Our results highlight\\nthe potential and power of photo-realistic synthetic images for training and\\nespecially fine-tuning deep neural nets. Our contributions are threefold, 1) we\\ncreated a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor\\nscenes, that we call Foggy Synscapes and plan to release publicly 2) we show\\nthat with this data we outperform previous approaches on real-world foggy test\\ndata 3) we show that a combination of our data and previously used data can\\neven further improve the performance on real-world foggy data.',\n",
       "    'author': [{'name': 'Martin Hahner'},\n",
       "     {'name': 'Dengxin Dai'},\n",
       "     {'name': 'Christos Sakaridis'},\n",
       "     {'name': 'Jan-Nico Zaech'},\n",
       "     {'name': 'Luc Van Gool'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ITSC.2019.8917518'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ITSC.2019.8917518',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1910.03997v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.03997v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'independent class IoU scores corrected for BiSiNet architecture'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.01103v1',\n",
       "    'updated': '2019-11-04T10:07:27Z',\n",
       "    'published': '2019-11-04T10:07:27Z',\n",
       "    'title': 'Learning One-Shot Imitation from Humans without Humans',\n",
       "    'summary': 'Humans can naturally learn to execute a new task by seeing it performed by\\nother individuals once, and then reproduce it in a variety of configurations.\\nEndowing robots with this ability of imitating humans from third person is a\\nvery immediate and natural way of teaching new tasks. Only recently, through\\nmeta-learning, there have been successful attempts to one-shot imitation\\nlearning from humans; however, these approaches require a lot of human\\nresources to collect the data in the real world to train the robot. But is\\nthere a way to remove the need for real world human demonstrations during\\ntraining? We show that with Task-Embedded Control Networks, we can infer\\ncontrol polices by embedding human demonstrations that can condition a control\\npolicy and achieve one-shot imitation learning. Importantly, we do not use a\\nreal human arm to supply demonstrations during training, but instead leverage\\ndomain randomisation in an application that has not been seen before:\\nsim-to-real transfer on humans. Upon evaluating our approach on pushing and\\nplacing tasks in both simulation and in the real world, we show that in\\ncomparison to a system that was trained on real-world data we are able to\\nachieve similar results by utilising only simulation data.',\n",
       "    'author': [{'name': 'Alessandro Bonardi'},\n",
       "     {'name': 'Stephen James'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Videos can be found here:\\n  https://sites.google.com/view/tecnets-humans'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.01103v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.01103v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.07246v1',\n",
       "    'updated': '2019-11-17T14:32:20Z',\n",
       "    'published': '2019-11-17T14:32:20Z',\n",
       "    'title': 'IKEA Furniture Assembly Environment for Long-Horizon Complex\\n  Manipulation Tasks',\n",
       "    'summary': 'The IKEA Furniture Assembly Environment is one of the first benchmarks for\\ntesting and accelerating the automation of complex manipulation tasks. The\\nenvironment is designed to advance reinforcement learning from simple toy tasks\\nto complex tasks requiring both long-term planning and sophisticated low-level\\ncontrol. Our environment supports over 80 different furniture models, Sawyer\\nand Baxter robot simulation, and domain randomization. The IKEA Furniture\\nAssembly Environment is a testbed for methods aiming to solve complex\\nmanipulation tasks. The environment is publicly available at\\nhttps://clvrai.com/furniture',\n",
       "    'author': [{'name': 'Youngwoon Lee'},\n",
       "     {'name': 'Edward S. Hu'},\n",
       "     {'name': 'Zhengyu Yang'},\n",
       "     {'name': 'Alex Yin'},\n",
       "     {'name': 'Joseph J. Lim'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Simulator'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.07246v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.07246v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.00177v2',\n",
       "    'updated': '2019-12-05T18:17:45Z',\n",
       "    'published': '2019-11-30T10:24:45Z',\n",
       "    'title': 'Urban Driving with Conditional Imitation Learning',\n",
       "    'summary': 'Hand-crafting generalised decision-making rules for real-world urban\\nautonomous driving is hard. Alternatively, learning behaviour from\\neasy-to-collect human driving demonstrations is appealing. Prior work has\\nstudied imitation learning (IL) for autonomous driving with a number of\\nlimitations. Examples include only performing lane-following rather than\\nfollowing a user-defined route, only using a single camera view or heavily\\ncropped frames lacking state observability, only lateral (steering) control,\\nbut not longitudinal (speed) control and a lack of interaction with traffic.\\nImportantly, the majority of such systems have been primarily evaluated in\\nsimulation - a simple domain, which lacks real-world complexities. Motivated by\\nthese challenges, we focus on learning representations of semantics, geometry\\nand motion with computer vision for IL from human driving demonstrations. As\\nour main contribution, we present an end-to-end conditional imitation learning\\napproach, combining both lateral and longitudinal control on a real vehicle for\\nfollowing urban routes with simple traffic. We address inherent dataset bias by\\ndata balancing, training our final policy on approximately 30 hours of\\ndemonstrations gathered over six months. We evaluate our method on an\\nautonomous vehicle by driving 35km of novel routes in European urban streets.',\n",
       "    'author': [{'name': 'Jeffrey Hawke'},\n",
       "     {'name': 'Richard Shen'},\n",
       "     {'name': 'Corina Gurau'},\n",
       "     {'name': 'Siddharth Sharma'},\n",
       "     {'name': 'Daniele Reda'},\n",
       "     {'name': 'Nikolay Nikolov'},\n",
       "     {'name': 'Przemyslaw Mazur'},\n",
       "     {'name': 'Sean Micklethwaite'},\n",
       "     {'name': 'Nicolas Griffiths'},\n",
       "     {'name': 'Amar Shah'},\n",
       "     {'name': 'Alex Kendall'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Under submission; added acknowledgements'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1912.00177v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.00177v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.06321v2',\n",
       "    'updated': '2020-08-17T03:26:55Z',\n",
       "    'published': '2019-12-13T04:29:38Z',\n",
       "    'title': 'Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World\\n  Performance?',\n",
       "    'summary': \"Does progress in simulation translate to progress on robots? If one method\\noutperforms another in simulation, how likely is that trend to hold in reality\\non a robot? We examine this question for embodied PointGoal navigation,\\ndeveloping engineering tools and a research paradigm for evaluating a simulator\\nby its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),\\na library for seamless execution of identical code on simulated agents and\\nrobots, transferring simulation-trained agents to a LoCoBot platform with a\\none-line code change. Second, we investigate the sim2real predictivity of\\nHabitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create\\na virtualized replica, and run parallel tests of 9 different models in reality\\nand simulation. We present a new metric called Sim-vs-Real Correlation\\nCoefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as\\nused for the CVPR19 challenge is low (0.18 for the success metric), suggesting\\nthat performance differences in this simulator-based challenge do not persist\\nafter physical deployment. This gap is largely due to AI agents learning to\\nexploit simulator imperfections, abusing collision dynamics to 'slide' along\\nwalls, leading to shortcuts through otherwise non-navigable space. Naturally,\\nsuch exploits do not work in the real world. Our experiments show that it is\\npossible to tune simulation parameters to improve sim2real predictivity (e.g.\\nimproving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that\\nin-simulation comparisons will translate to deployed systems in reality.\",\n",
       "    'author': [{'name': 'Abhishek Kadian'},\n",
       "     {'name': 'Joanne Truong'},\n",
       "     {'name': 'Aaron Gokaslan'},\n",
       "     {'name': 'Alexander Clegg'},\n",
       "     {'name': 'Erik Wijmans'},\n",
       "     {'name': 'Stefan Lee'},\n",
       "     {'name': 'Manolis Savva'},\n",
       "     {'name': 'Sonia Chernova'},\n",
       "     {'name': 'Dhruv Batra'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2020.3013848'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2020.3013848',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1912.06321v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.06321v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters (RA-L) 2020'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2001.04835v2',\n",
       "    'updated': '2020-01-15T07:07:10Z',\n",
       "    'published': '2020-01-14T15:00:09Z',\n",
       "    'title': 'Knowledge Representations in Technical Systems -- A Taxonomy',\n",
       "    'summary': 'The recent usage of technical systems in human-centric environments leads to\\nthe question, how to teach technical systems, e.g., robots, to understand,\\nlearn, and perform tasks desired by the human. Therefore, an accurate\\nrepresentation of knowledge is essential for the system to work as expected.\\nThis article mainly gives insight into different knowledge representation\\ntechniques and their categorization into various problem domains in artificial\\nintelligence. Additionally, applications of presented knowledge representations\\nare introduced in everyday robotics tasks. By means of the provided taxonomy,\\nthe search for a proper knowledge representation technique regarding a specific\\nproblem should be facilitated.',\n",
       "    'author': [{'name': 'Kristina Scharei'},\n",
       "     {'name': 'Florian Heidecker'},\n",
       "     {'name': 'Maarten Bieshaar'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '21 pages, 7 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2001.04835v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2001.04835v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.00504v1',\n",
       "    'updated': '2020-03-01T15:37:48Z',\n",
       "    'published': '2020-03-01T15:37:48Z',\n",
       "    'title': 'MonoPair: Monocular 3D Object Detection Using Pairwise Spatial\\n  Relationships',\n",
       "    'summary': 'Monocular 3D object detection is an essential component in autonomous driving\\nwhile challenging to solve, especially for those occluded samples which are\\nonly partially visible. Most detectors consider each 3D object as an\\nindependent training target, inevitably resulting in a lack of useful\\ninformation for occluded samples. To this end, we propose a novel method to\\nimprove the monocular 3D object detection by considering the relationship of\\npaired samples. This allows us to encode spatial constraints for\\npartially-occluded objects from their adjacent neighbors. Specifically, the\\nproposed detector computes uncertainty-aware predictions for object locations\\nand 3D distances for the adjacent object pairs, which are subsequently jointly\\noptimized by nonlinear least squares. Finally, the one-stage uncertainty-aware\\nprediction structure and the post-optimization module are dedicatedly\\nintegrated for ensuring the run-time efficiency. Experiments demonstrate that\\nour method yields the best performance on KITTI 3D detection benchmark, by\\noutperforming state-of-the-art competitors by wide margins, especially for the\\nhard samples.',\n",
       "    'author': [{'name': 'Yongjian Chen'},\n",
       "     {'name': 'Lei Tai'},\n",
       "     {'name': 'Kai Sun'},\n",
       "     {'name': 'Mingyang Li'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2020 accepted'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.00504v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.00504v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.11973v1',\n",
       "    'updated': '2020-03-22T03:24:31Z',\n",
       "    'published': '2020-03-22T03:24:31Z',\n",
       "    'title': 'GISNet: Graph-Based Information Sharing Network For Vehicle Trajectory\\n  Prediction',\n",
       "    'summary': 'The trajectory prediction is a critical and challenging problem in the design\\nof an autonomous driving system. Many AI-oriented companies, such as Google\\nWaymo, Uber and DiDi, are investigating more accurate vehicle trajectory\\nprediction algorithms. However, the prediction performance is governed by lots\\nof entangled factors, such as the stochastic behaviors of surrounding vehicles,\\nhistorical information of self-trajectory, and relative positions of neighbors,\\netc. In this paper, we propose a novel graph-based information sharing network\\n(GISNet) that allows the information sharing between the target vehicle and its\\nsurrounding vehicles. Meanwhile, the model encodes the historical trajectory\\ninformation of all the vehicles in the scene. Experiments are carried out on\\nthe public NGSIM US-101 and I-80 Dataset and the prediction performance is\\nmeasured by the Root Mean Square Error (RMSE). The quantitative and qualitative\\nexperimental results show that our model significantly improves the trajectory\\nprediction accuracy, by up to 50.00%, compared to existing models.',\n",
       "    'author': [{'name': 'Ziyi Zhao'},\n",
       "     {'name': 'Haowen Fang'},\n",
       "     {'name': 'Zhao Jin'},\n",
       "     {'name': 'Qinru Qiu'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.11973v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.11973v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.02673v1',\n",
       "    'updated': '2020-04-06T13:46:54Z',\n",
       "    'published': '2020-04-06T13:46:54Z',\n",
       "    'title': 'SHOP-VRB: A Visual Reasoning Benchmark for Object Perception',\n",
       "    'summary': \"In this paper we present an approach and a benchmark for visual reasoning in\\nrobotics applications, in particular small object grasping and manipulation.\\nThe approach and benchmark are focused on inferring object properties from\\nvisual and text data. It concerns small household objects with their\\nproperties, functionality, natural language descriptions as well as\\nquestion-answer pairs for visual reasoning queries along with their\\ncorresponding scene semantic representations. We also present a method for\\ngenerating synthetic data which allows to extend the benchmark to other objects\\nor scenes and propose an evaluation protocol that is more challenging than in\\nthe existing datasets. We propose a reasoning system based on symbolic program\\nexecution. A disentangled representation of the visual and textual inputs is\\nobtained and used to execute symbolic programs that represent a 'reasoning\\nprocess' of the algorithm. We perform a set of experiments on the proposed\\nbenchmark and compare to results for the state of the art methods. These\\nresults expose the shortcomings of the existing benchmarks that may lead to\\nmisleading conclusions on the actual performance of the visual reasoning\\nsystems.\",\n",
       "    'author': [{'name': 'Michal Nazarczuk'}, {'name': 'Krystian Mikolajczyk'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Robotics and Automation (ICRA) 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2004.02673v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.02673v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.04954v1',\n",
       "    'updated': '2020-04-10T08:59:32Z',\n",
       "    'published': '2020-04-10T08:59:32Z',\n",
       "    'title': 'Learning to Visually Navigate in Photorealistic Environments Without any\\n  Supervision',\n",
       "    'summary': 'Learning to navigate in a realistic setting where an agent must rely solely\\non visual inputs is a challenging task, in part because the lack of position\\ninformation makes it difficult to provide supervision during training. In this\\npaper, we introduce a novel approach for learning to navigate from image inputs\\nwithout external supervision or reward. Our approach consists of three stages:\\nlearning a good representation of first-person views, then learning to explore\\nusing memory, and finally learning to navigate by setting its own goals. The\\nmodel is trained with intrinsic rewards only so that it can be applied to any\\nenvironment with image observations. We show the benefits of our approach by\\ntraining an agent to navigate challenging photo-realistic environments from the\\nGibson dataset with RGB inputs only.',\n",
       "    'author': [{'name': 'Lina Mezghani'},\n",
       "     {'name': 'Sainbayar Sukhbaatar'},\n",
       "     {'name': 'Arthur Szlam'},\n",
       "     {'name': 'Armand Joulin'},\n",
       "     {'name': 'Piotr Bojanowski'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2004.04954v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.04954v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.05155v1',\n",
       "    'updated': '2020-04-10T17:57:29Z',\n",
       "    'published': '2020-04-10T17:57:29Z',\n",
       "    'title': 'Learning to Explore using Active Neural SLAM',\n",
       "    'summary': \"This work presents a modular and hierarchical approach to learn policies for\\nexploring 3D environments, called `Active Neural SLAM'. Our approach leverages\\nthe strengths of both classical and learning-based methods, by using analytical\\npath planners with learned SLAM module, and global and local policies. The use\\nof learning provides flexibility with respect to input modalities (in the SLAM\\nmodule), leverages structural regularities of the world (in global policies),\\nand provides robustness to errors in state estimation (in local policies). Such\\nuse of learning within each module retains its benefits, while at the same\\ntime, hierarchical decomposition and modular training allow us to sidestep the\\nhigh sample complexities associated with training end-to-end policies. Our\\nexperiments in visually and physically realistic simulated 3D environments\\ndemonstrate the effectiveness of our approach over past learning and\\ngeometry-based approaches. The proposed model can also be easily transferred to\\nthe PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal\\nNavigation Challenge.\",\n",
       "    'author': [{'name': 'Devendra Singh Chaplot'},\n",
       "     {'name': 'Dhiraj Gandhi'},\n",
       "     {'name': 'Saurabh Gupta'},\n",
       "     {'name': 'Abhinav Gupta'},\n",
       "     {'name': 'Ruslan Salakhutdinov'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in ICLR-2020. See the project webpage at\\n  https://devendrachaplot.github.io/projects/Neural-SLAM for supplementary\\n  videos. The code is available at\\n  https://github.com/devendrachaplot/Neural-SLAM'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2004.05155v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.05155v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.07950v2',\n",
       "    'updated': '2020-09-30T22:24:32Z',\n",
       "    'published': '2020-04-15T17:29:10Z',\n",
       "    'title': 'Learning visual policies for building 3D shape categories',\n",
       "    'summary': 'Manipulation and assembly tasks require non-trivial planning of actions\\ndepending on the environment and the final goal. Previous work in this domain\\noften assembles particular instances of objects from known sets of primitives.\\nIn contrast, we aim to handle varying sets of primitives and to construct\\ndifferent objects of a shape category. Given a single object instance of a\\ncategory, e.g. an arch, and a binary shape classifier, we learn a visual policy\\nto assemble other instances of the same category. In particular, we propose a\\ndisassembly procedure and learn a state policy that discovers new object\\ninstances and their assembly plans in state space. We then render simulated\\nstates in the observation space and learn a heatmap representation to predict\\nalternative actions from a given input image. To validate our approach, we\\nfirst demonstrate its efficiency for building object categories in state space.\\nWe then show the success of our visual policies for building arches from\\ndifferent primitives. Moreover, we demonstrate (i) the reactive ability of our\\nmethod to re-assemble objects using additional primitives and (ii) the robust\\nperformance of our policy for unseen primitives resembling building blocks used\\nduring training. Our visual assembly policies are trained with no real images\\nand reach up to 95% success rate when evaluated on a real robot.',\n",
       "    'author': [{'name': 'Alexander Pashevich'},\n",
       "     {'name': 'Igor Kalevatykh'},\n",
       "     {'name': 'Ivan Laptev'},\n",
       "     {'name': 'Cordelia Schmid'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IROS 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2004.07950v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.07950v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.08051v3',\n",
       "    'updated': '2021-04-08T19:52:37Z',\n",
       "    'published': '2020-04-17T03:36:50Z',\n",
       "    'title': 'Approximate Inverse Reinforcement Learning from Vision-based Imitation\\n  Learning',\n",
       "    'summary': 'In this work, we present a method for obtaining an implicit objective\\nfunction for vision-based navigation. The proposed methodology relies on\\nImitation Learning, Model Predictive Control (MPC), and an interpretation\\ntechnique used in Deep Neural Networks. We use Imitation Learning as a means to\\ndo Inverse Reinforcement Learning in order to create an approximate cost\\nfunction generator for a visual navigation challenge. The resulting cost\\nfunction, the costmap, is used in conjunction with MPC for real-time control\\nand outperforms other state-of-the-art costmap generators in novel\\nenvironments. The proposed process allows for simple training and robustness to\\nout-of-sample data. We apply our method to the task of vision-based autonomous\\ndriving in multiple real and simulated environments and show its\\ngeneralizability.',\n",
       "    'author': [{'name': 'Keuntaek Lee'},\n",
       "     {'name': 'Bogdan Vlahov'},\n",
       "     {'name': 'Jason Gibson'},\n",
       "     {'name': 'James M. Rehg'},\n",
       "     {'name': 'Evangelos A. Theodorou'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2004.08051v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.08051v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2005.12256v2',\n",
       "    'updated': '2020-05-28T22:56:12Z',\n",
       "    'published': '2020-05-25T17:56:29Z',\n",
       "    'title': 'Neural Topological SLAM for Visual Navigation',\n",
       "    'summary': 'This paper studies the problem of image-goal navigation which involves\\nnavigating to the location indicated by a goal image in a novel previously\\nunseen environment. To tackle this problem, we design topological\\nrepresentations for space that effectively leverage semantics and afford\\napproximate geometric reasoning. At the heart of our representations are nodes\\nwith associated semantic features, that are interconnected using coarse\\ngeometric information. We describe supervised learning-based algorithms that\\ncan build, maintain and use such representations under noisy actuation.\\nExperimental study in visually and physically realistic simulation suggests\\nthat our method builds effective representations that capture structural\\nregularities and efficiently solve long-horizon navigation problems. We observe\\na relative improvement of more than 50% over existing methods that study this\\ntask.',\n",
       "    'author': [{'name': 'Devendra Singh Chaplot'},\n",
       "     {'name': 'Ruslan Salakhutdinov'},\n",
       "     {'name': 'Abhinav Gupta'},\n",
       "     {'name': 'Saurabh Gupta'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in CVPR 2020. See the project webpage at\\n  https://devendrachaplot.github.io/projects/Neural-Topological-SLAM'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2005.12256v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2005.12256v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2006.10034v2',\n",
       "    'updated': '2020-10-27T05:46:46Z',\n",
       "    'published': '2020-06-17T17:56:00Z',\n",
       "    'title': 'Semantic Visual Navigation by Watching YouTube Videos',\n",
       "    'summary': \"Semantic cues and statistical regularities in real-world environment layouts\\ncan improve efficiency for navigation in novel environments. This paper learns\\nand leverages such semantic cues for navigating to objects of interest in novel\\nenvironments, by simply watching YouTube videos. This is challenging because\\nYouTube videos don't come with labels for actions or goals, and may not even\\nshowcase optimal behavior. Our method tackles these challenges through the use\\nof Q-learning on pseudo-labeled transition quadruples (image, action, next\\nimage, reward). We show that such off-policy Q-learning from passive data is\\nable to learn meaningful semantic cues for navigation. These cues, when used in\\na hierarchical navigation policy, lead to improved efficiency at the ObjectGoal\\ntask in visually realistic simulations. We observe a relative improvement of\\n15-83% over end-to-end RL, behavior cloning, and classical methods, while using\\nminimal direct interaction.\",\n",
       "    'author': [{'name': 'Matthew Chang'},\n",
       "     {'name': 'Arjun Gupta'},\n",
       "     {'name': 'Saurabh Gupta'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2020. Project website with code, models, and videos:\\n  https://matthewchang.github.io/value-learning-from-video'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2006.10034v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2006.10034v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.07792v2',\n",
       "    'updated': '2021-03-26T04:44:22Z',\n",
       "    'published': '2020-08-18T08:05:15Z',\n",
       "    'title': 'ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for\\n  Mobile Manipulation',\n",
       "    'summary': 'Many Reinforcement Learning (RL) approaches use joint control signals\\n(positions, velocities, torques) as action space for continuous control tasks.\\nWe propose to lift the action space to a higher level in the form of subgoals\\nfor a motion generator (a combination of motion planner and trajectory\\nexecutor). We argue that, by lifting the action space and by leveraging\\nsampling-based motion planners, we can efficiently use RL to solve complex,\\nlong-horizon tasks that could not be solved with existing RL methods in the\\noriginal action space. We propose ReLMoGen -- a framework that combines a\\nlearned policy to predict subgoals and a motion generator to plan and execute\\nthe motion needed to reach these subgoals. To validate our method, we apply\\nReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation\\nproblems where interactions with the environment are required to reach the\\ndestination, and 2) Mobile Manipulation tasks, manipulation tasks that require\\nmoving the robot base. These problems are challenging because they are usually\\nlong-horizon, hard to explore during training, and comprise alternating phases\\nof navigation and interaction. Our method is benchmarked on a diverse set of\\nseven robotics tasks in photo-realistic simulation environments. In all\\nsettings, ReLMoGen outperforms state-of-the-art Reinforcement Learning and\\nHierarchical Reinforcement Learning baselines. ReLMoGen also shows outstanding\\ntransferability between different motion generators at test time, indicating a\\ngreat potential to transfer to real robots.',\n",
       "    'author': [{'name': 'Fei Xia'},\n",
       "     {'name': 'Chengshu Li'},\n",
       "     {'name': 'Roberto Martín-Martín'},\n",
       "     {'name': 'Or Litany'},\n",
       "     {'name': 'Alexander Toshev'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'First two authors contributed equally. Access project website at\\n  http://svl.stanford.edu/projects/relmogen'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.07792v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.07792v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.11044v2',\n",
       "    'updated': '2020-09-30T13:09:32Z',\n",
       "    'published': '2020-09-23T10:40:03Z',\n",
       "    'title': 'Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem\\n  Formulation',\n",
       "    'summary': 'Event-based cameras record an asynchronous stream of per-pixel brightness\\nchanges. As such, they have numerous advantages over the standard frame-based\\ncameras, including high temporal resolution, high dynamic range, and no motion\\nblur. Due to the asynchronous nature, efficient learning of compact\\nrepresentation for event data is challenging. While it remains not explored the\\nextent to which the spatial and temporal event \"information\" is useful for\\npattern recognition tasks. In this paper, we focus on single-layer\\narchitectures. We analyze the performance of two general problem formulations:\\nthe direct and the inverse, for unsupervised feature learning from local event\\ndata (local volumes of events described in space-time). We identify and show\\nthe main advantages of each approach. Theoretically, we analyze guarantees for\\nan optimal solution, possibility for asynchronous, parallel parameter update,\\nand the computational complexity. We present numerical experiments for object\\nrecognition. We evaluate the solution under the direct and the inverse problem\\nand give a comparison with the state-of-the-art methods. Our empirical results\\nhighlight the advantages of both approaches for representation learning from\\nevent data. We show improvements of up to 9 % in the recognition accuracy\\ncompared to the state-of-the-art methods from the same class of methods.',\n",
       "    'author': [{'name': 'Dimche Kostadinov'}, {'name': 'Davide Scaramuzza'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IAPR IEEE/Computer Society International Conference on Pattern\\n  Recognition (ICPR), Milan, 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2009.11044v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.11044v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.04689v1',\n",
       "    'updated': '2020-10-09T17:21:42Z',\n",
       "    'published': '2020-10-09T17:21:42Z',\n",
       "    'title': 'LaND: Learning to Navigate from Disengagements',\n",
       "    'summary': \"Consistently testing autonomous mobile robots in real world scenarios is a\\nnecessary aspect of developing autonomous navigation systems. Each time the\\nhuman safety monitor disengages the robot's autonomy system due to the robot\\nperforming an undesirable maneuver, the autonomy developers gain insight into\\nhow to improve the autonomy system. However, we believe that these\\ndisengagements not only show where the system fails, which is useful for\\ntroubleshooting, but also provide a direct learning signal by which the robot\\ncan learn to navigate. We present a reinforcement learning approach for\\nlearning to navigate from disengagements, or LaND. LaND learns a neural network\\nmodel that predicts which actions lead to disengagements given the current\\nsensory observation, and then at test time plans and executes actions that\\navoid disengagements. Our results demonstrate LaND can successfully learn to\\nnavigate in diverse, real world sidewalk environments, outperforming both\\nimitation learning and reinforcement learning approaches. Videos, code, and\\nother material are available on our website\\nhttps://sites.google.com/view/sidewalk-learning\",\n",
       "    'author': [{'name': 'Gregory Kahn'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.04689v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.04689v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.06740v2',\n",
       "    'updated': '2020-11-27T20:33:03Z',\n",
       "    'published': '2020-10-13T23:42:40Z',\n",
       "    'title': 'Measuring Visual Generalization in Continuous Control from Pixels',\n",
       "    'summary': \"Self-supervised learning and data augmentation have significantly reduced the\\nperformance gap between state and image-based reinforcement learning agents in\\ncontinuous control tasks. However, it is still unclear whether current\\ntechniques can face a variety of visual conditions required by real-world\\nenvironments. We propose a challenging benchmark that tests agents' visual\\ngeneralization by adding graphical variety to existing continuous control\\ndomains. Our empirical analysis shows that current methods struggle to\\ngeneralize across a diverse set of visual changes, and we examine the specific\\nfactors of variation that make these tasks difficult. We find that data\\naugmentation techniques outperform self-supervised learning approaches and that\\nmore significant image transformations provide better visual generalization\\n\\\\footnote{The benchmark and our augmented actor-critic implementation are\\nopen-sourced @ https://github.com/QData/dmc_remastered)\",\n",
       "    'author': [{'name': 'Jake Grigsby'}, {'name': 'Yanjun Qi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'A total of 20 pages, 8 pages as the main text'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.06740v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.06740v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.08776v1',\n",
       "    'updated': '2020-10-17T12:25:18Z',\n",
       "    'published': '2020-10-17T12:25:18Z',\n",
       "    'title': 'The NVIDIA PilotNet Experiments',\n",
       "    'summary': 'Four years ago, an experimental system known as PilotNet became the first\\nNVIDIA system to steer an autonomous car along a roadway. This system\\nrepresents a departure from the classical approach for self-driving in which\\nthe process is manually decomposed into a series of modules, each performing a\\ndifferent task. In PilotNet, on the other hand, a single deep neural network\\n(DNN) takes pixels as input and produces a desired vehicle trajectory as\\noutput; there are no distinct internal modules connected by human-designed\\ninterfaces. We believe that handcrafted interfaces ultimately limit performance\\nby restricting information flow through the system and that a learned approach,\\nin combination with other artificial intelligence systems that add redundancy,\\nwill lead to better overall performing systems. We continue to conduct research\\ntoward that goal.\\n  This document describes the PilotNet lane-keeping effort, carried out over\\nthe past five years by our NVIDIA PilotNet group in Holmdel, New Jersey. Here\\nwe present a snapshot of system status in mid-2020 and highlight some of the\\nwork done by the PilotNet group.',\n",
       "    'author': [{'name': 'Mariusz Bojarski'},\n",
       "     {'name': 'Chenyi Chen'},\n",
       "     {'name': 'Joyjit Daw'},\n",
       "     {'name': 'Alperen Değirmenci'},\n",
       "     {'name': 'Joya Deri'},\n",
       "     {'name': 'Bernhard Firner'},\n",
       "     {'name': 'Beat Flepp'},\n",
       "     {'name': 'Sachin Gogri'},\n",
       "     {'name': 'Jesse Hong'},\n",
       "     {'name': 'Lawrence Jackel'},\n",
       "     {'name': 'Zhenhua Jia'},\n",
       "     {'name': 'BJ Lee'},\n",
       "     {'name': 'Bo Liu'},\n",
       "     {'name': 'Fei Liu'},\n",
       "     {'name': 'Urs Muller'},\n",
       "     {'name': 'Samuel Payne'},\n",
       "     {'name': 'Nischal Kota Nagendra Prasad'},\n",
       "     {'name': 'Artem Provodin'},\n",
       "     {'name': 'John Roach'},\n",
       "     {'name': 'Timur Rvachov'},\n",
       "     {'name': 'Neha Tadimeti'},\n",
       "     {'name': 'Jesper van Engelen'},\n",
       "     {'name': 'Haiguang Wen'},\n",
       "     {'name': 'Eric Yang'},\n",
       "     {'name': 'Zongyi Yang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.08776v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.08776v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.09164v3',\n",
       "    'updated': '2021-01-18T18:34:32Z',\n",
       "    'published': '2020-10-19T01:27:21Z',\n",
       "    'title': 'Evidential Sparsification of Multimodal Latent Spaces in Conditional\\n  Variational Autoencoders',\n",
       "    'summary': 'Discrete latent spaces in variational autoencoders have been shown to\\neffectively capture the data distribution for many real-world problems such as\\nnatural language understanding, human intent prediction, and visual scene\\nrepresentation. However, discrete latent spaces need to be sufficiently large\\nto capture the complexities of real-world data, rendering downstream tasks\\ncomputationally challenging. For instance, performing motion planning in a\\nhigh-dimensional latent representation of the environment could be intractable.\\nWe consider the problem of sparsifying the discrete latent space of a trained\\nconditional variational autoencoder, while preserving its learned\\nmultimodality. As a post hoc latent space reduction technique, we use\\nevidential theory to identify the latent classes that receive direct evidence\\nfrom a particular input condition and filter out those that do not. Experiments\\non diverse tasks, such as image generation and human behavior prediction,\\ndemonstrate the effectiveness of our proposed technique at reducing the\\ndiscrete latent sample space size of a model while maintaining its learned\\nmultimodality.',\n",
       "    'author': [{'name': 'Masha Itkina'},\n",
       "     {'name': 'Boris Ivanovic'},\n",
       "     {'name': 'Ransalu Senanayake'},\n",
       "     {'name': 'Mykel J. Kochenderfer'},\n",
       "     {'name': 'Marco Pavone'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '21 pages, 15 figures, 34th Conference on Neural Information\\n  Processing Systems (NeurIPS 2020)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.09164v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.09164v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.10; I.2.9; I.2.6',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.10903v1',\n",
       "    'updated': '2020-10-21T11:22:30Z',\n",
       "    'published': '2020-10-21T11:22:30Z',\n",
       "    'title': 'Visual Navigation in Real-World Indoor Environments Using End-to-End\\n  Deep Reinforcement Learning',\n",
       "    'summary': \"Visual navigation is essential for many applications in robotics, from\\nmanipulation, through mobile robotics to automated driving. Deep reinforcement\\nlearning (DRL) provides an elegant map-free approach integrating image\\nprocessing, localization, and planning in one module, which can be trained and\\ntherefore optimized for a given environment. However, to date, DRL-based visual\\nnavigation was validated exclusively in simulation, where the simulator\\nprovides information that is not available in the real world, e.g., the robot's\\nposition or image segmentation masks. This precludes the use of the learned\\npolicy on a real robot. Therefore, we propose a novel approach that enables a\\ndirect deployment of the trained policy on real robots. We have designed visual\\nauxiliary tasks, a tailored reward scheme, and a new powerful simulator to\\nfacilitate domain randomization. The policy is fine-tuned on images collected\\nfrom real-world environments. We have evaluated the method on a mobile robot in\\na real office environment. The training took ~30 hours on a single GPU. In 30\\nnavigation experiments, the robot reached a 0.3-meter neighborhood of the goal\\nin more than 86.7% of cases. This result makes the proposed method directly\\napplicable to tasks like mobile manipulation.\",\n",
       "    'author': [{'name': 'Jonáš Kulhánek'},\n",
       "     {'name': 'Erik Derner'},\n",
       "     {'name': 'Robert Babuška'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.10903v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.10903v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.13957v2',\n",
       "    'updated': '2021-01-11T16:09:19Z',\n",
       "    'published': '2020-10-26T23:50:30Z',\n",
       "    'title': 'MELD: Meta-Reinforcement Learning from Images via Latent State Models',\n",
       "    'summary': 'Meta-reinforcement learning algorithms can enable autonomous agents, such as\\nrobots, to quickly acquire new behaviors by leveraging prior experience in a\\nset of related training tasks. However, the onerous data requirements of\\nmeta-training compounded with the challenge of learning from sensory inputs\\nsuch as images have made meta-RL challenging to apply to real robotic systems.\\nLatent state models, which learn compact state representations from a sequence\\nof observations, can accelerate representation learning from visual inputs. In\\nthis paper, we leverage the perspective of meta-learning as task inference to\\nshow that latent state models can \\\\emph{also} perform meta-learning given an\\nappropriately defined observation space. Building on this insight, we develop\\nmeta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that\\nperforms inference in a latent state model to quickly acquire new skills given\\nobservations and rewards. MELD outperforms prior meta-RL methods on several\\nsimulated image-based robotic control problems, and enables a real WidowX\\nrobotic arm to insert an Ethernet cable into new locations given a sparse task\\ncompletion signal after only $8$ hours of real world meta-training. To our\\nknowledge, MELD is the first meta-RL algorithm trained in a real-world robotic\\ncontrol setting from images.',\n",
       "    'author': [{'name': 'Tony Z. Zhao'},\n",
       "     {'name': 'Anusha Nagabandi'},\n",
       "     {'name': 'Kate Rakelly'},\n",
       "     {'name': 'Chelsea Finn'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to CoRL 2020. Supplementary material at\\n  https://sites.google.com/view/meld-lsm/home . 16 pages, 19 figures. V2: add\\n  funding acknowledgements, reduce file size'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.13957v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.13957v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.14296v1',\n",
       "    'updated': '2020-10-27T13:54:37Z',\n",
       "    'published': '2020-10-27T13:54:37Z',\n",
       "    'title': 'Fit to Measure: Reasoning about Sizes for Robust Object Recognition',\n",
       "    'summary': 'Service robots can help with many of our daily tasks, especially in those\\ncases where it is inconvenient or unsafe for us to intervene: e.g., under\\nextreme weather conditions or when social distance needs to be maintained.\\nHowever, before we can successfully delegate complex tasks to robots, we need\\nto enhance their ability to make sense of dynamic, real world environments. In\\nthis context, the first prerequisite to improving the Visual Intelligence of a\\nrobot is building robust and reliable object recognition systems. While object\\nrecognition solutions are traditionally based on Machine Learning methods,\\naugmenting them with knowledge based reasoners has been shown to improve their\\nperformance. In particular, based on our prior work on identifying the\\nepistemic requirements of Visual Intelligence, we hypothesise that knowledge of\\nthe typical size of objects could significantly improve the accuracy of an\\nobject recognition system. To verify this hypothesis, in this paper we present\\nan approach to integrating knowledge about object sizes in a ML based\\narchitecture. Our experiments in a real world robotic scenario show that this\\ncombined approach ensures a significant performance increase over state of the\\nart Machine Learning methods.',\n",
       "    'author': [{'name': 'Agnese Chiatti'},\n",
       "     {'name': 'Enrico Motta'},\n",
       "     {'name': 'Enrico Daga'},\n",
       "     {'name': 'Gianluca Bardaro'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.14296v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.14296v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.15413v3',\n",
       "    'updated': '2021-09-10T06:55:37Z',\n",
       "    'published': '2020-10-29T08:25:43Z',\n",
       "    'title': 'Measuring and Harnessing Transference in Multi-Task Learning',\n",
       "    'summary': 'Multi-task learning can leverage information learned by one task to benefit\\nthe training of other tasks. Despite this capacity, naive formulations often\\ndegrade performance and in particular, identifying the tasks that would benefit\\nfrom co-training remains a challenging design question. In this paper, we\\nanalyze the dynamics of information transfer, or transference, across tasks\\nthroughout training. Specifically, we develop a similarity measure that can\\nquantify transference among tasks and use this quantity to both better\\nunderstand the optimization dynamics of multi-task learning as well as improve\\noverall learning performance. In the latter case, we propose two methods to\\nleverage our transference metric. The first operates at a macro-level by\\nselecting which tasks should train together while the second functions at a\\nmicro-level by determining how to combine task gradients at each training step.\\nWe find these methods can lead to significant improvement over prior work on\\nthree supervised multi-task learning benchmarks and one multi-task\\nreinforcement learning paradigm.',\n",
       "    'author': [{'name': 'Christopher Fifty'},\n",
       "     {'name': 'Ehsan Amid'},\n",
       "     {'name': 'Zhe Zhao'},\n",
       "     {'name': 'Tianhe Yu'},\n",
       "     {'name': 'Rohan Anil'},\n",
       "     {'name': 'Chelsea Finn'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.15413v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.15413v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.16298v1',\n",
       "    'updated': '2020-10-30T14:50:32Z',\n",
       "    'published': '2020-10-30T14:50:32Z',\n",
       "    'title': 'Learning Vision-based Reactive Policies for Obstacle Avoidance',\n",
       "    'summary': 'In this paper, we address the problem of vision-based obstacle avoidance for\\nrobotic manipulators. This topic poses challenges for both perception and\\nmotion generation. While most work in the field aims at improving one of those\\naspects, we provide a unified framework for approaching this problem. The main\\ngoal of this framework is to connect perception and motion by identifying the\\nrelationship between the visual input and the corresponding motion\\nrepresentation. To this end, we propose a method for learning reactive obstacle\\navoidance policies. We evaluate our method on goal-reaching tasks for single\\nand multiple obstacles scenarios. We show the ability of the proposed method to\\nefficiently learn stable obstacle avoidance strategies at a high success rate,\\nwhile maintaining closed-loop responsiveness required for critical applications\\nlike human-robot interaction.',\n",
       "    'author': [{'name': 'Elie Aljalbout'},\n",
       "     {'name': 'Ji Chen'},\n",
       "     {'name': 'Konstantin Ritt'},\n",
       "     {'name': 'Maximilian Ulmer'},\n",
       "     {'name': 'Sami Haddadin'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for publication at CoRL 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.16298v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.16298v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.6; I.2.10; I.5.0; J.2',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.01619v2',\n",
       "    'updated': '2021-06-29T05:52:38Z',\n",
       "    'published': '2020-11-03T11:00:10Z',\n",
       "    'title': 'Relational Graph Learning on Visual and Kinematics Embeddings for\\n  Accurate Gesture Recognition in Robotic Surgery',\n",
       "    'summary': 'Automatic surgical gesture recognition is fundamentally important to enable\\nintelligent cognitive assistance in robotic surgery. With recent advancement in\\nrobot-assisted minimally invasive surgery, rich information including surgical\\nvideos and robotic kinematics can be recorded, which provide complementary\\nknowledge for understanding surgical gestures. However, existing methods either\\nsolely adopt uni-modal data or directly concatenate multi-modal\\nrepresentations, which can not sufficiently exploit the informative\\ncorrelations inherent in visual and kinematics data to boost gesture\\nrecognition accuracies. In this regard, we propose a novel online approach of\\nmulti-modal relational graph network (i.e., MRG-Net) to dynamically integrate\\nvisual and kinematics information through interactive message propagation in\\nthe latent feature space. In specific, we first extract embeddings from video\\nand kinematics sequences with temporal convolutional networks and LSTM units.\\nNext, we identify multi-relations in these multi-modal embeddings and leverage\\nthem through a hierarchical relational graph learning module. The effectiveness\\nof our method is demonstrated with state-of-the-art results on the public\\nJIGSAWS dataset, outperforming current uni-modal and multi-modal methods on\\nboth suturing and knot typing tasks. Furthermore, we validated our method on\\nin-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)\\nplatforms in two centers, with consistent promising performance achieved.',\n",
       "    'author': [{'name': 'Yonghao Long'},\n",
       "     {'name': 'Jie Ying Wu'},\n",
       "     {'name': 'Bo Lu'},\n",
       "     {'name': 'Yueming Jin'},\n",
       "     {'name': 'Mathias Unberath'},\n",
       "     {'name': 'Yun-Hui Liu'},\n",
       "     {'name': 'Pheng Ann Heng'},\n",
       "     {'name': 'Qi Dou'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for ICRA 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.01619v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.01619v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.06464v1',\n",
       "    'updated': '2020-11-12T16:15:52Z',\n",
       "    'published': '2020-11-12T16:15:52Z',\n",
       "    'title': '3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators',\n",
       "    'summary': 'We propose an action-conditioned dynamics model that predicts scene changes\\ncaused by object and agent interactions in a viewpoint-invariant 3D neural\\nscene representation space, inferred from RGB-D videos. In this 3D feature\\nspace, objects do not interfere with one another and their appearance persists\\nover time and across viewpoints. This permits our model to predict future\\nscenes long in the future by simply \"moving\" 3D object features based on\\ncumulative object motion predictions. Object motion predictions are computed by\\na graph neural network that operates over the object features extracted from\\nthe 3D neural scene representation. Our model\\'s simulations can be decoded by a\\nneural renderer into2D image views from any desired viewpoint, which aids the\\ninterpretability of our latent 3D simulation space. We show our model\\ngeneralizes well its predictions across varying number and appearances of\\ninteracting objects as well as across camera viewpoints, outperforming existing\\n2D and 3D dynamics models. We further demonstrate sim-to-real transfer of the\\nlearnt dynamics by applying our model trained solely in simulation to\\nmodel-based control for pushing objects to desired locations under clutter on a\\nreal robotic setup',\n",
       "    'author': [{'name': 'Hsiao-Yu Fish Tung'},\n",
       "     {'name': 'Zhou Xian'},\n",
       "     {'name': 'Mihir Prabhudesai'},\n",
       "     {'name': 'Shamit Lal'},\n",
       "     {'name': 'Katerina Fragkiadaki'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.06464v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.06464v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.06498v1',\n",
       "    'updated': '2020-11-12T17:09:36Z',\n",
       "    'published': '2020-11-12T17:09:36Z',\n",
       "    'title': 'Fit2Form: 3D Generative Model for Robot Gripper Form Design',\n",
       "    'summary': \"The 3D shape of a robot's end-effector plays a critical role in determining\\nit's functionality and overall performance. Many industrial applications rely\\non task-specific gripper designs to ensure the system's robustness and\\naccuracy. However, the process of manual hardware design is both costly and\\ntime-consuming, and the quality of the resulting design is dependent on the\\nengineer's experience and domain expertise, which can easily be out-dated or\\ninaccurate. The goal of this work is to use machine learning algorithms to\\nautomate the design of task-specific gripper fingers. We propose Fit2Form, a 3D\\ngenerative design framework that generates pairs of finger shapes to maximize\\ndesign objectives (i.e., grasp success, stability, and robustness) for target\\ngrasp objects. We model the design objectives by training a Fitness network to\\npredict their values for pairs of gripper fingers and their corresponding grasp\\nobjects. This Fitness network then provides supervision to a 3D Generative\\nnetwork that produces a pair of 3D finger geometries for the target grasp\\nobject. Our experiments demonstrate that the proposed 3D generative design\\nframework generates parallel jaw gripper finger shapes that achieve more stable\\nand robust grasps compared to other general-purpose and task-specific gripper\\ndesign algorithms. Video can be found at https://youtu.be/utKHP3qb1bg.\",\n",
       "    'author': [{'name': 'Huy Ha'},\n",
       "     {'name': 'Shubham Agrawal'},\n",
       "     {'name': 'Shuran Song'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Conference on Robot Learning 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.06498v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.06498v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.10', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.06507v2',\n",
       "    'updated': '2021-11-04T20:07:57Z',\n",
       "    'published': '2020-11-12T17:15:48Z',\n",
       "    'title': 'Reinforcement Learning with Videos: Combining Offline Observations with\\n  Interaction',\n",
       "    'summary': \"Reinforcement learning is a powerful framework for robots to acquire skills\\nfrom experience, but often requires a substantial amount of online data\\ncollection. As a result, it is difficult to collect sufficiently diverse\\nexperiences that are needed for robots to generalize broadly. Videos of humans,\\non the other hand, are a readily available source of broad and interesting\\nexperiences. In this paper, we consider the question: can we perform\\nreinforcement learning directly on experience collected by humans? This problem\\nis particularly difficult, as such videos are not annotated with actions and\\nexhibit substantial visual domain shift relative to the robot's embodiment. To\\naddress these challenges, we propose a framework for reinforcement learning\\nwith videos (RLV). RLV learns a policy and value function using experience\\ncollected by humans in combination with data collected by robots. In our\\nexperiments, we find that RLV is able to leverage such videos to learn\\nchallenging vision-based skills with less than half as many samples as RL\\nmethods that learn from scratch.\",\n",
       "    'author': [{'name': 'Karl Schmeckpeper'},\n",
       "     {'name': 'Oleh Rybkin'},\n",
       "     {'name': 'Kostas Daniilidis'},\n",
       "     {'name': 'Sergey Levine'},\n",
       "     {'name': 'Chelsea Finn'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Conference on Robot Learning (2020)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.06507v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.06507v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.07748v3',\n",
       "    'updated': '2021-03-26T05:13:32Z',\n",
       "    'published': '2020-11-16T06:51:55Z',\n",
       "    'title': 'Fast Uncertainty Quantification for Deep Object Pose Estimation',\n",
       "    'summary': 'Deep learning-based object pose estimators are often unreliable and\\noverconfident especially when the input image is outside the training domain,\\nfor instance, with sim2real transfer. Efficient and robust uncertainty\\nquantification (UQ) in pose estimators is critically needed in many robotic\\ntasks. In this work, we propose a simple, efficient, and plug-and-play UQ\\nmethod for 6-DoF object pose estimation. We ensemble 2-3 pre-trained models\\nwith different neural network architectures and/or training data sources, and\\ncompute their average pairwise disagreement against one another to obtain the\\nuncertainty quantification. We propose four disagreement metrics, including a\\nlearned metric, and show that the average distance (ADD) is the best\\nlearning-free metric and it is only slightly worse than the learned metric,\\nwhich requires labeled target data. Our method has several advantages compared\\nto the prior art: 1) our method does not require any modification of the\\ntraining process or the model inputs; and 2) it needs only one forward pass for\\neach model. We evaluate the proposed UQ method on three tasks where our\\nuncertainty quantification yields much stronger correlations with pose\\nestimation errors than the baselines. Moreover, in a real robot grasping task,\\nour method increases the grasping success rate from 35% to 90%.',\n",
       "    'author': [{'name': 'Guanya Shi'},\n",
       "     {'name': 'Yifeng Zhu'},\n",
       "     {'name': 'Jonathan Tremblay'},\n",
       "     {'name': 'Stan Birchfield'},\n",
       "     {'name': 'Fabio Ramos'},\n",
       "     {'name': 'Animashree Anandkumar'},\n",
       "     {'name': 'Yuke Zhu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Video and code are available at https://sites.google.com/view/fastuq'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conferenceon Robotics and Automation (ICRA), 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.07748v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.07748v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.08518v1',\n",
       "    'updated': '2020-11-17T09:14:02Z',\n",
       "    'published': '2020-11-17T09:14:02Z',\n",
       "    'title': 'DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and\\n  Sequence-based Place Recognition',\n",
       "    'summary': 'Sequence-based place recognition methods for all-weather navigation are\\nwell-known for producing state-of-the-art results under challenging day-night\\nor summer-winter transitions. These systems, however, rely on complex\\nhandcrafted heuristics for sequential matching - which are applied on top of a\\npre-computed pairwise similarity matrix between reference and query image\\nsequences of a single route - to further reduce false-positive rates compared\\nto single-frame retrieval methods. As a result, performing multi-frame place\\nrecognition can be extremely slow for deployment on autonomous vehicles or\\nevaluation on large datasets, and fail when using relatively short parameter\\nvalues such as a sequence length of 2 frames. In this paper, we propose\\nDeepSeqSLAM: a trainable CNN+RNN architecture for jointly learning visual and\\npositional representations from a single monocular image sequence of a route.\\nWe demonstrate our approach on two large benchmark datasets, Nordland and\\nOxford RobotCar - recorded over 728 km and 10 km routes, respectively, each\\nduring 1 year with multiple seasons, weather, and lighting conditions. On\\nNordland, we compare our method to two state-of-the-art sequence-based methods\\nacross the entire route under summer-winter changes using a sequence length of\\n2 and show that our approach can get over 72% AUC compared to 27% AUC for Delta\\nDescriptors and 2% AUC for SeqSLAM; while drastically reducing the deployment\\ntime from around 1 hour to 1 minute against both. The framework code and video\\nare available at https://mchancan.github.io/deepseqslam',\n",
       "    'author': [{'name': 'Marvin Chancán'}, {'name': 'Michael Milford'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 6 figures, 2 tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2020 Workshop on Machine Learning for Autonomous Driving\\n  (ML4AD)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.08518v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.08518v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.03040v2',\n",
       "    'updated': '2022-01-14T13:20:15Z',\n",
       "    'published': '2020-12-05T14:39:14Z',\n",
       "    'title': \"Understanding Bird's-Eye View of Road Semantics using an Onboard Camera\",\n",
       "    'summary': \"Autonomous navigation requires scene understanding of the action-space to\\nmove or anticipate events. For planner agents moving on the ground plane, such\\nas autonomous vehicles, this translates to scene understanding in the\\nbird's-eye view (BEV). However, the onboard cameras of autonomous cars are\\ncustomarily mounted horizontally for a better view of the surrounding. In this\\nwork, we study scene understanding in the form of online estimation of semantic\\nBEV maps using the video input from a single onboard camera. We study three key\\naspects of this task, image-level understanding, BEV level understanding, and\\nthe aggregation of temporal information. Based on these three pillars we\\npropose a novel architecture that combines these three aspects. In our\\nextensive experiments, we demonstrate that the considered aspects are\\ncomplementary to each other for BEV understanding. Furthermore, the proposed\\narchitecture significantly surpasses the current state-of-the-art. Code:\\nhttps://github.com/ybarancan/BEV_feat_stitch.\",\n",
       "    'author': [{'name': 'Yigit Baran Can'},\n",
       "     {'name': 'Alexander Liniger'},\n",
       "     {'name': 'Ozan Unal'},\n",
       "     {'name': 'Danda Paudel'},\n",
       "     {'name': 'Luc Van Gool'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.03040v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.03040v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.03806v1',\n",
       "    'updated': '2020-12-07T15:48:26Z',\n",
       "    'published': '2020-12-07T15:48:26Z',\n",
       "    'title': 'Perspectives on Sim2Real Transfer for Robotics: A Summary of the R:SS\\n  2020 Workshop',\n",
       "    'summary': 'This report presents the debates, posters, and discussions of the Sim2Real\\nworkshop held in conjunction with the 2020 edition of the \"Robotics: Science\\nand System\" conference. Twelve leaders of the field took competing debate\\npositions on the definition, viability, and importance of transferring skills\\nfrom simulation to the real world in the context of robotics problems. The\\ndebaters also joined a large panel discussion, answering audience questions and\\noutlining the future of Sim2Real in robotics. Furthermore, we invited extended\\nabstracts to this workshop which are summarized in this report. Based on the\\nworkshop, this report concludes with directions for practitioners exploiting\\nthis technology and for researchers further exploring open problems in this\\narea.',\n",
       "    'author': [{'name': 'Sebastian Höfer'},\n",
       "     {'name': 'Kostas Bekris'},\n",
       "     {'name': 'Ankur Handa'},\n",
       "     {'name': 'Juan Camilo Gamboa'},\n",
       "     {'name': 'Florian Golemo'},\n",
       "     {'name': 'Melissa Mozifian'},\n",
       "     {'name': 'Chris Atkeson'},\n",
       "     {'name': 'Dieter Fox'},\n",
       "     {'name': 'Ken Goldberg'},\n",
       "     {'name': 'John Leonard'},\n",
       "     {'name': 'C. Karen Liu'},\n",
       "     {'name': 'Jan Peters'},\n",
       "     {'name': 'Shuran Song'},\n",
       "     {'name': 'Peter Welinder'},\n",
       "     {'name': 'Martha White'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Summary of the \"2nd Workshop on Closing the Reality Gap in Sim2Real\\n  Transfer for Robotics\" held in conjunction with \"Robotics: Science and System\\n  2020\". Website: https://sim2real.github.io/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.03806v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.03806v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.03912v1',\n",
       "    'updated': '2020-12-07T18:42:38Z',\n",
       "    'published': '2020-12-07T18:42:38Z',\n",
       "    'title': 'MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation',\n",
       "    'summary': 'Navigation tasks in photorealistic 3D environments are challenging because\\nthey require perception and effective planning under partial observability.\\nRecent work shows that map-like memory is useful for long-horizon navigation\\ntasks. However, a focused investigation of the impact of maps on navigation\\ntasks of varying complexity has not yet been performed. We propose the multiON\\ntask, which requires navigation to an episode-specific sequence of objects in a\\nrealistic environment. MultiON generalizes the ObjectGoal navigation task and\\nexplicitly tests the ability of navigation agents to locate previously observed\\ngoal objects. We perform a set of multiON experiments to examine how a variety\\nof agent models perform across a spectrum of navigation task complexities. Our\\nexperiments show that: i) navigation performance degrades dramatically with\\nescalating task complexity; ii) a simple semantic map agent performs\\nsurprisingly well relative to more complex neural image feature map agents; and\\niii) even oracle map agents achieve relatively low performance, indicating the\\npotential for future work in training embodied navigation agents using maps.\\nVideo summary: https://youtu.be/yqTlHNIcgnY',\n",
       "    'author': [{'name': 'Saim Wani'},\n",
       "     {'name': 'Shivansh Patel'},\n",
       "     {'name': 'Unnat Jain'},\n",
       "     {'name': 'Angel X. Chang'},\n",
       "     {'name': 'Manolis Savva'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page: https://shivanshpatel35.github.io/multi-ON/ ; the first\\n  three authors contributed equally'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.03912v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.03912v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.04060v2',\n",
       "    'updated': '2021-05-23T20:08:59Z',\n",
       "    'published': '2020-12-07T21:04:34Z',\n",
       "    'title': 'Semantic and Geometric Modeling with Neural Message Passing in 3D Scene\\n  Graphs for Hierarchical Mechanical Search',\n",
       "    'summary': \"Searching for objects in indoor organized environments such as homes or\\noffices is part of our everyday activities. When looking for a target object,\\nwe jointly reason about the rooms and containers the object is likely to be in;\\nthe same type of container will have a different probability of having the\\ntarget depending on the room it is in. We also combine geometric and semantic\\ninformation to infer what container is best to search, or what other objects\\nare best to move, if the target object is hidden from view. We propose to use a\\n3D scene graph representation to capture the hierarchical, semantic, and\\ngeometric aspects of this problem. To exploit this representation in a search\\nprocess, we introduce Hierarchical Mechanical Search (HMS), a method that\\nguides an agent's actions towards finding a target object specified with a\\nnatural language description. HMS is based on a novel neural network\\narchitecture that uses neural message passing of vectors with visual,\\ngeometric, and linguistic information to allow HMS to reason across layers of\\nthe graph while combining semantic and geometric cues. HMS is evaluated on a\\nnovel dataset of 500 3D scene graphs with dense placements of semantically\\nrelated objects in storage locations, and is shown to be significantly better\\nthan several baselines at finding objects and close to the oracle policy in\\nterms of the median number of actions required. Additional qualitative results\\ncan be found at https://ai.stanford.edu/mech-search/hms.\",\n",
       "    'author': [{'name': 'Andrey Kurenkov'},\n",
       "     {'name': 'Roberto Martín-Martín'},\n",
       "     {'name': 'Jeff Ichnowski'},\n",
       "     {'name': 'Ken Goldberg'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.04060v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.04060v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.04983v1',\n",
       "    'updated': '2020-12-09T11:19:50Z',\n",
       "    'published': '2020-12-09T11:19:50Z',\n",
       "    'title': 'Driving Behavior Explanation with Multi-level Fusion',\n",
       "    'summary': 'In this era of active development of autonomous vehicles, it becomes crucial\\nto provide driving systems with the capacity to explain their decisions. In\\nthis work, we focus on generating high-level driving explanations as the\\nvehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep\\narchitecture which explains the behavior of a trajectory prediction model.\\nSupervised by annotations of human driving decisions justifications, BEEF\\nlearns to fuse features from multiple levels. Leveraging recent advances in the\\nmulti-modal fusion literature, BEEF is carefully designed to model the\\ncorrelations between high-level decisions features and mid-level perceptual\\nfeatures. The flexibility and efficiency of our approach are validated with\\nextensive experiments on the HDD and BDD-X datasets.',\n",
       "    'author': [{'name': 'Hédi Ben-Younes'},\n",
       "     {'name': 'Éloi Zablocki'},\n",
       "     {'name': 'Patrick Pérez'},\n",
       "     {'name': 'Matthieu Cord'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.patcog.2021.108421'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.patcog.2021.108421',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2012.04983v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.04983v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at NeurIPS Workshop ML4AD 2020'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Pattern Recognition, Volume 123, March 2022, 108421'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.09242v1',\n",
       "    'updated': '2020-12-16T20:14:41Z',\n",
       "    'published': '2020-12-16T20:14:41Z',\n",
       "    'title': 'S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point\\n  Clouds',\n",
       "    'summary': 'With the increasing reliance of self-driving and similar robotic systems on\\nrobust 3D vision, the processing of LiDAR scans with deep convolutional neural\\nnetworks has become a trend in academia and industry alike. Prior attempts on\\nthe challenging Semantic Scene Completion task - which entails the inference of\\ndense 3D structure and associated semantic labels from \"sparse\" representations\\n- have been, to a degree, successful in small indoor scenes when provided with\\ndense point clouds or dense depth maps often fused with semantic segmentation\\nmaps from RGB images. However, the performance of these systems drop\\ndrastically when applied to large outdoor scenes characterized by dynamic and\\nexponentially sparser conditions. Likewise, processing of the entire sparse\\nvolume becomes infeasible due to memory limitations and workarounds introduce\\ncomputational inefficiency as practitioners are forced to divide the overall\\nvolume into multiple equal segments and infer on each individually, rendering\\nreal-time performance impossible. In this work, we formulate a method that\\nsubsumes the sparsity of large-scale environments and present S3CNet, a sparse\\nconvolution based neural network that predicts the semantically completed scene\\nfrom a single, unified LiDAR point cloud. We show that our proposed method\\noutperforms all counterparts on the 3D task, achieving state-of-the art results\\non the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet\\nwith a multi-view fusion strategy to complement our 3D network, providing\\nrobustness to occlusions and extreme sparsity in distant regions. We conduct\\nexperiments for the 2D semantic scene completion task and compare the results\\nof our sparse 2D network against several leading LiDAR segmentation models\\nadapted for bird\\'s eye view segmentation on two open-source datasets.',\n",
       "    'author': [{'name': 'Ran Cheng'},\n",
       "     {'name': 'Christopher Agia'},\n",
       "     {'name': 'Yuan Ren'},\n",
       "     {'name': 'Xinhai Li'},\n",
       "     {'name': 'Liu Bingbing'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.09242v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.09242v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.11717v3',\n",
       "    'updated': '2021-08-20T16:41:06Z',\n",
       "    'published': '2020-12-21T22:25:06Z',\n",
       "    'title': 'Social NCE: Contrastive Learning of Socially-aware Motion\\n  Representations',\n",
       "    'summary': 'Learning socially-aware motion representations is at the core of recent\\nadvances in multi-agent problems, such as human motion forecasting and robot\\nnavigation in crowds. Despite promising progress, existing representations\\nlearned with neural networks still struggle to generalize in closed-loop\\npredictions (e.g., output colliding trajectories). This issue largely arises\\nfrom the non-i.i.d. nature of sequential prediction in conjunction with\\nill-distributed training data. Intuitively, if the training data only comes\\nfrom human behaviors in safe spaces, i.e., from \"positive\" examples, it is\\ndifficult for learning algorithms to capture the notion of \"negative\" examples\\nlike collisions. In this work, we aim to address this issue by explicitly\\nmodeling negative examples through self-supervision: (i) we introduce a social\\ncontrastive loss that regularizes the extracted motion representation by\\ndiscerning the ground-truth positive events from synthetic negative ones; (ii)\\nwe construct informative negative samples based on our prior knowledge of rare\\nbut dangerous circumstances. Our method substantially reduces the collision\\nrates of recent trajectory forecasting, behavioral cloning and reinforcement\\nlearning algorithms, outperforming state-of-the-art methods on several\\nbenchmarks. Our code is available at https://github.com/vita-epfl/social-nce.',\n",
       "    'author': [{'name': 'Yuejiang Liu'},\n",
       "     {'name': 'Qi Yan'},\n",
       "     {'name': 'Alexandre Alahi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICCV 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.11717v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.11717v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.12259v2',\n",
       "    'updated': '2021-04-01T17:55:10Z',\n",
       "    'published': '2020-12-22T18:58:18Z',\n",
       "    'title': 'YolactEdge: Real-time Instance Segmentation on the Edge',\n",
       "    'summary': 'We propose YolactEdge, the first competitive instance segmentation approach\\nthat runs on small edge devices at real-time speeds. Specifically, YolactEdge\\nruns at up to 30.8 FPS on a Jetson AGX Xavier (and 172.7 FPS on an RTX 2080 Ti)\\nwith a ResNet-101 backbone on 550x550 resolution images. To achieve this, we\\nmake two improvements to the state-of-the-art image-based real-time method\\nYOLACT: (1) applying TensorRT optimization while carefully trading off speed\\nand accuracy, and (2) a novel feature warping module to exploit temporal\\nredundancy in videos. Experiments on the YouTube VIS and MS COCO datasets\\ndemonstrate that YolactEdge produces a 3-5x speed up over existing real-time\\nmethods while producing competitive mask and box detection accuracy. We also\\nconduct ablation studies to dissect our design choices and modules. Code and\\nmodels are available at https://github.com/haotian-liu/yolact_edge.',\n",
       "    'author': [{'name': 'Haotian Liu'},\n",
       "     {'name': 'Rafael A. Rivera Soto'},\n",
       "     {'name': 'Fanyi Xiao'},\n",
       "     {'name': 'Yong Jae Lee'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '\\\\c{opyright} 2021 IEEE. Personal use of this material is permitted.\\n  Permission from IEEE must be obtained for all other uses, in any current or\\n  future media, including reprinting/republishing this material for advertising\\n  or promotional purposes, creating new collective works, for resale or\\n  redistribution to servers or lists, or reuse of any copyrighted component of\\n  this work in other works'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.12259v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.12259v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.00316v1',\n",
       "    'updated': '2021-01-01T21:02:18Z',\n",
       "    'published': '2021-01-01T21:02:18Z',\n",
       "    'title': 'Energy-constrained Self-training for Unsupervised Domain Adaptation',\n",
       "    'summary': 'Unsupervised domain adaptation (UDA) aims to transfer the knowledge on a\\nlabeled source domain distribution to perform well on an unlabeled target\\ndomain. Recently, the deep self-training involves an iterative process of\\npredicting on the target domain and then taking the confident predictions as\\nhard pseudo-labels for retraining. However, the pseudo-labels are usually\\nunreliable, and easily leading to deviated solutions with propagated errors. In\\nthis paper, we resort to the energy-based model and constrain the training of\\nthe unlabeled target sample with the energy function minimization objective. It\\ncan be applied as a simple additional regularization. In this framework, it is\\npossible to gain the benefits of the energy-based model, while retaining strong\\ndiscriminative performance following a plug-and-play fashion. We deliver\\nextensive experiments on the most popular and large scale UDA benchmarks of\\nimage classification as well as semantic segmentation to demonstrate its\\ngenerality and effectiveness.',\n",
       "    'author': [{'name': 'Xiaofeng Liu'},\n",
       "     {'name': 'Bo Hu'},\n",
       "     {'name': 'Xiongchang Liu'},\n",
       "     {'name': 'Jun Lu'},\n",
       "     {'name': 'Jane You'},\n",
       "     {'name': 'Lingsheng Kong'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to 25th International Conference on Pattern Recognition\\n  (ICPR 2020)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.00316v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.00316v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.05307v2',\n",
       "    'updated': '2022-07-19T10:20:52Z',\n",
       "    'published': '2021-01-13T19:09:38Z',\n",
       "    'title': 'Explainability of deep vision-based autonomous driving systems: Review\\n  and challenges',\n",
       "    'summary': 'This survey reviews explainability methods for vision-based self-driving\\nsystems trained with behavior cloning. The concept of explainability has\\nseveral facets and the need for explainability is strong in driving, a\\nsafety-critical application. Gathering contributions from several research\\nfields, namely computer vision, deep learning, autonomous driving, explainable\\nAI (X-AI), this survey tackles several points. First, it discusses definitions,\\ncontext, and motivation for gaining more interpretability and explainability\\nfrom self-driving systems, as well as the challenges that are specific to this\\napplication. Second, methods providing explanations to a black-box self-driving\\nsystem in a post-hoc fashion are comprehensively organized and detailed. Third,\\napproaches from the literature that aim at building more interpretable\\nself-driving systems by design are presented and discussed in detail. Finally,\\nremaining open-challenges and potential future research directions are\\nidentified and examined.',\n",
       "    'author': [{'name': 'Éloi Zablocki'},\n",
       "     {'name': 'Hédi Ben-Younes'},\n",
       "     {'name': 'Patrick Pérez'},\n",
       "     {'name': 'Matthieu Cord'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IJCV 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.05307v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.05307v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.06541v1',\n",
       "    'updated': '2021-01-16T22:51:43Z',\n",
       "    'published': '2021-01-16T22:51:43Z',\n",
       "    'title': 'SceneGen: Learning to Generate Realistic Traffic Scenes',\n",
       "    'summary': \"We consider the problem of generating realistic traffic scenes automatically.\\nExisting methods typically insert actors into the scene according to a set of\\nhand-crafted heuristics and are limited in their ability to model the true\\ncomplexity and diversity of real traffic scenes, thus inducing a content gap\\nbetween synthesized traffic scenes versus real ones. As a result, existing\\nsimulators lack the fidelity necessary to train and test self-driving vehicles.\\nTo address this limitation, we present SceneGen, a neural autoregressive model\\nof traffic scenes that eschews the need for rules and heuristics. In\\nparticular, given the ego-vehicle state and a high definition map of\\nsurrounding area, SceneGen inserts actors of various classes into the scene and\\nsynthesizes their sizes, orientations, and velocities. We demonstrate on two\\nlarge-scale datasets SceneGen's ability to faithfully model distributions of\\nreal traffic scenes. Moreover, we show that SceneGen coupled with sensor\\nsimulation can be used to train perception models that generalize to the real\\nworld.\",\n",
       "    'author': [{'name': 'Shuhan Tan'},\n",
       "     {'name': 'Kelvin Wong'},\n",
       "     {'name': 'Shenlong Wang'},\n",
       "     {'name': 'Sivabalan Manivasagam'},\n",
       "     {'name': 'Mengye Ren'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.06541v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.06541v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.06547v3',\n",
       "    'updated': '2021-05-07T18:47:54Z',\n",
       "    'published': '2021-01-16T23:19:22Z',\n",
       "    'title': 'LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving',\n",
       "    'summary': 'In this paper, we present LookOut, a novel autonomy system that perceives the\\nenvironment, predicts a diverse set of futures of how the scene might unroll\\nand estimates the trajectory of the SDV by optimizing a set of contingency\\nplans over these future realizations. In particular, we learn a diverse joint\\ndistribution over multi-agent future trajectories in a traffic scene that\\ncovers a wide range of future modes with high sample efficiency while\\nleveraging the expressive power of generative models. Unlike previous work in\\ndiverse motion forecasting, our diversity objective explicitly rewards sampling\\nfuture scenarios that require distinct reactions from the self-driving vehicle\\nfor improved safety. Our contingency planner then finds comfortable and\\nnon-conservative trajectories that ensure safe reactions to a wide range of\\nfuture scenarios. Through extensive evaluations, we show that our model\\ndemonstrates significantly more diverse and sample-efficient motion forecasting\\nin a large-scale self-driving dataset as well as safer and less-conservative\\nmotion plans in long-term closed-loop simulations when compared to current\\nstate-of-the-art models.',\n",
       "    'author': [{'name': 'Alexander Cui'},\n",
       "     {'name': 'Sergio Casas'},\n",
       "     {'name': 'Abbas Sadat'},\n",
       "     {'name': 'Renjie Liao'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICCV48922.2021.01580'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICCV48922.2021.01580',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2101.06547v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.06547v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.06549v3',\n",
       "    'updated': '2022-01-08T21:50:56Z',\n",
       "    'published': '2021-01-16T23:23:12Z',\n",
       "    'title': 'AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles',\n",
       "    'summary': \"As self-driving systems become better, simulating scenarios where the\\nautonomy stack may fail becomes more important. Traditionally, those scenarios\\nare generated for a few scenes with respect to the planning module that takes\\nground-truth actor states as input. This does not scale and cannot identify all\\npossible autonomy failures, such as perception failures due to occlusion. In\\nthis paper, we propose AdvSim, an adversarial framework to generate\\nsafety-critical scenarios for any LiDAR-based autonomy system. Given an initial\\ntraffic scenario, AdvSim modifies the actors' trajectories in a physically\\nplausible manner and updates the LiDAR sensor data to match the perturbed\\nworld. Importantly, by simulating directly from sensor data, we obtain\\nadversarial scenarios that are safety-critical for the full autonomy stack. Our\\nexperiments show that our approach is general and can identify thousands of\\nsemantically meaningful safety-critical scenarios for a wide range of modern\\nself-driving systems. Furthermore, we show that the robustness and safety of\\nthese systems can be further improved by training them with scenarios generated\\nby AdvSim.\",\n",
       "    'author': [{'name': 'Jingkang Wang'},\n",
       "     {'name': 'Ava Pun'},\n",
       "     {'name': 'James Tu'},\n",
       "     {'name': 'Sivabalan Manivasagam'},\n",
       "     {'name': 'Abbas Sadat'},\n",
       "     {'name': 'Sergio Casas'},\n",
       "     {'name': 'Mengye Ren'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2021. Corrected typos in the adversarial objective'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.06549v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.06549v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.06557v1',\n",
       "    'updated': '2021-01-17T00:29:30Z',\n",
       "    'published': '2021-01-17T00:29:30Z',\n",
       "    'title': 'TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors',\n",
       "    'summary': 'Simulation has the potential to massively scale evaluation of self-driving\\nsystems enabling rapid development as well as safe deployment. To close the gap\\nbetween simulation and the real world, we need to simulate realistic\\nmulti-agent behaviors. Existing simulation environments rely on heuristic-based\\nmodels that directly encode traffic rules, which cannot capture irregular\\nmaneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding,\\nmerging). In contrast, we leverage real-world data to learn directly from human\\ndemonstration and thus capture a more diverse set of actor behaviors. To this\\nend, we propose TrafficSim, a multi-agent behavior model for realistic traffic\\nsimulation. In particular, we leverage an implicit latent variable model to\\nparameterize a joint actor policy that generates socially-consistent plans for\\nall actors in the scene jointly. To learn a robust policy amenable for long\\nhorizon simulation, we unroll the policy in training and optimize through the\\nfully differentiable simulation across time. Our learning objective\\nincorporates both human demonstrations as well as common sense. We show\\nTrafficSim generates significantly more realistic and diverse traffic scenarios\\nas compared to a diverse set of baselines. Notably, we can exploit trajectories\\ngenerated by TrafficSim as effective data augmentation for training better\\nmotion planner.',\n",
       "    'author': [{'name': 'Simon Suo'},\n",
       "     {'name': 'Sebastian Regalado'},\n",
       "     {'name': 'Sergio Casas'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.06557v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.06557v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.07907v1',\n",
       "    'updated': '2021-01-20T00:31:52Z',\n",
       "    'published': '2021-01-20T00:31:52Z',\n",
       "    'title': 'IntentNet: Learning to Predict Intention from Raw Sensor Data',\n",
       "    'summary': 'In order to plan a safe maneuver, self-driving vehicles need to understand\\nthe intent of other traffic participants. We define intent as a combination of\\ndiscrete high-level behaviors as well as continuous trajectories describing\\nfuture motion. In this paper, we develop a one-stage detector and forecaster\\nthat exploits both 3D point clouds produced by a LiDAR sensor as well as\\ndynamic maps of the environment. Our multi-task model achieves better accuracy\\nthan the respective separate modules while saving computation, which is\\ncritical to reducing reaction time in self-driving applications.',\n",
       "    'author': [{'name': 'Sergio Casas'},\n",
       "     {'name': 'Wenjie Luo'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.07907v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.07907v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.10892v2',\n",
       "    'updated': '2022-02-10T12:50:42Z',\n",
       "    'published': '2021-01-26T16:01:02Z',\n",
       "    'title': 'Online Body Schema Adaptation through Cost-Sensitive Active Learning',\n",
       "    'summary': \"Humanoid robots have complex bodies and kinematic chains with several\\nDegrees-of-Freedom (DoF) which are difficult to model. Learning the parameters\\nof a kinematic model can be achieved by observing the position of the robot\\nlinks during prospective motions and minimising the prediction errors. This\\nwork proposes a movement efficient approach for estimating online the\\nbody-schema of a humanoid robot arm in the form of Denavit-Hartenberg (DH)\\nparameters. A cost-sensitive active learning approach based on the A-Optimality\\ncriterion is used to select optimal joint configurations. The chosen joint\\nconfigurations simultaneously minimise the error in the estimation of the body\\nschema and minimise the movement between samples. This reduces energy\\nconsumption, along with mechanical fatigue and wear, while not compromising the\\nlearning accuracy. The work was implemented in a simulation environment, using\\nthe 7DoF arm of the iCub robot simulator. The hand pose is measured with a\\nsingle camera via markers placed in the palm and back of the robot's hand. A\\nnon-parametric occlusion model is proposed to avoid choosing joint\\nconfigurations where the markers are not visible, thus preventing worthless\\nattempts. The results show cost-sensitive active learning has similar accuracy\\nto the standard active learning approach, while reducing in about half the\\nexecuted movement.\",\n",
       "    'author': [{'name': 'Gonçalo Cunha'},\n",
       "     {'name': 'Pedro Vicente'},\n",
       "     {'name': 'Alexandre Bernardino'},\n",
       "     {'name': 'Ricardo Ribeiro'},\n",
       "     {'name': 'Plínio Moreno'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 7 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.10892v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.10892v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2101.11812v1',\n",
       "    'updated': '2021-01-28T04:35:15Z',\n",
       "    'published': '2021-01-28T04:35:15Z',\n",
       "    'title': 'SwingBot: Learning Physical Features from In-hand Tactile Exploration\\n  for Dynamic Swing-up Manipulation',\n",
       "    'summary': 'Several robot manipulation tasks are extremely sensitive to variations of the\\nphysical properties of the manipulated objects. One such task is manipulating\\nobjects by using gravity or arm accelerations, increasing the importance of\\nmass, center of mass, and friction information. We present SwingBot, a robot\\nthat is able to learn the physical features of a held object through tactile\\nexploration. Two exploration actions (tilting and shaking) provide the tactile\\ninformation used to create a physical feature embedding space. With this\\nembedding, SwingBot is able to predict the swing angle achieved by a robot\\nperforming dynamic swing-up manipulations on a previously unseen object. Using\\nthese predictions, it is able to search for the optimal control parameters for\\na desired swing-up angle. We show that with the learned physical features our\\nend-to-end self-supervised learning pipeline is able to substantially improve\\nthe accuracy of swinging up unseen objects. We also show that objects with\\nsimilar dynamics are closer to each other on the embedding space and that the\\nembedding can be disentangled into values of specific physical properties.',\n",
       "    'author': [{'name': 'Chen Wang'},\n",
       "     {'name': 'Shaoxiong Wang'},\n",
       "     {'name': 'Branden Romero'},\n",
       "     {'name': 'Filipe Veiga'},\n",
       "     {'name': 'Edward Adelson'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IROS 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2101.11812v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2101.11812v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.04738v2',\n",
       "    'updated': '2021-12-09T02:40:18Z',\n",
       "    'published': '2021-02-09T10:04:39Z',\n",
       "    'title': 'End-to-End Deep Learning of Lane Detection and Path Prediction for\\n  Real-Time Autonomous Driving',\n",
       "    'summary': \"Inspired by the UNet architecture of semantic image segmentation, we propose\\na lightweight UNet using depthwise separable convolutions (DSUNet) for\\nend-to-end learning of lane detection and path prediction (PP) in autonomous\\ndriving. We also design and integrate a PP algorithm with convolutional neural\\nnetwork (CNN) to form a simulation model (CNN-PP) that can be used to assess\\nCNN's performance qualitatively, quantitatively, and dynamically in a host\\nagent car driving along with other agents all in a real-time autonomous manner.\\nDSUNet is 5.16x lighter in model size and 1.61x faster in inference than UNet.\\nDSUNet-PP outperforms UNet-PP in mean average errors of predicted curvature and\\nlateral offset for path planning in dynamic simulation. DSUNet-PP outperforms a\\nmodified UNet in lateral error, which is tested in a real car on real road.\\nThese results show that DSUNet is efficient and effective for lane detection\\nand path prediction in autonomous driving.\",\n",
       "    'author': [{'name': 'Der-Hau Lee'}, {'name': 'Jinn-Liang Liu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 4 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.04738v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.04738v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.04750v1',\n",
       "    'updated': '2021-02-09T10:34:32Z',\n",
       "    'published': '2021-02-09T10:34:32Z',\n",
       "    'title': 'Where is my hand? Deep hand segmentation for visual self-recognition in\\n  humanoid robots',\n",
       "    'summary': 'The ability to distinguish between the self and the background is of\\nparamount importance for robotic tasks. The particular case of hands, as the\\nend effectors of a robotic system that more often enter into contact with other\\nelements of the environment, must be perceived and tracked with precision to\\nexecute the intended tasks with dexterity and without colliding with obstacles.\\nThey are fundamental for several applications, from Human-Robot Interaction\\ntasks to object manipulation. Modern humanoid robots are characterized by high\\nnumber of degrees of freedom which makes their forward kinematics models very\\nsensitive to uncertainty. Thus, resorting to vision sensing can be the only\\nsolution to endow these robots with a good perception of the self, being able\\nto localize their body parts with precision. In this paper, we propose the use\\nof a Convolution Neural Network (CNN) to segment the robot hand from an image\\nin an egocentric view. It is known that CNNs require a huge amount of data to\\nbe trained. To overcome the challenge of labeling real-world images, we propose\\nthe use of simulated datasets exploiting domain randomization techniques. We\\nfine-tuned the Mask-RCNN network for the specific task of segmenting the hand\\nof the humanoid robot Vizzy. We focus our attention on developing a methodology\\nthat requires low amounts of data to achieve reasonable performance while\\ngiving detailed insight on how to properly generate variability in the training\\ndataset. Moreover, we analyze the fine-tuning process within the complex model\\nof Mask-RCNN, understanding which weights should be transferred to the new task\\nof segmenting robot hands. Our final model was trained solely on synthetic\\nimages and achieves an average IoU of 82% on synthetic validation data and\\n56.3% on real test data. These results were achieved with only 1000 training\\nimages and 3 hours of training time using a single GPU.',\n",
       "    'author': [{'name': 'Alexandre Almeida'},\n",
       "     {'name': 'Pedro Vicente'},\n",
       "     {'name': 'Alexandre Bernardino'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '13 pages, 12 figures, Submitted to Journal of Robotics and Autonomous\\n  Systems'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.04750v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.04750v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.08945v1',\n",
       "    'updated': '2021-02-17T18:58:02Z',\n",
       "    'published': '2021-02-17T18:58:02Z',\n",
       "    'title': 'Weakly Supervised Learning of Rigid 3D Scene Flow',\n",
       "    'summary': 'We propose a data-driven scene flow estimation algorithm exploiting the\\nobservation that many 3D scenes can be explained by a collection of agents\\nmoving as rigid bodies. At the core of our method lies a deep architecture able\\nto reason at the \\\\textbf{object-level} by considering 3D scene flow in\\nconjunction with other 3D tasks. This object level abstraction, enables us to\\nrelax the requirement for dense scene flow supervision with simpler binary\\nbackground segmentation mask and ego-motion annotations. Our mild supervision\\nrequirements make our method well suited for recently released massive data\\ncollections for autonomous driving, which do not contain dense scene flow\\nannotations. As output, our model provides low-level cues like pointwise flow\\nand higher-level cues such as holistic scene understanding at the level of\\nrigid objects. We further propose a test-time optimization refining the\\npredicted rigid scene flow. We showcase the effectiveness and generalization\\ncapacity of our method on four different autonomous driving datasets. We\\nrelease our source code and pre-trained models under\\n\\\\url{github.com/zgojcic/Rigid3DSceneFlow}.',\n",
       "    'author': [{'name': 'Zan Gojcic'},\n",
       "     {'name': 'Or Litany'},\n",
       "     {'name': 'Andreas Wieser'},\n",
       "     {'name': 'Leonidas J. Guibas'},\n",
       "     {'name': 'Tolga Birdal'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.08945v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.08945v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.02074v1',\n",
       "    'updated': '2021-03-02T22:57:43Z',\n",
       "    'published': '2021-03-02T22:57:43Z',\n",
       "    'title': 'Sequential Place Learning: Heuristic-Free High-Performance Long-Term\\n  Place Recognition',\n",
       "    'summary': 'Sequential matching using hand-crafted heuristics has been standard practice\\nin route-based place recognition for enhancing pairwise similarity results for\\nnearly a decade. However, precision-recall performance of these algorithms\\ndramatically degrades when searching on short temporal window (TW) lengths,\\nwhile demanding high compute and storage costs on large robotic datasets for\\nautonomous navigation research. Here, influenced by biological systems that\\nrobustly navigate spacetime scales even without vision, we develop a joint\\nvisual and positional representation learning technique, via a sequential\\nprocess, and design a learning-based CNN+LSTM architecture, trainable via\\nbackpropagation through time, for viewpoint- and appearance-invariant place\\nrecognition. Our approach, Sequential Place Learning (SPL), is based on a CNN\\nfunction that visually encodes an environment from a single traversal, thus\\nreducing storage capacity, while an LSTM temporally fuses each visual embedding\\nwith corresponding positional data -- obtained from any source of motion\\nestimation -- for direct sequential inference. Contrary to classical two-stage\\npipelines, e.g., match-then-temporally-filter, our network directly eliminates\\nfalse-positive rates while jointly learning sequence matching from a single\\nmonocular image sequence, even using short TWs. Hence, we demonstrate that our\\nmodel outperforms 15 classical methods while setting new state-of-the-art\\nperformance standards on 4 challenging benchmark datasets, where one of them\\ncan be considered solved with recall rates of 100% at 100% precision, correctly\\nmatching all places under extreme sunlight-darkness changes. In addition, we\\nshow that SPL can be up to 70x faster to deploy than classical methods on a 729\\nkm route comprising 35,768 consecutive frames. Extensive experiments\\ndemonstrate the... Baseline code available at\\nhttps://github.com/mchancan/deepseqslam',\n",
       "    'author': [{'name': 'Marvin Chancán'}, {'name': 'Michael Milford'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to RSS 2021. 14 pages, 5 tables, 17 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.02074v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.02074v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.04174v3',\n",
       "    'updated': '2021-06-19T07:25:28Z',\n",
       "    'published': '2021-03-06T18:58:56Z',\n",
       "    'title': 'Greedy Hierarchical Variational Autoencoders for Large-Scale Video\\n  Prediction',\n",
       "    'summary': 'A video prediction model that generalizes to diverse scenes would enable\\nintelligent agents such as robots to perform a variety of tasks via planning\\nwith the model. However, while existing video prediction models have produced\\npromising results on small datasets, they suffer from severe underfitting when\\ntrained on large and diverse datasets. To address this underfitting challenge,\\nwe first observe that the ability to train larger video prediction models is\\noften bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep\\nhierarchical latent variable models can produce higher quality predictions by\\ncapturing the multi-level stochasticity of future observations, but end-to-end\\noptimization of such models is notably difficult. Our key insight is that\\ngreedy and modular optimization of hierarchical autoencoders can simultaneously\\naddress both the memory constraints and the optimization challenges of\\nlarge-scale video prediction. We introduce Greedy Hierarchical Variational\\nAutoencoders (GHVAEs), a method that learns high-fidelity video predictions by\\ngreedily training each level of a hierarchical autoencoder. In comparison to\\nstate-of-the-art models, GHVAEs provide 17-55% gains in prediction performance\\non four video datasets, a 35-40% higher success rate on real robot tasks, and\\ncan improve performance monotonically by simply adding more modules.',\n",
       "    'author': [{'name': 'Bohan Wu'},\n",
       "     {'name': 'Suraj Nair'},\n",
       "     {'name': 'Roberto Martin-Martin'},\n",
       "     {'name': 'Li Fei-Fei'},\n",
       "     {'name': 'Chelsea Finn'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Equal advising and contribution for last two authors'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.04174v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.04174v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.04351v1',\n",
       "    'updated': '2021-03-07T13:19:06Z',\n",
       "    'published': '2021-03-07T13:19:06Z',\n",
       "    'title': 'Learning a State Representation and Navigation in Cluttered and Dynamic\\n  Environments',\n",
       "    'summary': 'In this work, we present a learning-based pipeline to realise local\\nnavigation with a quadrupedal robot in cluttered environments with static and\\ndynamic obstacles. Given high-level navigation commands, the robot is able to\\nsafely locomote to a target location based on frames from a depth camera\\nwithout any explicit mapping of the environment. First, the sequence of images\\nand the current trajectory of the camera are fused to form a model of the world\\nusing state representation learning. The output of this lightweight module is\\nthen directly fed into a target-reaching and obstacle-avoiding policy trained\\nwith reinforcement learning. We show that decoupling the pipeline into these\\ncomponents results in a sample efficient policy learning stage that can be\\nfully trained in simulation in just a dozen minutes. The key part is the state\\nrepresentation, which is trained to not only estimate the hidden state of the\\nworld in an unsupervised fashion, but also helps bridging the reality gap,\\nenabling successful sim-to-real transfer. In our experiments with the\\nquadrupedal robot ANYmal in simulation and in reality, we show that our system\\ncan handle noisy depth images, avoid dynamic obstacles unseen during training,\\nand is endowed with local spatial awareness.',\n",
       "    'author': [{'name': 'David Hoeller'},\n",
       "     {'name': 'Lorenz Wellhausen'},\n",
       "     {'name': 'Farbod Farshidian'},\n",
       "     {'name': 'Marco Hutter'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 8 figures, 2 tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.04351v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.04351v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.09942v1',\n",
       "    'updated': '2021-03-17T23:09:28Z',\n",
       "    'published': '2021-03-17T23:09:28Z',\n",
       "    'title': 'Machine Vision based Sample-Tube Localization for Mars Sample Return',\n",
       "    'summary': 'A potential Mars Sample Return (MSR) architecture is being jointly studied by\\nNASA and ESA. As currently envisioned, the MSR campaign consists of a series of\\n3 missions: sample cache, fetch and return to Earth. In this paper, we focus on\\nthe fetch part of the MSR, and more specifically the problem of autonomously\\ndetecting and localizing sample tubes deposited on the Martian surface. Towards\\nthis end, we study two machine-vision based approaches: First, a\\ngeometry-driven approach based on template matching that uses hard-coded\\nfilters and a 3D shape model of the tube; and second, a data-driven approach\\nbased on convolutional neural networks (CNNs) and learned features.\\nFurthermore, we present a large benchmark dataset of sample-tube images,\\ncollected in representative outdoor environments and annotated with ground\\ntruth segmentation masks and locations. The dataset was acquired systematically\\nacross different terrain, illumination conditions and dust-coverage; and\\nbenchmarking was performed to study the feasibility of each approach, their\\nrelative strengths and weaknesses, and robustness in the presence of adverse\\nenvironmental conditions.',\n",
       "    'author': [{'name': 'Shreyansh Daftry'},\n",
       "     {'name': 'Barry Ridge'},\n",
       "     {'name': 'William Seto'},\n",
       "     {'name': 'Tu-Hoa Pham'},\n",
       "     {'name': 'Peter Ilhardt'},\n",
       "     {'name': 'Gerard Maggiolino'},\n",
       "     {'name': 'Mark Van der Merwe'},\n",
       "     {'name': 'Alex Brinkman'},\n",
       "     {'name': 'John Mayo'},\n",
       "     {'name': 'Eric Kulczyski'},\n",
       "     {'name': 'Renaud Detry'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Aerospace Conference, 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.09942v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.09942v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.14025v1',\n",
       "    'updated': '2021-03-25T17:59:08Z',\n",
       "    'published': '2021-03-25T17:59:08Z',\n",
       "    'title': 'The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion\\n  Planning Benchmark for Physically Realistic Embodied AI',\n",
       "    'summary': 'We introduce a visually-guided and physics-driven task-and-motion planning\\nbenchmark, which we call the ThreeDWorld Transport Challenge. In this\\nchallenge, an embodied agent equipped with two 9-DOF articulated arms is\\nspawned randomly in a simulated physical home environment. The agent is\\nrequired to find a small set of objects scattered around the house, pick them\\nup, and transport them to a desired final location. We also position containers\\naround the house that can be used as tools to assist with transporting objects\\nefficiently. To complete the task, an embodied agent must plan a sequence of\\nactions to change the state of a large number of objects in the face of\\nrealistic physical constraints. We build this benchmark challenge using the\\nThreeDWorld simulation: a virtual 3D environment where all objects respond to\\nphysics, and where can be controlled using fully physics-driven navigation and\\ninteraction API. We evaluate several existing agents on this benchmark.\\nExperimental results suggest that: 1) a pure RL model struggles on this\\nchallenge; 2) hierarchical planning-based agents can transport some objects but\\nstill far from solving this task. We anticipate that this benchmark will\\nempower researchers to develop more intelligent physics-driven robots for the\\nphysical world.',\n",
       "    'author': [{'name': 'Chuang Gan'},\n",
       "     {'name': 'Siyuan Zhou'},\n",
       "     {'name': 'Jeremy Schwartz'},\n",
       "     {'name': 'Seth Alter'},\n",
       "     {'name': 'Abhishek Bhandwaldar'},\n",
       "     {'name': 'Dan Gutfreund'},\n",
       "     {'name': 'Daniel L. K. Yamins'},\n",
       "     {'name': 'James J DiCarlo'},\n",
       "     {'name': 'Josh McDermott'},\n",
       "     {'name': 'Antonio Torralba'},\n",
       "     {'name': 'Joshua B. Tenenbaum'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page: http://tdw-transport.csail.mit.edu/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.14025v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.14025v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.14283v1',\n",
       "    'updated': '2021-03-26T06:11:05Z',\n",
       "    'published': '2021-03-26T06:11:05Z',\n",
       "    'title': 'OmniHang: Learning to Hang Arbitrary Objects using Contact Point\\n  Correspondences and Neural Collision Estimation',\n",
       "    'summary': 'In this paper, we explore whether a robot can learn to hang arbitrary objects\\nonto a diverse set of supporting items such as racks or hooks. Endowing robots\\nwith such an ability has applications in many domains such as domestic\\nservices, logistics, or manufacturing. Yet, it is a challenging manipulation\\ntask due to the large diversity of geometry and topology of everyday objects.\\nIn this paper, we propose a system that takes partial point clouds of an object\\nand a supporting item as input and learns to decide where and how to hang the\\nobject stably. Our system learns to estimate the contact point correspondences\\nbetween the object and supporting item to get an estimated stable pose. We then\\nrun a deep reinforcement learning algorithm to refine the predicted stable\\npose. Then, the robot needs to find a collision-free path to move the object\\nfrom its initial pose to stable hanging pose. To this end, we train a neural\\nnetwork based collision estimator that takes as input partial point clouds of\\nthe object and supporting item. We generate a new and challenging, large-scale,\\nsynthetic dataset annotated with stable poses of objects hung on various\\nsupporting items and their contact point correspondences. In this dataset, we\\nshow that our system is able to achieve a 68.3% success rate of predicting\\nstable object poses and has a 52.1% F1 score in terms of finding feasible\\npaths. Supplemental material and videos are available on our project webpage.',\n",
       "    'author': [{'name': 'Yifan You'},\n",
       "     {'name': 'Lin Shao'},\n",
       "     {'name': 'Toki Migimatsu'},\n",
       "     {'name': 'Jeannette Bohg'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE International Conference on Robotics and Automation\\n  (ICRA) 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.14283v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.14283v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.16775v1',\n",
       "    'updated': '2021-03-31T02:42:28Z',\n",
       "    'published': '2021-03-31T02:42:28Z',\n",
       "    'title': 'Attention, please! A survey of Neural Attention Models in Deep Learning',\n",
       "    'summary': \"In humans, Attention is a core property of all perceptual and cognitive\\noperations. Given our limited ability to process competing sources, attention\\nmechanisms select, modulate, and focus on the information most relevant to\\nbehavior. For decades, concepts and functions of attention have been studied in\\nphilosophy, psychology, neuroscience, and computing. For the last six years,\\nthis property has been widely explored in deep neural networks. Currently, the\\nstate-of-the-art in Deep Learning is represented by neural attention models in\\nseveral application domains. This survey provides a comprehensive overview and\\nanalysis of developments in neural attention models. We systematically reviewed\\nhundreds of architectures in the area, identifying and discussing those in\\nwhich attention has shown a significant impact. We also developed and made\\npublic an automated methodology to facilitate the development of reviews in the\\narea. By critically analyzing 650 works, we describe the primary uses of\\nattention in convolutional, recurrent networks and generative models,\\nidentifying common subgroups of uses and applications. Furthermore, we describe\\nthe impact of attention in different application domains and their impact on\\nneural networks' interpretability. Finally, we list possible trends and\\nopportunities for further research, hoping that this review will provide a\\nsuccinct overview of the main attentional models in the area and guide\\nresearchers in developing future approaches that will drive further\\nimprovements.\",\n",
       "    'author': [{'name': 'Alana de Santana Correia'},\n",
       "     {'name': 'Esther Luna Colombini'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '66 pages, 24 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.16775v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.16775v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.16817v1',\n",
       "    'updated': '2021-03-31T05:25:05Z',\n",
       "    'published': '2021-03-31T05:25:05Z',\n",
       "    'title': 'Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human\\n  Videos',\n",
       "    'summary': 'We are motivated by the goal of generalist robots that can complete a wide\\nrange of tasks across many environments. Critical to this is the robot\\'s\\nability to acquire some metric of task success or reward, which is necessary\\nfor reinforcement learning, planning, or knowing when to ask for help. For a\\ngeneral-purpose robot operating in the real world, this reward function must\\nalso be able to generalize broadly across environments, tasks, and objects,\\nwhile depending only on on-board sensor observations (e.g. RGB images). While\\ndeep learning on large and diverse datasets has shown promise as a path towards\\nsuch generalization in computer vision and natural language, collecting high\\nquality datasets of robotic interaction at scale remains an open challenge. In\\ncontrast, \"in-the-wild\" videos of humans (e.g. YouTube) contain an extensive\\ncollection of people doing interesting tasks across a diverse range of\\nsettings. In this work, we propose a simple approach, Domain-agnostic Video\\nDiscriminator (DVD), that learns multitask reward functions by training a\\ndiscriminator to classify whether two videos are performing the same task, and\\ncan generalize by virtue of learning from a small amount of robot data with a\\nbroad dataset of human videos. We find that by leveraging diverse human\\ndatasets, this reward function (a) can generalize zero shot to unseen\\nenvironments, (b) generalize zero shot to unseen tasks, and (c) can be combined\\nwith visual model predictive control to solve robotic manipulation tasks on a\\nreal WidowX200 robot in an unseen environment from a single human demo.',\n",
       "    'author': [{'name': 'Annie S. Chen'},\n",
       "     {'name': 'Suraj Nair'},\n",
       "     {'name': 'Chelsea Finn'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'https://sites.google.com/view/dvd-human-videos'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.16817v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.16817v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.00442v2',\n",
       "    'updated': '2021-06-26T04:55:32Z',\n",
       "    'published': '2021-04-01T12:49:29Z',\n",
       "    'title': 'Touch-based Curiosity for Sparse-Reward Tasks',\n",
       "    'summary': 'Robots in many real-world settings have access to force/torque sensors in\\ntheir gripper and tactile sensing is often necessary in tasks that involve\\ncontact-rich motion. In this work, we leverage surprise from mismatches in\\ntouch feedback to guide exploration in hard sparse-reward reinforcement\\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\\nrewarding interactions where the expectation and the experience don\\'t match. In\\nour proposed method, an initial task-independent exploration phase is followed\\nby an on-task learning phase, in which the original interactions are relabeled\\nwith on-task rewards. We test our approach on a range of touch-intensive robot\\narm tasks (e.g. pushing objects, opening doors), which we also release as part\\nof this work. Across multiple experiments in a simulated setting, we\\ndemonstrate that our method is able to learn these difficult tasks through\\nsparse reward and curiosity alone. We compare our cross-modal approach to\\nsingle-modality (touch- or vision-only) approaches as well as other\\ncuriosity-based methods and find that our method performs better and is more\\nsample-efficient.',\n",
       "    'author': [{'name': 'Sai Rajeswar'},\n",
       "     {'name': 'Cyril Ibrahim'},\n",
       "     {'name': 'Nitin Surya'},\n",
       "     {'name': 'Florian Golemo'},\n",
       "     {'name': 'David Vazquez'},\n",
       "     {'name': 'Aaron Courville'},\n",
       "     {'name': 'Pedro O. Pinheiro'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.00442v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.00442v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.01328v2',\n",
       "    'updated': '2021-11-12T04:18:05Z',\n",
       "    'published': '2021-04-03T07:12:31Z',\n",
       "    'title': 'Uncertainty for Identifying Open-Set Errors in Visual Object Detection',\n",
       "    'summary': 'Deployed into an open world, object detectors are prone to open-set errors,\\nfalse positive detections of object classes not present in the training\\ndataset. We propose GMM-Det, a real-time method for extracting epistemic\\nuncertainty from object detectors to identify and reject open-set errors.\\nGMM-Det trains the detector to produce a structured logit space that is\\nmodelled with class-specific Gaussian Mixture Models. At test time, open-set\\nerrors are identified by their low log-probability under all Gaussian Mixture\\nModels. We test two common detector architectures, Faster R-CNN and RetinaNet,\\nacross three varied datasets spanning robotics and computer vision. Our results\\nshow that GMM-Det consistently outperforms existing uncertainty techniques for\\nidentifying and rejecting open-set detections, especially at the low-error-rate\\noperating point required for safety-critical applications. GMM-Det maintains\\nobject detection performance, and introduces only minimal computational\\noverhead. We also introduce a methodology for converting existing object\\ndetection datasets into specific open-set datasets to evaluate open-set\\nperformance in object detection.',\n",
       "    'author': [{'name': 'Dimity Miller'},\n",
       "     {'name': 'Niko Sünderhauf'},\n",
       "     {'name': 'Michael Milford'},\n",
       "     {'name': 'Feras Dayoub'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2021.3123374'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2021.3123374',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2104.01328v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.01328v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters (January 2022), Volume 7,\\n  Issue 1, pages 215-222, ISSN 2377-3766'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.03956v1',\n",
       "    'updated': '2021-04-08T17:57:41Z',\n",
       "    'published': '2021-04-08T17:57:41Z',\n",
       "    'title': 'Just Label What You Need: Fine-Grained Active Selection for Perception\\n  and Prediction through Partially Labeled Scenes',\n",
       "    'summary': 'Self-driving vehicles must perceive and predict the future positions of\\nnearby actors in order to avoid collisions and drive safely. A learned deep\\nlearning module is often responsible for this task, requiring large-scale,\\nhigh-quality training datasets. As data collection is often significantly\\ncheaper than labeling in this domain, the decision of which subset of examples\\nto label can have a profound impact on model performance. Active learning\\ntechniques, which leverage the state of the current model to iteratively select\\nexamples for labeling, offer a promising solution to this problem. However,\\ndespite the appeal of this approach, there has been little scientific analysis\\nof active learning approaches for the perception and prediction (P&P) problem.\\nIn this work, we study active learning techniques for P&P and find that the\\ntraditional active learning formulation is ill-suited for the P&P setting. We\\nthus introduce generalizations that ensure that our approach is both cost-aware\\nand allows for fine-grained selection of examples through partially labeled\\nscenes. Our experiments on a real-world, large-scale self-driving dataset\\nsuggest that fine-grained selection can improve the performance across\\nperception, prediction, and downstream planning tasks.',\n",
       "    'author': [{'name': 'Sean Segal'},\n",
       "     {'name': 'Nishanth Kumar'},\n",
       "     {'name': 'Sergio Casas'},\n",
       "     {'name': 'Wenyuan Zeng'},\n",
       "     {'name': 'Mengye Ren'},\n",
       "     {'name': 'Jingkang Wang'},\n",
       "     {'name': 'Raquel Urtasun'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.03956v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.03956v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.09224v1',\n",
       "    'updated': '2021-04-19T11:48:13Z',\n",
       "    'published': '2021-04-19T11:48:13Z',\n",
       "    'title': 'Multi-Modal Fusion Transformer for End-to-End Autonomous Driving',\n",
       "    'summary': 'How should representations from complementary sensors be integrated for\\nautonomous driving? Geometry-based sensor fusion has shown great promise for\\nperception tasks such as object detection and motion forecasting. However, for\\nthe actual driving task, the global context of the 3D scene is key, e.g. a\\nchange in traffic light state can affect the behavior of a vehicle\\ngeometrically distant from that traffic light. Geometry alone may therefore be\\ninsufficient for effectively fusing representations in end-to-end driving\\nmodels. In this work, we demonstrate that imitation learning policies based on\\nexisting sensor fusion methods under-perform in the presence of a high density\\nof dynamic agents and complex scenarios, which require global contextual\\nreasoning, such as handling traffic oncoming from multiple directions at\\nuncontrolled intersections. Therefore, we propose TransFuser, a novel\\nMulti-Modal Fusion Transformer, to integrate image and LiDAR representations\\nusing attention. We experimentally validate the efficacy of our approach in\\nurban settings involving complex scenarios using the CARLA urban driving\\nsimulator. Our approach achieves state-of-the-art driving performance while\\nreducing collisions by 76% compared to geometry-based fusion.',\n",
       "    'author': [{'name': 'Aditya Prakash'},\n",
       "     {'name': 'Kashyap Chitta'},\n",
       "     {'name': 'Andreas Geiger'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.09224v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.09224v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.11165v1',\n",
       "    'updated': '2021-04-22T16:35:32Z',\n",
       "    'published': '2021-04-22T16:35:32Z',\n",
       "    'title': 'Hierarchical growing grid networks for skeleton based action recognition',\n",
       "    'summary': 'In this paper, a novel cognitive architecture for action recognition is\\ndeveloped by applying layers of growing grid neural networks.Using these layers\\nmakes the system capable of automatically arranging its representational\\nstructure. In addition to the expansion of the neural map during the growth\\nphase, the system is provided with a prior knowledge of the input space, which\\nincreases the processing speed of the learning phase. Apart from two layers of\\ngrowing grid networks the architecture is composed of a preprocessing layer, an\\nordered vector representation layer and a one-layer supervised neural network.\\nThese layers are designed to solve the action recognition problem. The\\nfirst-layer growing grid receives the input data of human actions and the\\nneural map generates an action pattern vector representing each action sequence\\nby connecting the elicited activation of the trained map. The pattern vectors\\nare then sent to the ordered vector representation layer to build the\\ntime-invariant input vectors of key activations for the second-layer growing\\ngrid. The second-layer growing grid categorizes the input vectors to the\\ncorresponding action clusters/sub-clusters and finally the one-layer supervised\\nneural network labels the shaped clusters with action labels. Three experiments\\nusing different datasets of actions show that the system is capable of learning\\nto categorize the actions quickly and efficiently. The performance of the\\ngrowing grid architecture is com-pared with the results from a system based on\\nSelf-Organizing Maps, showing that the growing grid architecture performs\\nsignificantly superior on the action recognition tasks.',\n",
       "    'author': {'name': 'Zahra Gharaee'},\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.cogsys.2020.05.002'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.cogsys.2020.05.002',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2104.11165v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.11165v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Cognitive Systems Research, vol.63, pp.11-29 (2020)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.00373v4',\n",
       "    'updated': '2022-05-04T06:00:55Z',\n",
       "    'published': '2021-05-02T01:52:18Z',\n",
       "    'title': 'Investigating the Impact of Multi-LiDAR Placement on Object Detection\\n  for Autonomous Driving',\n",
       "    'summary': 'The past few years have witnessed an increasing interest in improving the\\nperception performance of LiDARs on autonomous vehicles. While most of the\\nexisting works focus on developing new deep learning algorithms or model\\narchitectures, we study the problem from the physical design perspective, i.e.,\\nhow different placements of multiple LiDARs influence the learning-based\\nperception. To this end, we introduce an easy-to-compute information-theoretic\\nsurrogate metric to quantitatively and fast evaluate LiDAR placement for 3D\\ndetection of different types of objects. We also present a new data collection,\\ndetection model training and evaluation framework in the realistic CARLA\\nsimulator to evaluate disparate multi-LiDAR configurations. Using several\\nprevalent placements inspired by the designs of self-driving companies, we show\\nthe correlation between our surrogate metric and object detection performance\\nof different representative algorithms on KITTI through extensive experiments,\\nvalidating the effectiveness of our LiDAR placement evaluation approach. Our\\nresults show that sensor placement is non-negligible in 3D point cloud-based\\nobject detection, which will contribute up to 10% performance discrepancy in\\nterms of average precision in challenging 3D object detection settings. We\\nbelieve that this is one of the first studies to quantitatively investigate the\\ninfluence of LiDAR placement on perception performance. The code is available\\non https://github.com/HanjiangHu/Multi-LiDAR-Placement-for-3D-Detection.',\n",
       "    'author': [{'name': 'Hanjiang Hu'},\n",
       "     {'name': 'Zuxin Liu'},\n",
       "     {'name': 'Sharad Chitlangia'},\n",
       "     {'name': 'Akhil Agnihotri'},\n",
       "     {'name': 'Ding Zhao'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2022 camera-ready version:15 pages, 14 figures, 9 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.00373v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.00373v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.01060v2',\n",
       "    'updated': '2021-08-31T01:43:15Z',\n",
       "    'published': '2021-05-03T17:59:20Z',\n",
       "    'title': 'Curious Representation Learning for Embodied Intelligence',\n",
       "    'summary': 'Self-supervised representation learning has achieved remarkable success in\\nrecent years. By subverting the need for supervised labels, such approaches are\\nable to utilize the numerous unlabeled images that exist on the Internet and in\\nphotographic datasets. Yet to build truly intelligent agents, we must construct\\nrepresentation learning algorithms that can learn not only from datasets but\\nalso learn from environments. An agent in a natural environment will not\\ntypically be fed curated data. Instead, it must explore its environment to\\nacquire the data it will learn from. We propose a framework, curious\\nrepresentation learning (CRL), which jointly learns a reinforcement learning\\npolicy and a visual representation model. The policy is trained to maximize the\\nerror of the representation learner, and in doing so is incentivized to explore\\nits environment. At the same time, the learned representation becomes stronger\\nand stronger as the policy feeds it ever harder data to learn from. Our learned\\nrepresentations enable promising transfer to downstream navigation tasks,\\nperforming better than or comparably to ImageNet pretraining without using any\\nsupervision at all. In addition, despite being trained in simulation, our\\nlearned representations can obtain interpretable results on real images. Code\\nis available at https://yilundu.github.io/crl/.',\n",
       "    'author': [{'name': 'Yilun Du'},\n",
       "     {'name': 'Chuang Gan'},\n",
       "     {'name': 'Phillip Isola'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To apear at ICCV 2021. Code is available at\\n  https://yilundu.github.io/crl'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.01060v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.01060v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.09903v2',\n",
       "    'updated': '2021-08-09T16:56:21Z',\n",
       "    'published': '2021-05-20T17:07:36Z',\n",
       "    'title': 'Multi-Perspective Anomaly Detection',\n",
       "    'summary': 'Anomaly detection is a critical problem in the manufacturing industry. In\\nmany applications, images of objects to be analyzed are captured from multiple\\nperspectives which can be exploited to improve the robustness of anomaly\\ndetection. In this work, we build upon the deep support vector data description\\nalgorithm and address multi-perspective anomaly detection using three different\\nfusion techniques, i.e., early fusion, late fusion, and late fusion with\\nmultiple decoders. We employ different augmentation techniques with a denoising\\nprocess to deal with scarce one-class data, which further improves the\\nperformance (ROC AUC $= 80\\\\%$). Furthermore, we introduce the dices dataset,\\nwhich consists of over 2000 grayscale images of falling dices from multiple\\nperspectives, with 5\\\\% of the images containing rare anomalies (e.g., drill\\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\\nusing images from two different perspectives and also benchmark on the standard\\nMNIST dataset. Extensive experiments demonstrate that our proposed\\n{multi-perspective} approach exceeds the state-of-the-art {single-perspective\\nanomaly detection on both the MNIST and dices datasets}. To the best of our\\nknowledge, this is the first work that focuses on addressing multi-perspective\\nanomaly detection in images by jointly using different perspectives together\\nwith one single objective function for anomaly detection.',\n",
       "    'author': [{'name': 'Peter Jakob'},\n",
       "     {'name': 'Manav Madan'},\n",
       "     {'name': 'Tobias Schmid-Schirling'},\n",
       "     {'name': 'Abhinav Valada'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.3390/s21165311'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.3390/s21165311',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2105.09903v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.09903v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '20 pages, 5 figures, 8 tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Sensors. 2021; 21(16):5311'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.10014v1',\n",
       "    'updated': '2021-05-20T20:10:14Z',\n",
       "    'published': '2021-05-20T20:10:14Z',\n",
       "    'title': 'Evaluating Robustness over High Level Driving Instruction for Autonomous\\n  Driving',\n",
       "    'summary': 'In recent years, we have witnessed increasingly high performance in the field\\nof autonomous end-to-end driving. In particular, more and more research is\\nbeing done on driving in urban environments, where the car has to follow high\\nlevel commands to navigate. However, few evaluations are made on the ability of\\nthese agents to react in an unexpected situation. Specifically, no evaluations\\nare conducted on the robustness of driving agents in the event of a bad\\nhigh-level command. We propose here an evaluation method, namely a benchmark\\nthat allows to assess the robustness of an agent, and to appreciate its\\nunderstanding of the environment through its ability to keep a safe behavior,\\nregardless of the instruction.',\n",
       "    'author': [{'name': 'Florence Carton'},\n",
       "     {'name': 'David Filliat'},\n",
       "     {'name': 'Jaonary Rabarisoa'},\n",
       "     {'name': 'Quoc Cuong Pham'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IV21, 32nd IEEE Intelligent Vehicles Symposium'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.10014v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.10014v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.11283v2',\n",
       "    'updated': '2021-07-29T13:42:53Z',\n",
       "    'published': '2021-05-24T14:12:38Z',\n",
       "    'title': 'Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide\\n  Task Spaces',\n",
       "    'summary': 'In this paper, we study the problem of zero-shot sim-to-real when the task\\nrequires both highly precise control with sub-millimetre error tolerance, and\\nwide task space generalisation. Our framework involves a coarse-to-fine\\ncontroller, where trajectories begin with classical motion planning using\\nICP-based pose estimation, and transition to a learned end-to-end controller\\nwhich maps images to actions and is trained in simulation with domain\\nrandomisation. In this way, we achieve precise control whilst also generalising\\nthe controller across wide task spaces, and keeping the robustness of\\nvision-based, end-to-end control. Real-world experiments on a range of\\ndifferent tasks show that, by exploiting the best of both worlds, our framework\\nsignificantly outperforms purely motion planning methods, and purely\\nlearning-based methods. Furthermore, we answer a range of questions on best\\npractices for precise sim-to-real transfer, such as how different image sensor\\nmodalities and image feature representations perform.',\n",
       "    'author': [{'name': 'Eugene Valassakis'},\n",
       "     {'name': 'Norman Di Palo'},\n",
       "     {'name': 'Edward Johns'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To be published at IROS 2021. 8 pages, 6 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.11283v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.11283v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.12774v1',\n",
       "    'updated': '2021-05-26T18:19:21Z',\n",
       "    'published': '2021-05-26T18:19:21Z',\n",
       "    'title': 'DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially\\n  Trained Autoencoder',\n",
       "    'summary': 'Accurate reconstruction of static environments from LiDAR scans of scenes\\ncontaining dynamic objects, which we refer to as Dynamic to Static Translation\\n(DST), is an important area of research in Autonomous Navigation. This problem\\nhas been recently explored for visual SLAM, but to the best of our knowledge no\\nwork has been attempted to address DST for LiDAR scans. The problem is of\\ncritical importance due to wide-spread adoption of LiDAR in Autonomous\\nVehicles. We show that state-of the art methods developed for the visual domain\\nwhen adapted for LiDAR scans perform poorly.\\n  We develop DSLR, a deep generative model which learns a mapping between\\ndynamic scan to its static counterpart through an adversarially trained\\nautoencoder. Our model yields the first solution for DST on LiDAR that\\ngenerates static scans without using explicit segmentation labels. DSLR cannot\\nalways be applied to real world data due to lack of paired dynamic-static\\nscans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer\\nto real world data and experimentally show that this performs well in real\\nworld settings. Additionally, if segmentation information is available, we\\nextend DSLR to DSLR-Seg to further improve the reconstruction quality.\\n  DSLR gives the state of the art performance on simulated and real-world\\ndatasets and also shows at least 4x improvement. We show that DSLR, unlike the\\nexisting baselines, is a practically viable model with its reconstruction\\nquality within the tolerable limits for tasks pertaining to autonomous\\nnavigation like SLAM in dynamic environments.',\n",
       "    'author': [{'name': 'Prashant Kumar'},\n",
       "     {'name': 'Sabyasachi Sahoo'},\n",
       "     {'name': 'Vanshil Shah'},\n",
       "     {'name': 'Vineetha Kondameedi'},\n",
       "     {'name': 'Abhinav Jain'},\n",
       "     {'name': 'Akshaj Verma'},\n",
       "     {'name': 'Chiranjib Bhattacharyya'},\n",
       "     {'name': 'Vinay Viswanathan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '17 pages, 15 figures, Accepted at AAAI 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.12774v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.12774v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.12923v1',\n",
       "    'updated': '2021-05-27T03:26:40Z',\n",
       "    'published': '2021-05-27T03:26:40Z',\n",
       "    'title': 'Robust Navigation for Racing Drones based on Imitation Learning and\\n  Modularization',\n",
       "    'summary': 'This paper presents a vision-based modularized drone racing navigation system\\nthat uses a customized convolutional neural network (CNN) for the perception\\nmodule to produce high-level navigation commands and then leverages a\\nstate-of-the-art planner and controller to generate low-level control commands,\\nthus exploiting the advantages of both data-based and model-based approaches.\\nUnlike the state-of-the-art method which only takes the current camera image as\\nthe CNN input, we further add the latest three drone states as part of the\\ninputs. Our method outperforms the state-of-the-art method in various track\\nlayouts and offers two switchable navigation behaviors with a single trained\\nnetwork. The CNN-based perception module is trained to imitate an expert policy\\nthat automatically generates ground truth navigation commands based on the\\npre-computed global trajectories. Owing to the extensive randomization and our\\nmodified dataset aggregation (DAgger) policy during data collection, our\\nnavigation system, which is purely trained in simulation with synthetic\\ntextures, successfully operates in environments with randomly-chosen\\nphotorealistic textures without further fine-tuning.',\n",
       "    'author': [{'name': 'Tianqi Wang'}, {'name': 'Dong Eui Chang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at the 2021 International Conference on Robotics and\\n  Automation (ICRA 2021)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.12923v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.12923v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.14829v2',\n",
       "    'updated': '2022-02-04T01:01:14Z',\n",
       "    'published': '2021-05-31T09:44:16Z',\n",
       "    'title': 'Q-attention: Enabling Efficient Learning for Vision-based Robotic\\n  Manipulation',\n",
       "    'summary': 'Despite the success of reinforcement learning methods, they have yet to have\\ntheir breakthrough moment when applied to a broad range of robotic manipulation\\ntasks. This is partly due to the fact that reinforcement learning algorithms\\nare notoriously difficult and time consuming to train, which is exacerbated\\nwhen training from images rather than full-state inputs. As humans perform\\nmanipulation tasks, our eyes closely monitor every step of the process with our\\ngaze focusing sequentially on the objects being manipulated. With this in mind,\\nwe present our Attention-driven Robotic Manipulation (ARM) algorithm, which is\\na general manipulation algorithm that can be applied to a range of\\nsparse-rewarded tasks, given only a small number of demonstrations. ARM splits\\nthe complex task of manipulation into a 3 stage pipeline: (1) a Q-attention\\nagent extracts relevant pixel locations from RGB and point cloud inputs, (2) a\\nnext-best pose agent that accepts crops from the Q-attention agent and outputs\\nposes, and (3) a control agent that takes the goal pose and outputs joint\\nactions. We show that current learning algorithms fail on a range of RLBench\\ntasks, whilst ARM is successful.',\n",
       "    'author': [{'name': 'Stephen James'}, {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters, 2022 (+ presentation at ICRA\\n  2022). Videos and code found at: https://sites.google.com/view/q-attention'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2105.14829v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.14829v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.05966v1',\n",
       "    'updated': '2021-06-10T17:58:34Z',\n",
       "    'published': '2021-06-10T17:58:34Z',\n",
       "    'title': 'Learning by Watching',\n",
       "    'summary': \"When in a new situation or geographical location, human drivers have an\\nextraordinary ability to watch others and learn maneuvers that they themselves\\nmay have never performed. In contrast, existing techniques for learning to\\ndrive preclude such a possibility as they assume direct access to an\\ninstrumented ego-vehicle with fully known observations and expert driver\\nactions. However, such measurements cannot be directly accessed for the non-ego\\nvehicles when learning by watching others. Therefore, in an application where\\ndata is regarded as a highly valuable asset, current approaches completely\\ndiscard the vast portion of the training data that can be potentially obtained\\nthrough indirect observation of surrounding vehicles. Motivated by this key\\ninsight, we propose the Learning by Watching (LbW) framework which enables\\nlearning a driving policy without requiring full knowledge of neither the state\\nnor expert actions. To increase its data, i.e., with new perspectives and\\nmaneuvers, LbW makes use of the demonstrations of other vehicles in a given\\nscene by (1) transforming the ego-vehicle's observations to their points of\\nview, and (2) inferring their expert actions. Our LbW agent learns more robust\\ndriving policies while enabling data-efficient learning, including quick\\nadaptation of the policy to rare and novel scenarios. In particular, LbW drives\\nrobustly even with a fraction of available driving data required by existing\\nmethods, achieving an average success rate of 92% on the original CARLA\\nbenchmark with only 30 minutes of total driving data and 82% with only 10\\nminutes.\",\n",
       "    'author': [{'name': 'Jimuyang Zhang'}, {'name': 'Eshed Ohn-Bar'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.05966v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.05966v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.12534v2',\n",
       "    'updated': '2022-03-15T00:33:43Z',\n",
       "    'published': '2021-06-23T16:57:16Z',\n",
       "    'title': 'Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic\\n  Manipulation via Discretisation',\n",
       "    'summary': \"We present a coarse-to-fine discretisation method that enables the use of\\ndiscrete reinforcement learning approaches in place of unstable and\\ndata-inefficient actor-critic methods in continuous robotics domains. This\\napproach builds on the recently released ARM algorithm, which replaces the\\ncontinuous next-best pose agent with a discrete one, with coarse-to-fine\\nQ-attention. Given a voxelised scene, coarse-to-fine Q-attention learns what\\npart of the scene to 'zoom' into. When this 'zooming' behaviour is applied\\niteratively, it results in a near-lossless discretisation of the translation\\nspace, and allows the use of a discrete action, deep Q-learning method. We show\\nthat our new coarse-to-fine algorithm achieves state-of-the-art performance on\\nseveral difficult sparsely rewarded RLBench vision-based robotics tasks, and\\ncan train real-world policies, tabula rasa, in a matter of minutes, with as\\nlittle as 3 demonstrations.\",\n",
       "    'author': [{'name': 'Stephen James'},\n",
       "     {'name': 'Kentaro Wada'},\n",
       "     {'name': 'Tristan Laidlow'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\n  Recognition (CVPR 2022). Videos and code:\\n  https://sites.google.com/view/c2f-q-attention'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.12534v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.12534v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.02792v1',\n",
       "    'updated': '2021-07-06T17:59:02Z',\n",
       "    'published': '2021-07-06T17:59:02Z',\n",
       "    'title': 'Learned Visual Navigation for Under-Canopy Agricultural Robots',\n",
       "    'summary': 'We describe a system for visually guided autonomous navigation of\\nunder-canopy farm robots. Low-cost under-canopy robots can drive between crop\\nrows under the plant canopy and accomplish tasks that are infeasible for\\nover-the-canopy drones or larger agricultural equipment. However, autonomously\\nnavigating them under the canopy presents a number of challenges: unreliable\\nGPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to\\nleaves and weeds, and large variability in appearance over the season and\\nacross crop types. We address these challenges by building a modular system\\nthat leverages machine learning for robust and generalizable perception from\\nmonocular RGB images from low-cost cameras, and model predictive control for\\naccurate control in challenging terrain. Our system, CropFollow, is able to\\nautonomously drive 485 meters per intervention on average, outperforming a\\nstate-of-the-art LiDAR based system (286 meters per intervention) in extensive\\nfield testing spanning over 25 km.',\n",
       "    'author': [{'name': 'Arun Narenthiran Sivakumar'},\n",
       "     {'name': 'Sahil Modi'},\n",
       "     {'name': 'Mateus Valverde Gasparino'},\n",
       "     {'name': 'Che Ellis'},\n",
       "     {'name': 'Andres Eduardo Baquero Velasquez'},\n",
       "     {'name': 'Girish Chowdhary'},\n",
       "     {'name': 'Saurabh Gupta'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'RSS 2021. Project website with data and videos:\\n  https://ansivakumar.github.io/learned-visual-navigation/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.02792v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.02792v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.02974v1',\n",
       "    'updated': '2021-07-07T01:48:16Z',\n",
       "    'published': '2021-07-07T01:48:16Z',\n",
       "    'title': 'RAM-VO: Less is more in Visual Odometry',\n",
       "    'summary': \"Building vehicles capable of operating without human supervision requires the\\ndetermination of the agent's pose. Visual Odometry (VO) algorithms estimate the\\negomotion using only visual changes from the input images. The most recent VO\\nmethods implement deep-learning techniques using convolutional neural networks\\n(CNN) extensively, which add a substantial cost when dealing with\\nhigh-resolution images. Furthermore, in VO tasks, more input data does not mean\\na better prediction; on the contrary, the architecture may filter out useless\\ninformation. Therefore, the implementation of computationally efficient and\\nlightweight architectures is essential. In this work, we propose the RAM-VO, an\\nextension of the Recurrent Attention Model (RAM) for visual odometry tasks.\\nRAM-VO improves the visual and temporal representation of information and\\nimplements the Proximal Policy Optimization (PPO) algorithm to learn robust\\npolicies. The results indicate that RAM-VO can perform regressions with six\\ndegrees of freedom from monocular input images using approximately 3 million\\nparameters. In addition, experiments on the KITTI dataset demonstrate that\\nRAM-VO achieves competitive results using only 5.7% of the available visual\\ninformation.\",\n",
       "    'author': [{'name': 'Iury Cleveston'}, {'name': 'Esther L. Colombini'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.02974v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.02974v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.04000v1',\n",
       "    'updated': '2021-07-08T17:46:05Z',\n",
       "    'published': '2021-07-08T17:46:05Z',\n",
       "    'title': 'Active Safety Envelopes using Light Curtains with Probabilistic\\n  Guarantees',\n",
       "    'summary': 'To safely navigate unknown environments, robots must accurately perceive\\ndynamic obstacles. Instead of directly measuring the scene depth with a LiDAR\\nsensor, we explore the use of a much cheaper and higher resolution sensor:\\nprogrammable light curtains. Light curtains are controllable depth sensors that\\nsense only along a surface that a user selects. We use light curtains to\\nestimate the safety envelope of a scene: a hypothetical surface that separates\\nthe robot from all obstacles. We show that generating light curtains that sense\\nrandom locations (from a particular distribution) can quickly discover the\\nsafety envelope for scenes with unknown objects. Importantly, we produce\\ntheoretical safety guarantees on the probability of detecting an obstacle using\\nrandom curtains. We combine random curtains with a machine learning based model\\nthat forecasts and tracks the motion of the safety envelope efficiently. Our\\nmethod accurately estimates safety envelopes while providing probabilistic\\nsafety guarantees that can be used to certify the efficacy of a robot\\nperception system to detect and avoid dynamic obstacles. We evaluate our\\napproach in a simulated urban driving environment and a real-world environment\\nwith moving pedestrians using a light curtain device and show that we can\\nestimate safety envelopes efficiently and effectively. Project website:\\nhttps://siddancha.github.io/projects/active-safety-envelopes-with-guarantees',\n",
       "    'author': [{'name': 'Siddharth Ancha'},\n",
       "     {'name': 'Gaurav Pathak'},\n",
       "     {'name': 'Srinivasa G. Narasimhan'},\n",
       "     {'name': 'David Held'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.15607/rss.2021.xvii.045'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.15607/rss.2021.xvii.045',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2107.04000v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.04000v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '18 pages, Published at Robotics: Science and Systems (RSS) 2021'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.04013v1',\n",
       "    'updated': '2021-07-08T17:55:01Z',\n",
       "    'published': '2021-07-08T17:55:01Z',\n",
       "    'title': 'Multi-Modality Task Cascade for 3D Object Detection',\n",
       "    'summary': 'Point clouds and RGB images are naturally complementary modalities for 3D\\nvisual understanding - the former provides sparse but accurate locations of\\npoints on objects, while the latter contains dense color and texture\\ninformation. Despite this potential for close sensor fusion, many methods train\\ntwo models in isolation and use simple feature concatenation to represent 3D\\nsensor data. This separated training scheme results in potentially sub-optimal\\nperformance and prevents 3D tasks from being used to benefit 2D tasks that are\\noften useful on their own. To provide a more integrated approach, we propose a\\nnovel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box\\nproposals to improve 2D segmentation predictions, which are then used to\\nfurther refine the 3D boxes. We show that including a 2D network between two\\nstages of 3D modules significantly improves both 2D and 3D task performance.\\nMoreover, to prevent the 3D module from over-relying on the overfitted 2D\\npredictions, we propose a dual-head 2D segmentation training and inference\\nscheme, allowing the 2nd 3D module to learn to interpret imperfect 2D\\nsegmentation predictions. Evaluating our model on the challenging SUN RGB-D\\ndataset, we improve upon state-of-the-art results of both single modality and\\nfusion networks by a large margin ($\\\\textbf{+3.8}$ mAP@0.5). Code will be\\nreleased $\\\\href{https://github.com/Divadi/MTC_RCNN}{\\\\text{here.}}$',\n",
       "    'author': [{'name': 'Jinhyung Park'},\n",
       "     {'name': 'Xinshuo Weng'},\n",
       "     {'name': 'Yunze Man'},\n",
       "     {'name': 'Kris Kitani'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.04013v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.04013v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.04034v1',\n",
       "    'updated': '2021-07-08T17:59:59Z',\n",
       "    'published': '2021-07-08T17:59:59Z',\n",
       "    'title': 'RMA: Rapid Motor Adaptation for Legged Robots',\n",
       "    'summary': 'Successful real-world deployment of legged robots would require them to adapt\\nin real-time to unseen scenarios like changing terrains, changing payloads,\\nwear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to\\nsolve this problem of real-time online adaptation in quadruped robots. RMA\\nconsists of two components: a base policy and an adaptation module. The\\ncombination of these components enables the robot to adapt to novel situations\\nin fractions of a second. RMA is trained completely in simulation without using\\nany domain knowledge like reference trajectories or predefined foot trajectory\\ngenerators and is deployed on the A1 robot without any fine-tuning. We train\\nRMA on a varied terrain generator using bioenergetics-inspired rewards and\\ndeploy it on a variety of difficult terrains including rocky, slippery,\\ndeformable surfaces in environments with grass, long vegetation, concrete,\\npebbles, stairs, sand, etc. RMA shows state-of-the-art performance across\\ndiverse real-world as well as simulation experiments. Video results at\\nhttps://ashish-kmr.github.io/rma-legged-robots/',\n",
       "    'author': [{'name': 'Ashish Kumar'},\n",
       "     {'name': 'Zipeng Fu'},\n",
       "     {'name': 'Deepak Pathak'},\n",
       "     {'name': 'Jitendra Malik'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'RSS 2021. Webpage at https://ashish-kmr.github.io/rma-legged-robots/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.04034v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.04034v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.04619v1',\n",
       "    'updated': '2021-07-09T18:15:16Z',\n",
       "    'published': '2021-07-09T18:15:16Z',\n",
       "    'title': 'Diverse Video Generation using a Gaussian Process Trigger',\n",
       "    'summary': 'Generating future frames given a few context (or past) frames is a\\nchallenging task. It requires modeling the temporal coherence of videos and\\nmulti-modality in terms of diversity in the potential future states. Current\\nvariational approaches for video generation tend to marginalize over\\nmulti-modal future outcomes. Instead, we propose to explicitly model the\\nmulti-modality in the future outcomes and leverage it to sample diverse\\nfutures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to\\nlearn priors on future states given the past and maintains a probability\\ndistribution over possible futures given a particular sample. In addition, we\\nleverage the changes in this distribution over time to control the sampling of\\ndiverse future states by estimating the end of ongoing sequences. That is, we\\nuse the variance of GP over the output function space to trigger a change in an\\naction sequence. We achieve state-of-the-art results on diverse future frame\\ngeneration in terms of reconstruction quality and diversity of the generated\\nsequences.',\n",
       "    'author': [{'name': 'Gaurav Shrivastava'},\n",
       "     {'name': 'Abhinav Shrivastava'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Learning Representations, 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.04619v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.04619v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.09170v1',\n",
       "    'updated': '2021-07-19T21:51:06Z',\n",
       "    'published': '2021-07-19T21:51:06Z',\n",
       "    'title': 'DeepSocNav: Social Navigation by Imitating Human Behaviors',\n",
       "    'summary': \"Current datasets to train social behaviors are usually borrowed from\\nsurveillance applications that capture visual data from a bird's-eye\\nperspective. This leaves aside precious relationships and visual cues that\\ncould be captured through a first-person view of a scene. In this work, we\\npropose a strategy to exploit the power of current game engines, such as Unity,\\nto transform pre-existing bird's-eye view datasets into a first-person view, in\\nparticular, a depth view. Using this strategy, we are able to generate large\\nvolumes of synthetic data that can be used to pre-train a social navigation\\nmodel. To test our ideas, we present DeepSocNav, a deep learning based model\\nthat takes advantage of the proposed approach to generate synthetic data.\\nFurthermore, DeepSocNav includes a self-supervised strategy that is included as\\nan auxiliary task. This consists of predicting the next depth frame that the\\nagent will face. Our experiments show the benefits of the proposed model that\\nis able to outperform relevant baselines in terms of social navigation scores.\",\n",
       "    'author': [{'name': 'Juan Pablo de Vicente'}, {'name': 'Alvaro Soto'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, Accepted paper at the RSS Workshop on Social Robot\\n  Navigation 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.09170v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.09170v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.13389v1',\n",
       "    'updated': '2021-07-28T14:28:32Z',\n",
       "    'published': '2021-07-28T14:28:32Z',\n",
       "    'title': 'SimROD: A Simple Adaptation Method for Robust Object Detection',\n",
       "    'summary': 'This paper presents a Simple and effective unsupervised adaptation method for\\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\\nshift and pseudo-label noise, our method integrates a novel domain-centric\\naugmentation method, a gradual self-labeling adaptation procedure, and a\\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\\ncan be leveraged to adapt object detection models without changing the model\\narchitecture or generating synthetic data. When applied to image corruptions\\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\\nbenchmarks. On the image corruption benchmark, models adapted with our method\\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\\nand up to 4% on Watercolor dataset.',\n",
       "    'author': [{'name': 'Rindra Ramamonjison'},\n",
       "     {'name': 'Amin Banitalebi-Dehkordi'},\n",
       "     {'name': 'Xinyu Kang'},\n",
       "     {'name': 'Xiaolong Bai'},\n",
       "     {'name': 'Yong Zhang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICCV 2021 conference for full oral presentation'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.13389v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.13389v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.14229v3',\n",
       "    'updated': '2022-06-29T04:41:26Z',\n",
       "    'published': '2021-07-29T17:59:31Z',\n",
       "    'title': 'Physics-informed Guided Disentanglement in Generative Networks',\n",
       "    'summary': 'Image-to-image translation (i2i) networks suffer from entanglement effects in\\npresence of physics-related phenomena in target domain (such as occlusions,\\nfog, etc), lowering altogether the translation quality, controllability and\\nvariability. In this paper, we build upon collection of simple physics models\\nand present a comprehensive method for disentangling visual traits in target\\nimages, guiding the process with a physical model that renders some of the\\ntarget traits, and learning the remaining ones. Because it allows explicit and\\ninterpretable outputs, our physical models (optimally regressed on target)\\nallows generating unseen scenarios in a controllable manner. We also extend our\\nframework, showing versatility to neural-guided disentanglement. The results\\nshow our disentanglement strategies dramatically increase performances\\nqualitatively and quantitatively in several challenging scenarios for image\\ntranslation.',\n",
       "    'author': [{'name': 'Fabio Pizzati'},\n",
       "     {'name': 'Pietro Cerri'},\n",
       "     {'name': 'Raoul de Charette'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Journal submission'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.14229v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.14229v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.14483v5',\n",
       "    'updated': '2021-11-04T12:11:21Z',\n",
       "    'published': '2021-07-30T08:20:22Z',\n",
       "    'title': 'ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale\\n  Demonstrations',\n",
       "    'summary': 'Object manipulation from 3D visual inputs poses many challenges on building\\ngeneralizable perception and policy models. However, 3D assets in existing\\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\\nintra-class complexity in topology and geometry. Here we propose SAPIEN\\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\\nlarge intra-class topological and geometric variations. Tasks are carefully\\nchosen to cover distinct types of manipulation challenges. Latest progress in\\n3D vision also makes us believe that we should customize the benchmark so that\\nthe challenge is inviting to researchers working on 3D deep learning. To this\\nend, we simulate a moving panoramic camera that returns ego-centric point\\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\\nset of researchers interested in manipulation research. Besides supporting the\\nlearning of policies from interactions, we also support\\nlearning-from-demonstrations (LfD) methods, by providing a large number of\\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\\nbaselines) is open-sourced, and a challenge facing interdisciplinary\\nresearchers will be held based on the benchmark.',\n",
       "    'author': [{'name': 'Tongzhou Mu'},\n",
       "     {'name': 'Zhan Ling'},\n",
       "     {'name': 'Fanbo Xiang'},\n",
       "     {'name': 'Derek Yang'},\n",
       "     {'name': 'Xuanlin Li'},\n",
       "     {'name': 'Stone Tao'},\n",
       "     {'name': 'Zhiao Huang'},\n",
       "     {'name': 'Zhiwei Jia'},\n",
       "     {'name': 'Hao Su'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2021 Track on Datasets and Benchmarks; code:\\n  https://github.com/haosulab/ManiSkill'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.14483v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.14483v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.03272v4',\n",
       "    'updated': '2021-11-03T18:51:07Z',\n",
       "    'published': '2021-08-06T18:41:39Z',\n",
       "    'title': 'iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday\\n  Household Tasks',\n",
       "    'summary': 'Recent research in embodied AI has been boosted by the use of simulation\\nenvironments to develop and train robot learning approaches. However, the use\\nof simulation has skewed the attention to tasks that only require what robotics\\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\\nan open-source simulation environment that supports the simulation of a more\\ndiverse set of household tasks through three key innovations. First, iGibson\\n2.0 supports object states, including temperature, wetness level, cleanliness\\nlevel, and toggled and sliced states, necessary to cover a wider range of\\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\\nit. This functionality can generate potentially infinite instances of tasks\\nwith minimal effort from the users. The sampling mechanism allows our scenes to\\nbe more densely populated with small objects in semantically meaningful\\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\\nimmerse humans in its scenes to collect demonstrations. As a result, we can\\ncollect demonstrations from humans on these new types of tasks, and use them\\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\\nenable robot learning of novel tasks, in the hope of demonstrating the\\npotential of this new simulator to support new research in embodied AI. iGibson\\n2.0 and its new dataset are publicly available at\\nhttp://svl.stanford.edu/igibson/.',\n",
       "    'author': [{'name': 'Chengshu Li'},\n",
       "     {'name': 'Fei Xia'},\n",
       "     {'name': 'Roberto Martín-Martín'},\n",
       "     {'name': 'Michael Lingelbach'},\n",
       "     {'name': 'Sanjana Srivastava'},\n",
       "     {'name': 'Bokui Shen'},\n",
       "     {'name': 'Kent Vainio'},\n",
       "     {'name': 'Cem Gokmen'},\n",
       "     {'name': 'Gokul Dharan'},\n",
       "     {'name': 'Tanish Jain'},\n",
       "     {'name': 'Andrey Kurenkov'},\n",
       "     {'name': 'C. Karen Liu'},\n",
       "     {'name': 'Hyowon Gweon'},\n",
       "     {'name': 'Jiajun Wu'},\n",
       "     {'name': 'Li Fei-Fei'},\n",
       "     {'name': 'Silvio Savarese'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at Conference on Robot Learning (CoRL) 2021. Project\\n  website: http://svl.stanford.edu/igibson/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.03272v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.03272v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.05713v2',\n",
       "    'updated': '2022-06-02T15:22:32Z',\n",
       "    'published': '2021-08-08T11:29:16Z',\n",
       "    'title': 'Towards real-world navigation with deep differentiable planners',\n",
       "    'summary': 'We train embodied neural networks to plan and navigate unseen complex 3D\\nenvironments, emphasising real-world deployment. Rather than requiring prior\\nknowledge of the agent or environment, the planner learns to model the state\\ntransitions and rewards. To avoid the potentially hazardous trial-and-error of\\nreinforcement learning, we focus on differentiable planners such as Value\\nIteration Networks (VIN), which are trained offline from safe expert\\ndemonstrations. Although they work well in small simulations, we address two\\nmajor limitations that hinder their deployment. First, we observed that current\\ndifferentiable planners struggle to plan long-term in environments with a high\\nbranching complexity. While they should ideally learn to assign low rewards to\\nobstacles to avoid collisions, we posit that the constraints imposed on the\\nnetwork are not strong enough to guarantee the network to learn sufficiently\\nlarge penalties for every possible collision. We thus impose a structural\\nconstraint on the value iteration, which explicitly learns to model any\\nimpossible actions. Secondly, we extend the model to work with a limited\\nperspective camera under translation and rotation, which is crucial for real\\nrobot deployment. Many VIN-like planners assume a 360 degrees or overhead view\\nwithout rotation. In contrast, our method uses a memory-efficient lattice map\\nto aggregate CNN embeddings of partial observations, and models the rotational\\ndynamics explicitly using a 3D state-space grid (translation and rotation). Our\\nproposals significantly improve semantic navigation and exploration on several\\n2D and 3D environments, succeeding in settings that are otherwise challenging\\nfor this class of methods. As far as we know, we are the first to successfully\\nperform differentiable planning on the difficult Active Vision Dataset,\\nconsisting of real images captured from a robot.',\n",
       "    'author': [{'name': 'Shu Ishida'}, {'name': 'João F. Henriques'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in CVPR 2022 (Conference on Computer Vision and Pattern\\n  Recognition)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.05713v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.05713v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.08876v2',\n",
       "    'updated': '2021-10-05T09:31:08Z',\n",
       "    'published': '2021-08-19T18:54:19Z',\n",
       "    'title': 'Deep Learning-based Spacecraft Relative Navigation Methods: A Survey',\n",
       "    'summary': 'Autonomous spacecraft relative navigation technology has been planned for and\\napplied to many famous space missions. The development of on-board electronics\\nsystems has enabled the use of vision-based and LiDAR-based methods to achieve\\nbetter performances. Meanwhile, deep learning has reached great success in\\ndifferent areas, especially in computer vision, which has also attracted the\\nattention of space researchers. However, spacecraft navigation differs from\\nground tasks due to high reliability requirements but lack of large datasets.\\nThis survey aims to systematically investigate the current deep learning-based\\nautonomous spacecraft relative navigation methods, focusing on concrete orbital\\napplications such as spacecraft rendezvous and landing on small bodies or the\\nMoon. The fundamental characteristics, primary motivations, and contributions\\nof deep learning-based relative navigation algorithms are first summarised from\\nthree perspectives of spacecraft rendezvous, asteroid exploration, and terrain\\nnavigation. Furthermore, popular visual tracking benchmarks and their\\nrespective properties are compared and summarised. Finally, potential\\napplications are discussed, along with expected impediments.',\n",
       "    'author': [{'name': 'Jianing Song'},\n",
       "     {'name': 'Duarte Rondao'},\n",
       "     {'name': 'Nabil Aouf'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.actaastro.2021.10.025'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.actaastro.2021.10.025',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2108.08876v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.08876v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '41 pages; 17 figures; Submitted to Acta Astronautica, under review'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.10', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.10367v1',\n",
       "    'updated': '2021-08-23T19:08:20Z',\n",
       "    'published': '2021-08-23T19:08:20Z',\n",
       "    'title': 'Marine vessel tracking using a monocular camera',\n",
       "    'summary': 'In this paper, a new technique for camera calibration using only GPS data is\\npresented. A new way of tracking objects that move on a plane in a video is\\nachieved by using the location and size of the bounding box to estimate the\\ndistance, achieving an average prediction error of 5.55m per 100m distance from\\nthe camera. This solution can be run in real-time at the edge, achieving\\nefficient inference in a low-powered IoT environment while also being able to\\ntrack multiple different vessels.',\n",
       "    'author': [{'name': 'Tobias Jacob'},\n",
       "     {'name': 'Raffaele Galliera'},\n",
       "     {'name': 'Muddasar Ali'},\n",
       "     {'name': 'Sikha Bagui'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.5220/0010516000170028'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.5220/0010516000170028',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2108.10367v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.10367v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 9 figures, the paper is based on the submission for the AI\\n  Tracks at Sea challenge made by the same team taking to a 3rd place in the\\n  competition, included in DeLTA 2021 conference proceedings, published on\\n  SCITEPRESS Digital Library and available at\\n  https://www.scitepress.org/PublicationsDetail.aspx?ID=yzZS+b/VkZ4=&t=1'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.12375v1',\n",
       "    'updated': '2021-08-27T16:16:01Z',\n",
       "    'published': '2021-08-27T16:16:01Z',\n",
       "    'title': 'A Pedestrian Detection and Tracking Framework for Autonomous Cars:\\n  Efficient Fusion of Camera and LiDAR Data',\n",
       "    'summary': 'This paper presents a novel method for pedestrian detection and tracking by\\nfusing camera and LiDAR sensor data. To deal with the challenges associated\\nwith the autonomous driving scenarios, an integrated tracking and detection\\nframework is proposed. The detection phase is performed by converting LiDAR\\nstreams to computationally tractable depth images, and then, a deep neural\\nnetwork is developed to identify pedestrian candidates both in RGB and depth\\nimages. To provide accurate information, the detection phase is further\\nenhanced by fusing multi-modal sensor information using the Kalman filter. The\\ntracking phase is a combination of the Kalman filter prediction and an optical\\nflow algorithm to track multiple pedestrians in a scene. We evaluate our\\nframework on a real public driving dataset. Experimental results demonstrate\\nthat the proposed method achieves significant performance improvement over a\\nbaseline method that solely uses image-based pedestrian detection.',\n",
       "    'author': [{'name': 'Muhammad Mobaidul Islam'},\n",
       "     {'name': 'Abdullah Al Redwan Newaz'},\n",
       "     {'name': 'Ali Karimoddini'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.12375v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.12375v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.02711v1',\n",
       "    'updated': '2021-09-06T19:44:50Z',\n",
       "    'published': '2021-09-06T19:44:50Z',\n",
       "    'title': 'Graph Attention Layer Evolves Semantic Segmentation for Road Pothole\\n  Detection: A Benchmark and Algorithms',\n",
       "    'summary': 'Existing road pothole detection approaches can be classified as computer\\nvision-based or machine learning-based. The former approaches typically employ\\n2-D image analysis/understanding or 3-D point cloud modeling and segmentation\\nalgorithms to detect road potholes from vision sensor data. The latter\\napproaches generally address road pothole detection using convolutional neural\\nnetworks (CNNs) in an end-to-end manner. However, road potholes are not\\nnecessarily ubiquitous and it is challenging to prepare a large well-annotated\\ndataset for CNN training. In this regard, while computer vision-based methods\\nwere the mainstream research trend in the past decade, machine learning-based\\nmethods were merely discussed. Recently, we published the first stereo\\nvision-based road pothole detection dataset and a novel disparity\\ntransformation algorithm, whereby the damaged and undamaged road areas can be\\nhighly distinguished. However, there are no benchmarks currently available for\\nstate-of-the-art (SoTA) CNNs trained using either disparity images or\\ntransformed disparity images. Therefore, in this paper, we first discuss the\\nSoTA CNNs designed for semantic segmentation and evaluate their performance for\\nroad pothole detection with extensive experiments. Additionally, inspired by\\ngraph neural network (GNN), we propose a novel CNN layer, referred to as graph\\nattention layer (GAL), which can be easily deployed in any existing CNN to\\noptimize image feature representations for semantic segmentation. Our\\nexperiments compare GAL-DeepLabv3+, our best-performing implementation, with\\nnine SoTA CNNs on three modalities of training data: RGB images, disparity\\nimages, and transformed disparity images. The experimental results suggest that\\nour proposed GAL-DeepLabv3+ achieves the best overall pothole detection\\naccuracy on all training data modalities.',\n",
       "    'author': [{'name': 'Rui Fan'},\n",
       "     {'name': 'Hengli Wang'},\n",
       "     {'name': 'Yuan Wang'},\n",
       "     {'name': 'Ming Liu'},\n",
       "     {'name': 'Ioannis Pitas'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TIP.2021.3112316'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TIP.2021.3112316',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2109.02711v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.02711v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'accepted as a regular paper to IEEE Transactions on Image Processing'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.03805v3',\n",
       "    'updated': '2021-12-23T19:16:51Z',\n",
       "    'published': '2021-09-08T17:45:37Z',\n",
       "    'title': 'Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic\\n  Segmentation and Tracking',\n",
       "    'summary': 'Panoptic scene understanding and tracking of dynamic agents are essential for\\nrobots and automated vehicles to navigate in urban environments. As LiDARs\\nprovide accurate illumination-independent geometric depictions of the scene,\\nperforming these tasks using LiDAR point clouds provides reliable predictions.\\nHowever, existing datasets lack diversity in the type of urban scenes and have\\na limited number of dynamic object instances which hinders both learning of\\nthese tasks as well as credible benchmarking of the developed methods. In this\\npaper, we introduce the large-scale Panoptic nuScenes benchmark dataset that\\nextends our popular nuScenes dataset with point-wise groundtruth annotations\\nfor semantic segmentation, panoptic segmentation, and panoptic tracking tasks.\\nTo facilitate comparison, we provide several strong baselines for each of these\\ntasks on our proposed dataset. Moreover, we analyze the drawbacks of the\\nexisting metrics for panoptic tracking and propose the novel instance-centric\\nPAT metric that addresses the concerns. We present exhaustive experiments that\\ndemonstrate the utility of Panoptic nuScenes compared to existing datasets and\\nmake the online evaluation server available at nuScenes.org. We believe that\\nthis extension will accelerate the research of novel methods for scene\\nunderstanding of dynamic urban environments.',\n",
       "    'author': [{'name': 'Whye Kit Fong'},\n",
       "     {'name': 'Rohit Mohan'},\n",
       "     {'name': 'Juana Valeria Hurtado'},\n",
       "     {'name': 'Lubing Zhou'},\n",
       "     {'name': 'Holger Caesar'},\n",
       "     {'name': 'Oscar Beijbom'},\n",
       "     {'name': 'Abhinav Valada'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The benchmark is available at https://www.nuscenes.org'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.03805v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.03805v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.04456v1',\n",
       "    'updated': '2021-09-09T17:55:28Z',\n",
       "    'published': '2021-09-09T17:55:28Z',\n",
       "    'title': 'NEAT: Neural Attention Fields for End-to-End Autonomous Driving',\n",
       "    'summary': \"Efficient reasoning about the semantic, spatial, and temporal structure of a\\nscene is a crucial prerequisite for autonomous driving. We present NEural\\nATtention fields (NEAT), a novel representation that enables such reasoning for\\nend-to-end imitation learning models. NEAT is a continuous function which maps\\nlocations in Bird's Eye View (BEV) scene coordinates to waypoints and\\nsemantics, using intermediate attention maps to iteratively compress\\nhigh-dimensional 2D image features into a compact representation. This allows\\nour model to selectively attend to relevant regions in the input while ignoring\\ninformation irrelevant to the driving task, effectively associating the images\\nwith the BEV representation. In a new evaluation setting involving adverse\\nenvironmental conditions and challenging scenarios, NEAT outperforms several\\nstrong baselines and achieves driving scores on par with the privileged CARLA\\nexpert used to generate its training data. Furthermore, visualizing the\\nattention maps for models with NEAT intermediate representations provides\\nimproved interpretability.\",\n",
       "    'author': [{'name': 'Kashyap Chitta'},\n",
       "     {'name': 'Aditya Prakash'},\n",
       "     {'name': 'Andreas Geiger'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICCV 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.04456v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.04456v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.07703v3',\n",
       "    'updated': '2022-04-29T06:11:42Z',\n",
       "    'published': '2021-09-16T03:53:52Z',\n",
       "    'title': 'ROS-X-Habitat: Bridging the ROS Ecosystem with Embodied AI',\n",
       "    'summary': 'We introduce ROS-X-Habitat, a software interface that bridges the AI Habitat\\nplatform for embodied learning-based agents with other robotics resources via\\nROS. This interface not only offers standardized communication protocols\\nbetween embodied agents and simulators, but also enables physically and\\nphotorealistic simulation that benefits the training and/or testing of\\nvision-based embodied agents. With this interface, roboticists can evaluate\\ntheir own Habitat RL agents in another ROS-based simulator or use Habitat Sim\\nv2 as the test bed for their own robotic algorithms. Through in silico\\nexperiments, we demonstrate that ROS-X-Habitat has minimal impact on the\\nnavigation performance and simulation speed of a Habitat RGBD agent; that a\\nstandard set of ROS mapping, planning and navigation tools can run in Habitat\\nSim v2; and that a Habitat agent can run in the standard ROS simulator Gazebo.',\n",
       "    'author': [{'name': 'Guanxiong Chen'},\n",
       "     {'name': 'Haoyu Yang'},\n",
       "     {'name': 'Ian M. Mitchell'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Camera-ready version submitted to Canadian Conference on Computer and\\n  Robot Vision (CRV) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.07703v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.07703v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.10322v1',\n",
       "    'updated': '2021-09-21T17:19:09Z',\n",
       "    'published': '2021-09-21T17:19:09Z',\n",
       "    'title': 'CondNet: Conditional Classifier for Scene Segmentation',\n",
       "    'summary': 'The fully convolutional network (FCN) has achieved tremendous success in\\ndense visual recognition tasks, such as scene segmentation. The last layer of\\nFCN is typically a global classifier (1x1 convolution) to recognize each pixel\\nto a semantic label. We empirically show that this global classifier, ignoring\\nthe intra-class distinction, may lead to sub-optimal results.\\n  In this work, we present a conditional classifier to replace the traditional\\nglobal classifier, where the kernels of the classifier are generated\\ndynamically conditioned on the input. The main advantages of the new classifier\\nconsist of: (i) it attends on the intra-class distinction, leading to stronger\\ndense recognition capability; (ii) the conditional classifier is simple and\\nflexible to be integrated into almost arbitrary FCN architectures to improve\\nthe prediction. Extensive experiments demonstrate that the proposed classifier\\nperforms favourably against the traditional classifier on the FCN architecture.\\nThe framework equipped with the conditional classifier (called CondNet)\\nachieves new state-of-the-art performances on two datasets. The code and models\\nare available at https://git.io/CondNet.',\n",
       "    'author': [{'name': 'Changqian Yu'},\n",
       "     {'name': 'Yuanjie Shao'},\n",
       "     {'name': 'Changxin Gao'},\n",
       "     {'name': 'Nong Sang'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LSP.2021.3070472'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LSP.2021.3070472',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2109.10322v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.10322v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE SPL. 4 pages, 3 figures, 4 tables'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.11644v1',\n",
       "    'updated': '2021-09-23T20:53:55Z',\n",
       "    'published': '2021-09-23T20:53:55Z',\n",
       "    'title': 'A Learned Stereo Depth System for Robotic Manipulation in Homes',\n",
       "    'summary': 'We present a passive stereo depth system that produces dense and accurate\\npoint clouds optimized for human environments, including dark, textureless,\\nthin, reflective and specular surfaces and objects, at 2560x2048 resolution,\\nwith 384 disparities, in 30 ms. The system consists of an algorithm combining\\nlearned stereo matching with engineered filtering, a training and data-mixing\\nmethodology, and a sensor hardware design. Our architecture is 15x faster than\\napproaches that perform similarly on the Middlebury and Flying Things Stereo\\nBenchmarks. To effectively supervise the training of this model, we combine\\nreal data labelled using off-the-shelf depth sensors, as well as a number of\\ndifferent rendered, simulated labeled datasets. We demonstrate the efficacy of\\nour system by presenting a large number of qualitative results in the form of\\ndepth maps and point-clouds, experiments validating the metric accuracy of our\\nsystem and comparisons to other sensors on challenging objects and scenes. We\\nalso show the competitiveness of our algorithm compared to state-of-the-art\\nlearned models using the Middlebury and FlyingThings datasets.',\n",
       "    'author': [{'name': 'Krishna Shankar'},\n",
       "     {'name': 'Mark Tjersland'},\n",
       "     {'name': 'Jeremy Ma'},\n",
       "     {'name': 'Kevin Stone'},\n",
       "     {'name': 'Max Bajracharya'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.11644v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.11644v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.12109v3',\n",
       "    'updated': '2022-04-13T04:09:03Z',\n",
       "    'published': '2021-09-27T02:52:46Z',\n",
       "    'title': 'Autonomy and Perception for Space Mining',\n",
       "    'summary': 'Future Moon bases will likely be constructed using resources mined from the\\nsurface of the Moon. The difficulty of maintaining a human workforce on the\\nMoon and communications lag with Earth means that mining will need to be\\nconducted using collaborative robots with a high degree of autonomy. In this\\npaper, we describe our solution for Phase 2 of the NASA Space Robotics\\nChallenge, which provided a simulated lunar environment in which teams were\\ntasked to develop software systems to achieve autonomous collaborative robots\\nfor mining on the Moon. Our 3rd place and innovation award winning solution\\nshows how machine learning-enabled vision could alleviate major challenges\\nposed by the lunar environment towards autonomous space mining, chiefly the\\nlack of satellite positioning systems, hazardous terrain, and delicate robot\\ninteractions. A robust multi-robot coordinator was also developed to achieve\\nlong-term operation and effective collaboration between robots.',\n",
       "    'author': [{'name': 'Ragav Sachdeva'},\n",
       "     {'name': 'Ravi Hammond'},\n",
       "     {'name': 'James Bockman'},\n",
       "     {'name': 'Alec Arthur'},\n",
       "     {'name': 'Brandon Smart'},\n",
       "     {'name': 'Dustin Craggs'},\n",
       "     {'name': 'Anh-Dzung Doan'},\n",
       "     {'name': 'Thomas Rowntree'},\n",
       "     {'name': 'Elijah Schutz'},\n",
       "     {'name': 'Adrian Orenstein'},\n",
       "     {'name': 'Andy Yu'},\n",
       "     {'name': 'Tat-Jun Chin'},\n",
       "     {'name': 'Ian Reid'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This paper describes our 3rd place and innovation award winning\\n  solution to the NASA Space Robotics Challenge Phase 2'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.12109v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.12109v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.13333v1',\n",
       "    'updated': '2021-09-27T20:19:18Z',\n",
       "    'published': '2021-09-27T20:19:18Z',\n",
       "    'title': 'Urban Driver: Learning to Drive from Real-world Demonstrations Using\\n  Policy Gradients',\n",
       "    'summary': 'In this work we are the first to present an offline policy gradient method\\nfor learning imitative policies for complex urban driving from a large corpus\\nof real-world demonstrations. This is achieved by building a differentiable\\ndata-driven simulator on top of perception outputs and high-fidelity HD maps of\\nthe area. It allows us to synthesize new driving experiences from existing\\ndemonstrations using mid-level representations. Using this simulator we then\\ntrain a policy network in closed-loop employing policy gradients. We train our\\nproposed method on 100 hours of expert demonstrations on urban roads and show\\nthat it learns complex driving policies that generalize well and can perform a\\nvariety of driving maneuvers. We demonstrate this in simulation as well as\\ndeploy our model to self-driving vehicles in the real-world. Our method\\noutperforms previously demonstrated state-of-the-art for urban driving\\nscenarios -- all this without the need for complex state perturbations or\\ncollecting additional on-policy data during training. We make code and data\\npublicly available.',\n",
       "    'author': [{'name': 'Oliver Scheel'},\n",
       "     {'name': 'Luca Bergamini'},\n",
       "     {'name': 'Maciej Wołczyk'},\n",
       "     {'name': 'Błażej Osiński'},\n",
       "     {'name': 'Peter Ondruska'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.13333v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.13333v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.13602v1',\n",
       "    'updated': '2021-09-28T10:23:46Z',\n",
       "    'published': '2021-09-28T10:23:46Z',\n",
       "    'title': 'SafetyNet: Safe planning for real-world self-driving vehicles using\\n  machine-learned policies',\n",
       "    'summary': \"In this paper we present the first safe system for full control of\\nself-driving vehicles trained from human demonstrations and deployed in\\nchallenging, real-world, urban environments. Current industry-standard\\nsolutions use rule-based systems for planning. Although they perform reasonably\\nwell in common scenarios, the engineering complexity renders this approach\\nincompatible with human-level performance. On the other hand, the performance\\nof machine-learned (ML) planning solutions can be improved by simply adding\\nmore exemplar data. However, ML methods cannot offer safety guarantees and\\nsometimes behave unpredictably. To combat this, our approach uses a simple yet\\neffective rule-based fallback layer that performs sanity checks on an ML\\nplanner's decisions (e.g. avoiding collision, assuring physical feasibility).\\nThis allows us to leverage ML to handle complex situations while still assuring\\nthe safety, reducing ML planner-only collisions by 95%. We train our ML planner\\non 300 hours of expert driving demonstrations using imitation learning and\\ndeploy it along with the fallback layer in downtown San Francisco, where it\\ntakes complete control of a real vehicle and navigates a wide variety of\\nchallenging urban driving scenarios.\",\n",
       "    'author': [{'name': 'Matt Vitelli'},\n",
       "     {'name': 'Yan Chang'},\n",
       "     {'name': 'Yawei Ye'},\n",
       "     {'name': 'Maciej Wołczyk'},\n",
       "     {'name': 'Błażej Osiński'},\n",
       "     {'name': 'Moritz Niendorf'},\n",
       "     {'name': 'Hugo Grimmett'},\n",
       "     {'name': 'Qiangui Huang'},\n",
       "     {'name': 'Ashesh Jain'},\n",
       "     {'name': 'Peter Ondruska'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.13602v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.13602v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.00087v1',\n",
       "    'updated': '2021-09-30T21:09:09Z',\n",
       "    'published': '2021-09-30T21:09:09Z',\n",
       "    'title': 'Seeing Glass: Joint Point Cloud and Depth Completion for Transparent\\n  Objects',\n",
       "    'summary': 'The basis of many object manipulation algorithms is RGB-D input. Yet,\\ncommodity RGB-D sensors can only provide distorted depth maps for a wide range\\nof transparent objects due light refraction and absorption. To tackle the\\nperception challenges posed by transparent objects, we propose TranspareNet, a\\njoint point cloud and depth completion method, with the ability to complete the\\ndepth of transparent objects in cluttered and complex scenes, even with\\npartially filled fluid contents within the vessels. To address the shortcomings\\nof existing transparent object data collection schemes in literature, we also\\npropose an automated dataset creation workflow that consists of\\nrobot-controlled image collection and vision-based automatic annotation.\\nThrough this automated workflow, we created Toronto Transparent Objects Depth\\nDataset (TODD), which consists of nearly 15000 RGB-D images. Our experimental\\nevaluation demonstrates that TranspareNet outperforms existing state-of-the-art\\ndepth completion methods on multiple datasets, including ClearGrasp, and that\\nit also handles cluttered scenes when trained on TODD. Code and dataset will be\\nreleased at https://www.pair.toronto.edu/TranspareNet/',\n",
       "    'author': [{'name': 'Haoping Xu'},\n",
       "     {'name': 'Yi Ru Wang'},\n",
       "     {'name': 'Sagi Eppel'},\n",
       "     {'name': 'Alàn Aspuru-Guzik'},\n",
       "     {'name': 'Florian Shkurti'},\n",
       "     {'name': 'Animesh Garg'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for Oral at Conference on Robot Learning (CoRL) 2021;\\n  Haoping Xu and Yi Ru Wang contributed equally; 8 pages, 6 figures, 3 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.00087v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.00087v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.00784v1',\n",
       "    'updated': '2021-10-02T11:15:04Z',\n",
       "    'published': '2021-10-02T11:15:04Z',\n",
       "    'title': 'Seeking Visual Discomfort: Curiosity-driven Representations for\\n  Reinforcement Learning',\n",
       "    'summary': 'Vision-based reinforcement learning (RL) is a promising approach to solve\\ncontrol tasks involving images as the main observation. State-of-the-art RL\\nalgorithms still struggle in terms of sample efficiency, especially when using\\nimage observations. This has led to increased attention on integrating state\\nrepresentation learning (SRL) techniques into the RL pipeline. Work in this\\nfield demonstrates a substantial improvement in sample efficiency among other\\nbenefits. However, to take full advantage of this paradigm, the quality of\\nsamples used for training plays a crucial role. More importantly, the diversity\\nof these samples could affect the sample efficiency of vision-based RL, but\\nalso its generalization capability. In this work, we present an approach to\\nimprove sample diversity for state representation learning. Our method enhances\\nthe exploration capability of RL algorithms, by taking advantage of the SRL\\nsetup. Our experiments show that our proposed approach boosts the visitation of\\nproblematic states, improves the learned state representation, and outperforms\\nthe baselines for all tested environments. These results are most apparent for\\nenvironments where the baseline methods struggle. Even in simple environments,\\nour method stabilizes the training, reduces the reward variance, and promotes\\nsample efficiency.',\n",
       "    'author': [{'name': 'Elie Aljalbout'},\n",
       "     {'name': 'Maximilian Ulmer'},\n",
       "     {'name': 'Rudolph Triebel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: substantial text overlap with arXiv:2109.13588'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.00784v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.00784v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.6; I.2.8; I.2.9; I.2.10; I.5.0',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.00804v2',\n",
       "    'updated': '2021-10-16T02:14:06Z',\n",
       "    'published': '2021-10-02T13:46:32Z',\n",
       "    'title': 'ProTo: Program-Guided Transformer for Program-Guided Tasks',\n",
       "    'summary': 'Programs, consisting of semantic and structural information, play an\\nimportant role in the communication between humans and agents. Towards learning\\ngeneral program executors to unify perception, reasoning, and decision making,\\nwe formulate program-guided tasks which require learning to execute a given\\nprogram on the observed task specification. Furthermore, we propose the\\nProgram-guided Transformer (ProTo), which integrates both semantic and\\nstructural guidance of a program by leveraging cross-attention and masked\\nself-attention to pass messages between the specification and routines in the\\nprogram. ProTo executes a program in a learned latent space and enjoys stronger\\nrepresentation ability than previous neural-symbolic approaches. We demonstrate\\nthat ProTo significantly outperforms the previous state-of-the-art methods on\\nGQA visual reasoning and 2D Minecraft policy learning datasets. Additionally,\\nProTo demonstrates better generalization to unseen, complex, and human-written\\nprograms.',\n",
       "    'author': [{'name': 'Zelin Zhao'},\n",
       "     {'name': 'Karan Samel'},\n",
       "     {'name': 'Binghong Chen'},\n",
       "     {'name': 'Le Song'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted in NeurIPS 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.00804v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.00804v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.02739v2',\n",
       "    'updated': '2021-11-04T18:10:36Z',\n",
       "    'published': '2021-09-28T13:50:21Z',\n",
       "    'title': 'A Step Towards Efficient Evaluation of Complex Perception Tasks in\\n  Simulation',\n",
       "    'summary': 'There has been increasing interest in characterising the error behaviour of\\nsystems which contain deep learning models before deploying them into any\\nsafety-critical scenario. However, characterising such behaviour usually\\nrequires large-scale testing of the model that can be extremely computationally\\nexpensive for complex real-world tasks. For example, tasks involving compute\\nintensive object detectors as one of their components. In this work, we propose\\nan approach that enables efficient large-scale testing using simplified\\nlow-fidelity simulators and without the computational cost of executing\\nexpensive deep learning models. Our approach relies on designing an efficient\\nsurrogate model corresponding to the compute intensive components of the task\\nunder test. We demonstrate the efficacy of our methodology by evaluating the\\nperformance of an autonomous driving task in the Carla simulator with reduced\\ncomputational expense by training efficient surrogate models for PIXOR and\\nCenterPoint LiDAR detectors, whilst demonstrating that the accuracy of the\\nsimulation is maintained.',\n",
       "    'author': [{'name': 'Jonathan Sadeghi'},\n",
       "     {'name': 'Blaine Rogers'},\n",
       "     {'name': 'James Gunn'},\n",
       "     {'name': 'Thomas Saunders'},\n",
       "     {'name': 'Sina Samangooei'},\n",
       "     {'name': 'Puneet Kumar Dokania'},\n",
       "     {'name': 'John Redford'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in NeurIPS 2021 Workshop on Machine Learning for Autonomous\\n  Driving (ML4AD)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.02739v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.02739v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.06558v1',\n",
       "    'updated': '2021-10-13T08:15:08Z',\n",
       "    'published': '2021-10-13T08:15:08Z',\n",
       "    'title': 'LENS: Localization enhanced by NeRF synthesis',\n",
       "    'summary': 'Neural Radiance Fields (NeRF) have recently demonstrated photo-realistic\\nresults for the task of novel view synthesis. In this paper, we propose to\\napply novel view synthesis to the robot relocalization problem: we demonstrate\\nimprovement of camera pose regression thanks to an additional synthetic dataset\\nrendered by the NeRF class of algorithm. To avoid spawning novel views in\\nirrelevant places we selected virtual camera locations from NeRF internal\\nrepresentation of the 3D geometry of the scene. We further improved\\nlocalization accuracy of pose regressors using synthesized realistic and\\ngeometry consistent images as data augmentation during training. At the time of\\npublication, our approach improved state of the art with a 60% lower error on\\nCambridge Landmarks and 7-scenes datasets. Hence, the resulting accuracy\\nbecomes comparable to structure-based methods, without any architecture\\nmodification or domain adaptation constraints. Since our method allows almost\\ninfinite generation of training data, we investigated limitations of camera\\npose regression depending on size and distribution of data used for training on\\npublic benchmarks. We concluded that pose regression accuracy is mostly bounded\\nby relatively small and biased datasets rather than capacity of the pose\\nregression model to solve the localization task.',\n",
       "    'author': [{'name': 'Arthur Moreau'},\n",
       "     {'name': 'Nathan Piasco'},\n",
       "     {'name': 'Dzmitry Tsishkou'},\n",
       "     {'name': 'Bogdan Stanciulescu'},\n",
       "     {'name': 'Arnaud de La Fortelle'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at CoRL 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.06558v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.06558v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.06922v1',\n",
       "    'updated': '2021-10-13T17:59:35Z',\n",
       "    'published': '2021-10-13T17:59:35Z',\n",
       "    'title': 'DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries',\n",
       "    'summary': 'We introduce a framework for multi-camera 3D object detection. In contrast to\\nexisting works, which estimate 3D bounding boxes directly from monocular images\\nor use depth prediction networks to generate input for 3D object detection from\\n2D information, our method manipulates predictions directly in 3D space. Our\\narchitecture extracts 2D features from multiple camera images and then uses a\\nsparse set of 3D object queries to index into these 2D features, linking 3D\\npositions to multi-view images using camera transformation matrices. Finally,\\nour model makes a bounding box prediction per object query, using a set-to-set\\nloss to measure the discrepancy between the ground-truth and the prediction.\\nThis top-down approach outperforms its bottom-up counterpart in which object\\nbounding box prediction follows per-pixel depth estimation, since it does not\\nsuffer from the compounding error introduced by a depth prediction model.\\nMoreover, our method does not require post-processing such as non-maximum\\nsuppression, dramatically improving inference speed. We achieve\\nstate-of-the-art performance on the nuScenes autonomous driving benchmark.',\n",
       "    'author': [{'name': 'Yue Wang'},\n",
       "     {'name': 'Vitor Guizilini'},\n",
       "     {'name': 'Tianyuan Zhang'},\n",
       "     {'name': 'Yilun Wang'},\n",
       "     {'name': 'Hang Zhao'},\n",
       "     {'name': 'Justin Solomon'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to CORL 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.06922v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.06922v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.06923v1',\n",
       "    'updated': '2021-10-13T17:59:38Z',\n",
       "    'published': '2021-10-13T17:59:38Z',\n",
       "    'title': 'Object DGCNN: 3D Object Detection using Dynamic Graphs',\n",
       "    'summary': '3D object detection often involves complicated training and testing\\npipelines, which require substantial domain knowledge about individual\\ndatasets. Inspired by recent non-maximum suppression-free 2D object detection\\nmodels, we propose a 3D object detection architecture on point clouds. Our\\nmethod models 3D object detection as message passing on a dynamic graph,\\ngeneralizing the DGCNN framework to predict a set of objects. In our\\nconstruction, we remove the necessity of post-processing via object confidence\\naggregation or non-maximum suppression. To facilitate object detection from\\nsparse point clouds, we also propose a set-to-set distillation approach\\ncustomized to 3D detection. This approach aligns the outputs of the teacher\\nmodel and the student model in a permutation-invariant fashion, significantly\\nsimplifying knowledge distillation for the 3D detection task. Our method\\nachieves state-of-the-art performance on autonomous driving benchmarks. We also\\nprovide abundant analysis of the detection model and distillation framework.',\n",
       "    'author': [{'name': 'Yue Wang'}, {'name': 'Justin Solomon'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to NeurIPS 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.06923v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.06923v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.11599v1',\n",
       "    'updated': '2021-10-22T05:27:24Z',\n",
       "    'published': '2021-10-22T05:27:24Z',\n",
       "    'title': 'High Fidelity 3D Reconstructions with Limited Physical Views',\n",
       "    'summary': 'Multi-view triangulation is the gold standard for 3D reconstruction from 2D\\ncorrespondences given known calibration and sufficient views. However in\\npractice, expensive multi-view setups -- involving tens sometimes hundreds of\\ncameras -- are required in order to obtain the high fidelity 3D reconstructions\\nnecessary for many modern applications. In this paper we present a novel\\napproach that leverages recent advances in 2D-3D lifting using neural shape\\npriors while also enforcing multi-view equivariance. We show how our method can\\nachieve comparable fidelity to expensive calibrated multi-view rigs using a\\nlimited (2-3) number of uncalibrated camera views.',\n",
       "    'author': [{'name': 'Mosam Dabhi'},\n",
       "     {'name': 'Chaoyang Wang'},\n",
       "     {'name': 'Kunal Saluja'},\n",
       "     {'name': 'Laszlo Jeni'},\n",
       "     {'name': 'Ian Fasel'},\n",
       "     {'name': 'Simon Lucey'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/3DV53792.2021.00137'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/3DV53792.2021.00137',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2110.11599v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.11599v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to 3DV 2021 (project page & code:\\n  https://sites.google.com/view/high-fidelity-3d-neural-prior)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.15360v1',\n",
       "    'updated': '2021-10-28T17:59:30Z',\n",
       "    'published': '2021-10-28T17:59:30Z',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action\\n  Primitives',\n",
       "    'summary': 'Despite the potential of reinforcement learning (RL) for building\\ngeneral-purpose robotic systems, training RL agents to solve robotics tasks\\nstill remains challenging due to the difficulty of exploration in purely\\ncontinuous action spaces. Addressing this problem is an active area of research\\nwith the majority of focus on improving RL methods via better optimization or\\nmore efficient exploration. An alternate but important component to consider\\nimproving is the interface of the RL algorithm with the robot. In this work, we\\nmanually specify a library of robot action primitives (RAPS), parameterized\\nwith arguments that are learned by an RL policy. These parameterized primitives\\nare expressive, simple to implement, enable efficient exploration and can be\\ntransferred across robots, tasks and environments. We perform a thorough\\nempirical study across challenging tasks in three distinct domains with image\\ninput and a sparse terminal reward. We find that our simple change to the\\naction interface substantially improves both the learning efficiency and task\\nperformance irrespective of the underlying RL algorithm, significantly\\noutperforming prior methods which learn skills from offline expert data. Code\\nand videos at https://mihdalal.github.io/raps/',\n",
       "    'author': [{'name': 'Murtaza Dalal'},\n",
       "     {'name': 'Deepak Pathak'},\n",
       "     {'name': 'Ruslan Salakhutdinov'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at NeurIPS 2021. Website at\\n  https://mihdalal.github.io/raps/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.15360v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.15360v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.00210v2',\n",
       "    'updated': '2021-12-12T04:24:08Z',\n",
       "    'published': '2021-10-30T09:13:39Z',\n",
       "    'title': 'Mastering Atari Games with Limited Data',\n",
       "    'summary': \"Reinforcement learning has achieved great success in many applications.\\nHowever, sample efficiency remains a key challenge, with prominent methods\\nrequiring millions (or even billions) of environment steps to train. Recently,\\nthere has been significant progress in sample efficient image-based RL\\nalgorithms; however, consistent human-level performance on the Atari game\\nbenchmark remains an elusive goal. We propose a sample efficient model-based\\nvisual RL algorithm built on MuZero, which we name EfficientZero. Our method\\nachieves 194.3% mean human performance and 109.0% median performance on the\\nAtari 100k benchmark with only two hours of real-time game experience and\\noutperforms the state SAC in some tasks on the DMControl 100k benchmark. This\\nis the first time an algorithm achieves super-human performance on Atari games\\nwith such little data. EfficientZero's performance is also close to DQN's\\nperformance at 200 million frames while we consume 500 times less data.\\nEfficientZero's low sample complexity and high performance can bring RL closer\\nto real-world applicability. We implement our algorithm in an\\neasy-to-understand manner and it is available at\\nhttps://github.com/YeWR/EfficientZero. We hope it will accelerate the research\\nof MCTS-based RL algorithms in the wider community.\",\n",
       "    'author': [{'name': 'Weirui Ye'},\n",
       "     {'name': 'Shaohuai Liu'},\n",
       "     {'name': 'Thanard Kurutach'},\n",
       "     {'name': 'Pieter Abbeel'},\n",
       "     {'name': 'Yang Gao'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at NeurIPS 2021; Homepage:\\n  https://yewr.github.io/projects/efficientzero/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.00210v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.00210v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.09858v1',\n",
       "    'updated': '2021-11-18T18:36:05Z',\n",
       "    'published': '2021-11-18T18:36:05Z',\n",
       "    'title': 'Successor Feature Landmarks for Long-Horizon Goal-Conditioned\\n  Reinforcement Learning',\n",
       "    'summary': 'Operating in the real-world often requires agents to learn about a complex\\nenvironment and apply this understanding to achieve a breadth of goals. This\\nproblem, known as goal-conditioned reinforcement learning (GCRL), becomes\\nespecially challenging for long-horizon goals. Current methods have tackled\\nthis problem by augmenting goal-conditioned policies with graph-based planning\\nalgorithms. However, they struggle to scale to large, high-dimensional state\\nspaces and assume access to exploration mechanisms for efficiently collecting\\ntraining data. In this work, we introduce Successor Feature Landmarks (SFL), a\\nframework for exploring large, high-dimensional environments so as to obtain a\\npolicy that is proficient for any goal. SFL leverages the ability of successor\\nfeatures (SF) to capture transition dynamics, using it to drive exploration by\\nestimating state-novelty and to enable high-level planning by abstracting the\\nstate-space as a non-parametric landmark-based graph. We further exploit SF to\\ndirectly compute a goal-conditioned policy for inter-landmark traversal, which\\nwe use to execute plans to \"frontier\" landmarks at the edge of the explored\\nstate space. We show in our experiments on MiniGrid and ViZDoom that SFL\\nenables efficient exploration of large, high-dimensional state spaces and\\noutperforms state-of-the-art baselines on long-horizon GCRL tasks.',\n",
       "    'author': [{'name': 'Christopher Hoang'},\n",
       "     {'name': 'Sungryull Sohn'},\n",
       "     {'name': 'Jongwook Choi'},\n",
       "     {'name': 'Wilka Carvalho'},\n",
       "     {'name': 'Honglak Lee'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2021. Video and code at https://2016choang.github.io/sfl'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.09858v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.09858v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.13681v3',\n",
       "    'updated': '2022-07-20T10:26:06Z',\n",
       "    'published': '2021-11-26T18:59:58Z',\n",
       "    'title': 'ManiFest: Manifold Deformation for Few-shot Image Translation',\n",
       "    'summary': 'Most image-to-image translation methods require a large number of training\\nimages, which restricts their applicability. We instead propose ManiFest: a\\nframework for few-shot image translation that learns a context-aware\\nrepresentation of a target domain from a few images only. To enforce feature\\nconsistency, our framework learns a style manifold between source and proxy\\nanchor domains (assumed to be composed of large numbers of images). The learned\\nmanifold is interpolated and deformed towards the few-shot target domain via\\npatch-based adversarial and feature statistics alignment losses. All of these\\ncomponents are trained simultaneously during a single end-to-end loop. In\\naddition to the general few-shot translation task, our approach can\\nalternatively be conditioned on a single exemplar image to reproduce its\\nspecific style. Extensive experiments demonstrate the efficacy of ManiFest on\\nmultiple tasks, outperforming the state-of-the-art on all metrics and in both\\nthe general- and exemplar-based scenarios. Our code is available at\\nhttps://github.com/cv-rits/Manifest .',\n",
       "    'author': [{'name': 'Fabio Pizzati'},\n",
       "     {'name': 'Jean-François Lalonde'},\n",
       "     {'name': 'Raoul de Charette'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ECCV 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.13681v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.13681v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.14693v3',\n",
       "    'updated': '2022-03-02T06:22:59Z',\n",
       "    'published': '2021-11-29T16:53:49Z',\n",
       "    'title': 'SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional,\\n  and Incremental Robot Learning',\n",
       "    'summary': \"Building general-purpose robots to perform a diverse range of tasks in a\\nlarge variety of environments in the physical world at the human level is\\nextremely challenging. It requires the robot learning to be sample-efficient,\\ngeneralizable, compositional, and incremental. In this work, we introduce a\\nsystematic learning framework called SAGCI-system towards achieving these above\\nfour requirements. Our system first takes the raw point clouds gathered by the\\ncamera mounted on the robot's wrist as the inputs and produces initial modeling\\nof the surrounding environment represented as a file of Unified Robot\\nDescription Format (URDF). Our system adopts a learning-augmented\\ndifferentiable simulation that loads the URDF. The robot then utilizes the\\ninteractive perception to interact with the environment to online verify and\\nmodify the URDF. Leveraging the differentiable simulation, we propose a\\nmodel-based learning algorithm combining object-centric and robot-centric\\nstages to efficiently produce policies to accomplish manipulation tasks. We\\napply our system to perform articulated object manipulation tasks, both in the\\nsimulation and the real world. Extensive experiments demonstrate the\\neffectiveness of our proposed learning framework. Supplemental materials and\\nvideos are available on https://sites.google.com/view/egci.\",\n",
       "    'author': [{'name': 'Jun Lv'},\n",
       "     {'name': 'Qiaojun Yu'},\n",
       "     {'name': 'Lin Shao'},\n",
       "     {'name': 'Wenhai Liu'},\n",
       "     {'name': 'Wenqiang Xu'},\n",
       "     {'name': 'Cewu Lu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE International Conference on Robotics and Automation\\n  (ICRA) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.14693v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.14693v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.14973v3',\n",
       "    'updated': '2021-12-22T04:39:40Z',\n",
       "    'published': '2021-11-29T21:36:53Z',\n",
       "    'title': 'MultiPath++: Efficient Information Fusion and Trajectory Aggregation for\\n  Behavior Prediction',\n",
       "    'summary': 'Predicting the future behavior of road users is one of the most challenging\\nand important problems in autonomous driving. Applying deep learning to this\\nproblem requires fusing heterogeneous world state in the form of rich\\nperception signals and map information, and inferring highly multi-modal\\ndistributions over possible futures. In this paper, we present MultiPath++, a\\nfuture prediction model that achieves state-of-the-art performance on popular\\nbenchmarks. MultiPath++ improves the MultiPath architecture by revisiting many\\ndesign choices. The first key design difference is a departure from dense\\nimage-based encoding of the input world state in favor of a sparse encoding of\\nheterogeneous scene elements: MultiPath++ consumes compact and efficient\\npolylines to describe road features, and raw agent state information directly\\n(e.g., position, velocity, acceleration). We propose a context-aware fusion of\\nthese elements and develop a reusable multi-context gating fusion component.\\nSecond, we reconsider the choice of pre-defined, static anchors, and develop a\\nway to learn latent anchor embeddings end-to-end in the model. Lastly, we\\nexplore ensembling and output aggregation techniques -- common in other ML\\ndomains -- and find effective variants for our probabilistic multimodal output\\nrepresentation. We perform an extensive ablation on these design choices, and\\nshow that our proposed model achieves state-of-the-art performance on the\\nArgoverse Motion Forecasting Competition and the Waymo Open Dataset Motion\\nPrediction Challenge.',\n",
       "    'author': [{'name': 'Balakrishnan Varadarajan'},\n",
       "     {'name': 'Ahmed Hefny'},\n",
       "     {'name': 'Avikalp Srivastava'},\n",
       "     {'name': 'Khaled S. Refaat'},\n",
       "     {'name': 'Nigamaa Nayakanti'},\n",
       "     {'name': 'Andre Cornman'},\n",
       "     {'name': 'Kan Chen'},\n",
       "     {'name': 'Bertrand Douillard'},\n",
       "     {'name': 'Chi Pang Lam'},\n",
       "     {'name': 'Dragomir Anguelov'},\n",
       "     {'name': 'Benjamin Sapp'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.14973v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.14973v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.01001v1',\n",
       "    'updated': '2021-12-02T06:26:38Z',\n",
       "    'published': '2021-12-02T06:26:38Z',\n",
       "    'title': 'SEAL: Self-supervised Embodied Active Learning using Exploration and 3D\\n  Consistency',\n",
       "    'summary': 'In this paper, we explore how we can build upon the data and models of\\nInternet images and use them to adapt to robot vision without requiring any\\nextra labels. We present a framework called Self-supervised Embodied Active\\nLearning (SEAL). It utilizes perception models trained on internet images to\\nlearn an active exploration policy. The observations gathered by this\\nexploration policy are labelled using 3D consistency and used to improve the\\nperception model. We build and utilize 3D semantic maps to learn both action\\nand perception in a completely self-supervised manner. The semantic map is used\\nto compute an intrinsic motivation reward for training the exploration policy\\nand for labelling the agent observations using spatio-temporal 3D consistency\\nand label propagation. We demonstrate that the SEAL framework can be used to\\nclose the action-perception loop: it improves object detection and instance\\nsegmentation performance of a pretrained perception model by just moving around\\nin training environments and the improved perception model can be used to\\nimprove Object Goal Navigation.',\n",
       "    'author': [{'name': 'Devendra Singh Chaplot'},\n",
       "     {'name': 'Murtaza Dalal'},\n",
       "     {'name': 'Saurabh Gupta'},\n",
       "     {'name': 'Jitendra Malik'},\n",
       "     {'name': 'Ruslan Salakhutdinov'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at NeurIPS 2021. See project webpage at\\n  https://devendrachaplot.github.io/projects/seal'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.01001v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.01001v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.01010v1',\n",
       "    'updated': '2021-12-02T06:48:16Z',\n",
       "    'published': '2021-12-02T06:48:16Z',\n",
       "    'title': 'Differentiable Spatial Planning using Transformers',\n",
       "    'summary': 'We consider the problem of spatial path planning. In contrast to the\\nclassical solutions which optimize a new plan from scratch and assume access to\\nthe full map with ground truth obstacle locations, we learn a planner from the\\ndata in a differentiable manner that allows us to leverage statistical\\nregularities from past data. We propose Spatial Planning Transformers (SPT),\\nwhich given an obstacle map learns to generate actions by planning over\\nlong-range spatial dependencies, unlike prior data-driven planners that\\npropagate information locally via convolutional structure in an iterative\\nmanner. In the setting where the ground truth map is not known to the agent, we\\nleverage pre-trained SPTs in an end-to-end framework that has the structure of\\nmapper and planner built into it which allows seamless generalization to\\nout-of-distribution maps and goals. SPTs outperform prior state-of-the-art\\ndifferentiable planners across all the setups for both manipulation and\\nnavigation tasks, leading to an absolute improvement of 7-19%.',\n",
       "    'author': [{'name': 'Devendra Singh Chaplot'},\n",
       "     {'name': 'Deepak Pathak'},\n",
       "     {'name': 'Jitendra Malik'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published at ICML 2021. See project webpage at\\n  https://devendrachaplot.github.io/projects/spatial-planning-transformers'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.01010v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.01010v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.01511v2',\n",
       "    'updated': '2021-12-06T16:46:37Z',\n",
       "    'published': '2021-12-02T18:58:09Z',\n",
       "    'title': 'The Surprising Effectiveness of Representation Learning for Visual\\n  Imitation',\n",
       "    'summary': 'While visual imitation learning offers one of the most effective ways of\\nlearning from visual demonstrations, generalizing from them requires either\\nhundreds of diverse demonstrations, task specific priors, or large,\\nhard-to-train parametric models. One reason such complexities arise is because\\nstandard visual imitation frameworks try to solve two coupled problems at once:\\nlearning a succinct but good representation from the diverse visual data, while\\nsimultaneously learning to associate the demonstrated actions with such\\nrepresentations. Such joint learning causes an interdependence between these\\ntwo problems, which often results in needing large amounts of demonstrations\\nfor learning. To address this challenge, we instead propose to decouple\\nrepresentation learning from behavior learning for visual imitation. First, we\\nlearn a visual representation encoder from offline data using standard\\nsupervised and self-supervised learning methods. Once the representations are\\ntrained, we use non-parametric Locally Weighted Regression to predict the\\nactions. We experimentally show that this simple decoupling improves the\\nperformance of visual imitation models on both offline demonstration datasets\\nand real-robot door opening compared to prior work in visual imitation. All of\\nour generated data, code, and robot videos are publicly available at\\nhttps://jyopari.github.io/VINN/.',\n",
       "    'author': [{'name': 'Jyothish Pari'},\n",
       "     {'name': 'Nur Muhammad Shafiullah'},\n",
       "     {'name': 'Sridhar Pandian Arunachalam'},\n",
       "     {'name': 'Lerrel Pinto'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The first two authors contributed equally'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.01511v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.01511v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.02094v2',\n",
       "    'updated': '2022-07-24T06:10:34Z',\n",
       "    'published': '2021-12-03T18:59:59Z',\n",
       "    'title': 'Coupling Vision and Proprioception for Navigation of Legged Robots',\n",
       "    'summary': 'We exploit the complementary strengths of vision and proprioception to\\ndevelop a point-goal navigation system for legged robots, called VP-Nav. Legged\\nsystems are capable of traversing more complex terrain than wheeled robots, but\\nto fully utilize this capability, we need a high-level path planner in the\\nnavigation system to be aware of the walking capabilities of the low-level\\nlocomotion policy in varying environments. We achieve this by using\\nproprioceptive feedback to ensure the safety of the planned path by sensing\\nunexpected obstacles like glass walls, terrain properties like slipperiness or\\nsoftness of the ground and robot properties like extra payload that are likely\\nmissed by vision. The navigation system uses onboard cameras to generate an\\noccupancy map and a corresponding cost map to reach the goal. A fast marching\\nplanner then generates a target path. A velocity command generator takes this\\nas input to generate the desired velocity for the walking policy. A safety\\nadvisor module adds sensed unexpected obstacles to the occupancy map and\\nenvironment-determined speed limits to the velocity command generator. We show\\nsuperior performance compared to wheeled robot baselines, and ablation studies\\nwhich have disjoint high-level planning and low-level control. We also show the\\nreal-world deployment of VP-Nav on a quadruped robot with onboard sensors and\\ncomputation. Videos at https://navigation-locomotion.github.io',\n",
       "    'author': [{'name': 'Zipeng Fu'},\n",
       "     {'name': 'Ashish Kumar'},\n",
       "     {'name': 'Ananye Agarwal'},\n",
       "     {'name': 'Haozhi Qi'},\n",
       "     {'name': 'Jitendra Malik'},\n",
       "     {'name': 'Deepak Pathak'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2022 final version. Website at\\n  https://navigation-locomotion.github.io'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.02094v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.02094v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.02205v1',\n",
       "    'updated': '2021-12-04T00:48:30Z',\n",
       "    'published': '2021-12-04T00:48:30Z',\n",
       "    'title': 'Behind the Curtain: Learning Occluded Shapes for 3D Object Detection',\n",
       "    'summary': 'Advances in LiDAR sensors provide rich 3D data that supports 3D scene\\nunderstanding. However, due to occlusion and signal miss, LiDAR point clouds\\nare in practice 2.5D as they cover only partial underlying shapes, which poses\\na fundamental challenge to 3D perception. To tackle the challenge, we present a\\nnovel LiDAR-based 3D object detection model, dubbed Behind the Curtain Detector\\n(BtcDet), which learns the object shape priors and estimates the complete\\nobject shapes that are partially occluded (curtained) in point clouds. BtcDet\\nfirst identifies the regions that are affected by occlusion and signal miss. In\\nthese regions, our model predicts the probability of occupancy that indicates\\nif a region contains object shapes. Integrated with this probability map,\\nBtcDet can generate high-quality 3D proposals. Finally, the probability of\\noccupancy is also integrated into a proposal refinement module to generate the\\nfinal bounding boxes. Extensive experiments on the KITTI Dataset and the Waymo\\nOpen Dataset demonstrate the effectiveness of BtcDet. Particularly, for the 3D\\ndetection of both cars and cyclists on the KITTI benchmark, BtcDet surpasses\\nall of the published state-of-the-art methods by remarkable margins. Code is\\nreleased\\n(https://github.com/Xharlie/BtcDet}{https://github.com/Xharlie/BtcDet).',\n",
       "    'author': [{'name': 'Qiangeng Xu'},\n",
       "     {'name': 'Yiqi Zhong'},\n",
       "     {'name': 'Ulrich Neumann'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'AAAI2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.02205v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.02205v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.05124v1',\n",
       "    'updated': '2021-12-09T18:57:15Z',\n",
       "    'published': '2021-12-09T18:57:15Z',\n",
       "    'title': 'Neural Descriptor Fields: SE(3)-Equivariant Object Representations for\\n  Manipulation',\n",
       "    'summary': 'We present Neural Descriptor Fields (NDFs), an object representation that\\nencodes both points and relative poses between an object and a target (such as\\na robot gripper or a rack used for hanging) via category-level descriptors. We\\nemploy this representation for object manipulation, where given a task\\ndemonstration, we want to repeat the same task on a new object instance from\\nthe same category. We propose to achieve this objective by searching (via\\noptimization) for the pose whose descriptor matches that observed in the\\ndemonstration. NDFs are conveniently trained in a self-supervised fashion via a\\n3D auto-encoding task that does not rely on expert-labeled keypoints. Further,\\nNDFs are SE(3)-equivariant, guaranteeing performance that generalizes across\\nall possible 3D object translations and rotations. We demonstrate learning of\\nmanipulation tasks from few (5-10) demonstrations both in simulation and on a\\nreal robot. Our performance generalizes across both object instances and 6-DoF\\nobject poses, and significantly outperforms a recent baseline that relies on 2D\\ndescriptors. Project website: https://yilundu.github.io/ndf/.',\n",
       "    'author': [{'name': 'Anthony Simeonov'},\n",
       "     {'name': 'Yilun Du'},\n",
       "     {'name': 'Andrea Tagliasacchi'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Alberto Rodriguez'},\n",
       "     {'name': 'Pulkit Agrawal'},\n",
       "     {'name': 'Vincent Sitzmann'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Website: https://yilundu.github.io/ndf/. First two authors\\n  contributed equally (order determined by coin flip), last two authors equal\\n  advising'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.05124v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.05124v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2112.05847v1',\n",
       "    'updated': '2021-12-01T18:42:08Z',\n",
       "    'published': '2021-12-01T18:42:08Z',\n",
       "    'title': 'A Novel Gaussian Process Based Ground Segmentation Algorithm with\\n  Local-Smoothness Estimation',\n",
       "    'summary': \"Autonomous Land Vehicles (ALV) shall efficiently recognize the ground in\\nunknown environments. A novel $\\\\mathcal{GP}$-based method is proposed for the\\nground segmentation task in rough driving scenarios. A non-stationary\\ncovariance function is utilized as the kernel for the $\\\\mathcal{GP}$. The\\nground surface behavior is assumed to only demonstrate local-smoothness. Thus,\\npoint estimates of the kernel's length-scales are obtained. Thus, two Gaussian\\nprocesses are introduced to separately model the observation and local\\ncharacteristics of the data. While, the \\\\textit{observation process} is used to\\nmodel the ground, the \\\\textit{latent process} is put on length-scale values to\\nestimate point values of length-scales at each input location. Input locations\\nfor this latent process are chosen in a physically-motivated procedure to\\nrepresent an intuition about ground condition. Furthermore, an intuitive guess\\nof length-scale value is represented by assuming the existence of hypothetical\\nsurfaces in the environment that every bunch of data points may be assumed to\\nbe resulted from measurements from this surfaces. Bayesian inference is\\nimplemented using \\\\textit{maximum a Posteriori} criterion. The log-marginal\\nlikelihood function is assumed to be a multi-task objective function, to\\nrepresent a whole-frame unbiased view of the ground at each frame. Simulation\\nresults shows the effectiveness of the proposed method even in an uneven, rough\\nscene which outperforms similar Gaussian process based ground segmentation\\nmethods. While adjacent segments do not have similar ground structure in an\\nuneven scene, the proposed method gives an efficient ground estimation based on\\na whole-frame viewpoint instead of just estimating segment-wise probable ground\\nsurfaces.\",\n",
       "    'author': [{'name': 'Pouria Mehrabi'}, {'name': 'Hamid D. Taghirad'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: substantial text overlap with arXiv:2111.10638'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2112.05847v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2112.05847v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.05200v3',\n",
       "    'updated': '2022-03-12T17:05:46Z',\n",
       "    'published': '2022-02-10T18:03:28Z',\n",
       "    'title': 'Visual Servoing for Pose Control of Soft Continuum Arm in a Structured\\n  Environment',\n",
       "    'summary': 'For soft continuum arms, visual servoing is a popular control strategy that\\nrelies on visual feedback to close the control loop. However, robust visual\\nservoing is challenging as it requires reliable feature extraction from the\\nimage, accurate control models and sensors to perceive the shape of the arm,\\nboth of which can be hard to implement in a soft robot. This letter circumvents\\nthese challenges by presenting a deep neural network-based method to perform\\nsmooth and robust 3D positioning tasks on a soft arm by visual servoing using a\\ncamera mounted at the distal end of the arm. A convolutional neural network is\\ntrained to predict the actuations required to achieve the desired pose in a\\nstructured environment. Integrated and modular approaches for estimating the\\nactuations from the image are proposed and are experimentally compared. A\\nproportional control law is implemented to reduce the error between the desired\\nand current image as seen by the camera. The model together with the\\nproportional feedback control makes the described approach robust to several\\nvariations such as new targets, lighting, loads, and diminution of the soft\\narm. Furthermore, the model lends itself to be transferred to a new environment\\nwith minimal effort.',\n",
       "    'author': [{'name': 'Shivani Kamtikar'},\n",
       "     {'name': 'Samhita Marri'},\n",
       "     {'name': 'Benjamin Walt'},\n",
       "     {'name': 'Naveen Kumar Uppalapati'},\n",
       "     {'name': 'Girish Krishnan'},\n",
       "     {'name': 'Girish Chowdhary'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2022.3155821'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2022.3155821',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2202.05200v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.05200v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in RA-L + RoboSoft 2022'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE RA-L 2022'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.05832v2',\n",
       "    'updated': '2022-03-01T09:14:01Z',\n",
       "    'published': '2022-02-11T18:55:10Z',\n",
       "    'title': 'SafePicking: Learning Safe Object Extraction via Object-Level Mapping',\n",
       "    'summary': 'Robots need object-level scene understanding to manipulate objects while\\nreasoning about contact, support, and occlusion among objects. Given a pile of\\nobjects, object recognition and reconstruction can identify the boundary of\\nobject instances, giving important cues as to how the objects form and support\\nthe pile. In this work, we present a system, SafePicking, that integrates\\nobject-level mapping and learning-based motion planning to generate a motion\\nthat safely extracts occluded target objects from a pile. Planning is done by\\nlearning a deep Q-network that receives observations of predicted poses and a\\ndepth-based heightmap to output a motion trajectory, trained to maximize a\\nsafety metric reward. Our results show that the observation fusion of poses and\\ndepth-sensing gives both better performance and robustness to the model. We\\nevaluate our methods using the YCB objects in both simulation and the real\\nworld, achieving safe object extraction from piles.',\n",
       "    'author': [{'name': 'Kentaro Wada'},\n",
       "     {'name': 'Stephen James'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 6 figures, IEEE International Conference on Robotics and\\n  Automation (ICRA) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.05832v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.05832v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.10324v2',\n",
       "    'updated': '2022-09-24T09:45:01Z',\n",
       "    'published': '2022-02-17T09:51:32Z',\n",
       "    'title': 'VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning',\n",
       "    'summary': 'We propose VRL3, a powerful data-driven framework with a simple design for\\nsolving challenging visual deep reinforcement learning (DRL) tasks. We analyze\\na number of major obstacles in taking a data-driven approach, and present a\\nsuite of design principles, novel findings, and critical insights about\\ndata-driven visual DRL. Our framework has three stages: in stage 1, we leverage\\nnon-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations;\\nin stage 2, we use offline RL data (e.g. a limited number of expert\\ndemonstrations) to convert the task-agnostic representations into more powerful\\ntask-specific representations; in stage 3, we fine-tune the agent with online\\nRL. On a set of challenging hand manipulation tasks with sparse reward and\\nrealistic visual inputs, compared to the previous SOTA, VRL3 achieves an\\naverage of 780% better sample efficiency. And on the hardest task, VRL3 is\\n1220% more sample efficient (2440% when using a wider encoder) and solves the\\ntask with only 10% of the computation. These significant results clearly\\ndemonstrate the great potential of data-driven deep reinforcement learning.',\n",
       "    'author': [{'name': 'Che Wang'},\n",
       "     {'name': 'Xufang Luo'},\n",
       "     {'name': 'Keith Ross'},\n",
       "     {'name': 'Dongsheng Li'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '41 pages, under camera-ready revision, accepted to NeurIPS 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.10324v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.10324v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.11092v1',\n",
       "    'updated': '2022-02-22T18:46:52Z',\n",
       "    'published': '2022-02-22T18:46:52Z',\n",
       "    'title': 'ReorientBot: Learning Object Reorientation for Specific-Posed Placement',\n",
       "    'summary': 'Robots need the capability of placing objects in arbitrary, specific poses to\\nrearrange the world and achieve various valuable tasks. Object reorientation\\nplays a crucial role in this as objects may not initially be oriented such that\\nthe robot can grasp and then immediately place them in a specific goal pose. In\\nthis work, we present a vision-based manipulation system, ReorientBot, which\\nconsists of 1) visual scene understanding with pose estimation and volumetric\\nreconstruction using an onboard RGB-D camera; 2) learned waypoint selection for\\nsuccessful and efficient motion generation for reorientation; 3) traditional\\nmotion planning to generate a collision-free trajectory from the selected\\nwaypoints. We evaluate our method using the YCB objects in both simulation and\\nthe real world, achieving 93% overall success, 81% improvement in success rate,\\nand 22% improvement in execution time compared to a heuristic approach. We\\ndemonstrate extended multi-object rearrangement showing the general capability\\nof the system.',\n",
       "    'author': [{'name': 'Kentaro Wada'},\n",
       "     {'name': 'Stephen James'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 6 figures, IEEE International Conference on Robotics and\\n  Automation (ICRA) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.11092v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.11092v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.12555v2',\n",
       "    'updated': '2022-11-07T08:30:48Z',\n",
       "    'published': '2022-02-25T08:41:13Z',\n",
       "    'title': '6D Rotation Representation For Unconstrained Head Pose Estimation',\n",
       "    'summary': 'In this paper, we present a method for unconstrained end-to-end head pose\\nestimation. We address the problem of ambiguous rotation labels by introducing\\nthe rotation matrix formalism for our ground truth data and propose a\\ncontinuous 6D rotation matrix representation for efficient and robust direct\\nregression. This way, our method can learn the full rotation appearance which\\nis contrary to previous approaches that restrict the pose prediction to a\\nnarrow-angle for satisfactory results. In addition, we propose a geodesic\\ndistance-based loss to penalize our network with respect to the SO(3) manifold\\ngeometry. Experiments on the public AFLW2000 and BIWI datasets demonstrate that\\nour proposed method significantly outperforms other state-of-the-art methods by\\nup to 20\\\\%. We open-source our training and testing code along with our\\npre-trained models: https://github.com/thohemp/6DRepNet.',\n",
       "    'author': [{'name': 'Thorsten Hempel'},\n",
       "     {'name': 'Ahmed A. Abdelrahman'},\n",
       "     {'name': 'Ayoub Al-Hamadi'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICIP46576.2022.9897219'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICIP46576.2022.9897219',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2202.12555v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.12555v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICIP 2022'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2022 IEEE International Conference on Image Processing (ICIP),\\n  2022, pp. 2496-2500'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.00352v1',\n",
       "    'updated': '2022-03-01T11:00:35Z',\n",
       "    'published': '2022-03-01T11:00:35Z',\n",
       "    'title': 'Affordance Learning from Play for Sample-Efficient Policy Learning',\n",
       "    'summary': 'Robots operating in human-centered environments should have the ability to\\nunderstand how objects function: what can be done with each object, where this\\ninteraction may occur, and how the object is used to achieve a goal. To this\\nend, we propose a novel approach that extracts a self-supervised visual\\naffordance model from human teleoperated play data and leverages it to enable\\nefficient policy learning and motion planning. We combine model-based planning\\nwith model-free deep reinforcement learning (RL) to learn policies that favor\\nthe same object regions favored by people, while requiring minimal robot\\ninteractions with the environment. We evaluate our algorithm, Visual\\nAffordance-guided Policy Optimization (VAPO), with both diverse simulation\\nmanipulation tasks and real world robot tidy-up experiments to demonstrate the\\neffectiveness of our affordance-guided policies. We find that our policies\\ntrain 4x faster than the baselines and generalize better to novel objects\\nbecause our visual affordance model can anticipate their affordance regions.',\n",
       "    'author': [{'name': 'Jessica Borja-Diaz'},\n",
       "     {'name': 'Oier Mees'},\n",
       "     {'name': 'Gabriel Kalweit'},\n",
       "     {'name': 'Lukas Hermann'},\n",
       "     {'name': 'Joschka Boedecker'},\n",
       "     {'name': 'Wolfram Burgard'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at the 2022 IEEE International Conference on Robotics and\\n  Automation (ICRA). Videos at http://vapo.cs.uni-freiburg.de/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.00352v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.00352v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.02634v1',\n",
       "    'updated': '2022-03-05T01:23:13Z',\n",
       "    'published': '2022-03-05T01:23:13Z',\n",
       "    'title': 'Important Object Identification with Semi-Supervised Learning for\\n  Autonomous Driving',\n",
       "    'summary': 'Accurate identification of important objects in the scene is a prerequisite\\nfor safe and high-quality decision making and motion planning of intelligent\\nagents (e.g., autonomous vehicles) that navigate in complex and dynamic\\nenvironments. Most existing approaches attempt to employ attention mechanisms\\nto learn importance weights associated with each object indirectly via various\\ntasks (e.g., trajectory prediction), which do not enforce direct supervision on\\nthe importance estimation. In contrast, we tackle this task in an explicit way\\nand formulate it as a binary classification (\"important\" or \"unimportant\")\\nproblem. We propose a novel approach for important object identification in\\negocentric driving scenarios with relational reasoning on the objects in the\\nscene. Besides, since human annotations are limited and expensive to obtain, we\\npresent a semi-supervised learning pipeline to enable the model to learn from\\nunlimited unlabeled data. Moreover, we propose to leverage the auxiliary tasks\\nof ego vehicle behavior prediction to further improve the accuracy of\\nimportance estimation. The proposed approach is evaluated on a public\\negocentric driving dataset (H3D) collected in complex traffic scenarios. A\\ndetailed ablative study is conducted to demonstrate the effectiveness of each\\nmodel component and the training strategy. Our approach also outperforms\\nrule-based baselines by a large margin.',\n",
       "    'author': [{'name': 'Jiachen Li'},\n",
       "     {'name': 'Haiming Gang'},\n",
       "     {'name': 'Hengbo Ma'},\n",
       "     {'name': 'Masayoshi Tomizuka'},\n",
       "     {'name': 'Chiho Choi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICRA 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.02634v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.02634v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.03580v2',\n",
       "    'updated': '2022-08-08T22:32:21Z',\n",
       "    'published': '2022-03-07T18:26:14Z',\n",
       "    'title': 'The Unsurprising Effectiveness of Pre-Trained Vision Models for Control',\n",
       "    'summary': 'Recent years have seen the emergence of pre-trained representations as a\\npowerful abstraction for AI applications in computer vision, natural language,\\nand speech. However, policy learning for control is still dominated by a\\ntabula-rasa learning paradigm, with visuo-motor policies often trained from\\nscratch using data from deployment environments. In this context, we revisit\\nand study the role of pre-trained visual representations for control, and in\\nparticular representations trained on large-scale computer vision datasets.\\nThrough extensive empirical evaluation in diverse control domains (Habitat,\\nDeepMind Control, Adroit, Franka Kitchen), we isolate and study the importance\\nof different representation training methods, data augmentations, and feature\\nhierarchies. Overall, we find that pre-trained visual representations can be\\ncompetitive or even better than ground-truth state representations to train\\ncontrol policies. This is in spite of using only out-of-domain data from\\nstandard vision datasets, without any in-domain data from the deployment\\nenvironments. Source code and more at\\nhttps://sites.google.com/view/pvr-control.',\n",
       "    'author': [{'name': 'Simone Parisi'},\n",
       "     {'name': 'Aravind Rajeswaran'},\n",
       "     {'name': 'Senthil Purushwalkam'},\n",
       "     {'name': 'Abhinav Gupta'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'First two authors contributed equally'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'International Conference on Machine Learning (ICML), 2022,\\n  162:17359-17371'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.03580v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.03580v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.04563v1',\n",
       "    'updated': '2022-03-09T07:53:15Z',\n",
       "    'published': '2022-03-09T07:53:15Z',\n",
       "    'title': 'MLNav: Learning to Safely Navigate on Martian Terrains',\n",
       "    'summary': 'We present MLNav, a learning-enhanced path planning framework for\\nsafety-critical and resource-limited systems operating in complex environments,\\nsuch as rovers navigating on Mars. MLNav makes judicious use of machine\\nlearning to enhance the efficiency of path planning while fully respecting\\nsafety constraints. In particular, the dominant computational cost in such\\nsafety-critical settings is running a model-based safety checker on the\\nproposed paths. Our learned search heuristic can simultaneously predict the\\nfeasibility for all path options in a single run, and the model-based safety\\nchecker is only invoked on the top-scoring paths. We validate in high-fidelity\\nsimulations using both real Martian terrain data collected by the Perseverance\\nrover, as well as a suite of challenging synthetic terrains. Our experiments\\nshow that: (i) compared to the baseline ENav path planner on board the\\nPerserverance rover, MLNav can provide a significant improvement in multiple\\nkey metrics, such as a 10x reduction in collision checks when navigating real\\nMartian terrains, despite being trained with synthetic terrains; and (ii) MLNav\\ncan successfully navigate highly challenging terrains where the baseline ENav\\nfails to find a feasible path before timing out.',\n",
       "    'author': [{'name': 'Shreyansh Daftry'},\n",
       "     {'name': 'Neil Abcouwer'},\n",
       "     {'name': 'Tyler Del Sesto'},\n",
       "     {'name': 'Siddarth Venkatraman'},\n",
       "     {'name': 'Jialin Song'},\n",
       "     {'name': 'Lucas Igel'},\n",
       "     {'name': 'Amos Byon'},\n",
       "     {'name': 'Ugo Rosolia'},\n",
       "     {'name': 'Yisong Yue'},\n",
       "     {'name': 'Masahiro Ono'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters (RA-L) and ICRA 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.04563v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.04563v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.04566v2',\n",
       "    'updated': '2022-03-13T07:51:46Z',\n",
       "    'published': '2022-03-09T08:03:07Z',\n",
       "    'title': 'All You Need is LUV: Unsupervised Collection of Labeled Images using\\n  Invisible UV Fluorescent Indicators',\n",
       "    'summary': 'Large-scale semantic image annotation is a significant challenge for\\nlearning-based perception systems in robotics. Current approaches often rely on\\nhuman labelers, which can be expensive, or simulation data, which can visually\\nor physically differ from real data. This paper proposes Labels from\\nUltraViolet (LUV), a novel framework that enables rapid, labeled data\\ncollection in real manipulation environments without human labeling. LUV uses\\ntransparent, ultraviolet-fluorescent paint with programmable ultraviolet LEDs\\nto collect paired images of a scene in standard lighting and UV lighting to\\nautonomously extract segmentation masks and keypoints via color segmentation.\\nWe apply LUV to a suite of diverse robot perception tasks to evaluate its\\nlabeling quality, flexibility, and data collection rate. Results suggest that\\nLUV is 180-2500 times faster than a human labeler across the tasks. We show\\nthat LUV provides labels consistent with human annotations on unpainted test\\nimages. The networks trained on these labels are used to smooth and fold\\ncrumpled towels with 83% success rate and achieve 1.7mm position error with\\nrespect to human labels on a surgical needle pose estimation task. The low cost\\nof LUV makes it ideal as a lightweight replacement for human labeling systems,\\nwith the one-time setup costs at $300 equivalent to the cost of collecting\\naround 200 semantic segmentation labels on Amazon Mechanical Turk. Code,\\ndatasets, visualizations, and supplementary material can be found at\\nhttps://sites.google.com/berkeley.edu/luv',\n",
       "    'author': [{'name': 'Brijen Thananjeyan'},\n",
       "     {'name': 'Justin Kerr'},\n",
       "     {'name': 'Huang Huang'},\n",
       "     {'name': 'Joseph E. Gonzalez'},\n",
       "     {'name': 'Ken Goldberg'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.04566v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.04566v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.05698v2',\n",
       "    'updated': '2022-08-01T11:45:39Z',\n",
       "    'published': '2022-03-11T01:12:00Z',\n",
       "    'title': 'Learning-based Localizability Estimation for Robust LiDAR Localization',\n",
       "    'summary': 'LiDAR-based localization and mapping is one of the core components in many\\nmodern robotic systems due to the direct integration of range and geometry,\\nallowing for precise motion estimation and generation of high quality maps in\\nreal-time. Yet, as a consequence of insufficient environmental constraints\\npresent in the scene, this dependence on geometry can result in localization\\nfailure, happening in self-symmetric surroundings such as tunnels. This work\\naddresses precisely this issue by proposing a neural network-based estimation\\napproach for detecting (non-)localizability during robot operation. Special\\nattention is given to the localizability of scan-to-scan registration, as it is\\na crucial component in many LiDAR odometry estimation pipelines. In contrast to\\nprevious, mostly traditional detection approaches, the proposed method enables\\nearly detection of failure by estimating the localizability on raw sensor\\nmeasurements without evaluating the underlying registration optimization.\\nMoreover, previous approaches remain limited in their ability to generalize\\nacross environments and sensor types, as heuristic-tuning of degeneracy\\ndetection thresholds is required. The proposed approach avoids this problem by\\nlearning from a collection of different environments, allowing the network to\\nfunction over various scenarios. Furthermore, the network is trained\\nexclusively on simulated data, avoiding arduous data collection in challenging\\nand degenerate, often hard-to-access, environments. The presented method is\\ntested during field experiments conducted across challenging environments and\\non two different sensor types without any modifications. The observed detection\\nperformance is on par with state-of-the-art methods after environment-specific\\nthreshold tuning.',\n",
       "    'author': [{'name': 'Julian Nubert'},\n",
       "     {'name': 'Etienne Walther'},\n",
       "     {'name': 'Shehryar Khattak'},\n",
       "     {'name': 'Marco Hutter'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 7 figures, 4 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.05698v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.05698v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.09510v1',\n",
       "    'updated': '2022-03-17T17:58:00Z',\n",
       "    'published': '2022-03-17T17:58:00Z',\n",
       "    'title': 'DetMatch: Two Teachers are Better Than One for Joint 2D and 3D\\n  Semi-Supervised Object Detection',\n",
       "    'summary': 'While numerous 3D detection works leverage the complementary relationship\\nbetween RGB images and point clouds, developments in the broader framework of\\nsemi-supervised object recognition remain uninfluenced by multi-modal fusion.\\nCurrent methods develop independent pipelines for 2D and 3D semi-supervised\\nlearning despite the availability of paired image and point cloud frames.\\nObserving that the distinct characteristics of each sensor cause them to be\\nbiased towards detecting different objects, we propose DetMatch, a flexible\\nframework for joint semi-supervised learning on 2D and 3D modalities. By\\nidentifying objects detected in both sensors, our pipeline generates a cleaner,\\nmore robust set of pseudo-labels that both demonstrates stronger performance\\nand stymies single-modality error propagation. Further, we leverage the richer\\nsemantics of RGB images to rectify incorrect 3D class predictions and improve\\nlocalization of 3D boxes. Evaluating on the challenging KITTI and Waymo\\ndatasets, we improve upon strong semi-supervised learning methods and observe\\nhigher quality pseudo-labels. Code will be released at\\nhttps://github.com/Divadi/DetMatch',\n",
       "    'author': [{'name': 'Jinhyung Park'},\n",
       "     {'name': 'Chenfeng Xu'},\n",
       "     {'name': 'Yiyang Zhou'},\n",
       "     {'name': 'Masayoshi Tomizuka'},\n",
       "     {'name': 'Wei Zhan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'https://github.com/Divadi/DetMatch'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.09510v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.09510v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.10194v1',\n",
       "    'updated': '2022-03-18T23:51:09Z',\n",
       "    'published': '2022-03-18T23:51:09Z',\n",
       "    'title': 'Analysis and Adaptation of YOLOv4 for Object Detection in Aerial Images',\n",
       "    'summary': 'The recent and rapid growth in Unmanned Aerial Vehicles (UAVs) deployment for\\nvarious computer vision tasks has paved the path for numerous opportunities to\\nmake them more effective and valuable. Object detection in aerial images is\\nchallenging due to variations in appearance, pose, and scale. Autonomous aerial\\nflight systems with their inherited limited memory and computational power\\ndemand accurate and computationally efficient detection algorithms for\\nreal-time applications. Our work shows the adaptation of the popular YOLOv4\\nframework for predicting the objects and their locations in aerial images with\\nhigh accuracy and inference speed. We utilized transfer learning for faster\\nconvergence of the model on the VisDrone DET aerial object detection dataset.\\nThe trained model resulted in a mean average precision (mAP) of 45.64% with an\\ninference speed reaching 8.7 FPS on the Tesla K80 GPU and was highly accurate\\nin detecting truncated and occluded objects. We experimentally evaluated the\\nimpact of varying network resolution sizes and training epochs on the\\nperformance. A comparative study with several contemporary aerial object\\ndetectors proved that YOLOv4 performed better, implying a more suitable\\ndetection algorithm to incorporate on aerial platforms.',\n",
       "    'author': [{'name': 'Aryaman Singh Samyal'},\n",
       "     {'name': 'Akshatha K R'},\n",
       "     {'name': 'Soham Hans'},\n",
       "     {'name': 'Karunakar A K'},\n",
       "     {'name': 'Satish Shenoy B'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.10194v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.10194v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.11194v2',\n",
       "    'updated': '2022-11-28T16:02:45Z',\n",
       "    'published': '2022-03-21T17:59:50Z',\n",
       "    'title': 'Test-time adaptation with slot-centric models',\n",
       "    'summary': 'Current supervised visual detectors, though impressive within their training\\ndistribution, often fail to segment out-of-distribution scenes into their\\nconstituent entities. Recent test-time adaptation methods use auxiliary\\nself-supervised losses to adapt the network parameters to each test example\\nindependently and have shown promising results towards generalization outside\\nthe training distribution for the task of image classification. In our work, we\\nfind evidence that these losses can be insufficient for instance segmentation\\ntasks, without also considering architectural inductive biases. For image\\nsegmentation, recent slot-centric generative models break such dependence on\\nsupervision by attempting to segment scenes into entities in a self-supervised\\nmanner by reconstructing pixels. Drawing upon these two lines of work, we\\npropose Slot-TTA, a semi-supervised instance segmentation model equipped with a\\nslot-centric inductive bias, that is adapted per scene at test time through\\ngradient descent on reconstruction or novel view synthesis objectives. We show\\nthat test-time adaptation in Slot-TTA greatly improves instance segmentation in\\nout-of-distribution scenes. We evaluate Slot-TTA in several 3D and 2D scene\\ninstance segmentation benchmarks and show substantial out-of-distribution\\nperformance improvements against state-of-the-art supervised feed-forward\\ndetectors and self-supervised test-time adaptation methods.',\n",
       "    'author': [{'name': 'Mihir Prabhudesai'},\n",
       "     {'name': 'Anirudh Goyal'},\n",
       "     {'name': 'Sujoy Paul'},\n",
       "     {'name': 'Sjoerd van Steenkiste'},\n",
       "     {'name': 'Mehdi S. M. Sajjadi'},\n",
       "     {'name': 'Gaurav Aggarwal'},\n",
       "     {'name': 'Thomas Kipf'},\n",
       "     {'name': 'Deepak Pathak'},\n",
       "     {'name': 'Katerina Fragkiadaki'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project website at https://mihirp1998.github.io/project_pages/slottta'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.11194v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.11194v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.12601v3',\n",
       "    'updated': '2022-11-18T05:57:09Z',\n",
       "    'published': '2022-03-23T17:55:09Z',\n",
       "    'title': 'R3M: A Universal Visual Representation for Robot Manipulation',\n",
       "    'summary': 'We study how visual representations pre-trained on diverse human video data\\ncan enable data-efficient learning of downstream robotic manipulation tasks.\\nConcretely, we pre-train a visual representation using the Ego4D human video\\ndataset using a combination of time-contrastive learning, video-language\\nalignment, and an L1 penalty to encourage sparse and compact representations.\\nThe resulting representation, R3M, can be used as a frozen perception module\\nfor downstream policy learning. Across a suite of 12 simulated robot\\nmanipulation tasks, we find that R3M improves task success by over 20% compared\\nto training from scratch and by over 10% compared to state-of-the-art visual\\nrepresentations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika\\nPanda arm to learn a range of manipulation tasks in a real, cluttered apartment\\ngiven just 20 demonstrations. Code and pre-trained models are available at\\nhttps://tinyurl.com/robotr3m.',\n",
       "    'author': [{'name': 'Suraj Nair'},\n",
       "     {'name': 'Aravind Rajeswaran'},\n",
       "     {'name': 'Vikash Kumar'},\n",
       "     {'name': 'Chelsea Finn'},\n",
       "     {'name': 'Abhinav Gupta'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Conference on Robot Learning (CoRL) 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.12601v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.12601v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.13251v1',\n",
       "    'updated': '2022-03-24T17:58:54Z',\n",
       "    'published': '2022-03-24T17:58:54Z',\n",
       "    'title': 'Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient\\n  Dexterous Manipulation',\n",
       "    'summary': \"Optimizing behaviors for dexterous manipulation has been a longstanding\\nchallenge in robotics, with a variety of methods from model-based control to\\nmodel-free reinforcement learning having been previously explored in\\nliterature. Perhaps one of the most powerful techniques to learn complex\\nmanipulation strategies is imitation learning. However, collecting and learning\\nfrom demonstrations in dexterous manipulation is quite challenging. The\\ncomplex, high-dimensional action-space involved with multi-finger control often\\nleads to poor sample efficiency of learning-based methods. In this work, we\\npropose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning\\nframework for dexterous manipulation. DIME only requires a single RGB camera to\\nobserve a human operator and teleoperate our robotic hand. Once demonstrations\\nare collected, DIME employs standard imitation learning methods to train\\ndexterous manipulation policies. On both simulation and real robot benchmarks\\nwe demonstrate that DIME can be used to solve complex, in-hand manipulation\\ntasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro\\nhand. Our framework along with pre-collected demonstrations is publicly\\navailable at https://nyu-robot-learning.github.io/dime.\",\n",
       "    'author': [{'name': 'Sridhar Pandian Arunachalam'},\n",
       "     {'name': 'Sneha Silwal'},\n",
       "     {'name': 'Ben Evans'},\n",
       "     {'name': 'Lerrel Pinto'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The first two authors contributed equally'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.13251v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.13251v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.14708v1',\n",
       "    'updated': '2022-03-24T09:16:56Z',\n",
       "    'published': '2022-03-24T09:16:56Z',\n",
       "    'title': 'Object Memory Transformer for Object Goal Navigation',\n",
       "    'summary': 'This paper presents a reinforcement learning method for object goal\\nnavigation (ObjNav) where an agent navigates in 3D indoor environments to reach\\na target object based on long-term observations of objects and scenes. To this\\nend, we propose Object Memory Transformer (OMT) that consists of two key ideas:\\n1) Object-Scene Memory (OSM) that enables to store long-term scenes and object\\nsemantics, and 2) Transformer that attends to salient objects in the sequence\\nof previously observed scenes and objects stored in OSM. This mechanism allows\\nthe agent to efficiently navigate in the indoor environment without prior\\nknowledge about the environments, such as topological maps or 3D meshes. To the\\nbest of our knowledge, this is the first work that uses a long-term memory of\\nobject semantics in a goal-oriented navigation task. Experimental results\\nconducted on the AI2-THOR dataset show that OMT outperforms previous approaches\\nin navigating in unknown environments. In particular, we show that utilizing\\nthe long-term object semantics information improves the efficiency of\\nnavigation.',\n",
       "    'author': [{'name': 'Rui Fukushima'},\n",
       "     {'name': 'Kei Ota'},\n",
       "     {'name': 'Asako Kanezaki'},\n",
       "     {'name': 'Yoko Sasaki'},\n",
       "     {'name': 'Yusuke Yoshiyasu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '7 pages, 3 figures, Accepted at ICRA 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.14708v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.14708v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.15174v2',\n",
       "    'updated': '2022-07-23T03:51:45Z',\n",
       "    'published': '2022-03-29T01:36:11Z',\n",
       "    'title': 'Disentangling Object Motion and Occlusion for Unsupervised Multi-frame\\n  Monocular Depth',\n",
       "    'summary': 'Conventional self-supervised monocular depth prediction methods are based on\\na static environment assumption, which leads to accuracy degradation in dynamic\\nscenes due to the mismatch and occlusion problems introduced by object motions.\\nExisting dynamic-object-focused methods only partially solved the mismatch\\nproblem at the training loss level. In this paper, we accordingly propose a\\nnovel multi-frame monocular depth prediction method to solve these problems at\\nboth the prediction and supervision loss levels. Our method, called\\nDynamicDepth, is a new framework trained via a self-supervised cycle consistent\\nlearning scheme. A Dynamic Object Motion Disentanglement (DOMD) module is\\nproposed to disentangle object motions to solve the mismatch problem. Moreover,\\nnovel occlusion-aware Cost Volume and Re-projection Loss are designed to\\nalleviate the occlusion effects of object motions. Extensive analyses and\\nexperiments on the Cityscapes and KITTI datasets show that our method\\nsignificantly outperforms the state-of-the-art monocular depth prediction\\nmethods, especially in the areas of dynamic objects. Code is available at\\nhttps://github.com/AutoAILab/DynamicDepth',\n",
       "    'author': [{'name': 'Ziyue Feng'},\n",
       "     {'name': 'Liang Yang'},\n",
       "     {'name': 'Longlong Jing'},\n",
       "     {'name': 'Haiyan Wang'},\n",
       "     {'name': 'YingLi Tian'},\n",
       "     {'name': 'Bing Li'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '[ECCV 2022]'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.15174v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.15174v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.15983v2',\n",
       "    'updated': '2022-08-01T20:29:24Z',\n",
       "    'published': '2022-03-30T01:43:15Z',\n",
       "    'title': 'VI-IKD: High-Speed Accurate Off-Road Navigation using Learned\\n  Visual-Inertial Inverse Kinodynamics',\n",
       "    'summary': 'One of the key challenges in high speed off road navigation on ground\\nvehicles is that the kinodynamics of the vehicle terrain interaction can differ\\ndramatically depending on the terrain. Previous approaches to addressing this\\nchallenge have considered learning an inverse kinodynamics (IKD) model,\\nconditioned on inertial information of the vehicle to sense the kinodynamic\\ninteractions. In this paper, we hypothesize that to enable accurate high-speed\\noff-road navigation using a learned IKD model, in addition to inertial\\ninformation from the past, one must also anticipate the kinodynamic\\ninteractions of the vehicle with the terrain in the future. To this end, we\\nintroduce Visual-Inertial Inverse Kinodynamics (VI-IKD), a novel learning based\\nIKD model that is conditioned on visual information from a terrain patch ahead\\nof the robot in addition to past inertial information, enabling it to\\nanticipate kinodynamic interactions in the future. We validate the\\neffectiveness of VI-IKD in accurate high-speed off-road navigation\\nexperimentally on a scale 1/5 UT-AlphaTruck off-road autonomous vehicle in both\\nindoor and outdoor environments and show that compared to other\\nstate-of-the-art approaches, VI-IKD enables more accurate and robust off-road\\nnavigation on a variety of different terrains at speeds of up to 3.5 m/s.',\n",
       "    'author': [{'name': 'Haresh Karnan'},\n",
       "     {'name': 'Kavan Singh Sikand'},\n",
       "     {'name': 'Pranav Atreya'},\n",
       "     {'name': 'Sadegh Rabiee'},\n",
       "     {'name': 'Xuesu Xiao'},\n",
       "     {'name': 'Garrett Warnell'},\n",
       "     {'name': 'Peter Stone'},\n",
       "     {'name': 'Joydeep Biswas'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE/RSJ International Conference on Intelligent Robots and\\n  Systems (IROS 2022)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.15983v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.15983v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.02091v1',\n",
       "    'updated': '2022-04-05T10:03:52Z',\n",
       "    'published': '2022-04-05T10:03:52Z',\n",
       "    'title': 'P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior',\n",
       "    'summary': 'Monocular depth estimation is vital for scene understanding and downstream\\ntasks. We focus on the supervised setup, in which ground-truth depth is\\navailable only at training time. Based on knowledge about the high regularity\\nof real 3D scenes, we propose a method that learns to selectively leverage\\ninformation from coplanar pixels to improve the predicted depth. In particular,\\nwe introduce a piecewise planarity prior which states that for each pixel,\\nthere is a seed pixel which shares the same planar 3D surface with the former.\\nMotivated by this prior, we design a network with two heads. The first head\\noutputs pixel-level plane coefficients, while the second one outputs a dense\\noffset vector field that identifies the positions of seed pixels. The plane\\ncoefficients of seed pixels are then used to predict depth at each position.\\nThe resulting prediction is adaptively fused with the initial prediction from\\nthe first head via a learned confidence to account for potential deviations\\nfrom precise local planarity. The entire architecture is trained end-to-end\\nthanks to the differentiability of the proposed modules and it learns to\\npredict regular depth maps, with sharp edges at occlusion boundaries. An\\nextensive evaluation of our method shows that we set the new state of the art\\nin supervised monocular depth estimation, surpassing prior methods on NYU\\nDepth-v2 and on the Garg split of KITTI. Our method delivers depth maps that\\nyield plausible 3D reconstructions of the input scenes. Code is available at:\\nhttps://github.com/SysCV/P3Depth',\n",
       "    'author': [{'name': 'Vaishakh Patil'},\n",
       "     {'name': 'Christos Sakaridis'},\n",
       "     {'name': 'Alexander Liniger'},\n",
       "     {'name': 'Luc Van Gool'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at CVPR 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.02091v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.02091v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.02390v2',\n",
       "    'updated': '2022-06-30T07:36:51Z',\n",
       "    'published': '2022-04-05T17:55:58Z',\n",
       "    'title': 'Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower',\n",
       "    'summary': 'We investigate pneumatic non-prehensile manipulation (i.e., blowing) as a\\nmeans of efficiently moving scattered objects into a target receptacle. Due to\\nthe chaotic nature of aerodynamic forces, a blowing controller must (i)\\ncontinually adapt to unexpected changes from its actions, (ii) maintain\\nfine-grained control, since the slightest misstep can result in large\\nunintended consequences (e.g., scatter objects already in a pile), and (iii)\\ninfer long-range plans (e.g., move the robot to strategic blowing locations).\\nWe tackle these challenges in the context of deep reinforcement learning,\\nintroducing a multi-frequency version of the spatial action maps framework.\\nThis allows for efficient learning of vision-based policies that effectively\\ncombine high-level planning and low-level closed-loop control for dynamic\\nmobile manipulation. Experiments show that our system learns efficient\\nbehaviors for the task, demonstrating in particular that blowing achieves\\nbetter downstream performance than pushing, and that our policies improve\\nperformance over baselines. Moreover, we show that our system naturally\\nencourages emergent specialization between the different subpolicies spanning\\nlow-level fine-grained control and high-level planning. On a real mobile robot\\nequipped with a miniature air blower, we show that our simulation-trained\\npolicies transfer well to a real environment and can generalize to novel\\nobjects.',\n",
       "    'author': [{'name': 'Jimmy Wu'},\n",
       "     {'name': 'Xingyuan Sun'},\n",
       "     {'name': 'Andy Zeng'},\n",
       "     {'name': 'Shuran Song'},\n",
       "     {'name': 'Szymon Rusinkiewicz'},\n",
       "     {'name': 'Thomas Funkhouser'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2022.3187833'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2022.3187833',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2204.02390v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.02390v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE Robotics and Automation Letters (RA-L), 2022 and\\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),\\n  2022. Project page: https://learning-dynamic-manipulation.cs.princeton.edu'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.10419v2',\n",
       "    'updated': '2023-01-20T07:11:44Z',\n",
       "    'published': '2022-04-21T21:59:24Z',\n",
       "    'title': 'Learning Sequential Latent Variable Models from Multimodal Time Series\\n  Data',\n",
       "    'summary': 'Sequential modelling of high-dimensional data is an important problem that\\nappears in many domains including model-based reinforcement learning and\\ndynamics identification for control. Latent variable models applied to\\nsequential data (i.e., latent dynamics models) have been shown to be a\\nparticularly effective probabilistic approach to solve this problem, especially\\nwhen dealing with images. However, in many application areas (e.g., robotics),\\ninformation from multiple sensing modalities is available -- existing latent\\ndynamics methods have not yet been extended to effectively make use of such\\nmultimodal sequential data. Multimodal sensor streams can be correlated in a\\nuseful manner and often contain complementary information across modalities. In\\nthis work, we present a self-supervised generative modelling framework to\\njointly learn a probabilistic latent state representation of multimodal data\\nand the respective dynamics. Using synthetic and real-world datasets from a\\nmultimodal robotic planar pushing task, we demonstrate that our approach leads\\nto significant improvements in prediction and representation quality.\\nFurthermore, we compare to the common learning baseline of concatenating each\\nmodality in the latent space and show that our principled probabilistic\\nformulation performs better. Finally, despite being fully self-supervised, we\\ndemonstrate that our method is nearly as effective as an existing supervised\\napproach that relies on ground truth labels.',\n",
       "    'author': [{'name': 'Oliver Limoyo'},\n",
       "     {'name': 'Trevor Ablett'},\n",
       "     {'name': 'Jonathan Kelly'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/978-3-031-22216-0_35'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/978-3-031-22216-0_35',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2204.10419v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.10419v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"In: Petrovic, I., Menegatti, E., Markovi\\\\'c, I. (eds) Intelligent\\n  Autonomous Systems 17. IAS 2022. Lecture Notes in Networks and Systems, vol\\n  577. Springer, Cham\"},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2204.12471v2',\n",
       "    'updated': '2022-05-02T15:38:48Z',\n",
       "    'published': '2022-04-26T17:41:28Z',\n",
       "    'title': 'Coarse-to-fine Q-attention with Tree Expansion',\n",
       "    'summary': 'Coarse-to-fine Q-attention enables sample-efficient robot manipulation by\\ndiscretizing the translation space in a coarse-to-fine manner, where the\\nresolution gradually increases at each layer in the hierarchy. Although\\neffective, Q-attention suffers from \"coarse ambiguity\" - when voxelization is\\nsignificantly coarse, it is not feasible to distinguish similar-looking objects\\nwithout first inspecting at a finer resolution. To combat this, we propose to\\nenvision Q-attention as a tree that can be expanded and used to accumulate\\nvalue estimates across the top-k voxels at each Q-attention depth. When our\\nextension, Q-attention with Tree Expansion (QTE), replaces standard Q-attention\\nin the Attention-driven Robot Manipulation (ARM) system, we are able to\\naccomplish a larger set of tasks; especially on those that suffer from \"coarse\\nambiguity\". In addition to evaluating our approach across 12 RLBench tasks, we\\nalso show that the improved performance is visible in a real-world task\\ninvolving small objects.',\n",
       "    'author': [{'name': 'Stephen James'}, {'name': 'Pieter Abbeel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project page and code: https://sites.google.com/view/q-attention-qte'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2204.12471v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2204.12471v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.02111v2',\n",
       "    'updated': '2022-08-01T14:59:42Z',\n",
       "    'published': '2022-05-03T06:29:03Z',\n",
       "    'title': 'Improved Orientation Estimation and Detection with Hybrid Object\\n  Detection Networks for Automotive Radar',\n",
       "    'summary': \"This paper presents novel hybrid architectures that combine grid- and\\npoint-based processing to improve the detection performance and orientation\\nestimation of radar-based object detection networks. Purely grid-based\\ndetection models operate on a bird's-eye-view (BEV) projection of the input\\npoint cloud. These approaches suffer from a loss of detailed information\\nthrough the discrete grid resolution. This applies in particular to radar\\nobject detection, where relatively coarse grid resolutions are commonly used to\\naccount for the sparsity of radar point clouds. In contrast, point-based models\\nare not affected by this problem as they process point clouds without\\ndiscretization. However, they generally exhibit worse detection performances\\nthan grid-based methods.\\n  We show that a point-based model can extract neighborhood features,\\nleveraging the exact relative positions of points, before grid rendering. This\\nhas significant benefits for a subsequent grid-based convolutional detection\\nbackbone. In experiments on the public nuScenes dataset our hybrid architecture\\nachieves improvements in terms of detection performance (19.7% higher mAP for\\ncar class than next-best radar-only submission) and orientation estimates\\n(11.5% relative orientation improvement) over networks from previous\\nliterature.\",\n",
       "    'author': [{'name': 'Michael Ulrich'},\n",
       "     {'name': 'Sascha Braun'},\n",
       "     {'name': 'Daniel Köhler'},\n",
       "     {'name': 'Daniel Niederlöhner'},\n",
       "     {'name': 'Florian Faion'},\n",
       "     {'name': 'Claudius Gläser'},\n",
       "     {'name': 'Holger Blume'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '(c) 2022 IEEE. Personal use of this material is permitted. Permission\\n  from IEEE must be obtained for all other uses, in any current or future\\n  media, including reprinting/republishing this material for advertising or\\n  promotional purposes, creating new collective works, for resale or\\n  redistribution to servers or lists, or reuse of any copyrighted component of\\n  this work in other works'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.02111v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.02111v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.06333v1',\n",
       "    'updated': '2022-05-12T19:48:11Z',\n",
       "    'published': '2022-05-12T19:48:11Z',\n",
       "    'title': 'Visuomotor Control in Multi-Object Scenes Using Object-Aware\\n  Representations',\n",
       "    'summary': 'Perceptual understanding of the scene and the relationship between its\\ndifferent components is important for successful completion of robotic tasks.\\nRepresentation learning has been shown to be a powerful technique for this, but\\nmost of the current methodologies learn task specific representations that do\\nnot necessarily transfer well to other tasks. Furthermore, representations\\nlearned by supervised methods require large labeled datasets for each task that\\nare expensive to collect in the real world. Using self-supervised learning to\\nobtain representations from unlabeled data can mitigate this problem. However,\\ncurrent self-supervised representation learning methods are mostly object\\nagnostic, and we demonstrate that the resulting representations are\\ninsufficient for general purpose robotics tasks as they fail to capture the\\ncomplexity of scenes with many components. In this paper, we explore the\\neffectiveness of using object-aware representation learning techniques for\\nrobotic tasks. Our self-supervised representations are learned by observing the\\nagent freely interacting with different parts of the environment and is queried\\nin two different settings: (i) policy learning and (ii) object location\\nprediction. We show that our model learns control policies in a\\nsample-efficient manner and outperforms state-of-the-art object agnostic\\ntechniques as well as methods trained on raw RGB images. Our results show a 20\\npercent increase in performance in low data regimes (1000 trajectories) in\\npolicy training using implicit behavioral cloning (IBC). Furthermore, our\\nmethod outperforms the baselines for the task of object localization in\\nmulti-object scenes.',\n",
       "    'author': [{'name': 'Negin Heravi'},\n",
       "     {'name': 'Ayzaan Wahid'},\n",
       "     {'name': 'Corey Lynch'},\n",
       "     {'name': 'Pete Florence'},\n",
       "     {'name': 'Travis Armstrong'},\n",
       "     {'name': 'Jonathan Tompson'},\n",
       "     {'name': 'Pierre Sermanet'},\n",
       "     {'name': 'Jeannette Bohg'},\n",
       "     {'name': 'Debidatta Dwibedi'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.06333v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.06333v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.09753v1',\n",
       "    'updated': '2022-04-30T07:08:30Z',\n",
       "    'published': '2022-04-30T07:08:30Z',\n",
       "    'title': 'HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory\\n  Prediction via Scene Encoding',\n",
       "    'summary': 'One essential task for autonomous driving is to encode the information of a\\ndriving scene into vector representations so that the downstream task such as\\ntrajectory prediction could perform well. The driving scene is complicated, and\\nthere exists heterogeneity within elements, where they own diverse types of\\ninformation i.e., agent dynamics, map routing, road lines, etc. Meanwhile,\\nthere also exist relativity across elements - meaning they have spatial\\nrelations with each other; such relations should be canonically represented\\nregarding the relative measurements since the absolute value of the coordinate\\nis meaningless. Taking these two observations into consideration, we propose a\\nnovel backbone, namely Heterogeneous Driving Graph Transformer (HDGT), which\\nmodels the driving scene as a heterogeneous graph with different types of nodes\\nand edges. For graph construction, each node represents either an agent or a\\nroad element and each edge represents their semantics relations such as\\nPedestrian-To-Crosswalk, Lane-To-Left-Lane. As for spatial relation encoding,\\ninstead of setting a fixed global reference, the coordinate information of the\\nnode as well as its in-edges is transformed to the local node-centric\\ncoordinate system. For the aggregation module in the graph neural network\\n(GNN), we adopt the transformer structure in a hierarchical way to fit the\\nheterogeneous nature of inputs. Experimental results show that the proposed\\nmethod achieves new state-of-the-art on INTERACTION Prediction Challenge and\\nWaymo Open Motion Challenge, in which we rank 1st and 2nd respectively\\nregarding the minADE/minFDE metric.',\n",
       "    'author': [{'name': 'Xiaosong Jia'},\n",
       "     {'name': 'Penghao Wu'},\n",
       "     {'name': 'Li Chen'},\n",
       "     {'name': 'Hongyang Li'},\n",
       "     {'name': 'Yu Liu'},\n",
       "     {'name': 'Junchi Yan'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.09753v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.09753v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.10223v1',\n",
       "    'updated': '2022-04-30T21:01:03Z',\n",
       "    'published': '2022-04-30T21:01:03Z',\n",
       "    'title': 'Mosaic Zonotope Shadow Matching for Risk-Aware Autonomous Localization\\n  in Harsh Urban Environments',\n",
       "    'summary': \"Risk-aware urban localization with the Global Navigation Satellite System\\n(GNSS) remains an unsolved problem with frequent misdetection of the user's\\nstreet or side of the street. Significant advances in 3D map-aided GNSS use\\ngrid-based GNSS shadow matching alongside AI-driven line-of-sight (LOS)\\nclassifiers and server-based processing to improve localization accuracy,\\nespecially in the cross-street direction. Our prior work introduces a new\\nparadigm for shadow matching that proposes set-valued localization with\\ncomputationally efficient zonotope set representations. While existing\\nliterature improved accuracy and efficiency, the current state of shadow\\nmatching theory does not address the needs of risk-aware autonomous systems. We\\nextend our prior work to propose Mosaic Zonotope Shadow Matching (MZSM) that\\nemploys a classifier-agnostic polytope mosaic architecture to provide\\nrisk-awareness and certifiable guarantees on urban positioning. We formulate a\\nrecursively expanding binary tree that refines an initial location estimate\\nwith set operations into smaller polytopes. Together, the smaller polytopes\\nform a mosaic. We weight the tree branches with the probability that the user\\nis in line of sight of the satellite and expand the tree with each new\\nsatellite observation. Our method yields an exact shadow matching distribution\\nfrom which we guarantee uncertainty bounds on the user localization. We perform\\nhigh-fidelity simulations using a 3D building map of San Francisco to validate\\nour algorithm's risk-aware improvements. We demonstrate that MZSM provides\\ncertifiable guarantees across varied data-driven LOS classifier accuracies and\\nyields a more precise understanding of the uncertainty over existing methods.\\nWe validate that our tree-based construction is efficient and tractable,\\ncomputing a mosaic from 14 satellites in 0.63 seconds and growing quadratically\\nin the satellite number.\",\n",
       "    'author': [{'name': 'Daniel Neamati'},\n",
       "     {'name': 'Sriramya Bhamidipati'},\n",
       "     {'name': 'Grace Gao'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to AIJ Special Issue on Risk-Aware Autonomous Systems:\\n  Theory and Practice'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.10223v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.10223v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.11110v1',\n",
       "    'updated': '2022-05-23T07:58:50Z',\n",
       "    'published': '2022-05-23T07:58:50Z',\n",
       "    'title': 'Meta-Learning Regrasping Strategies for Physical-Agnostic Objects',\n",
       "    'summary': 'Grasping inhomogeneous objects, practical use in real-world applications,\\nremains a challenging task due to the unknown physical properties such as mass\\ndistribution and coefficient of friction. In this study, we propose a\\nvision-based meta-learning algorithm to learn physical properties in an\\nagnostic way. In particular, we employ Conditional Neural Processes (CNPs) on\\ntop of DexNet-2.0. CNPs learn physical embeddings rapidly from a few\\nobservations where each observation is composed of i) the cropped depth image,\\nii) the grasping height between the gripper and estimated grasping point, and\\niii) the binary grasping result. Our modified conditional DexNet-2.0\\n(DexNet-CNP) updates the predicted grasping quality iteratively from new\\nobservations, which can be executed in an online fashion. We evaluate our\\nmethod in the Pybullet simulator using various shape primitive objects with\\ndifferent physical parameters. The results show that our model outperforms the\\noriginal DexNet-2.0 and is able to generalize on unseen objects with different\\nshapes.',\n",
       "    'author': [{'name': 'Ruijie Chen'},\n",
       "     {'name': 'Ning Gao'},\n",
       "     {'name': 'Ngo Anh Vien'},\n",
       "     {'name': 'Hanna Ziesche'},\n",
       "     {'name': 'Gerhard Neumann'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted as spotlight in ICRA 2022 Workshop: Scaling Robot Learning'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.11110v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.11110v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2205.15997v1',\n",
       "    'updated': '2022-05-31T17:57:19Z',\n",
       "    'published': '2022-05-31T17:57:19Z',\n",
       "    'title': 'TransFuser: Imitation with Transformer-Based Sensor Fusion for\\n  Autonomous Driving',\n",
       "    'summary': \"How should we integrate representations from complementary sensors for\\nautonomous driving? Geometry-based fusion has shown promise for perception\\n(e.g. object detection, motion forecasting). However, in the context of\\nend-to-end driving, we find that imitation learning based on existing sensor\\nfusion methods underperforms in complex driving scenarios with a high density\\nof dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate\\nimage and LiDAR representations using self-attention. Our approach uses\\ntransformer modules at multiple resolutions to fuse perspective view and bird's\\neye view feature maps. We experimentally validate its efficacy on a challenging\\nnew benchmark with long routes and dense traffic, as well as the official\\nleaderboard of the CARLA urban driving simulator. At the time of submission,\\nTransFuser outperforms all prior work on the CARLA leaderboard in terms of\\ndriving score by a large margin. Compared to geometry-based fusion, TransFuser\\nreduces the average collisions per kilometer by 48%.\",\n",
       "    'author': [{'name': 'Kashyap Chitta'},\n",
       "     {'name': 'Aditya Prakash'},\n",
       "     {'name': 'Bernhard Jaeger'},\n",
       "     {'name': 'Zehao Yu'},\n",
       "     {'name': 'Katrin Renz'},\n",
       "     {'name': 'Andreas Geiger'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: text overlap with arXiv:2104.09224'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2205.15997v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2205.15997v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.02622v2',\n",
       "    'updated': '2022-11-04T16:31:24Z',\n",
       "    'published': '2022-06-06T14:05:25Z',\n",
       "    'title': 'Hardware-accelerated Mars Sample Localization via deep transfer learning\\n  from photorealistic simulations',\n",
       "    'summary': 'The goal of the Mars Sample Return campaign is to collect soil samples from\\nthe surface of Mars and return them to Earth for further study. The samples\\nwill be acquired and stored in metal tubes by the Perseverance rover and\\ndeposited on the Martian surface. As part of this campaign, it is expected that\\nthe Sample Fetch Rover will be in charge of localizing and gathering up to 35\\nsample tubes over 150 Martian sols. Autonomous capabilities are critical for\\nthe success of the overall campaign and for the Sample Fetch Rover in\\nparticular. This work proposes a novel system architecture for the autonomous\\ndetection and pose estimation of the sample tubes. For the detection stage, a\\nDeep Neural Network and transfer learning from a synthetic dataset are\\nproposed. The dataset is created from photorealistic 3D simulations of Martian\\nscenarios. Additionally, the sample tubes poses are estimated using Computer\\nVision techniques such as contour detection and line fitting on the detected\\narea. Finally, laboratory tests of the Sample Localization procedure are\\nperformed using the ExoMars Testing Rover on a Mars-like testbed. These tests\\nvalidate the proposed approach in different hardware architectures, providing\\npromising results related to the sample detection and pose estimation.',\n",
       "    'author': [{'name': 'Raúl Castilla-Arquillo'},\n",
       "     {'name': 'Carlos Jesús Pérez-del-Pulgar'},\n",
       "     {'name': 'Gonzalo Jesús Paz-Delgado'},\n",
       "     {'name': 'Levin Gerdes'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2022.3219306'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2022.3219306',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2206.02622v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.02622v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Preprint version only. Final version at IEEE Xplore. Accepted for\\n  IEEE Robotics and Automation Letters'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.03970v2',\n",
       "    'updated': '2022-06-10T17:44:21Z',\n",
       "    'published': '2022-06-08T15:36:31Z',\n",
       "    'title': 'Narrowing the Coordinate-frame Gap in Behavior Prediction Models:\\n  Distillation for Efficient and Accurate Scene-centric Motion Forecasting',\n",
       "    'summary': 'Behavior prediction models have proliferated in recent years, especially in\\nthe popular real-world robotics application of autonomous driving, where\\nrepresenting the distribution over possible futures of moving agents is\\nessential for safe and comfortable motion planning. In these models, the choice\\nof coordinate frames to represent inputs and outputs has crucial trade offs\\nwhich broadly fall into one of two categories. Agent-centric models transform\\ninputs and perform inference in agent-centric coordinates. These models are\\nintrinsically invariant to translation and rotation between scene elements, are\\nbest-performing on public leaderboards, but scale quadratically with the number\\nof agents and scene elements. Scene-centric models use a fixed coordinate\\nsystem to process all agents. This gives them the advantage of sharing\\nrepresentations among all agents, offering efficient amortized inference\\ncomputation which scales linearly with the number of agents. However, these\\nmodels have to learn invariance to translation and rotation between scene\\nelements, and typically underperform agent-centric models. In this work, we\\ndevelop knowledge distillation techniques between probabilistic motion\\nforecasting models, and apply these techniques to close the gap in performance\\nbetween agent-centric and scene-centric models. This improves scene-centric\\nmodel performance by 13.2% on the public Argoverse benchmark, 7.8% on Waymo\\nOpen Dataset and up to 9.4% on a large In-House dataset. These improved\\nscene-centric models rank highly in public leaderboards and are up to 15 times\\nmore efficient than their agent-centric teacher counterparts in busy scenes.',\n",
       "    'author': [{'name': 'DiJia Su'},\n",
       "     {'name': 'Bertrand Douillard'},\n",
       "     {'name': 'Rami Al-Rfou'},\n",
       "     {'name': 'Cheolho Park'},\n",
       "     {'name': 'Benjamin Sapp'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at ICRA 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.03970v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.03970v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.07162v1',\n",
       "    'updated': '2022-06-14T20:46:09Z',\n",
       "    'published': '2022-06-14T20:46:09Z',\n",
       "    'title': 'Category-Agnostic 6D Pose Estimation with Conditional Neural Processes',\n",
       "    'summary': 'We present a novel meta-learning approach for 6D pose estimation on unknown\\nobjects. In contrast to \"instance-level\" pose estimation methods, our algorithm\\nlearns object representation in a category-agnostic way, which endows it with\\nstrong generalization capabilities within and across object categories.\\nSpecifically, we employ a conditional neural process-based meta-learning\\napproach to train an encoder to capture texture and geometry of an object in a\\nlatent representation, based on very few RGB-D images and ground-truth\\nkeypoints. The latent representation is then used by a simultaneously\\nmeta-trained decoder to predict the 6D pose of the object in new images. To\\nevaluate our algorithm, experiments are conducted on our new fully-annotated\\nsynthetic datasets generated from Multiple Categories in Multiple Scenes\\n(MCMS). Experimental results demonstrate that our model performs well on unseen\\nobjects with various shapes and appearances.',\n",
       "    'author': [{'name': 'Yumeng Li'},\n",
       "     {'name': 'Ning Gao'},\n",
       "     {'name': 'Hanna Ziesche'},\n",
       "     {'name': 'Gerhard Neumann'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at CVPR2022 workshop: Women in Computer Vision (WiCV)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.07162v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.07162v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.08077v1',\n",
       "    'updated': '2022-06-16T10:45:17Z',\n",
       "    'published': '2022-06-16T10:45:17Z',\n",
       "    'title': 'Neural Scene Representation for Locomotion on Structured Terrain',\n",
       "    'summary': \"We propose a learning-based method to reconstruct the local terrain for\\nlocomotion with a mobile robot traversing urban environments. Using a stream of\\ndepth measurements from the onboard cameras and the robot's trajectory, the\\nalgorithm estimates the topography in the robot's vicinity. The raw\\nmeasurements from these cameras are noisy and only provide partial and occluded\\nobservations that in many cases do not show the terrain the robot stands on.\\nTherefore, we propose a 3D reconstruction model that faithfully reconstructs\\nthe scene, despite the noisy measurements and large amounts of missing data\\ncoming from the blind spots of the camera arrangement. The model consists of a\\n4D fully convolutional network on point clouds that learns the geometric priors\\nto complete the scene from the context and an auto-regressive feedback to\\nleverage spatio-temporal consistency and use evidence from the past. The\\nnetwork can be solely trained with synthetic data, and due to extensive\\naugmentation, it is robust in the real world, as shown in the validation on a\\nquadrupedal robot, ANYmal, traversing challenging settings. We run the pipeline\\non the robot's onboard low-power computer using an efficient sparse tensor\\nimplementation and show that the proposed method outperforms classical map\\nrepresentations.\",\n",
       "    'author': [{'name': 'David Hoeller'},\n",
       "     {'name': 'Nikita Rudin'},\n",
       "     {'name': 'Christopher Choy'},\n",
       "     {'name': 'Animashree Anandkumar'},\n",
       "     {'name': 'Marco Hutter'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.08077v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.08077v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.11251v2',\n",
       "    'updated': '2022-10-11T22:49:49Z',\n",
       "    'published': '2022-06-22T17:57:08Z',\n",
       "    'title': 'Behavior Transformers: Cloning $k$ modes with one stone',\n",
       "    'summary': 'While behavior learning has made impressive progress in recent times, it lags\\nbehind computer vision and natural language processing due to its inability to\\nleverage large, human-generated datasets. Human behaviors have wide variance,\\nmultiple modes, and human demonstrations typically do not come with reward\\nlabels. These properties limit the applicability of current methods in Offline\\nRL and Behavioral Cloning to learn from large, pre-collected datasets. In this\\nwork, we present Behavior Transformer (BeT), a new technique to model unlabeled\\ndemonstration data with multiple modes. BeT retrofits standard transformer\\narchitectures with action discretization coupled with a multi-task action\\ncorrection inspired by offset prediction in object detection. This allows us to\\nleverage the multi-modal modeling ability of modern transformers to predict\\nmulti-modal continuous actions. We experimentally evaluate BeT on a variety of\\nrobotic manipulation and self-driving behavior datasets. We show that BeT\\nsignificantly improves over prior state-of-the-art work on solving demonstrated\\ntasks while capturing the major modes present in the pre-collected datasets.\\nFinally, through an extensive ablation study, we analyze the importance of\\nevery crucial component in BeT. Videos of behavior generated by BeT are\\navailable at https://notmahi.github.io/bet',\n",
       "    'author': [{'name': 'Nur Muhammad Mahi Shafiullah'},\n",
       "     {'name': 'Zichen Jeff Cui'},\n",
       "     {'name': 'Ariuntuya Altanzaya'},\n",
       "     {'name': 'Lerrel Pinto'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Code and data available at https://github.com/notmahi/bet'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.11251v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.11251v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.13387v1',\n",
       "    'updated': '2022-06-18T00:00:02Z',\n",
       "    'published': '2022-06-18T00:00:02Z',\n",
       "    'title': 'ScePT: Scene-consistent, Policy-based Trajectory Predictions for\\n  Planning',\n",
       "    'summary': \"Trajectory prediction is a critical functionality of autonomous systems that\\nshare environments with uncontrolled agents, one prominent example being\\nself-driving vehicles. Currently, most prediction methods do not enforce scene\\nconsistency, i.e., there are a substantial amount of self-collisions between\\npredicted trajectories of different agents in the scene. Moreover, many\\napproaches generate individual trajectory predictions per agent instead of\\njoint trajectory predictions of the whole scene, which makes downstream\\nplanning difficult. In this work, we present ScePT, a policy planning-based\\ntrajectory prediction model that generates accurate, scene-consistent\\ntrajectory predictions suitable for autonomous system motion planning. It\\nexplicitly enforces scene consistency and learns an agent interaction policy\\nthat can be used for conditional prediction. Experiments on multiple real-world\\npedestrians and autonomous vehicle datasets show that ScePT} matches current\\nstate-of-the-art prediction accuracy with significantly improved scene\\nconsistency. We also demonstrate ScePT's ability to work with a downstream\\ncontingency planner.\",\n",
       "    'author': [{'name': 'Yuxiao Chen'},\n",
       "     {'name': 'Boris Ivanovic'},\n",
       "     {'name': 'Marco Pavone'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.13387v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.13387v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.13396v2',\n",
       "    'updated': '2022-08-09T20:47:35Z',\n",
       "    'published': '2022-06-21T02:33:57Z',\n",
       "    'title': 'A Simple Approach for Visual Rearrangement: 3D Mapping and Semantic\\n  Search',\n",
       "    'summary': \"Physically rearranging objects is an important capability for embodied\\nagents. Visual room rearrangement evaluates an agent's ability to rearrange\\nobjects in a room to a desired goal based solely on visual input. We propose a\\nsimple yet effective method for this problem: (1) search for and map which\\nobjects need to be rearranged, and (2) rearrange each object until the task is\\ncomplete. Our approach consists of an off-the-shelf semantic segmentation\\nmodel, voxel-based semantic map, and semantic search policy to efficiently find\\nobjects that need to be rearranged. On the AI2-THOR Rearrangement Challenge,\\nour method improves on current state-of-the-art end-to-end reinforcement\\nlearning-based methods that learn visual rearrangement policies from 0.53%\\ncorrect rearrangement to 16.56%, using only 2.7% as many samples from the\\nenvironment.\",\n",
       "    'author': [{'name': 'Brandon Trabucco'},\n",
       "     {'name': 'Gunnar Sigurdsson'},\n",
       "     {'name': 'Robinson Piramuthu'},\n",
       "     {'name': 'Gaurav S. Sukhatme'},\n",
       "     {'name': 'Ruslan Salakhutdinov'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Winner of the Rearrangement Challenge at CVPR 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.13396v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.13396v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.00401v2',\n",
       "    'updated': '2022-07-26T10:01:28Z',\n",
       "    'published': '2022-07-01T13:17:45Z',\n",
       "    'title': 'Autonomous Intraluminal Navigation of a Soft Robot using\\n  Deep-Learning-based Visual Servoing',\n",
       "    'summary': \"Navigation inside luminal organs is an arduous task that requires\\nnon-intuitive coordination between the movement of the operator's hand and the\\ninformation obtained from the endoscopic video. The development of tools to\\nautomate certain tasks could alleviate the physical and mental load of doctors\\nduring interventions, allowing them to focus on diagnosis and decision-making\\ntasks. In this paper, we present a synergic solution for intraluminal\\nnavigation consisting of a 3D printed endoscopic soft robot that can move\\nsafely inside luminal structures. Visual servoing, based on Convolutional\\nNeural Networks (CNNs) is used to achieve the autonomous navigation task. The\\nCNN is trained with phantoms and in-vivo data to segment the lumen, and a\\nmodel-less approach is presented to control the movement in constrained\\nenvironments. The proposed robot is validated in anatomical phantoms in\\ndifferent path configurations. We analyze the movement of the robot using\\ndifferent metrics such as task completion time, smoothness, error in the\\nsteady-state, and mean and maximum error. We show that our method is suitable\\nto navigate safely in hollow environments and conditions which are different\\nthan the ones the network was originally trained on.\",\n",
       "    'author': [{'name': 'Jorge F. Lazo'},\n",
       "     {'name': 'Chun-Feng Lai'},\n",
       "     {'name': 'Sara Moccia'},\n",
       "     {'name': 'Benoit Rosa'},\n",
       "     {'name': 'Michele Catellani'},\n",
       "     {'name': 'Michel de Mathelin'},\n",
       "     {'name': 'Giancarlo Ferrigno'},\n",
       "     {'name': 'Paul Breedveld'},\n",
       "     {'name': 'Jenny Dankelman'},\n",
       "     {'name': 'Elena De Momi'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.00401v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.00401v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.02959v1',\n",
       "    'updated': '2022-07-06T20:33:32Z',\n",
       "    'published': '2022-07-06T20:33:32Z',\n",
       "    'title': 'NeuralGrasps: Learning Implicit Representations for Grasps of Multiple\\n  Robotic Hands',\n",
       "    'summary': 'We introduce a neural implicit representation for grasps of objects from\\nmultiple robotic hands. Different grasps across multiple robotic hands are\\nencoded into a shared latent space. Each latent vector is learned to decode to\\nthe 3D shape of an object and the 3D shape of a robotic hand in a grasping pose\\nin terms of the signed distance functions of the two 3D shapes. In addition,\\nthe distance metric in the latent space is learned to preserve the similarity\\nbetween grasps across different robotic hands, where the similarity of grasps\\nis defined according to contact regions of the robotic hands. This property\\nenables our method to transfer grasps between different grippers including a\\nhuman hand, and grasp transfer has the potential to share grasping skills\\nbetween robots and enable robots to learn grasping skills from humans.\\nFurthermore, the encoded signed distance functions of objects and grasps in our\\nimplicit representation can be used for 6D object pose estimation with grasping\\ncontact optimization from partial point clouds, which enables robotic grasping\\nin the real world.',\n",
       "    'author': [{'name': 'Ninad Khargonkar'},\n",
       "     {'name': 'Neil Song'},\n",
       "     {'name': 'Zesheng Xu'},\n",
       "     {'name': 'Balakrishnan Prabhakaran'},\n",
       "     {'name': 'Yu Xiang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.02959v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.02959v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.03146v1',\n",
       "    'updated': '2022-07-07T08:15:12Z',\n",
       "    'published': '2022-07-07T08:15:12Z',\n",
       "    'title': 'Self-Supervised Velocity Estimation for Automotive Radar Object\\n  Detection Networks',\n",
       "    'summary': \"This paper presents a method to learn the Cartesian velocity of objects using\\nan object detection network on automotive radar data. The proposed method is\\nself-supervised in terms of generating its own training signal for the\\nvelocities. Labels are only required for single-frame, oriented bounding boxes\\n(OBBs). Labels for the Cartesian velocities or contiguous sequences, which are\\nexpensive to obtain, are not required. The general idea is to pre-train an\\nobject detection network without velocities using single-frame OBB labels, and\\nthen exploit the network's OBB predictions on unlabelled data for velocity\\ntraining. In detail, the network's OBB predictions of the unlabelled frames are\\nupdated to the timestamp of a labelled frame using the predicted velocities and\\nthe distances between the updated OBBs of the unlabelled frame and the OBB\\npredictions of the labelled frame are used to generate a self-supervised\\ntraining signal for the velocities. The detection network architecture is\\nextended by a module to account for the temporal relation of multiple scans and\\na module to represent the radars' radial velocity measurements explicitly. A\\ntwo-step approach of first training only OBB detection, followed by training\\nOBB detection and velocities is used. Further, a pre-training with\\npseudo-labels generated from radar radial velocity measurements bootstraps the\\nself-supervised method of this paper. Experiments on the publicly available\\nnuScenes dataset show that the proposed method almost reaches the velocity\\nestimation performance of a fully supervised training, but does not require\\nexpensive velocity labels. Furthermore, we outperform a baseline method which\\nuses only radial velocity measurements as labels.\",\n",
       "    'author': [{'name': 'Daniel Niederlöhner'},\n",
       "     {'name': 'Michael Ulrich'},\n",
       "     {'name': 'Sascha Braun'},\n",
       "     {'name': 'Daniel Köhler'},\n",
       "     {'name': 'Florian Faion'},\n",
       "     {'name': 'Claudius Gläser'},\n",
       "     {'name': 'André Treptow'},\n",
       "     {'name': 'Holger Blume'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/IV51971.2022.9827295'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/IV51971.2022.9827295',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2207.03146v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.03146v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for presentation at the 2022 33rd IEEE Intelligent Vehicles\\n  Symposium (IV) (IV 2022), June 5-9, 2022, in Aachen, Germany'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2022 IEEE Intelligent Vehicles Symposium (IV), 04-09 June 2022,\\n  Aachen Germany, pp. 352-359'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.03333v2',\n",
       "    'updated': '2022-11-08T16:55:14Z',\n",
       "    'published': '2022-07-06T05:57:24Z',\n",
       "    'title': 'FewSOL: A Dataset for Few-Shot Object Learning in Robotic Environments',\n",
       "    'summary': 'We introduce the Few-Shot Object Learning (FewSOL) dataset for object\\nrecognition with a few images per object. We captured 336 real-world objects\\nwith 9 RGB-D images per object from different views. Object segmentation masks,\\nobject poses and object attributes are provided. In addition, synthetic images\\ngenerated using 330 3D object models are used to augment the dataset. We\\ninvestigated (i) few-shot object classification and (ii) joint object\\nsegmentation and few-shot classification with the state-of-the-art methods for\\nfew-shot learning and meta-learning using our dataset. The evaluation results\\nshow that there is still a large margin to be improved for few-shot object\\nclassification in robotic environments. Our dataset can be used to study a set\\nof few-shot object recognition problems such as classification, detection and\\nsegmentation, shape reconstruction, pose estimation, keypoint correspondences\\nand attribute recognition. The dataset and code are available at\\nhttps://irvlutd.github.io/FewSOL.',\n",
       "    'author': [{'name': 'Jishnu Jaykumar P'},\n",
       "     {'name': 'Yu-Wei Chao'},\n",
       "     {'name': 'Yu Xiang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.03333v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.03333v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.14024v5',\n",
       "    'updated': '2022-12-07T16:22:37Z',\n",
       "    'published': '2022-07-28T11:36:21Z',\n",
       "    'title': 'Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion\\n  Transformer',\n",
       "    'summary': 'Large-scale deployment of autonomous vehicles has been continually delayed\\ndue to safety concerns. On the one hand, comprehensive scene understanding is\\nindispensable, a lack of which would result in vulnerability to rare but\\ncomplex traffic situations, such as the sudden emergence of unknown objects.\\nHowever, reasoning from a global context requires access to sensors of multiple\\ntypes and adequate fusion of multi-modal sensor signals, which is difficult to\\nachieve. On the other hand, the lack of interpretability in learning models\\nalso hampers the safety with unverifiable failure causes. In this paper, we\\npropose a safety-enhanced autonomous driving framework, named Interpretable\\nSensor Fusion Transformer(InterFuser), to fully process and fuse information\\nfrom multi-modal multi-view sensors for achieving comprehensive scene\\nunderstanding and adversarial event detection. Besides, intermediate\\ninterpretable features are generated from our framework, which provide more\\nsemantics and are exploited to better constrain actions to be within the safe\\nsets. We conducted extensive experiments on CARLA benchmarks, where our model\\noutperforms prior methods, ranking the first on the public CARLA Leaderboard.\\nOur code will be made available at https://github.com/opendilab/InterFuser',\n",
       "    'author': [{'name': 'Hao Shao'},\n",
       "     {'name': 'Letian Wang'},\n",
       "     {'name': 'RuoBing Chen'},\n",
       "     {'name': 'Hongsheng Li'},\n",
       "     {'name': 'Yu Liu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at CoRL 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2207.14024v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.14024v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.02408v1',\n",
       "    'updated': '2022-08-30T11:00:59Z',\n",
       "    'published': '2022-08-30T11:00:59Z',\n",
       "    'title': 'Robustness and invariance properties of image classifiers',\n",
       "    'summary': 'Deep neural networks have achieved impressive results in many image\\nclassification tasks. However, since their performance is usually measured in\\ncontrolled settings, it is important to ensure that their decisions remain\\ncorrect when deployed in noisy environments. In fact, deep networks are not\\nrobust to a large variety of semantic-preserving image modifications, even to\\nimperceptible image changes known as adversarial perturbations. The poor\\nrobustness of image classifiers to small data distribution shifts raises\\nserious concerns regarding their trustworthiness. To build reliable machine\\nlearning models, we must design principled methods to analyze and understand\\nthe mechanisms that shape robustness and invariance. This is exactly the focus\\nof this thesis.\\n  First, we study the problem of computing sparse adversarial perturbations. We\\nexploit the geometry of the decision boundaries of image classifiers for\\ncomputing sparse perturbations very fast, and reveal a qualitative connection\\nbetween adversarial examples and the data features that image classifiers\\nlearn. Then, to better understand this connection, we propose a geometric\\nframework that connects the distance of data samples to the decision boundary,\\nwith the features existing in the data. We show that deep classifiers have a\\nstrong inductive bias towards invariance to non-discriminative features, and\\nthat adversarial training exploits this property to confer robustness. Finally,\\nwe focus on the challenging problem of generalization to unforeseen corruptions\\nof the data, and we propose a novel data augmentation scheme for achieving\\nstate-of-the-art robustness to common corruptions of the images.\\n  Overall, our results contribute to the understanding of the fundamental\\nmechanisms of deep image classifiers, and pave the way for building more\\nreliable machine learning systems that can be deployed in real-world\\nenvironments.',\n",
       "    'author': {'name': 'Apostolos Modas'},\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.5075/epfl-thesis-9646'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.5075/epfl-thesis-9646',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2209.02408v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.02408v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"PhD Thesis No. 9646, Ecole Polytechnique F\\\\'ed\\\\'erale de Lausanne\\n  (EPFL). Open access through EPFL library:\\n  https://infoscience.epfl.ch/record/295828\"},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.03910v1',\n",
       "    'updated': '2022-09-08T16:36:24Z',\n",
       "    'published': '2022-09-08T16:36:24Z',\n",
       "    'title': 'PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and\\n  Feature-metric Alignment',\n",
       "    'summary': 'We present PixTrack, a vision based object pose tracking framework using\\nnovel view synthesis and deep feature-metric alignment. Our evaluations\\ndemonstrate that our method produces highly accurate, robust, and jitter-free\\n6DoF pose estimates of objects in RGB images without the need of any data\\nannotation or trajectory smoothing. Our method is also computationally\\nefficient making it easy to have multi-object tracking with no alteration to\\nour method and just using CPU multiprocessing.',\n",
       "    'author': [{'name': 'Prajwal Chidananda'},\n",
       "     {'name': 'Saurabh Nair'},\n",
       "     {'name': 'Douglas Lee'},\n",
       "     {'name': 'Adrian Kaehler'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.03910v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.03910v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.08772v1',\n",
       "    'updated': '2022-09-19T05:54:26Z',\n",
       "    'published': '2022-09-19T05:54:26Z',\n",
       "    'title': 'TANDEM3D: Active Tactile Exploration for 3D Object Recognition',\n",
       "    'summary': 'Tactile recognition of 3D objects remains a challenging task. Compared to 2D\\nshapes, the complex geometry of 3D surfaces requires richer tactile signals,\\nmore dexterous actions, and more advanced encoding techniques. In this work, we\\npropose TANDEM3D, a method that applies a co-training framework for exploration\\nand decision making to 3D object recognition with tactile signals. Starting\\nwith our previous work, which introduced a co-training paradigm for 2D\\nrecognition problems, we introduce a number of advances that enable us to scale\\nup to 3D. TANDEM3D is based on a novel encoder that builds 3D object\\nrepresentation from contact positions and normals using PointNet++.\\nFurthermore, by enabling 6DOF movement, TANDEM3D explores and collects\\ndiscriminative touch information with high efficiency. Our method is trained\\nentirely in simulation and validated with real-world experiments. Compared to\\nstate-of-the-art baselines, TANDEM3D achieves higher accuracy and a lower\\nnumber of actions in recognizing 3D objects and is also shown to be more robust\\nto different types and amounts of sensor noise. Video is available at\\nhttps://jxu.ai/tandem3d.',\n",
       "    'author': [{'name': 'Jingxi Xu'},\n",
       "     {'name': 'Han Lin'},\n",
       "     {'name': 'Shuran Song'},\n",
       "     {'name': 'Matei Ciocarlie'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.08772v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.08772v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.08959v1',\n",
       "    'updated': '2022-09-19T12:27:15Z',\n",
       "    'published': '2022-09-19T12:27:15Z',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning',\n",
       "    'summary': 'Everyday tasks of long-horizon and comprising a sequence of multiple implicit\\nsubtasks still impose a major challenge in offline robot control. While a\\nnumber of prior methods aimed to address this setting with variants of\\nimitation and offline reinforcement learning, the learned behavior is typically\\nnarrow and often struggles to reach configurable long-horizon goals. As both\\nparadigms have complementary strengths and weaknesses, we propose a novel\\nhierarchical approach that combines the strengths of both methods to learn\\ntask-agnostic long-horizon policies from high-dimensional camera observations.\\nConcretely, we combine a low-level policy that learns latent skills via\\nimitation learning and a high-level policy learned from offline reinforcement\\nlearning for skill-chaining the latent behavior priors. Experiments in various\\nsimulated and real robot control tasks show that our formulation enables\\nproducing previously unseen combinations of skills to reach temporally extended\\ngoals by \"stitching\" together latent skills through goal chaining with an\\norder-of-magnitude improvement in performance upon state-of-the-art baselines.\\nWe even learn one multi-task visuomotor policy for 25 distinct manipulation\\ntasks in the real world which outperforms both imitation learning and offline\\nreinforcement learning techniques.',\n",
       "    'author': [{'name': 'Erick Rosete-Beas'},\n",
       "     {'name': 'Oier Mees'},\n",
       "     {'name': 'Gabriel Kalweit'},\n",
       "     {'name': 'Joschka Boedecker'},\n",
       "     {'name': 'Wolfram Burgard'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2022. Project website: http://tacorl.cs.uni-freiburg.de/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.08959v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.08959v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.10767v2',\n",
       "    'updated': '2022-10-05T21:09:10Z',\n",
       "    'published': '2022-09-22T03:53:56Z',\n",
       "    'title': 'DRAMA: Joint Risk Localization and Captioning in Driving',\n",
       "    'summary': 'Considering the functionality of situational awareness in safety-critical\\nautomation systems, the perception of risk in driving scenes and its\\nexplainability is of particular importance for autonomous and cooperative\\ndriving. Toward this goal, this paper proposes a new research direction of\\njoint risk localization in driving scenes and its risk explanation as a natural\\nlanguage description. Due to the lack of standard benchmarks, we collected a\\nlarge-scale dataset, DRAMA (Driving Risk Assessment Mechanism with A captioning\\nmodule), which consists of 17,785 interactive driving scenarios collected in\\nTokyo, Japan. Our DRAMA dataset accommodates video- and object-level questions\\non driving risks with associated important objects to achieve the goal of\\nvisual captioning as a free-form language description utilizing closed and\\nopen-ended responses for multi-level questions, which can be used to evaluate a\\nrange of visual captioning capabilities in driving scenarios. We make this data\\navailable to the community for further research. Using DRAMA, we explore\\nmultiple facets of joint risk localization and captioning in interactive\\ndriving scenarios. In particular, we benchmark various multi-task prediction\\narchitectures and provide a detailed analysis of joint risk localization and\\nrisk captioning. The data set is available at https://usa.honda-ri.com/drama',\n",
       "    'author': [{'name': 'Srikanth Malla'},\n",
       "     {'name': 'Chiho Choi'},\n",
       "     {'name': 'Isht Dwivedi'},\n",
       "     {'name': 'Joon Hee Choi'},\n",
       "     {'name': 'Jiachen Li'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'WACV 2023 (Winter Conference on Applications of Computer Vision)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.10767v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.10767v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.11133v2',\n",
       "    'updated': '2022-09-23T18:14:39Z',\n",
       "    'published': '2022-09-22T16:20:17Z',\n",
       "    'title': 'PACT: Perception-Action Causal Transformer for Autoregressive Robotics\\n  Pre-Training',\n",
       "    'summary': 'Robotics has long been a field riddled with complex systems architectures\\nwhose modules and connections, whether traditional or learning-based, require\\nsignificant human expertise and prior knowledge. Inspired by large pre-trained\\nlanguage models, this work introduces a paradigm for pre-training a general\\npurpose representation that can serve as a starting point for multiple tasks on\\na given robot. We present the Perception-Action Causal Transformer (PACT), a\\ngenerative transformer-based architecture that aims to build representations\\ndirectly from robot data in a self-supervised fashion. Through autoregressive\\nprediction of states and actions over time, our model implicitly encodes\\ndynamics and behaviors for a particular robot. Our experimental evaluation\\nfocuses on the domain of mobile agents, where we show that this robot-specific\\nrepresentation can function as a single starting point to achieve distinct\\ntasks such as safe navigation, localization and mapping. We evaluate two form\\nfactors: a wheeled robot that uses a LiDAR sensor as perception input (MuSHR),\\nand a simulated agent that uses first-person RGB images (Habitat). We show that\\nfinetuning small task-specific networks on top of the larger pretrained model\\nresults in significantly better performance compared to training a single model\\nfrom scratch for all tasks simultaneously, and comparable performance to\\ntraining a separate large model for each task independently. By sharing a\\ncommon good-quality representation across tasks we can lower overall model\\ncapacity and speed up the real-time deployment of such systems.',\n",
       "    'author': [{'name': 'Rogerio Bonatti'},\n",
       "     {'name': 'Sai Vemprala'},\n",
       "     {'name': 'Shuang Ma'},\n",
       "     {'name': 'Felipe Frujeri'},\n",
       "     {'name': 'Shuhang Chen'},\n",
       "     {'name': 'Ashish Kapoor'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.11133v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.11133v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.00030v1',\n",
       "    'updated': '2022-09-30T18:14:07Z',\n",
       "    'published': '2022-09-30T18:14:07Z',\n",
       "    'title': 'VIP: Towards Universal Visual Reward and Representation via\\n  Value-Implicit Pre-Training',\n",
       "    'summary': \"Reward and representation learning are two long-standing challenges for\\nlearning an expanding set of robot manipulation skills from sensory\\nobservations. Given the inherent cost and scarcity of in-domain, task-specific\\nrobot data, learning from large, diverse, offline human videos has emerged as a\\npromising path towards acquiring a generally useful visual representation for\\ncontrol; however, how these human videos can be used for general-purpose reward\\nlearning remains an open question. We introduce\\n$\\\\textbf{V}$alue-$\\\\textbf{I}$mplicit $\\\\textbf{P}$re-training (VIP), a\\nself-supervised pre-trained visual representation capable of generating dense\\nand smooth reward functions for unseen robotic tasks. VIP casts representation\\nlearning from human videos as an offline goal-conditioned reinforcement\\nlearning problem and derives a self-supervised dual goal-conditioned\\nvalue-function objective that does not depend on actions, enabling pre-training\\non unlabeled human videos. Theoretically, VIP can be understood as a novel\\nimplicit time contrastive objective that generates a temporally smooth\\nembedding, enabling the value function to be implicitly defined via the\\nembedding distance, which can then be used to construct the reward for any\\ngoal-image specified downstream task. Trained on large-scale Ego4D human videos\\nand without any fine-tuning on in-domain, task-specific data, VIP's frozen\\nrepresentation can provide dense visual reward for an extensive set of\\nsimulated and $\\\\textbf{real-robot}$ tasks, enabling diverse reward-based visual\\ncontrol methods and significantly outperforming all prior pre-trained\\nrepresentations. Notably, VIP can enable simple, $\\\\textbf{few-shot}$ offline RL\\non a suite of real-world robot tasks with as few as 20 trajectories.\",\n",
       "    'author': [{'name': 'Yecheng Jason Ma'},\n",
       "     {'name': 'Shagun Sodhani'},\n",
       "     {'name': 'Dinesh Jayaraman'},\n",
       "     {'name': 'Osbert Bastani'},\n",
       "     {'name': 'Vikash Kumar'},\n",
       "     {'name': 'Amy Zhang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project website: https://sites.google.com/view/vip-rl'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.00030v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.00030v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.01597v2',\n",
       "    'updated': '2022-10-05T11:42:42Z',\n",
       "    'published': '2022-10-04T13:22:19Z',\n",
       "    'title': 'ROAD-R: The Autonomous Driving Dataset with Logical Requirements',\n",
       "    'summary': 'Neural networks have proven to be very powerful at computer vision tasks.\\nHowever, they often exhibit unexpected behaviours, violating known requirements\\nexpressing background knowledge. This calls for models (i) able to learn from\\nthe requirements, and (ii) guaranteed to be compliant with the requirements\\nthemselves. Unfortunately, the development of such models is hampered by the\\nlack of datasets equipped with formally specified requirements. In this paper,\\nwe introduce the ROad event Awareness Dataset with logical Requirements\\n(ROAD-R), the first publicly available dataset for autonomous driving with\\nrequirements expressed as logical constraints. Given ROAD-R, we show that\\ncurrent state-of-the-art models often violate its logical constraints, and that\\nit is possible to exploit them to create models that (i) have a better\\nperformance, and (ii) are guaranteed to be compliant with the requirements\\nthemselves.',\n",
       "    'author': [{'name': 'Eleonora Giunchiglia'},\n",
       "     {'name': 'Mihaela Cătălina Stoian'},\n",
       "     {'name': 'Salman Khan'},\n",
       "     {'name': 'Fabio Cuzzolin'},\n",
       "     {'name': 'Thomas Lukasiewicz'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.01597v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.01597v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.04458v1',\n",
       "    'updated': '2022-10-10T07:01:08Z',\n",
       "    'published': '2022-10-10T07:01:08Z',\n",
       "    'title': 'OGC: Unsupervised 3D Object Segmentation from Rigid Dynamics of Point\\n  Clouds',\n",
       "    'summary': 'In this paper, we study the problem of 3D object segmentation from raw point\\nclouds. Unlike all existing methods which usually require a large amount of\\nhuman annotations for full supervision, we propose the first unsupervised\\nmethod, called OGC, to simultaneously identify multiple 3D objects in a single\\nforward pass, without needing any type of human annotations. The key to our\\napproach is to fully leverage the dynamic motion patterns over sequential point\\nclouds as supervision signals to automatically discover rigid objects. Our\\nmethod consists of three major components, 1) the object segmentation network\\nto directly estimate multi-object masks from a single point cloud frame, 2) the\\nauxiliary self-supervised scene flow estimator, and 3) our core object geometry\\nconsistency component. By carefully designing a series of loss functions, we\\neffectively take into account the multi-object rigid consistency and the object\\nshape invariance in both temporal and spatial scales. This allows our method to\\ntruly discover the object geometry even in the absence of annotations. We\\nextensively evaluate our method on five datasets, demonstrating the superior\\nperformance for object part instance segmentation and general object\\nsegmentation in both indoor and the challenging outdoor scenarios.',\n",
       "    'author': [{'name': 'Ziyang Song'}, {'name': 'Bo Yang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2022. Code and data are available at:\\n  https://github.com/vLAR-group/OGC'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.04458v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.04458v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.04887v1',\n",
       "    'updated': '2022-10-10T17:58:45Z',\n",
       "    'published': '2022-10-10T17:58:45Z',\n",
       "    'title': 'In-Hand Object Rotation via Rapid Motor Adaptation',\n",
       "    'summary': 'Generalized in-hand manipulation has long been an unsolved challenge of\\nrobotics. As a small step towards this grand goal, we demonstrate how to design\\nand learn a simple adaptive controller to achieve in-hand object rotation using\\nonly fingertips. The controller is trained entirely in simulation on only\\ncylindrical objects, which then - without any fine-tuning - can be directly\\ndeployed to a real robot hand to rotate dozens of objects with diverse sizes,\\nshapes, and weights over the z-axis. This is achieved via rapid online\\nadaptation of the controller to the object properties using only proprioception\\nhistory. Furthermore, natural and stable finger gaits automatically emerge from\\ntraining the control policy via reinforcement learning. Code and more videos\\nare available at https://haozhi.io/hora',\n",
       "    'author': [{'name': 'Haozhi Qi'},\n",
       "     {'name': 'Ashish Kumar'},\n",
       "     {'name': 'Roberto Calandra'},\n",
       "     {'name': 'Yi Ma'},\n",
       "     {'name': 'Jitendra Malik'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2022. Code and Website: https://haozhi.io/hora'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.04887v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.04887v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.04932v1',\n",
       "    'updated': '2022-10-10T18:06:30Z',\n",
       "    'published': '2022-10-10T18:06:30Z',\n",
       "    'title': 'NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills\\n  using Neural Radiance Fields',\n",
       "    'summary': 'We present a system for applying sim2real approaches to \"in the wild\" scenes\\nwith realistic visuals, and to policies which rely on active perception using\\nRGB cameras. Given a short video of a static scene collected using a generic\\nphone, we learn the scene\\'s contact geometry and a function for novel view\\nsynthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering\\nof the static scene by overlaying the rendering of other dynamic objects (e.g.\\nthe robot\\'s own body, a ball). A simulation is then created using the rendering\\nengine in a physics simulator which computes contact dynamics from the static\\nscene geometry (estimated from the NeRF volume density) and the dynamic\\nobjects\\' geometry and physical properties (assumed known). We demonstrate that\\nwe can use this simulation to learn vision-based whole body navigation and ball\\npushing policies for a 20 degrees of freedom humanoid robot with an actuated\\nhead-mounted RGB camera, and we successfully transfer these policies to a real\\nrobot. Project video is available at\\nhttps://sites.google.com/view/nerf2real/home',\n",
       "    'author': [{'name': 'Arunkumar Byravan'},\n",
       "     {'name': 'Jan Humplik'},\n",
       "     {'name': 'Leonard Hasenclever'},\n",
       "     {'name': 'Arthur Brussee'},\n",
       "     {'name': 'Francesco Nori'},\n",
       "     {'name': 'Tuomas Haarnoja'},\n",
       "     {'name': 'Ben Moran'},\n",
       "     {'name': 'Steven Bohez'},\n",
       "     {'name': 'Fereshteh Sadeghi'},\n",
       "     {'name': 'Bojan Vujatovic'},\n",
       "     {'name': 'Nicolas Heess'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.04932v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.04932v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.07450v1',\n",
       "    'updated': '2022-10-14T01:32:15Z',\n",
       "    'published': '2022-10-14T01:32:15Z',\n",
       "    'title': 'ExAug: Robot-Conditioned Navigation Policies via Geometric Experience\\n  Augmentation',\n",
       "    'summary': 'Machine learning techniques rely on large and diverse datasets for\\ngeneralization. Computer vision, natural language processing, and other\\napplications can often reuse public datasets to train many different models.\\nHowever, due to differences in physical configurations, it is challenging to\\nleverage public datasets for training robotic control policies on new robot\\nplatforms or for new tasks. In this work, we propose a novel framework, ExAug\\nto augment the experiences of different robot platforms from multiple datasets\\nin diverse environments. ExAug leverages a simple principle: by extracting 3D\\ninformation in the form of a point cloud, we can create much more complex and\\nstructured augmentations, utilizing both generating synthetic images and\\ngeometric-aware penalization that would have been suitable in the same\\nsituation for a different robot, with different size, turning radius, and\\ncamera placement. The trained policy is evaluated on two new robot platforms\\nwith three different cameras in indoor and outdoor environments with obstacles.',\n",
       "    'author': [{'name': 'Noriaki Hirose'},\n",
       "     {'name': 'Dhruv Shah'},\n",
       "     {'name': 'Ajay Sridhar'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages, 9 figures, 2 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.07450v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.07450v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.09834v1',\n",
       "    'updated': '2022-10-17T08:05:38Z',\n",
       "    'published': '2022-10-17T08:05:38Z',\n",
       "    'title': 'Learning Less Generalizable Patterns with an Asymmetrically Trained\\n  Double Classifier for Better Test-Time Adaptation',\n",
       "    'summary': 'Deep neural networks often fail to generalize outside of their training\\ndistribution, in particular when only a single data domain is available during\\ntraining. While test-time adaptation has yielded encouraging results in this\\nsetting, we argue that, to reach further improvements, these approaches should\\nbe combined with training procedure modifications aiming to learn a more\\ndiverse set of patterns. Indeed, test-time adaptation methods usually have to\\nrely on a limited representation because of the shortcut learning phenomenon:\\nonly a subset of the available predictive patterns is learned with standard\\ntraining. In this paper, we first show that the combined use of existing\\ntraining-time strategies, and test-time batch normalization, a simple\\nadaptation method, does not always improve upon the test-time adaptation alone\\non the PACS benchmark. Furthermore, experiments on Office-Home show that very\\nfew training-time methods improve upon standard training, with or without\\ntest-time batch normalization. We therefore propose a novel approach using a\\npair of classifiers and a shortcut patterns avoidance loss that mitigates the\\nshortcut learning behavior by reducing the generalization ability of the\\nsecondary classifier, using the additional shortcut patterns avoidance loss\\nthat encourages the learning of samples specific patterns. The primary\\nclassifier is trained normally, resulting in the learning of both the natural\\nand the more complex, less generalizable, features. Our experiments show that\\nour method improves upon the state-of-the-art results on both benchmarks and\\nbenefits the most to test-time batch normalization.',\n",
       "    'author': [{'name': 'Thomas Duboudin',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'imagine'}},\n",
       "     {'name': 'Emmanuel Dellandréa'},\n",
       "     {'name': 'Corentin Abgrall'},\n",
       "     {'name': 'Gilles Hénaff'},\n",
       "     {'name': 'Liming Chen'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.09834v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.09834v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.10047v3',\n",
       "    'updated': '2022-12-15T23:47:39Z',\n",
       "    'published': '2022-10-18T17:59:55Z',\n",
       "    'title': 'From Play to Policy: Conditional Behavior Generation from Uncurated\\n  Robot Data',\n",
       "    'summary': 'While large-scale sequence modeling from offline data has led to impressive\\nperformance gains in natural language and image generation, directly\\ntranslating such ideas to robotics has been challenging. One critical reason\\nfor this is that uncurated robot demonstration data, i.e. play data, collected\\nfrom non-expert human demonstrators are often noisy, diverse, and\\ndistributionally multi-modal. This makes extracting useful, task-centric\\nbehaviors from such data a difficult generative modeling problem. In this work,\\nwe present Conditional Behavior Transformers (C-BeT), a method that combines\\nthe multi-modal generation ability of Behavior Transformer with\\nfuture-conditioned goal specification. On a suite of simulated benchmark tasks,\\nwe find that C-BeT improves upon prior state-of-the-art work in learning from\\nplay data by an average of 45.7%. Further, we demonstrate for the first time\\nthat useful task-centric behaviors can be learned on a real-world robot purely\\nfrom play data without any task labels or reward information. Robot videos are\\nbest viewed on our project website: https://play-to-policy.github.io',\n",
       "    'author': [{'name': 'Zichen Jeff Cui'},\n",
       "     {'name': 'Yibin Wang'},\n",
       "     {'name': 'Nur Muhammad Mahi Shafiullah'},\n",
       "     {'name': 'Lerrel Pinto'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Code and data available at: https://play-to-policy.github.io; (fixed\\n  metadata author name format)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.10047v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.10047v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.12126v2',\n",
       "    'updated': '2022-11-06T01:32:26Z',\n",
       "    'published': '2022-10-21T17:33:14Z',\n",
       "    'title': 'Neural Fields for Robotic Object Manipulation from a Single Image',\n",
       "    'summary': 'We present a unified and compact representation for object rendering, 3D\\nreconstruction, and grasp pose prediction that can be inferred from a single\\nimage within a few seconds. We achieve this by leveraging recent advances in\\nthe Neural Radiance Field (NeRF) literature that learn category-level priors\\nand fine-tune on novel objects with minimal data and time. Our insight is that\\nwe can learn a compact shape representation and extract meaningful additional\\ninformation from it, such as grasping poses. We believe this to be the first\\nwork to retrieve grasping poses directly from a NeRF-based representation using\\na single viewpoint (RGB-only), rather than going through a secondary network\\nand/or representation. When compared to prior art, our method is two to three\\norders of magnitude smaller while achieving comparable performance at view\\nreconstruction and grasping. Accompanying our method, we also propose a new\\ndataset of rendered shoes for training a sim-2-real NeRF method with grasping\\nposes for different widths of grippers.',\n",
       "    'author': [{'name': 'Valts Blukis'},\n",
       "     {'name': 'Taeyeop Lee'},\n",
       "     {'name': 'Jonathan Tremblay'},\n",
       "     {'name': 'Bowen Wen'},\n",
       "     {'name': 'In So Kweon'},\n",
       "     {'name': 'Kuk-Jin Yoon'},\n",
       "     {'name': 'Dieter Fox'},\n",
       "     {'name': 'Stan Birchfield'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICRA 2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.12126v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.12126v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.14791v2',\n",
       "    'updated': '2023-01-14T20:19:59Z',\n",
       "    'published': '2022-10-26T15:38:28Z',\n",
       "    'title': 'ViNL: Visual Navigation and Locomotion Over Obstacles',\n",
       "    'summary': 'We present Visual Navigation and Locomotion over obstacles (ViNL), which\\nenables a quadrupedal robot to navigate unseen apartments while stepping over\\nsmall obstacles that lie in its path (e.g., shoes, toys, cables), similar to\\nhow humans and pets lift their feet over objects as they walk. ViNL consists\\nof: (1) a visual navigation policy that outputs linear and angular velocity\\ncommands that guides the robot to a goal coordinate in unfamiliar indoor\\nenvironments; and (2) a visual locomotion policy that controls the robot\\'s\\njoints to avoid stepping on obstacles while following provided velocity\\ncommands. Both the policies are entirely \"model-free\", i.e. sensors-to-actions\\nneural networks trained end-to-end. The two are trained independently in two\\nentirely different simulators and then seamlessly co-deployed by feeding the\\nvelocity commands from the navigator to the locomotor, entirely \"zero-shot\"\\n(without any co-training). While prior works have developed learning methods\\nfor visual navigation or visual locomotion, to the best of our knowledge, this\\nis the first fully learned approach that leverages vision to accomplish both\\n(1) intelligent navigation in new environments, and (2) intelligent visual\\nlocomotion that aims to traverse cluttered environments without disrupting\\nobstacles. On the task of navigation to distant goals in unknown environments,\\nViNL using just egocentric vision significantly outperforms prior work on\\nrobust locomotion using privileged terrain maps (+32.8% success and -4.42\\ncollisions per meter). Additionally, we ablate our locomotion policy to show\\nthat each aspect of our approach helps reduce obstacle collisions. Videos and\\ncode at http://www.joannetruong.com/projects/vinl.html',\n",
       "    'author': [{'name': 'Simar Kareer'},\n",
       "     {'name': 'Naoki Yokoyama'},\n",
       "     {'name': 'Dhruv Batra'},\n",
       "     {'name': 'Sehoon Ha'},\n",
       "     {'name': 'Joanne Truong'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.14791v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.14791v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.17421v1',\n",
       "    'updated': '2022-10-28T16:28:26Z',\n",
       "    'published': '2022-10-28T16:28:26Z',\n",
       "    'title': 'I am Only Happy When There is Light: The Impact of Environmental Changes\\n  on Affective Facial Expressions Recognition',\n",
       "    'summary': 'Human-robot interaction (HRI) benefits greatly from advances in the machine\\nlearning field as it allows researchers to employ high-performance models for\\nperceptual tasks like detection and recognition. Especially deep learning\\nmodels, either pre-trained for feature extraction or used for classification,\\nare now established methods to characterize human behaviors in HRI scenarios\\nand to have social robots that understand better those behaviors. As HRI\\nexperiments are usually small-scale and constrained to particular lab\\nenvironments, the questions are how well can deep learning models generalize to\\nspecific interaction scenarios, and further, how good is their robustness\\ntowards environmental changes? These questions are important to address if the\\nHRI field wishes to put social robotic companions into real environments acting\\nconsistently, i.e. changing lighting conditions or moving people should still\\nproduce the same recognition results. In this paper, we study the impact of\\ndifferent image conditions on the recognition of arousal and valence from human\\nfacial expressions using the FaceChannel framework \\\\cite{Barro20}. Our results\\nshow how the interpretation of human affective states can differ greatly in\\neither the positive or negative direction even when changing only slightly the\\nimage properties. We conclude the paper with important points to consider when\\nemploying deep learning models to ensure sound interpretation of HRI\\nexperiments.',\n",
       "    'author': [{'name': 'Doreen Jirak'},\n",
       "     {'name': 'Alessandra Sciutti'},\n",
       "     {'name': 'Pablo Barros'},\n",
       "     {'name': 'Francesco Rea'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Paper contribution to the Social and Cognitive Interactions for\\n  Assistive Robotics Workshop (SCIAR) In Conjunction with IROS 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.17421v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.17421v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.00191v1',\n",
       "    'updated': '2022-10-31T23:17:59Z',\n",
       "    'published': '2022-10-31T23:17:59Z',\n",
       "    'title': 'Edge Grasp Network: A Graph-Based SE(3)-invariant Approach to Grasp\\n  Detection',\n",
       "    'summary': 'Given point cloud input, the problem of 6-DoF grasp pose detection is to\\nidentify a set of hand poses in SE(3) from which an object can be successfully\\ngrasped. This important problem has many practical applications. Here we\\npropose a novel method and neural network model that enables better grasp\\nsuccess rates relative to what is available in the literature. The method takes\\nstandard point cloud data as input and works well with single-view point clouds\\nobserved from arbitrary viewing directions.',\n",
       "    'author': [{'name': 'Haojie Huang'},\n",
       "     {'name': 'Dian Wang'},\n",
       "     {'name': 'Xupeng Zhu'},\n",
       "     {'name': 'Robin Walters'},\n",
       "     {'name': 'Robert Platt'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'https://haojhuang.github.io/edge_grasp_page/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.00191v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.00191v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.09019v1',\n",
       "    'updated': '2022-11-16T16:26:48Z',\n",
       "    'published': '2022-11-16T16:26:48Z',\n",
       "    'title': 'Learning Reward Functions for Robotic Manipulation by Observing Humans',\n",
       "    'summary': 'Observing a human demonstrator manipulate objects provides a rich, scalable\\nand inexpensive source of data for learning robotic policies. However,\\ntransferring skills from human videos to a robotic manipulator poses several\\nchallenges, not least a difference in action and observation spaces. In this\\nwork, we use unlabeled videos of humans solving a wide range of manipulation\\ntasks to learn a task-agnostic reward function for robotic manipulation\\npolicies. Thanks to the diversity of this training data, the learned reward\\nfunction sufficiently generalizes to image observations from a previously\\nunseen robot embodiment and environment to provide a meaningful prior for\\ndirected exploration in reinforcement learning. The learned rewards are based\\non distances to a goal in an embedding space learned using a time-contrastive\\nobjective. By conditioning the function on a goal image, we are able to reuse\\none model across a variety of tasks. Unlike prior work on leveraging human\\nvideos to teach robots, our method, Human Offline Learned Distances (HOLD)\\nrequires neither a priori data from the robot environment, nor a set of\\ntask-specific human demonstrations, nor a predefined notion of correspondence\\nacross morphologies, yet it is able to accelerate training of several\\nmanipulation tasks on a simulated robot arm compared to using only a sparse\\nreward obtained from task completion.',\n",
       "    'author': [{'name': 'Minttu Alakuijala'},\n",
       "     {'name': 'Gabriel Dulac-Arnold'},\n",
       "     {'name': 'Julien Mairal'},\n",
       "     {'name': 'Jean Ponce'},\n",
       "     {'name': 'Cordelia Schmid'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.09019v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.09019v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.13508v2',\n",
       "    'updated': '2022-11-28T18:25:26Z',\n",
       "    'published': '2022-11-24T09:59:13Z',\n",
       "    'title': '1st Workshop on Maritime Computer Vision (MaCVi) 2023: Challenge Results',\n",
       "    'summary': 'The 1$^{\\\\text{st}}$ Workshop on Maritime Computer Vision (MaCVi) 2023 focused\\non maritime computer vision for Unmanned Aerial Vehicles (UAV) and Unmanned\\nSurface Vehicle (USV), and organized several subchallenges in this domain: (i)\\nUAV-based Maritime Object Detection, (ii) UAV-based Maritime Object Tracking,\\n(iii) USV-based Maritime Obstacle Segmentation and (iv) USV-based Maritime\\nObstacle Detection. The subchallenges were based on the SeaDronesSee and MODS\\nbenchmarks. This report summarizes the main findings of the individual\\nsubchallenges and introduces a new benchmark, called SeaDronesSee Object\\nDetection v2, which extends the previous benchmark by including more classes\\nand footage. We provide statistical and qualitative analyses, and assess trends\\nin the best-performing methodologies of over 130 submissions. The methods are\\nsummarized in the appendix. The datasets, evaluation code and the leaderboard\\nare publicly available at https://seadronessee.cs.uni-tuebingen.de/macvi.',\n",
       "    'author': [{'name': 'Benjamin Kiefer'},\n",
       "     {'name': 'Matej Kristan'},\n",
       "     {'name': 'Janez Perš'},\n",
       "     {'name': 'Lojze Žust'},\n",
       "     {'name': 'Fabio Poiesi'},\n",
       "     {'name': 'Fabio Augusto de Alcantara Andrade'},\n",
       "     {'name': 'Alexandre Bernardino'},\n",
       "     {'name': 'Matthew Dawkins'},\n",
       "     {'name': 'Jenni Raitoharju'},\n",
       "     {'name': 'Yitong Quan'},\n",
       "     {'name': 'Adem Atmaca'},\n",
       "     {'name': 'Timon Höfer'},\n",
       "     {'name': 'Qiming Zhang'},\n",
       "     {'name': 'Yufei Xu'},\n",
       "     {'name': 'Jing Zhang'},\n",
       "     {'name': 'Dacheng Tao'},\n",
       "     {'name': 'Lars Sommer'},\n",
       "     {'name': 'Raphael Spraul'},\n",
       "     {'name': 'Hangyue Zhao'},\n",
       "     {'name': 'Hongpu Zhang'},\n",
       "     {'name': 'Yanyun Zhao'},\n",
       "     {'name': 'Jan Lukas Augustin'},\n",
       "     {'name': 'Eui-ik Jeon'},\n",
       "     {'name': 'Impyeong Lee'},\n",
       "     {'name': 'Luca Zedda'},\n",
       "     {'name': 'Andrea Loddo'},\n",
       "     {'name': 'Cecilia Di Ruberto'},\n",
       "     {'name': 'Sagar Verma'},\n",
       "     {'name': 'Siddharth Gupta'},\n",
       "     {'name': 'Shishir Muralidhara'},\n",
       "     {'name': 'Niharika Hegde'},\n",
       "     {'name': 'Daitao Xing'},\n",
       "     {'name': 'Nikolaos Evangeliou'},\n",
       "     {'name': 'Anthony Tzes'},\n",
       "     {'name': 'Vojtěch Bartl'},\n",
       "     {'name': 'Jakub Špaňhel'},\n",
       "     {'name': 'Adam Herout'},\n",
       "     {'name': 'Neelanjan Bhowmik'},\n",
       "     {'name': 'Toby P. Breckon'},\n",
       "     {'name': 'Shivanand Kundargi'},\n",
       "     {'name': 'Tejas Anvekar'},\n",
       "     {'name': 'Chaitra Desai'},\n",
       "     {'name': 'Ramesh Ashok Tabib'},\n",
       "     {'name': 'Uma Mudengudi'},\n",
       "     {'name': 'Arpita Vats'},\n",
       "     {'name': 'Yang Song'},\n",
       "     {'name': 'Delong Liu'},\n",
       "     {'name': 'Yonglin Li'},\n",
       "     {'name': 'Shuman Li'},\n",
       "     {'name': 'Chenhao Tan'},\n",
       "     {'name': 'Long Lan'},\n",
       "     {'name': 'Vladimir Somers'},\n",
       "     {'name': 'Christophe De Vleeschouwer'},\n",
       "     {'name': 'Alexandre Alahi'},\n",
       "     {'name': 'Hsiang-Wei Huang'},\n",
       "     {'name': 'Cheng-Yen Yang'},\n",
       "     {'name': 'Jenq-Neng Hwang'},\n",
       "     {'name': 'Pyong-Kun Kim'},\n",
       "     {'name': 'Kwangju Kim'},\n",
       "     {'name': 'Kyoungoh Lee'},\n",
       "     {'name': 'Shuai Jiang'},\n",
       "     {'name': 'Haiwen Li'},\n",
       "     {'name': 'Zheng Ziqiang'},\n",
       "     {'name': 'Tuan-Anh Vu'},\n",
       "     {'name': 'Hai Nguyen-Truong'},\n",
       "     {'name': 'Sai-Kit Yeung'},\n",
       "     {'name': 'Zhuang Jia'},\n",
       "     {'name': 'Sophia Yang'},\n",
       "     {'name': 'Chih-Chung Hsu'},\n",
       "     {'name': 'Xiu-Yu Hou'},\n",
       "     {'name': 'Yu-An Jhang'},\n",
       "     {'name': 'Simon Yang'},\n",
       "     {'name': 'Mau-Tsuen Yang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'MaCVi 2023 was part of WACV 2023. This report (38 pages) discusses\\n  the competition as part of MaCVi'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.13508v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.13508v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.13858v1',\n",
       "    'updated': '2022-11-25T02:07:57Z',\n",
       "    'published': '2022-11-25T02:07:57Z',\n",
       "    'title': 'Far3Det: Towards Far-Field 3D Detection',\n",
       "    'summary': 'We focus on the task of far-field 3D detection (Far3Det) of objects beyond a\\ncertain distance from an observer, e.g., $>$50m. Far3Det is particularly\\nimportant for autonomous vehicles (AVs) operating at highway speeds, which\\nrequire detections of far-field obstacles to ensure sufficient braking\\ndistances. However, contemporary AV benchmarks such as nuScenes underemphasize\\nthis problem because they evaluate performance only up to a certain distance\\n(50m). One reason is that obtaining far-field 3D annotations is difficult,\\nparticularly for lidar sensors that produce very few point returns for far-away\\nobjects. Indeed, we find that almost 50% of far-field objects (beyond 50m)\\ncontain zero lidar points. Secondly, current metrics for 3D detection employ a\\n\"one-size-fits-all\" philosophy, using the same tolerance thresholds for near\\nand far objects, inconsistent with tolerances for both human vision and stereo\\ndisparities. Both factors lead to an incomplete analysis of the Far3Det task.\\nFor example, while conventional wisdom tells us that high-resolution RGB\\nsensors should be vital for 3D detection of far-away objects, lidar-based\\nmethods still rank higher compared to RGB counterparts on the current benchmark\\nleaderboards. As a first step towards a Far3Det benchmark, we develop a method\\nto find well-annotated scenes from the nuScenes dataset and derive a\\nwell-annotated far-field validation set. We also propose a Far3Det evaluation\\nprotocol and explore various 3D detection methods for Far3Det. Our result\\nconvincingly justifies the long-held conventional wisdom that high-resolution\\nRGB improves 3D detection in the far-field. We further propose a simple yet\\neffective method that fuses detections from RGB and lidar detectors based on\\nnon-maximum suppression, which remarkably outperforms state-of-the-art 3D\\ndetectors in the far-field.',\n",
       "    'author': [{'name': 'Shubham Gupta'},\n",
       "     {'name': 'Jeet Kanjani'},\n",
       "     {'name': 'Mengtian Li'},\n",
       "     {'name': 'Francesco Ferroni'},\n",
       "     {'name': 'James Hays'},\n",
       "     {'name': 'Deva Ramanan'},\n",
       "     {'name': 'Shu Kong'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'WACV 2023 12 Pages, 8 Figures, 10 Tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.13858v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.13858v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.00767v1',\n",
       "    'updated': '2022-12-01T18:52:46Z',\n",
       "    'published': '2022-12-01T18:52:46Z',\n",
       "    'title': 'Exploiting Socially-Aware Tasks for Embodied Social Navigation',\n",
       "    'summary': 'Learning how to navigate among humans in an occluded and spatially\\nconstrained indoor environment, is a key ability required to embodied agent to\\nbe integrated into our society. In this paper, we propose an end-to-end\\narchitecture that exploits Socially-Aware Tasks (referred as to Risk and Social\\nCompass) to inject into a reinforcement learning navigation policy the ability\\nto infer common-sense social behaviors. To this end, our tasks exploit the\\nnotion of immediate and future dangers of collision. Furthermore, we propose an\\nevaluation protocol specifically designed for the Social Navigation Task in\\nsimulated environments. This is done to capture fine-grained features and\\ncharacteristics of the policy by analyzing the minimal unit of human-robot\\nspatial interaction, called Encounter. We validate our approach on Gibson4+ and\\nHabitat-Matterport3D datasets.',\n",
       "    'author': [{'name': 'Enrico Cancelli'},\n",
       "     {'name': 'Tommaso Campari'},\n",
       "     {'name': 'Luciano Serafini'},\n",
       "     {'name': 'Angel X. Chang'},\n",
       "     {'name': 'Lamberto Ballan'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.00767v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.00767v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.04581v1',\n",
       "    'updated': '2022-12-08T22:11:49Z',\n",
       "    'published': '2022-12-08T22:11:49Z',\n",
       "    'title': 'PALMER: Perception-Action Loop with Memory for Long-Horizon Planning',\n",
       "    'summary': 'To achieve autonomy in a priori unknown real-world scenarios, agents should\\nbe able to: i) act from high-dimensional sensory observations (e.g., images),\\nii) learn from past experience to adapt and improve, and iii) be capable of\\nlong horizon planning. Classical planning algorithms (e.g. PRM, RRT) are\\nproficient at handling long-horizon planning. Deep learning based methods in\\nturn can provide the necessary representations to address the others, by\\nmodeling statistical contingencies between observations. In this direction, we\\nintroduce a general-purpose planning algorithm called PALMER that combines\\nclassical sampling-based planning algorithms with learning-based perceptual\\nrepresentations. For training these perceptual representations, we combine\\nQ-learning with contrastive representation learning to create a latent space\\nwhere the distance between the embeddings of two states captures how easily an\\noptimal policy can traverse between them. For planning with these perceptual\\nrepresentations, we re-purpose classical sampling-based planning algorithms to\\nretrieve previously observed trajectory segments from a replay buffer and\\nrestitch them into approximately optimal paths that connect any given pair of\\nstart and goal states. This creates a tight feedback loop between\\nrepresentation learning, memory, reinforcement learning, and sampling-based\\nplanning. The end result is an experiential framework for long-horizon planning\\nthat is significantly more robust and sample efficient compared to existing\\nmethods.',\n",
       "    'author': [{'name': 'Onur Beker'},\n",
       "     {'name': 'Mohammad Mohammadi'},\n",
       "     {'name': 'Amir Zamir'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Website: https://palmer.epfl.ch'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.04581v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.04581v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.06437v1',\n",
       "    'updated': '2022-12-13T09:05:21Z',\n",
       "    'published': '2022-12-13T09:05:21Z',\n",
       "    'title': 'DiffStack: A Differentiable and Modular Control Stack for Autonomous\\n  Vehicles',\n",
       "    'summary': 'Autonomous vehicle (AV) stacks are typically built in a modular fashion, with\\nexplicit components performing detection, tracking, prediction, planning,\\ncontrol, etc. While modularity improves reusability, interpretability, and\\ngeneralizability, it also suffers from compounding errors, information\\nbottlenecks, and integration challenges. To overcome these challenges, a\\nprominent approach is to convert the AV stack into an end-to-end neural network\\nand train it with data. While such approaches have achieved impressive results,\\nthey typically lack interpretability and reusability, and they eschew\\nprincipled analytical components, such as planning and control, in favor of\\ndeep neural networks. To enable the joint optimization of AV stacks while\\nretaining modularity, we present DiffStack, a differentiable and modular stack\\nfor prediction, planning, and control. Crucially, our model-based planning and\\ncontrol algorithms leverage recent advancements in differentiable optimization\\nto produce gradients, enabling optimization of upstream components, such as\\nprediction, via backpropagation through planning and control. Our results on\\nthe nuScenes dataset indicate that end-to-end training with DiffStack yields\\nsubstantial improvements in open-loop and closed-loop planning metrics by,\\ne.g., learning to make fewer prediction errors that would affect planning.\\nBeyond these immediate benefits, DiffStack opens up new opportunities for fully\\ndata-driven yet modular and interpretable AV architectures. Project website:\\nhttps://sites.google.com/view/diffstack',\n",
       "    'author': [{'name': 'Peter Karkus'},\n",
       "     {'name': 'Boris Ivanovic'},\n",
       "     {'name': 'Shie Mannor'},\n",
       "     {'name': 'Marco Pavone'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CoRL 2022 camera ready'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.06437v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.06437v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2212.09902v1',\n",
       "    'updated': '2022-12-19T22:50:40Z',\n",
       "    'published': '2022-12-19T22:50:40Z',\n",
       "    'title': 'Dexterous Manipulation from Images: Autonomous Real-World RL via Substep\\n  Guidance',\n",
       "    'summary': 'Complex and contact-rich robotic manipulation tasks, particularly those that\\ninvolve multi-fingered hands and underactuated object manipulation, present a\\nsignificant challenge to any control method. Methods based on reinforcement\\nlearning offer an appealing choice for such settings, as they can enable robots\\nto learn to delicately balance contact forces and dexterously reposition\\nobjects without strong modeling assumptions. However, running reinforcement\\nlearning on real-world dexterous manipulation systems often requires\\nsignificant manual engineering. This negates the benefits of autonomous data\\ncollection and ease of use that reinforcement learning should in principle\\nprovide. In this paper, we describe a system for vision-based dexterous\\nmanipulation that provides a \"programming-free\" approach for users to define\\nnew tasks and enable robots with complex multi-fingered hands to learn to\\nperform them through interaction. The core principle underlying our system is\\nthat, in a vision-based setting, users should be able to provide high-level\\nintermediate supervision that circumvents challenges in teleoperation or\\nkinesthetic teaching which allow a robot to not only learn a task efficiently\\nbut also to autonomously practice. Our system includes a framework for users to\\ndefine a final task and intermediate sub-tasks with image examples, a\\nreinforcement learning procedure that learns the task autonomously without\\ninterventions, and experimental results with a four-finger robotic hand\\nlearning multi-stage object manipulation tasks directly in the real world,\\nwithout simulation, manual modeling, or reward engineering.',\n",
       "    'author': [{'name': 'Kelvin Xu'},\n",
       "     {'name': 'Zheyuan Hu'},\n",
       "     {'name': 'Ria Doshi'},\n",
       "     {'name': 'Aaron Rovinsky'},\n",
       "     {'name': 'Vikash Kumar'},\n",
       "     {'name': 'Abhishek Gupta'},\n",
       "     {'name': 'Sergey Levine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'First two authors contributed equally'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2212.09902v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2212.09902v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.00452v1',\n",
       "    'updated': '2023-01-01T18:05:25Z',\n",
       "    'published': '2023-01-01T18:05:25Z',\n",
       "    'title': 'Human-in-the-loop Embodied Intelligence with Interactive Simulation\\n  Environment for Surgical Robot Learning',\n",
       "    'summary': 'Surgical robot automation has attracted increasing research interest over the\\npast decade, expecting its huge potential to benefit surgeons, nurses and\\npatients. Recently, the learning paradigm of embodied AI has demonstrated\\npromising ability to learn good control policies for various complex tasks,\\nwhere embodied AI simulators play an essential role to facilitate relevant\\nresearchers. However, existing open-sourced simulators for surgical robot are\\nstill not sufficiently supporting human interactions through physical input\\ndevices, which further limits effective investigations on how human\\ndemonstrations would affect policy learning. In this paper, we study\\nhuman-in-the-loop embodied intelligence with a new interactive simulation\\nplatform for surgical robot learning. Specifically, we establish our platform\\nbased on our previously released SurRoL simulator with several new features\\nco-developed to allow high-quality human interaction via an input device. With\\nthese, we further propose to collect human demonstrations and imitate the\\naction patterns to achieve more effective policy learning. We showcase the\\nimprovement of our simulation environment with the designed new features and\\ntasks, and validate state-of-the-art reinforcement learning algorithms using\\nthe interactive environment. Promising results are obtained, with which we hope\\nto pave the way for future research on surgical embodied intelligence. Our\\nplatform is released and will be continuously updated in the website:\\nhttps://med-air.github.io/SurRoL/',\n",
       "    'author': [{'name': 'Yonghao Long'},\n",
       "     {'name': 'Wang Wei'},\n",
       "     {'name': 'Tao Huang'},\n",
       "     {'name': 'Yuehao Wang'},\n",
       "     {'name': 'Qi Dou'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICRA 2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.00452v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.00452v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.03512v1',\n",
       "    'updated': '2023-01-09T17:05:28Z',\n",
       "    'published': '2023-01-09T17:05:28Z',\n",
       "    'title': 'SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural\\n  Networks',\n",
       "    'summary': 'Understanding traffic scenes requires considering heterogeneous information\\nabout dynamic agents and the static infrastructure. In this work we propose\\nSCENE, a methodology to encode diverse traffic scenes in heterogeneous graphs\\nand to reason about these graphs using a heterogeneous Graph Neural Network\\nencoder and task-specific decoders. The heterogeneous graphs, whose structures\\nare defined by an ontology, consist of different nodes with type-specific node\\nfeatures and different relations with type-specific edge features. In order to\\nexploit all the information given by these graphs, we propose to use cascaded\\nlayers of graph convolution. The result is an encoding of the scene.\\nTask-specific decoders can be applied to predict desired attributes of the\\nscene. Extensive evaluation on two diverse binary node classification tasks\\nshow the main strength of this methodology: despite being generic, it even\\nmanages to outperform task-specific baselines. The further application of our\\nmethodology to the task of node classification in various knowledge graphs\\nshows its transferability to other domains.',\n",
       "    'author': [{'name': 'Thomas Monninger'},\n",
       "     {'name': 'Julian Schmidt'},\n",
       "     {'name': 'Jan Rupprecht'},\n",
       "     {'name': 'David Raba'},\n",
       "     {'name': 'Julian Jordan'},\n",
       "     {'name': 'Daniel Frank'},\n",
       "     {'name': 'Steffen Staab'},\n",
       "     {'name': 'Klaus Dietmayer'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2023.3234771'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2023.3234771',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2301.03512v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.03512v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Thomas Monninger and Julian Schmidt are co-first authors. The order\\n  was determined alphabetically'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters (RA-L), 2023'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.03844v1',\n",
       "    'updated': '2023-01-10T08:25:24Z',\n",
       "    'published': '2023-01-10T08:25:24Z',\n",
       "    'title': 'Look Beyond Bias with Entropic Adversarial Data Augmentation',\n",
       "    'summary': \"Deep neural networks do not discriminate between spurious and causal\\npatterns, and will only learn the most predictive ones while ignoring the\\nothers. This shortcut learning behaviour is detrimental to a network's ability\\nto generalize to an unknown test-time distribution in which the spurious\\ncorrelations do not hold anymore. Debiasing methods were developed to make\\nnetworks robust to such spurious biases but require to know in advance if a\\ndataset is biased and make heavy use of minority counterexamples that do not\\ndisplay the majority bias of their class. In this paper, we argue that such\\nsamples should not be necessarily needed because the ''hidden'' causal\\ninformation is often also contained in biased images. To study this idea, we\\npropose 3 publicly released synthetic classification benchmarks, exhibiting\\npredictive classification shortcuts, each of a different and challenging\\nnature, without any minority samples acting as counterexamples. First, we\\ninvestigate the effectiveness of several state-of-the-art strategies on our\\nbenchmarks and show that they do not yield satisfying results on them. Then, we\\npropose an architecture able to succeed on our benchmarks, despite their\\nunusual properties, using an entropic adversarial data augmentation training\\nscheme. An encoder-decoder architecture is tasked to produce images that are\\nnot recognized by a classifier, by maximizing the conditional entropy of its\\noutputs, and keep as much as possible of the initial content. A precise control\\nof the information destroyed, via a disentangling process, enables us to remove\\nthe shortcut and leave everything else intact. Furthermore, results competitive\\nwith the state-of-the-art on the BAR dataset ensure the applicability of our\\nmethod in real-life situations.\",\n",
       "    'author': [{'name': 'Thomas Duboudin',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'imagine'}},\n",
       "     {'name': 'Emmanuel Dellandréa'},\n",
       "     {'name': 'Corentin Abgrall'},\n",
       "     {'name': 'Gilles Hénaff'},\n",
       "     {'name': 'Liming Chen'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"International Conference on Pattern Recognition 2022, Aug 2022,\\n  Montr{\\\\'e}al, Canada\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.03844v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.03844v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.08957v1',\n",
       "    'updated': '2023-01-21T14:33:02Z',\n",
       "    'published': '2023-01-21T14:33:02Z',\n",
       "    'title': 'Slice Transformer and Self-supervised Learning for 6DoF Localization in\\n  3D Point Cloud Maps',\n",
       "    'summary': 'Precise localization is critical for autonomous vehicles. We present a\\nself-supervised learning method that employs Transformers for the first time\\nfor the task of outdoor localization using LiDAR data. We propose a pre-text\\ntask that reorganizes the slices of a $360^\\\\circ$ LiDAR scan to leverage its\\naxial properties. Our model, called Slice Transformer, employs multi-head\\nattention while systematically processing the slices. To the best of our\\nknowledge, this is the first instance of leveraging multi-head attention for\\noutdoor point clouds. We additionally introduce the Perth-WA dataset, which\\nprovides a large-scale LiDAR map of Perth city in Western Australia, covering\\n$\\\\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The\\nproposed localization method is thoroughly evaluated on Perth-WA and\\nAppollo-SouthBay datasets. We also establish the efficacy of our\\nself-supervised learning approach for the common downstream task of object\\nclassification using ModelNet40 and ScanNN datasets. The code and Perth-WA data\\nwill be publicly released.',\n",
       "    'author': [{'name': 'Muhammad Ibrahim'},\n",
       "     {'name': 'Naveed Akhtar'},\n",
       "     {'name': 'Saeed Anwar'},\n",
       "     {'name': 'Michael Wise'},\n",
       "     {'name': 'Ajmal Mian'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted in IEEE International Conference on Robotics and Automation\\n  (ICRA), 2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.08957v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.08957v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.04232v1',\n",
       "    'updated': '2019-06-10T19:04:09Z',\n",
       "    'published': '2019-06-10T19:04:09Z',\n",
       "    'title': 'BowNet: Dilated Convolution Neural Network for Ultrasound Tongue Contour\\n  Extraction',\n",
       "    'summary': 'Ultrasound imaging is safe, relatively affordable, and capable of real-time\\nperformance. One application of this technology is to visualize and to\\ncharacterize human tongue shape and motion during a real-time speech to study\\nhealthy or impaired speech production. Due to the noisy nature of ultrasound\\nimages with low-contrast characteristic, it might require expertise for\\nnon-expert users to recognize organ shape such as tongue surface (dorsum). To\\nalleviate this difficulty for quantitative analysis of tongue shape and motion,\\ntongue surface can be extracted, tracked, and visualized instead of the whole\\ntongue region. Delineating the tongue surface from each frame is a cumbersome,\\nsubjective, and error-prone task. Furthermore, the rapidity and complexity of\\ntongue gestures have made it a challenging task, and manual segmentation is not\\na feasible solution for real-time applications. Employing the power of\\nstate-of-the-art deep neural network models and training techniques, it is\\nfeasible to implement new fully-automatic, accurate, and robust segmentation\\nmethods with the capability of real-time performance, applicable for tracking\\nof the tongue contours during the speech. This paper presents two novel deep\\nneural network models named BowNet and wBowNet benefits from the ability of\\nglobal prediction of decoding-encoding models, with integrated multi-scale\\ncontextual information, and capability of full-resolution (local) extraction of\\ndilated convolutions. Experimental results using several ultrasound tongue\\nimage datasets revealed that the combination of both localization and\\nglobalization searching could improve prediction result significantly.\\nAssessment of BowNet models using both qualitatively and quantitatively studies\\nshowed them outstanding achievements in terms of accuracy and robustness in\\ncomparison with similar techniques.',\n",
       "    'author': [{'name': 'M. Hamed Mozaffari'}, {'name': 'Won-Sook Lee'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1121/1.5137212'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1121/1.5137212',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1906.04232v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.04232v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '23 pages, 15 figures, 10 tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'BowNet: Dilated convolutional neural network for ultrasound tongue\\n  contour extraction, 2019, The Journal of the Acoustical Society of America,\\n  pages 2940-2941, volume 146, number 4'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.IV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.IV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1908.07656v2',\n",
       "    'updated': '2019-12-01T03:30:37Z',\n",
       "    'published': '2019-08-16T16:40:49Z',\n",
       "    'title': 'Survey on Deep Neural Networks in Speech and Vision Systems',\n",
       "    'summary': 'This survey presents a review of state-of-the-art deep neural network\\narchitectures, algorithms, and systems in vision and speech applications.\\nRecent advances in deep artificial neural network algorithms and architectures\\nhave spurred rapid innovation and development of intelligent vision and speech\\nsystems. With availability of vast amounts of sensor data and cloud computing\\nfor processing and training of deep neural networks, and with increased\\nsophistication in mobile and embedded technology, the next-generation\\nintelligent systems are poised to revolutionize personal and commercial\\ncomputing. This survey begins by providing background and evolution of some of\\nthe most successful deep learning models for intelligent vision and speech\\nsystems to date. An overview of large-scale industrial research and development\\nefforts is provided to emphasize future trends and prospects of intelligent\\nvision and speech systems. Robust and efficient intelligent systems demand\\nlow-latency and high fidelity in resource-constrained hardware platforms such\\nas mobile devices, robots, and automobiles. Therefore, this survey also\\nprovides a summary of key challenges and recent successes in running deep\\nneural networks on hardware-restricted platforms, i.e. within limited memory,\\nbattery life, and processing capabilities. Finally, emerging applications of\\nvision and speech across disciplines such as affective computing, intelligent\\ntransportation, and precision medicine are discussed. To our knowledge, this\\npaper provides one of the most comprehensive surveys on the latest developments\\nin intelligent vision and speech applications from the perspectives of both\\nsoftware and hardware systems. Many of these emerging technologies using deep\\nneural networks show tremendous promise to revolutionize research and\\ndevelopment for future vision and speech systems.',\n",
       "    'author': [{'name': 'Mahbubul Alam'},\n",
       "     {'name': 'Manar D. Samad'},\n",
       "     {'name': 'Lasitha Vidyaratne'},\n",
       "     {'name': 'Alexander Glandon'},\n",
       "     {'name': 'Khan M. Iftekharuddin'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1908.07656v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1908.07656v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SP', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.00063v2',\n",
       "    'updated': '2021-04-13T11:05:06Z',\n",
       "    'published': '2020-02-28T20:56:24Z',\n",
       "    'title': 'Bio-Inspired Modality Fusion for Active Speaker Detection',\n",
       "    'summary': 'Human beings have developed fantastic abilities to integrate information from\\nvarious sensory sources exploring their inherent complementarity. Perceptual\\ncapabilities are therefore heightened, enabling, for instance, the well-known\\n\"cocktail party\" and McGurk effects, i.e., speech disambiguation from a panoply\\nof sound signals. This fusion ability is also key in refining the perception of\\nsound source location, as in distinguishing whose voice is being heard in a\\ngroup conversation. Furthermore, neuroscience has successfully identified the\\nsuperior colliculus region in the brain as the one responsible for this\\nmodality fusion, with a handful of biological models having been proposed to\\napproach its underlying neurophysiological process. Deriving inspiration from\\none of these models, this paper presents a methodology for effectively fusing\\ncorrelated auditory and visual information for active speaker detection. Such\\nan ability can have a wide range of applications, from teleconferencing systems\\nto social robotics. The detection approach initially routes auditory and visual\\ninformation through two specialized neural network structures. The resulting\\nembeddings are fused via a novel layer based on the superior colliculus, whose\\ntopological structure emulates spatial neuron cross-mapping of unimodal\\nperceptual fields. The validation process employed two publicly available\\ndatasets, with achieved results confirming and greatly surpassing initial\\nexpectations.',\n",
       "    'author': [{'name': 'Gustavo Assunção'},\n",
       "     {'name': 'Nuno Gonçalves'},\n",
       "     {'name': 'Paulo Menezes'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.3390/app11083397'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.3390/app11083397',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2003.00063v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.00063v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Appl. Sci. 2021, 11(8), 3397'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.04835v2',\n",
       "    'updated': '2019-08-06T17:16:22Z',\n",
       "    'published': '2019-05-13T02:24:19Z',\n",
       "    'title': 'Multi-Agent Image Classification via Reinforcement Learning',\n",
       "    'summary': 'We investigate a classification problem using multiple mobile agents capable\\nof collecting (partial) pose-dependent observations of an unknown environment.\\nThe objective is to classify an image over a finite time horizon. We propose a\\nnetwork architecture on how agents should form a local belief, take local\\nactions, and extract relevant features from their raw partial observations.\\nAgents are allowed to exchange information with their neighboring agents to\\nupdate their own beliefs. It is shown how reinforcement learning techniques can\\nbe utilized to achieve decentralized implementation of the classification\\nproblem by running a decentralized consensus protocol. Our experimental results\\non the MNIST handwritten digit dataset demonstrates the effectiveness of our\\nproposed framework.',\n",
       "    'author': [{'name': 'Hossein K. Mousavi'},\n",
       "     {'name': 'Mohammadreza Nazari'},\n",
       "     {'name': 'Martin Takáč'},\n",
       "     {'name': 'Nader Motee'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Preprint of the paper to be published in IROS'19 proceedings\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.04835v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.04835v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.01262v1',\n",
       "    'updated': '2021-08-03T02:56:21Z',\n",
       "    'published': '2021-08-03T02:56:21Z',\n",
       "    'title': 'SABER: Data-Driven Motion Planner for Autonomously Navigating\\n  Heterogeneous Robots',\n",
       "    'summary': 'We present an end-to-end online motion planning framework that uses a\\ndata-driven approach to navigate a heterogeneous robot team towards a global\\ngoal while avoiding obstacles in uncertain environments. First, we use\\nstochastic model predictive control (SMPC) to calculate control inputs that\\nsatisfy robot dynamics, and consider uncertainty during obstacle avoidance with\\nchance constraints. Second, recurrent neural networks are used to provide a\\nquick estimate of future state uncertainty considered in the SMPC finite-time\\nhorizon solution, which are trained on uncertainty outputs of various\\nsimultaneous localization and mapping algorithms. When two or more robots are\\nin communication range, these uncertainties are then updated using a\\ndistributed Kalman filtering approach. Lastly, a Deep Q-learning agent is\\nemployed to serve as a high-level path planner, providing the SMPC with target\\npositions that move the robots towards a desired global goal. Our complete\\nmethods are demonstrated on a ground and aerial robot simultaneously (code\\navailable at: https://github.com/AlexS28/SABER).',\n",
       "    'author': [{'name': 'Alexander Schperberg'},\n",
       "     {'name': 'Stephanie Tsuei'},\n",
       "     {'name': 'Stefano Soatto'},\n",
       "     {'name': 'Dennis Hong'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2021.3103054'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2021.3103054',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2108.01262v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.01262v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE Robotics and Automation Letters (RA-L) 2021.\\n  Pre-print version. The video link of the paper is:\\n  https://www.youtube.com/watch?v=EKCCQtN5Z6A'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1711.11017v1',\n",
       "    'updated': '2017-11-29T18:45:59Z',\n",
       "    'published': '2017-11-29T18:45:59Z',\n",
       "    'title': 'HoME: a Household Multimodal Environment',\n",
       "    'summary': 'We introduce HoME: a Household Multimodal Environment for artificial agents\\nto learn from vision, audio, semantics, physics, and interaction with objects\\nand other agents, all within a realistic context. HoME integrates over 45,000\\ndiverse 3D house layouts based on the SUNCG dataset, a scale which may\\nfacilitate learning, generalization, and transfer. HoME is an open-source,\\nOpenAI Gym-compatible platform extensible to tasks in reinforcement learning,\\nlanguage grounding, sound-based navigation, robotics, multi-agent learning, and\\nmore. We hope HoME better enables artificial agents to learn as humans do: in\\nan interactive, multimodal, and richly contextualized setting.',\n",
       "    'author': [{'name': 'Simon Brodeur'},\n",
       "     {'name': 'Ethan Perez'},\n",
       "     {'name': 'Ankesh Anand'},\n",
       "     {'name': 'Florian Golemo'},\n",
       "     {'name': 'Luca Celotti'},\n",
       "     {'name': 'Florian Strub'},\n",
       "     {'name': 'Jean Rouat'},\n",
       "     {'name': 'Hugo Larochelle'},\n",
       "     {'name': 'Aaron Courville'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"Presented at NIPS 2017's Visually-Grounded Interaction and Language\\n  Workshop\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1711.11017v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1711.11017v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.00418v1',\n",
       "    'updated': '2020-03-01T06:42:43Z',\n",
       "    'published': '2020-03-01T06:42:43Z',\n",
       "    'title': 'Towards Automatic Face-to-Face Translation',\n",
       "    'summary': 'In light of the recent breakthroughs in automatic machine translation\\nsystems, we propose a novel approach that we term as \"Face-to-Face\\nTranslation\". As today\\'s digital communication becomes increasingly visual, we\\nargue that there is a need for systems that can automatically translate a video\\nof a person speaking in language A into a target language B with realistic lip\\nsynchronization. In this work, we create an automatic pipeline for this problem\\nand demonstrate its impact on multiple real-world applications. First, we build\\na working speech-to-speech translation system by bringing together multiple\\nexisting modules from speech and language. We then move towards \"Face-to-Face\\nTranslation\" by incorporating a novel visual module, LipGAN for generating\\nrealistic talking faces from the translated audio. Quantitative evaluation of\\nLipGAN on the standard LRW test set shows that it significantly outperforms\\nexisting approaches across all standard metrics. We also subject our\\nFace-to-Face Translation pipeline, to multiple human evaluations and show that\\nit can significantly improve the overall user experience for consuming and\\ninteracting with multimodal content across languages. Code, models and demo\\nvideo are made publicly available.\\n  Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0\\n  Code and models: https://github.com/Rudrabha/LipGAN',\n",
       "    'author': [{'name': 'Prajwal K R'},\n",
       "     {'name': 'Rudrabha Mukhopadhyay'},\n",
       "     {'name': 'Jerin Philip'},\n",
       "     {'name': 'Abhishek Jha'},\n",
       "     {'name': 'Vinay Namboodiri'},\n",
       "     {'name': 'C. V. Jawahar'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1145/3343031.3351066'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1145/3343031.3351066',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2003.00418v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.00418v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages (including references), 5 figures, Published in ACM\\n  Multimedia, 2019'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"MM '19: Proceedings of the 27th ACM International Conference on\\n  Multimedia; October 2019; Pages 1428-1436\"},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.09321v3',\n",
       "    'updated': '2020-11-12T09:29:36Z',\n",
       "    'published': '2020-09-19T23:23:52Z',\n",
       "    'title': 'Learning a Lie Algebra from Unlabeled Data Pairs',\n",
       "    'summary': 'Deep convolutional networks (convnets) show a remarkable ability to learn\\ndisentangled representations. In recent years, the generalization of deep\\nlearning to Lie groups beyond rigid motion in $\\\\mathbb{R}^n$ has allowed to\\nbuild convnets over datasets with non-trivial symmetries, such as patterns over\\nthe surface of a sphere. However, one limitation of this approach is the need\\nto explicitly define the Lie group underlying the desired invariance property\\nbefore training the convnet. Whereas rotations on the sphere have a well-known\\nsymmetry group ($\\\\mathrm{SO}(3)$), the same cannot be said of many real-world\\nfactors of variability. For example, the disentanglement of pitch, intensity\\ndynamics, and playing technique remains a challenging task in music information\\nretrieval.\\n  This article proposes a machine learning method to discover a nonlinear\\ntransformation of the space $\\\\mathbb{R}^n$ which maps a collection of\\n$n$-dimensional vectors $(\\\\boldsymbol{x}_i)_i$ onto a collection of target\\nvectors $(\\\\boldsymbol{y}_i)_i$. The key idea is to approximate every target\\n$\\\\boldsymbol{y}_i$ by a matrix--vector product of the form\\n$\\\\boldsymbol{\\\\widetilde{y}}_i = \\\\boldsymbol{\\\\phi}(t_i) \\\\boldsymbol{x}_i$, where\\nthe matrix $\\\\boldsymbol{\\\\phi}(t_i)$ belongs to a one-parameter subgroup of\\n$\\\\mathrm{GL}_n (\\\\mathbb{R})$. Crucially, the value of the parameter $t_i \\\\in\\n\\\\mathbb{R}$ may change between data pairs $(\\\\boldsymbol{x}_i,\\n\\\\boldsymbol{y}_i)$ and does not need to be known in advance.',\n",
       "    'author': [{'name': 'Christopher Ick'}, {'name': 'Vincent Lostanlen'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2 pages, 1 figure. Presented at the first DeepMath conference, New\\n  York City, NY, USA, November 2020'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2009.09321v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.09321v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.03206v2',\n",
       "    'updated': '2021-06-23T00:25:31Z',\n",
       "    'published': '2021-03-04T18:20:50Z',\n",
       "    'title': 'Perceiver: General Perception with Iterative Attention',\n",
       "    'summary': 'Biological systems perceive the world by simultaneously processing\\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\\nproprioception, etc. The perception models used in deep learning on the other\\nhand are designed for individual modalities, often relying on domain-specific\\nassumptions such as the local grid structures exploited by virtually all\\nexisting vision models. These priors introduce helpful inductive biases, but\\nalso lock models to individual modalities. In this paper we introduce the\\nPerceiver - a model that builds upon Transformers and hence makes few\\narchitectural assumptions about the relationship between its inputs, but that\\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\\nleverages an asymmetric attention mechanism to iteratively distill inputs into\\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\\nshow that this architecture is competitive with or outperforms strong,\\nspecialized models on classification tasks across various modalities: images,\\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\\nattending to 50,000 pixels. It is also competitive in all modalities in\\nAudioSet.',\n",
       "    'author': [{'name': 'Andrew Jaegle'},\n",
       "     {'name': 'Felix Gimeno'},\n",
       "     {'name': 'Andrew Brock'},\n",
       "     {'name': 'Andrew Zisserman'},\n",
       "     {'name': 'Oriol Vinyals'},\n",
       "     {'name': 'Joao Carreira'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.03206v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.03206v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.03725v1',\n",
       "    'updated': '2021-04-08T12:19:10Z',\n",
       "    'published': '2021-04-08T12:19:10Z',\n",
       "    'title': 'On tuning consistent annealed sampling for denoising score matching',\n",
       "    'summary': 'Score-based generative models provide state-of-the-art quality for image and\\naudio synthesis. Sampling from these models is performed iteratively, typically\\nemploying a discretized series of noise levels and a predefined scheme. In this\\nnote, we first overview three common sampling schemes for models trained with\\ndenoising score matching. Next, we focus on one of them, consistent annealed\\nsampling, and study its hyper-parameter boundaries. We then highlight a\\npossible formulation of such hyper-parameter that explicitly considers those\\nboundaries and facilitates tuning when using few or a variable number of steps.\\nFinally, we highlight some connections of the formulation with other sampling\\nschemes.',\n",
       "    'author': [{'name': 'Joan Serrà'},\n",
       "     {'name': 'Santiago Pascual'},\n",
       "     {'name': 'Jordi Pons'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '3 pages and 1 figure'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.03725v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.03725v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.04283v1',\n",
       "    'updated': '2021-06-08T12:22:29Z',\n",
       "    'published': '2021-06-08T12:22:29Z',\n",
       "    'title': 'NWT: Towards natural audio-to-video generation with representation\\n  learning',\n",
       "    'summary': \"In this work we introduce NWT, an expressive speech-to-video model. Unlike\\napproaches that use domain-specific intermediate representations such as pose\\nkeypoints, NWT learns its own latent representations, with minimal assumptions\\nabout the audio and video content. To this end, we propose a novel discrete\\nvariational autoencoder with adversarial loss, dVAE-Adv, which learns a new\\ndiscrete latent representation we call Memcodes. Memcodes are straightforward\\nto implement, require no additional loss terms, are stable to train compared\\nwith other approaches, and show evidence of interpretability. To predict on the\\nMemcode space, we use an autoregressive encoder-decoder model conditioned on\\naudio. Additionally, our model can control latent attributes in the generated\\nvideo that are not annotated in the data. We train NWT on clips from HBO's Last\\nWeek Tonight with John Oliver. NWT consistently scores above other approaches\\nin Mean Opinion Score (MOS) on tests of overall video naturalness, facial\\nnaturalness and expressiveness, and lipsync quality. This work sets a strong\\nbaseline for generalized audio-to-video synthesis. Samples are available at\\nhttps://next-week-tonight.github.io/NWT/.\",\n",
       "    'author': [{'name': 'Rayhane Mama'},\n",
       "     {'name': 'Marc S. Tyndel'},\n",
       "     {'name': 'Hashiam Kadhim'},\n",
       "     {'name': 'Cole Clifford'},\n",
       "     {'name': 'Ragavan Thurairatnam'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.04283v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.04283v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.09262v1',\n",
       "    'updated': '2021-07-20T04:59:26Z',\n",
       "    'published': '2021-07-20T04:59:26Z',\n",
       "    'title': 'FoleyGAN: Visually Guided Generative Adversarial Network-Based\\n  Synchronous Sound Generation in Silent Videos',\n",
       "    'summary': 'Deep learning based visual to sound generation systems essentially need to be\\ndeveloped particularly considering the synchronicity aspects of visual and\\naudio features with time. In this research we introduce a novel task of guiding\\na class conditioned generative adversarial network with the temporal visual\\ninformation of a video input for visual to sound generation task adapting the\\nsynchronicity traits between audio-visual modalities. Our proposed FoleyGAN\\nmodel is capable of conditioning action sequences of visual events leading\\ntowards generating visually aligned realistic sound tracks. We expand our\\npreviously proposed Automatic Foley dataset to train with FoleyGAN and evaluate\\nour synthesized sound through human survey that shows noteworthy (on average\\n81\\\\%) audio-visual synchronicity performance. Our approach also outperforms in\\nstatistical experiments compared with other baseline models and audio-visual\\ndatasets.',\n",
       "    'author': [{'name': 'Sanchita Ghose'}, {'name': 'John J. Prevost'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This article is under review in IEEE Transaction on Multimedia. It\\n  contains total 12 pages, 6 figures, 4 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.09262v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.09262v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T10 (primary) 68T07, 68U10(secondary)',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.5.4; I.2.10; J.5',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.02096v2',\n",
       "    'updated': '2021-10-10T16:22:00Z',\n",
       "    'published': '2021-09-05T15:06:53Z',\n",
       "    'title': 'Timbre Transfer with Variational Auto Encoding and Cycle-Consistent\\n  Adversarial Networks',\n",
       "    'summary': \"This research project investigates the application of deep learning to timbre\\ntransfer, where the timbre of a source audio can be converted to the timbre of\\na target audio with minimal loss in quality. The adopted approach combines\\nVariational Autoencoders with Generative Adversarial Networks to construct\\nmeaningful representations of the source audio and produce realistic\\ngenerations of the target audio and is applied to the Flickr 8k Audio dataset\\nfor transferring the vocal timbre between speakers and the URMP dataset for\\ntransferring the musical timbre between instruments. Furthermore, variations of\\nthe adopted approach are trained, and generalised performance is compared using\\nthe metrics SSIM (Structural Similarity Index) and FAD (Frech\\\\'et Audio\\nDistance). It was found that a many-to-many approach supersedes a one-to-one\\napproach in terms of reconstructive capabilities, and that the adoption of a\\nbasic over a bottleneck residual block design is more suitable for enriching\\ncontent information about a latent space. It was also found that the decision\\non whether cyclic loss takes on a variational autoencoder or vanilla\\nautoencoder approach does not have a significant impact on reconstructive and\\nadversarial translation aspects of the model.\",\n",
       "    'author': [{'name': 'Russell Sammut Bonnici'},\n",
       "     {'name': 'Charalampos Saitis'},\n",
       "     {'name': 'Martin Benning'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 3 main figures, 4 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.02096v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.02096v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.11955v1',\n",
       "    'updated': '2021-09-24T13:40:51Z',\n",
       "    'published': '2021-09-24T13:40:51Z',\n",
       "    'title': 'Visual Scene Graphs for Audio Source Separation',\n",
       "    'summary': 'State-of-the-art approaches for visually-guided audio source separation\\ntypically assume sources that have characteristic sounds, such as musical\\ninstruments. These approaches often ignore the visual context of these sound\\nsources or avoid modeling object interactions that may be useful to better\\ncharacterize the sources, especially when the same object class may produce\\nvaried sounds from distinct interactions. To address this challenging problem,\\nwe propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning\\nmodel that embeds the visual structure of the scene as a graph and segments\\nthis graph into subgraphs, each subgraph being associated with a unique sound\\nobtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a\\nrecursive neural network that emits mutually-orthogonal sub-graph embeddings of\\nthe visual graph using multi-head attention. These embeddings are used for\\nconditioning an audio encoder-decoder towards source separation. Our pipeline\\nis trained end-to-end via a self-supervised task consisting of separating audio\\nsources using the visual graph from artificially mixed sounds. In this paper,\\nwe also introduce an \"in the wild\\'\\' video dataset for sound source separation\\nthat contains multiple non-musical sources, which we call Audio Separation in\\nthe Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and\\nprovides a challenging, natural, and daily-life setting for source separation.\\nThorough experiments on the proposed ASIW and the standard MUSIC datasets\\ndemonstrate state-of-the-art sound separation performance of our method against\\nrecent prior approaches.',\n",
       "    'author': [{'name': 'Moitreya Chatterjee'},\n",
       "     {'name': 'Jonathan Le Roux'},\n",
       "     {'name': 'Narendra Ahuja'},\n",
       "     {'name': 'Anoop Cherian'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at ICCV 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.11955v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.11955v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.08791v1',\n",
       "    'updated': '2021-10-17T11:14:00Z',\n",
       "    'published': '2021-10-17T11:14:00Z',\n",
       "    'title': 'Taming Visually Guided Sound Generation',\n",
       "    'summary': 'Recent advances in visually-induced audio generation are based on sampling\\nshort, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio\\nfrom the state-of-the-art model takes minutes on a high-end GPU. In this work,\\nwe propose a single model capable of generating visually relevant,\\nhigh-fidelity sounds prompted with a set of frames from open-domain videos in\\nless time than it takes to play it on a single GPU.\\n  We train a transformer to sample a new spectrogram from the pre-trained\\nspectrogram codebook given the set of video features. The codebook is obtained\\nusing a variant of VQGAN trained to produce a compact sampling space with a\\nnovel spectrogram-based perceptual loss. The generated spectrogram is\\ntransformed into a waveform using a window-based GAN that significantly speeds\\nup generation. Considering the lack of metrics for automatic evaluation of\\ngenerated spectrograms, we also build a family of metrics called FID and MKL.\\nThese metrics are based on a novel sound classifier, called Melception, and\\ndesigned to evaluate the fidelity and relevance of open-domain samples.\\n  Both qualitative and quantitative studies are conducted on small- and\\nlarge-scale datasets to evaluate the fidelity and relevance of generated\\nsamples. We also compare our model to the state-of-the-art and observe a\\nsubstantial improvement in quality, size, and computation time. Code, demo, and\\nsamples: v-iashin.github.io/SpecVQGAN',\n",
       "    'author': [{'name': 'Vladimir Iashin'}, {'name': 'Esa Rahtu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted as an oral presentation for the BMVC 2021. Code:\\n  https://github.com/v-iashin/SpecVQGAN Project page:\\n  https://v-iashin.github.io/SpecVQGAN'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.08791v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.08791v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.07765v2',\n",
       "    'updated': '2022-06-14T16:55:16Z',\n",
       "    'published': '2022-02-15T22:31:42Z',\n",
       "    'title': 'General-purpose, long-context autoregressive modeling with Perceiver AR',\n",
       "    'summary': 'Real-world data is high-dimensional: a book, image, or musical performance\\ncan easily contain hundreds of thousands of elements even after compression.\\nHowever, the most commonly used autoregressive models, Transformers, are\\nprohibitively expensive to scale to the number of inputs and layers needed to\\ncapture this long-range structure. We develop Perceiver AR, an autoregressive,\\nmodality-agnostic architecture which uses cross-attention to map long-range\\ninputs to a small number of latents while also maintaining end-to-end causal\\nmasking. Perceiver AR can directly attend to over a hundred thousand tokens,\\nenabling practical long-context density estimation without the need for\\nhand-crafted sparsity patterns or memory mechanisms. When trained on images or\\nmusic, Perceiver AR generates outputs with clear long-term coherence and\\nstructure. Our architecture also obtains state-of-the-art likelihood on\\nlong-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.',\n",
       "    'author': [{'name': 'Curtis Hawthorne'},\n",
       "     {'name': 'Andrew Jaegle'},\n",
       "     {'name': 'Cătălina Cangea'},\n",
       "     {'name': 'Sebastian Borgeaud'},\n",
       "     {'name': 'Charlie Nash'},\n",
       "     {'name': 'Mateusz Malinowski'},\n",
       "     {'name': 'Sander Dieleman'},\n",
       "     {'name': 'Oriol Vinyals'},\n",
       "     {'name': 'Matthew Botvinick'},\n",
       "     {'name': 'Ian Simon'},\n",
       "     {'name': 'Hannah Sheahan'},\n",
       "     {'name': 'Neil Zeghidour'},\n",
       "     {'name': 'Jean-Baptiste Alayrac'},\n",
       "     {'name': 'João Carreira'},\n",
       "     {'name': 'Jesse Engel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.07765v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.07765v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.08509v1',\n",
       "    'updated': '2022-02-17T08:26:25Z',\n",
       "    'published': '2022-02-17T08:26:25Z',\n",
       "    'title': 'A Study of Designing Compact Audio-Visual Wake Word Spotting System\\n  Based on Iterative Fine-Tuning in Neural Network Pruning',\n",
       "    'summary': 'Audio-only-based wake word spotting (WWS) is challenging under noisy\\nconditions due to environmental interference in signal transmission. In this\\npaper, we investigate on designing a compact audio-visual WWS system by\\nutilizing visual information to alleviate the degradation. Specifically, in\\norder to use visual information, we first encode the detected lips to\\nfixed-size vectors with MobileNet and concatenate them with acoustic features\\nfollowed by the fusion network for WWS. However, the audio-visual model based\\non neural networks requires a large footprint and a high computational\\ncomplexity. To meet the application requirements, we introduce a neural network\\npruning strategy via the lottery ticket hypothesis in an iterative fine-tuning\\nmanner (LTH-IF), to the single-modal and multi-modal models, respectively.\\nTested on our in-house corpus for audio-visual WWS in a home TV scene, the\\nproposed audio-visual system achieves significant performance improvements over\\nthe single-modality (audio-only or video-only) system under different noisy\\nconditions. Moreover, LTH-IF pruning can largely reduce the network parameters\\nand computations with no degradation of WWS performance, leading to a potential\\nproduct solution for the TV wake-up scenario.',\n",
       "    'author': [{'name': 'Hengshun Zhou'},\n",
       "     {'name': 'Jun Du'},\n",
       "     {'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'Shifu Xiong'},\n",
       "     {'name': 'Chin-Hui Lee'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICASSP 2022. H. Zhou et al'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.08509v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.08509v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.12307v1',\n",
       "    'updated': '2022-02-24T19:00:03Z',\n",
       "    'published': '2022-02-24T19:00:03Z',\n",
       "    'title': 'Retriever: Learning Content-Style Representation as a Token-Level\\n  Bipartite Graph',\n",
       "    'summary': 'This paper addresses the unsupervised learning of content-style decomposed\\nrepresentation. We first give a definition of style and then model the\\ncontent-style representation as a token-level bipartite graph. An unsupervised\\nframework, named Retriever, is proposed to learn such representations. First, a\\ncross-attention module is employed to retrieve permutation invariant (P.I.)\\ninformation, defined as style, from the input data. Second, a vector\\nquantization (VQ) module is used, together with man-induced constraints, to\\nproduce interpretable content tokens. Last, an innovative link attention module\\nserves as the decoder to reconstruct data from the decomposed content and\\nstyle, with the help of the linking keys. Being modal-agnostic, the proposed\\nRetriever is evaluated in both speech and image domains. The state-of-the-art\\nzero-shot voice conversion performance confirms the disentangling ability of\\nour framework. Top performance is also achieved in the part discovery task for\\nimages, verifying the interpretability of our representation. In addition, the\\nvivid part-based style transfer quality demonstrates the potential of Retriever\\nto support various fascinating generative tasks. Project page at\\nhttps://ydcustc.github.io/retriever-demo/.',\n",
       "    'author': [{'name': 'Dacheng Yin'},\n",
       "     {'name': 'Xuanchi Ren'},\n",
       "     {'name': 'Chong Luo'},\n",
       "     {'name': 'Yuwang Wang'},\n",
       "     {'name': 'Zhiwei Xiong'},\n",
       "     {'name': 'Wenjun Zeng'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to ICLR 2022. Project page at\\n  https://ydcustc.github.io/retriever-demo/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.12307v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.12307v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.13717v1',\n",
       "    'updated': '2022-08-29T16:56:35Z',\n",
       "    'published': '2022-08-29T16:56:35Z',\n",
       "    'title': 'StableFace: Analyzing and Improving Motion Stability for Talking Face\\n  Generation',\n",
       "    'summary': 'While previous speech-driven talking face generation methods have made\\nsignificant progress in improving the visual quality and lip-sync quality of\\nthe synthesized videos, they pay less attention to lip motion jitters which\\ngreatly undermine the realness of talking face videos. What causes motion\\njitters, and how to mitigate the problem? In this paper, we conduct systematic\\nanalyses on the motion jittering problem based on a state-of-the-art pipeline\\nthat uses 3D face representations to bridge the input audio and output video,\\nand improve the motion stability with a series of effective designs. We find\\nthat several issues can lead to jitters in synthesized talking face video: 1)\\njitters from the input 3D face representations; 2) training-inference mismatch;\\n3) lack of dependency modeling among video frames. Accordingly, we propose\\nthree effective solutions to address this issue: 1) we propose a gaussian-based\\nadaptive smoothing module to smooth the 3D face representations to eliminate\\njitters in the input; 2) we add augmented erosions on the input data of the\\nneural renderer in training to simulate the distortion in inference to reduce\\nmismatch; 3) we develop an audio-fused transformer generator to model\\ndependency among video frames. Besides, considering there is no off-the-shelf\\nmetric for measuring motion jitters in talking face video, we devise an\\nobjective metric (Motion Stability Index, MSI), to quantitatively measure the\\nmotion jitters by calculating the reciprocal of variance acceleration.\\nExtensive experimental results show the superiority of our method on\\nmotion-stable face video generation, with better quality than previous systems.',\n",
       "    'author': [{'name': 'Jun Ling'},\n",
       "     {'name': 'Xu Tan'},\n",
       "     {'name': 'Liyang Chen'},\n",
       "     {'name': 'Runnan Li'},\n",
       "     {'name': 'Yuchao Zhang'},\n",
       "     {'name': 'Sheng Zhao'},\n",
       "     {'name': 'Li Song'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages,'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.13717v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.13717v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.17416v1',\n",
       "    'updated': '2022-10-27T09:57:47Z',\n",
       "    'published': '2022-10-27T09:57:47Z',\n",
       "    'title': 'Efficient Similarity-based Passive Filter Pruning for Compressing CNNs',\n",
       "    'summary': 'Convolution neural networks (CNNs) have shown great success in various\\napplications. However, the computational complexity and memory storage of CNNs\\nis a bottleneck for their deployment on resource-constrained devices. Recent\\nefforts towards reducing the computation cost and the memory overhead of CNNs\\ninvolve similarity-based passive filter pruning methods. Similarity-based\\npassive filter pruning methods compute a pairwise similarity matrix for the\\nfilters and eliminate a few similar filters to obtain a small pruned CNN.\\nHowever, the computational complexity of computing the pairwise similarity\\nmatrix is high, particularly when a convolutional layer has many filters. To\\nreduce the computational complexity in obtaining the pairwise similarity\\nmatrix, we propose to use an efficient method where the complete pairwise\\nsimilarity matrix is approximated from only a few of its columns by using a\\nNystr\\\\\"om approximation method. The proposed efficient similarity-based passive\\nfilter pruning method is 3 times faster and gives same accuracy at the same\\nreduction in computations for CNNs compared to that of the similarity-based\\npruning method that computes a complete pairwise similarity matrix. Apart from\\nthis, the proposed efficient similarity-based pruning method performs similarly\\nor better than the existing norm-based pruning methods. The efficacy of the\\nproposed pruning method is evaluated on CNNs such as DCASE 2021 Task 1A\\nbaseline network and a VGGish network designed for acoustic scene\\nclassification.',\n",
       "    'author': [{'name': 'Arshdeep Singh'}, {'name': 'Mark D. Plumbley'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICASSP 2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.17416v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.17416v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.06267v2',\n",
       "    'updated': '2023-01-18T02:13:43Z',\n",
       "    'published': '2023-01-16T05:40:42Z',\n",
       "    'title': 'Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with\\n  Multimodal Models',\n",
       "    'summary': 'The ability to quickly learn a new task with minimal instruction - known as\\nfew-shot learning - is a central aspect of intelligent agents. Classical\\nfew-shot benchmarks make use of few-shot samples from a single modality, but\\nsuch samples may not be sufficient to characterize an entire concept class. In\\ncontrast, humans use cross-modal information to learn new concepts efficiently.\\nIn this work, we demonstrate that one can indeed build a better ${\\\\bf visual}$\\ndog classifier by ${\\\\bf read}$ing about dogs and ${\\\\bf listen}$ing to them\\nbark. To do so, we exploit the fact that recent multimodal foundation models\\nsuch as CLIP are inherently cross-modal, mapping different modalities to the\\nsame representation space. Specifically, we propose a simple cross-modal\\nadaptation approach that learns from few-shot examples spanning different\\nmodalities. By repurposing class names as additional one-shot training samples,\\nwe achieve SOTA results with an embarrassingly simple linear classifier for\\nvision-language adaptation. Furthermore, we show that our approach can benefit\\nexisting methods such as prefix tuning, adapters, and classifier ensembling.\\nFinally, to explore other modalities beyond vision and language, we construct\\nthe first (to our knowledge) audiovisual few-shot benchmark and use cross-modal\\ntraining to improve the performance of both image and audio classification.',\n",
       "    'author': [{'name': 'Zhiqiu Lin'},\n",
       "     {'name': 'Samuel Yu'},\n",
       "     {'name': 'Zhiyi Kuang'},\n",
       "     {'name': 'Deepak Pathak'},\n",
       "     {'name': 'Deva Ramanan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Project website: https://linzhiqiu.github.io/papers/cross_modal/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.06267v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.06267v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.08614v1',\n",
       "    'updated': '2021-04-17T18:39:22Z',\n",
       "    'published': '2021-04-17T18:39:22Z',\n",
       "    'title': 'Cetacean Translation Initiative: a roadmap to deciphering the\\n  communication of sperm whales',\n",
       "    'summary': 'The past decade has witnessed a groundbreaking rise of machine learning for\\nhuman language analysis, with current methods capable of automatically\\naccurately recovering various aspects of syntax and semantics - including\\nsentence structure and grounded word meaning - from large data collections.\\nRecent research showed the promise of such tools for analyzing acoustic\\ncommunication in nonhuman species. We posit that machine learning will be the\\ncornerstone of future collection, processing, and analysis of multimodal\\nstreams of data in animal communication studies, including bioacoustic,\\nbehavioral, biological, and environmental data. Cetaceans are unique non-human\\nmodel species as they possess sophisticated acoustic communications, but\\nutilize a very different encoding system that evolved in an aquatic rather than\\nterrestrial medium. Sperm whales, in particular, with their highly-developed\\nneuroanatomical features, cognitive abilities, social structures, and discrete\\nclick-based encoding make for an excellent starting point for advanced machine\\nlearning tools that can be applied to other animals in the future. This paper\\ndetails a roadmap toward this goal based on currently existing technology and\\nmultidisciplinary scientific community effort. We outline the key elements\\nrequired for the collection and processing of massive bioacoustic data of sperm\\nwhales, detecting their basic communication units and language-like\\nhigher-level structures, and validating these models through interactive\\nplayback experiments. The technological capabilities developed by such an\\nundertaking are likely to yield cross-applications and advancements in broader\\ncommunities investigating non-human communication and animal behavioral\\nresearch.',\n",
       "    'author': [{'name': 'Jacob Andreas'},\n",
       "     {'name': 'Gašper Beguš'},\n",
       "     {'name': 'Michael M. Bronstein'},\n",
       "     {'name': 'Roee Diamant'},\n",
       "     {'name': 'Denley Delaney'},\n",
       "     {'name': 'Shane Gero'},\n",
       "     {'name': 'Shafi Goldwasser'},\n",
       "     {'name': 'David F. Gruber'},\n",
       "     {'name': 'Sarah de Haas'},\n",
       "     {'name': 'Peter Malkin'},\n",
       "     {'name': 'Roger Payne'},\n",
       "     {'name': 'Giovanni Petri'},\n",
       "     {'name': 'Daniela Rus'},\n",
       "     {'name': 'Pratyusha Sharma'},\n",
       "     {'name': 'Dan Tchernov'},\n",
       "     {'name': 'Pernille Tønnesen'},\n",
       "     {'name': 'Antonio Torralba'},\n",
       "     {'name': 'Daniel Vogt'},\n",
       "     {'name': 'Robert J. Wood'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.08614v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.08614v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1410.0736v4',\n",
       "    'updated': '2015-05-16T03:36:32Z',\n",
       "    'published': '2014-10-03T01:17:20Z',\n",
       "    'title': 'HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale\\n  Visual Recognition',\n",
       "    'summary': 'In image classification, visual separability between different object\\ncategories is highly uneven, and some categories are more difficult to\\ndistinguish than others. Such difficult categories demand more dedicated\\nclassifiers. However, existing deep convolutional neural networks (CNN) are\\ntrained as flat N-way classifiers, and few efforts have been made to leverage\\nthe hierarchical structure of categories. In this paper, we introduce\\nhierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category\\nhierarchy. An HD-CNN separates easy classes using a coarse category classifier\\nwhile distinguishing difficult classes using fine category classifiers. During\\nHD-CNN training, component-wise pretraining is followed by global finetuning\\nwith a multinomial logistic loss regularized by a coarse category consistency\\nterm. In addition, conditional executions of fine category classifiers and\\nlayer parameter compression make HD-CNNs scalable for large-scale visual\\nrecognition. We achieve state-of-the-art results on both CIFAR100 and\\nlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, we\\nbuild up three different HD-CNNs and they lower the top-1 error of the standard\\nCNNs by 2.65%, 3.1% and 1.1%, respectively.',\n",
       "    'author': [{'name': 'Zhicheng Yan'},\n",
       "     {'name': 'Hao Zhang'},\n",
       "     {'name': 'Robinson Piramuthu'},\n",
       "     {'name': 'Vignesh Jagadeesh'},\n",
       "     {'name': 'Dennis DeCoste'},\n",
       "     {'name': 'Wei Di'},\n",
       "     {'name': 'Yizhou Yu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Add new results on ImageNet using VGG-16-layer building block net'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1410.0736v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1410.0736v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1605.08104v5',\n",
       "    'updated': '2017-03-01T01:00:54Z',\n",
       "    'published': '2016-05-25T23:58:55Z',\n",
       "    'title': 'Deep Predictive Coding Networks for Video Prediction and Unsupervised\\n  Learning',\n",
       "    'summary': 'While great strides have been made in using deep learning algorithms to solve\\nsupervised learning tasks, the problem of unsupervised learning - leveraging\\nunlabeled examples to learn about the structure of a domain - remains a\\ndifficult unsolved challenge. Here, we explore prediction of future frames in a\\nvideo sequence as an unsupervised learning rule for learning about the\\nstructure of the visual world. We describe a predictive neural network\\n(\"PredNet\") architecture that is inspired by the concept of \"predictive coding\"\\nfrom the neuroscience literature. These networks learn to predict future frames\\nin a video sequence, with each layer in the network making local predictions\\nand only forwarding deviations from those predictions to subsequent network\\nlayers. We show that these networks are able to robustly learn to predict the\\nmovement of synthetic (rendered) objects, and that in doing so, the networks\\nlearn internal representations that are useful for decoding latent object\\nparameters (e.g. pose) that support object recognition with fewer training\\nviews. We also show that these networks can scale to complex natural image\\nstreams (car-mounted camera videos), capturing key aspects of both egocentric\\nmovement and the movement of objects in the visual scene, and the\\nrepresentation learned in this setting is useful for estimating the steering\\nangle. Altogether, these results suggest that prediction represents a powerful\\nframework for unsupervised learning, allowing for implicit learning of object\\nand scene structure.',\n",
       "    'author': [{'name': 'William Lotter'},\n",
       "     {'name': 'Gabriel Kreiman'},\n",
       "     {'name': 'David Cox'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Code and example video clips can be found here:\\n  https://coxlab.github.io/prednet/'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1605.08104v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1605.08104v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1610.01076v1',\n",
       "    'updated': '2016-10-04T16:29:28Z',\n",
       "    'published': '2016-10-04T16:29:28Z',\n",
       "    'title': 'Tutorial on Answering Questions about Images with Deep Learning',\n",
       "    'summary': 'Together with the development of more accurate methods in Computer Vision and\\nNatural Language Understanding, holistic architectures that answer on questions\\nabout the content of real-world images have emerged. In this tutorial, we build\\na neural-based approach to answer questions about images. We base our tutorial\\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\\nmodels that we present here can achieve a competitive performance on both\\ndatasets, in fact, they are among the best methods that use a combination of\\nLSTM with a global, full frame CNN representation of an image. We hope that\\nafter reading this tutorial, the reader will be able to use Deep Learning\\nframeworks, such as Keras and introduced Kraino, to build various architectures\\nthat will lead to a further performance improvement on this challenging task.',\n",
       "    'author': [{'name': 'Mateusz Malinowski'}, {'name': 'Mario Fritz'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"The tutorial was presented at '2nd Summer School on Integrating\\n  Vision and Language: Deep Learning' in Malta, 2016\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1610.01076v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1610.01076v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1706.05507v2',\n",
       "    'updated': '2017-11-28T18:47:37Z',\n",
       "    'published': '2017-06-17T09:48:55Z',\n",
       "    'title': 'Variants of RMSProp and Adagrad with Logarithmic Regret Bounds',\n",
       "    'summary': 'Adaptive gradient methods have become recently very popular, in particular as\\nthey have been shown to be useful in the training of deep neural networks. In\\nthis paper we have analyzed RMSProp, originally proposed for the training of\\ndeep neural networks, in the context of online convex optimization and show\\n$\\\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and\\nSC-RMSProp for which we show logarithmic regret bounds for strongly convex\\nfunctions. Finally, we demonstrate in the experiments that these new variants\\noutperform other adaptive gradient techniques or stochastic gradient descent in\\nthe optimization of strongly convex functions as well as in training of deep\\nneural networks.',\n",
       "    'author': [{'name': 'Mahesh Chandra Mukkamala'},\n",
       "     {'name': 'Matthias Hein'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2017, 16 pages, 23 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1706.05507v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1706.05507v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1709.01215v2',\n",
       "    'updated': '2017-11-05T03:58:52Z',\n",
       "    'published': '2017-09-05T02:18:06Z',\n",
       "    'title': 'ALICE: Towards Understanding Adversarial Learning for Joint Distribution\\n  Matching',\n",
       "    'summary': 'We investigate the non-identifiability issues associated with bidirectional\\nadversarial training for joint distribution matching. Within a framework of\\nconditional entropy, we propose both adversarial and non-adversarial approaches\\nto learn desirable matched joint distributions for unsupervised and supervised\\ntasks. We unify a broad family of adversarial models as joint distribution\\nmatching problems. Our approach stabilizes learning of unsupervised\\nbidirectional adversarial learning methods. Further, we introduce an extension\\nfor semi-supervised learning tasks. Theoretical results are validated in\\nsynthetic data and real-world applications.',\n",
       "    'author': [{'name': 'Chunyuan Li'},\n",
       "     {'name': 'Hao Liu'},\n",
       "     {'name': 'Changyou Chen'},\n",
       "     {'name': 'Yunchen Pu'},\n",
       "     {'name': 'Liqun Chen'},\n",
       "     {'name': 'Ricardo Henao'},\n",
       "     {'name': 'Lawrence Carin'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NIPS 2017 (22 pages); short version (9 pages):\\n  http://people.duke.edu/~cl319/doc/papers/nips_2017_alice.pdf'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1709.01215v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1709.01215v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'stat.ML',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'stat.ML',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1710.10686v1',\n",
       "    'updated': '2017-10-29T20:27:51Z',\n",
       "    'published': '2017-10-29T20:27:51Z',\n",
       "    'title': 'Regularization for Deep Learning: A Taxonomy',\n",
       "    'summary': 'Regularization is one of the crucial ingredients of deep learning, yet the\\nterm regularization has various definitions, and regularization methods are\\noften studied separately from each other. In our work we present a systematic,\\nunifying taxonomy to categorize existing methods. We distinguish methods that\\naffect data, network architectures, error terms, regularization terms, and\\noptimization procedures. We do not provide all details about the listed\\nmethods; instead, we present an overview of how the methods can be sorted into\\nmeaningful categories and sub-categories. This helps revealing links and\\nfundamental similarities between them. Finally, we include practical\\nrecommendations both for users and for developers of new regularization\\nmethods.',\n",
       "    'author': [{'name': 'Jan Kukačka'},\n",
       "     {'name': 'Vladimir Golkov'},\n",
       "     {'name': 'Daniel Cremers'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1710.10686v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1710.10686v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '62M45', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.6; I.5', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.01890v2',\n",
       "    'updated': '2018-05-31T16:08:33Z',\n",
       "    'published': '2018-05-03T19:36:43Z',\n",
       "    'title': 'RMDL: Random Multimodel Deep Learning for Classification',\n",
       "    'summary': 'The continually increasing number of complex datasets each year necessitates\\never improving machine learning methods for robust and accurate categorization\\nof these data. This paper introduces Random Multimodel Deep Learning (RMDL): a\\nnew ensemble, deep learning approach for classification. Deep learning models\\nhave achieved state-of-the-art results across many domains. RMDL solves the\\nproblem of finding the best deep learning structure and architecture while\\nsimultaneously improving robustness and accuracy through ensembles of deep\\nlearning architectures. RDML can accept as input a variety data to include\\ntext, video, images, and symbolic. This paper describes RMDL and shows test\\nresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,\\nand 20newsgroup. These test results show that RDML produces consistently better\\nperformance than standard methods over a broad range of data types and\\nclassification problems.',\n",
       "    'author': [{'name': 'Kamran Kowsari'},\n",
       "     {'name': 'Mojtaba Heidarysafa'},\n",
       "     {'name': 'Donald E. Brown'},\n",
       "     {'name': 'Kiana Jafari Meimandi'},\n",
       "     {'name': 'Laura E. Barnes'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1145/3206098.3206111'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1145/3206098.3206111',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1805.01890v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.01890v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Best Paper award ACM ICISDM'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1806.05759v3',\n",
       "    'updated': '2018-10-23T18:59:02Z',\n",
       "    'published': '2018-06-14T22:34:11Z',\n",
       "    'title': 'Insights on representational similarity in neural networks with\\n  canonical correlation',\n",
       "    'summary': 'Comparing different neural network representations and determining how\\nrepresentations evolve over time remain challenging open questions in our\\nunderstanding of the function of neural networks. Comparing representations in\\nneural networks is fundamentally difficult as the structure of representations\\nvaries greatly, even across groups of networks trained on identical tasks, and\\nover the course of training. Here, we develop projection weighted CCA\\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\\nfirst improve the core method, showing how to differentiate between signal and\\nnoise, and then apply this technique to compare across a group of CNNs,\\ndemonstrating that networks which generalize converge to more similar\\nrepresentations than networks which memorize, that wider networks converge to\\nmore similar solutions than narrow networks, and that trained networks with\\nidentical topology but different learning rates converge to distinct clusters\\nwith diverse representations. We also investigate the representational dynamics\\nof RNNs, across both training and sequential timesteps, finding that RNNs\\nconverge in a bottom-up pattern over the course of training and that the hidden\\nstate is highly variable over the course of a sequence, even when accounting\\nfor linear transforms. Together, these results provide new insights into the\\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\\nunderstand representations.',\n",
       "    'author': [{'name': 'Ari S. Morcos'},\n",
       "     {'name': 'Maithra Raghu'},\n",
       "     {'name': 'Samy Bengio'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NIPS 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1806.05759v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1806.05759v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'stat.ML',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'stat.ML',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1810.02244v5',\n",
       "    'updated': '2021-11-30T15:37:55Z',\n",
       "    'published': '2018-10-04T14:31:57Z',\n",
       "    'title': 'Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks',\n",
       "    'summary': 'In recent years, graph neural networks (GNNs) have emerged as a powerful\\nneural architecture to learn vector representations of nodes and graphs in a\\nsupervised, end-to-end fashion. Up to now, GNNs have only been evaluated\\nempirically -- showing promising results. The following work investigates GNNs\\nfrom a theoretical point of view and relates them to the $1$-dimensional\\nWeisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have\\nthe same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic\\n(sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on\\nthis, we propose a generalization of GNNs, so-called $k$-dimensional GNNs\\n($k$-GNNs), which can take higher-order graph structures at multiple scales\\ninto account. These higher-order structures play an essential role in the\\ncharacterization of social networks and molecule graphs. Our experimental\\nevaluation confirms our theoretical findings as well as confirms that\\nhigher-order information is useful in the task of graph classification and\\nregression.',\n",
       "    'author': [{'name': 'Christopher Morris'},\n",
       "     {'name': 'Martin Ritzert'},\n",
       "     {'name': 'Matthias Fey'},\n",
       "     {'name': 'William L. Hamilton'},\n",
       "     {'name': 'Jan Eric Lenssen'},\n",
       "     {'name': 'Gaurav Rattan'},\n",
       "     {'name': 'Martin Grohe'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Extended version with proofs, accepted at AAAI 2019, added units of\\n  measurement of QM9 dataset into appendix, removed results from Wu et al.,\\n  2018 due to different units'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1810.02244v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1810.02244v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.03389v1',\n",
       "    'updated': '2019-05-08T23:35:02Z',\n",
       "    'published': '2019-05-08T23:35:02Z',\n",
       "    'title': 'Learning to Evolve',\n",
       "    'summary': 'Evolution and learning are two of the fundamental mechanisms by which life\\nadapts in order to survive and to transcend limitations. These biological\\nphenomena inspired successful computational methods such as evolutionary\\nalgorithms and deep learning. Evolution relies on random mutations and on\\nrandom genetic recombination. Here we show that learning to evolve, i.e.\\nlearning to mutate and recombine better than at random, improves the result of\\nevolution in terms of fitness increase per generation and even in terms of\\nattainable fitness. We use deep reinforcement learning to learn to dynamically\\nadjust the strategy of evolutionary algorithms to varying circumstances. Our\\nmethods outperform classical evolutionary algorithms on combinatorial and\\ncontinuous optimization problems.',\n",
       "    'author': [{'name': 'Jan Schuchardt'},\n",
       "     {'name': 'Vladimir Golkov'},\n",
       "     {'name': 'Daniel Cremers'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.03389v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.03389v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '62M45, 68T05, 68W25, 68T20, 90C40, 91A22, 92D15, 92D25',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'G.1.6; I.2.6; I.2.8; G.3; I.5.1',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.12857v3',\n",
       "    'updated': '2020-09-10T06:22:06Z',\n",
       "    'published': '2020-03-28T17:56:31Z',\n",
       "    'title': 'NPENAS: Neural Predictor Guided Evolution for Neural Architecture Search',\n",
       "    'summary': 'Neural architecture search (NAS) is a promising method for automatically\\ndesign neural architectures. NAS adopts a search strategy to explore the\\npredefined search space to find outstanding performance architecture with the\\nminimum searching costs. Bayesian optimization and evolutionary algorithms are\\ntwo commonly used search strategies, but they suffer from computationally\\nexpensive, challenge to implement or inefficient exploration ability. In this\\npaper, we propose a neural predictor guided evolutionary algorithm to enhance\\nthe exploration ability of EA for NAS (NPENAS) and design two kinds of neural\\npredictors. The first predictor is defined from Bayesian optimization and we\\npropose a graph-based uncertainty estimation network as a surrogate model that\\nis easy to implement and computationally efficient. The second predictor is a\\ngraph-based neural network that directly outputs the performance prediction of\\nthe input neural architecture. The NPENAS using the two neural predictors are\\ndenoted as NPENAS-BO and NPENAS-NP respectively. In addition, we introduce a\\nnew random architecture sampling method to overcome the drawbacks of the\\nexisting sampling method. Extensive experiments demonstrate the superiority of\\nNPENAS. Quantitative results on three NAS search spaces indicate that both\\nNPENAS-BO and NPENAS-NP outperform most existing NAS algorithms, with NPENAS-BO\\nachieving state-of-the-art performance on NASBench-201 and NPENAS-NP on\\nNASBench-101 and DARTS, respectively.',\n",
       "    'author': [{'name': 'Chen Wei'},\n",
       "     {'name': 'Chuang Niu'},\n",
       "     {'name': 'Yiping Tang'},\n",
       "     {'name': 'Yue Wang'},\n",
       "     {'name': 'Haihong Hu'},\n",
       "     {'name': 'Jimin Liang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.12857v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.12857v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.00554v1',\n",
       "    'updated': '2021-01-31T22:48:50Z',\n",
       "    'published': '2021-01-31T22:48:50Z',\n",
       "    'title': 'Sparsity in Deep Learning: Pruning and growth for efficient inference\\n  and training in neural networks',\n",
       "    'summary': 'The growing energy and performance costs of deep learning have driven the\\ncommunity to reduce the size of neural networks by selectively pruning\\ncomponents. Similarly to their biological counterparts, sparse networks\\ngeneralize just as well, if not better than, the original dense networks.\\nSparsity can reduce the memory footprint of regular networks to fit mobile\\ndevices, as well as shorten training time for ever growing networks. In this\\npaper, we survey prior work on sparsity in deep learning and provide an\\nextensive tutorial of sparsification for both inference and training. We\\ndescribe approaches to remove and add elements of neural networks, different\\ntraining strategies to achieve model sparsity, and mechanisms to exploit\\nsparsity in practice. Our work distills ideas from more than 300 research\\npapers and provides guidance to practitioners who wish to utilize sparsity\\ntoday, as well as to researchers whose goal is to push the frontier forward. We\\ninclude the necessary background on mathematical methods in sparsification,\\ndescribe phenomena such as early structure adaptation, the intricate relations\\nbetween sparsity and the training process, and show techniques for achieving\\nacceleration on real hardware. We also define a metric of pruned parameter\\nefficiency that could serve as a baseline for comparison of different sparse\\nnetworks. We close by speculating on how sparsity can improve future workloads\\nand outline major open problems in the field.',\n",
       "    'author': [{'name': 'Torsten Hoefler'},\n",
       "     {'name': 'Dan Alistarh'},\n",
       "     {'name': 'Tal Ben-Nun'},\n",
       "     {'name': 'Nikoli Dryden'},\n",
       "     {'name': 'Alexandra Peste'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '90 pages, 26 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.00554v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.00554v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AR', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.11237v1',\n",
       "    'updated': '2021-02-22T18:15:39Z',\n",
       "    'published': '2021-02-22T18:15:39Z',\n",
       "    'title': 'Image Captioning using Deep Stacked LSTMs, Contextual Word Embeddings\\n  and Data Augmentation',\n",
       "    'summary': 'Image Captioning, or the automatic generation of descriptions for images, is\\none of the core problems in Computer Vision and has seen considerable progress\\nusing Deep Learning Techniques. We propose to use Inception-ResNet\\nConvolutional Neural Network as encoder to extract features from images,\\nHierarchical Context based Word Embeddings for word representations and a Deep\\nStacked Long Short Term Memory network as decoder, in addition to using Image\\nData Augmentation to avoid over-fitting. For data Augmentation, we use\\nHorizontal and Vertical Flipping in addition to Perspective Transformations on\\nthe images. We evaluate our proposed methods with two image captioning\\nframeworks- Encoder-Decoder and Soft Attention. Evaluation on widely used\\nmetrics have shown that our approach leads to considerable improvement in model\\nperformance.',\n",
       "    'author': [{'name': 'Sulabh Katiyar'}, {'name': 'Samir Kumar Borgohain'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for publication in Springer Book Series: Advances in\\n  Intelligent Systems and Computing - ISSN 2194-5357. Upon publication, this\\n  article will point to the published one'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.11237v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.11237v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.10005v4',\n",
       "    'updated': '2021-06-15T06:52:21Z',\n",
       "    'published': '2021-05-20T19:38:03Z',\n",
       "    'title': 'Robust Unsupervised Multi-Object Tracking in Noisy Environments',\n",
       "    'summary': \"Physical processes, camera movement, and unpredictable environmental\\nconditions like the presence of dust can induce noise and artifacts in video\\nfeeds. We observe that popular unsupervised MOT methods are dependent on\\nnoise-free inputs. We show that the addition of a small amount of artificial\\nrandom noise causes a sharp degradation in model performance on benchmark\\nmetrics. We resolve this problem by introducing a robust unsupervised\\nmulti-object tracking (MOT) model: AttU-Net. The proposed single-head attention\\nmodel helps limit the negative impact of noise by learning visual\\nrepresentations at different segment scales. AttU-Net shows better unsupervised\\nMOT tracking performance over variational inference-based state-of-the-art\\nbaselines. We evaluate our method in the MNIST-MOT and the Atari game video\\nbenchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''\\nwhich consists of moving Japanese characters and ``Fashion-MNIST MOT'' to\\nvalidate the effectiveness of the MOT models.\",\n",
       "    'author': [{'name': 'C. -H. Huck Yang'},\n",
       "     {'name': 'Mohit Chhabra'},\n",
       "     {'name': 'Y. -C. Liu'},\n",
       "     {'name': 'Quan Kong'},\n",
       "     {'name': 'Tomoaki Yoshinaga'},\n",
       "     {'name': 'Tomokazu Murakami'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICIP42928.2021.9506029'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICIP42928.2021.9506029',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2105.10005v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.10005v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE ICIP 2021'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2021 IEEE International Conference on Image Processing (ICIP)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.04540v1',\n",
       "    'updated': '2021-06-08T17:20:50Z',\n",
       "    'published': '2021-06-08T17:20:50Z',\n",
       "    'title': 'Object Based Attention Through Internal Gating',\n",
       "    'summary': 'Object-based attention is a key component of the visual system, relevant for\\nperception, learning, and memory. Neurons tuned to features of attended objects\\ntend to be more active than those associated with non-attended objects. There\\nis a rich set of models of this phenomenon in computational neuroscience.\\nHowever, there is currently a divide between models that successfully match\\nphysiological data but can only deal with extremely simple problems and models\\nof attention used in computer vision. For example, attention in the brain is\\nknown to depend on top-down processing, whereas self-attention in deep learning\\ndoes not. Here, we propose an artificial neural network model of object-based\\nattention that captures the way in which attention is both top-down and\\nrecurrent. Our attention model works well both on simple test stimuli, such as\\nthose using images of handwritten digits, and on more complex stimuli, such as\\nnatural images drawn from the COCO dataset. We find that our model replicates a\\nrange of findings from neuroscience, including attention-invariant tuning,\\ninhibition of return, and attention-mediated scaling of activity. Understanding\\nobject based attention is both computationally interesting and a key problem\\nfor computational neuroscience.',\n",
       "    'author': [{'name': 'Jordan Lei'},\n",
       "     {'name': 'Ari S. Benjamin'},\n",
       "     {'name': 'Konrad P. Kording'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.04540v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.04540v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'q-bio.NC',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'q-bio.NC',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.06475v1',\n",
       "    'updated': '2021-07-14T03:58:02Z',\n",
       "    'published': '2021-07-14T03:58:02Z',\n",
       "    'title': 'Generative and reproducible benchmarks for comprehensive evaluation of\\n  machine learning classifiers',\n",
       "    'summary': 'Understanding the strengths and weaknesses of machine learning (ML)\\nalgorithms is crucial for determine their scope of application. Here, we\\nintroduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of\\nsynthetic datasets for comprehensive, reproducible, and interpretable\\nbenchmarking of machine learning algorithms for classification of binary\\noutcomes. The DIGEN resource consists of 40 mathematical functions which map\\ncontinuous features to discrete endpoints for creating synthetic datasets.\\nThese 40 functions were discovered using a heuristic algorithm designed to\\nmaximize the diversity of performance among multiple popular machine learning\\nalgorithms thus providing a useful test suite for evaluating and comparing new\\nmethods. Access to the generative functions facilitates understanding of why a\\nmethod performs poorly compared to other algorithms thus providing ideas for\\nimprovement. The resource with extensive documentation and analyses is\\nopen-source and available on GitHub.',\n",
       "    'author': [{'name': 'Patryk Orzechowski'}, {'name': 'Jason H. Moore'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 3 figures with subfigures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.06475v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.06475v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T09 (Primary) 62R07, 68-04, 68-11 (Secondary)',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.5.2; I.1.2; I.5.1; I.6.5; I.2.0; G.1.6',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.00364v2',\n",
       "    'updated': '2022-10-11T13:20:30Z',\n",
       "    'published': '2022-06-01T10:03:24Z',\n",
       "    'title': 'Elucidating the Design Space of Diffusion-Based Generative Models',\n",
       "    'summary': 'We argue that the theory and practice of diffusion-based generative models\\nare currently unnecessarily convoluted and seek to remedy the situation by\\npresenting a design space that clearly separates the concrete design choices.\\nThis lets us identify several changes to both the sampling and training\\nprocesses, as well as preconditioning of the score networks. Together, our\\nimprovements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a\\nclass-conditional setting and 1.97 in an unconditional setting, with much\\nfaster sampling (35 network evaluations per image) than prior designs. To\\nfurther demonstrate their modular nature, we show that our design changes\\ndramatically improve both the efficiency and quality obtainable with\\npre-trained score networks from previous work, including improving the FID of a\\npreviously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after\\nre-training with our proposed improvements to a new SOTA of 1.36.',\n",
       "    'author': [{'name': 'Tero Karras'},\n",
       "     {'name': 'Miika Aittala'},\n",
       "     {'name': 'Timo Aila'},\n",
       "     {'name': 'Samuli Laine'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.00364v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.00364v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.03358v2',\n",
       "    'updated': '2022-12-12T19:35:01Z',\n",
       "    'published': '2022-09-07T17:05:48Z',\n",
       "    'title': 'Securing the Spike: On the Transferability and Security of Spiking\\n  Neural Networks to Adversarial Examples',\n",
       "    'summary': 'Spiking neural networks (SNNs) have attracted much attention for their high\\nenergy efficiency and for recent advances in their classification performance.\\nHowever, unlike traditional deep learning approaches, the analysis and study of\\nthe robustness of SNNs to adversarial examples remain relatively\\nunderdeveloped. In this work we focus on advancing the adversarial attack side\\nof SNNs and make three major contributions. First, we show that successful\\nwhite-box adversarial attacks on SNNs are highly dependent on the underlying\\nsurrogate gradient technique. Second, using the best surrogate gradient\\ntechnique, we analyze the transferability of adversarial attacks on SNNs and\\nother state-of-the-art architectures like Vision Transformers (ViTs) and Big\\nTransfer Convolutional Neural Networks (CNNs). We demonstrate that SNNs are not\\noften deceived by adversarial examples generated by Vision Transformers and\\ncertain types of CNNs. Third, due to the lack of an ubiquitous white-box attack\\nthat is effective across both the SNN and CNN/ViT domains, we develop a new\\nwhite-box attack, the Auto Self-Attention Gradient Attack (Auto SAGA). Our\\nnovel attack generates adversarial examples capable of fooling both SNN models\\nand non-SNN models simultaneously. Auto SAGA is as much as $87.9\\\\%$ more\\neffective on SNN/ViT model ensembles than conventional white-box attacks like\\nPGD. Our experiments and analyses are broad and rigorous covering three\\ndatasets (CIFAR-10, CIFAR-100 and ImageNet), five different white-box attacks\\nand nineteen different classifier models (seven for each CIFAR dataset and five\\ndifferent models for ImageNet).',\n",
       "    'author': [{'name': 'Nuo Xu'},\n",
       "     {'name': 'Kaleel Mahmood'},\n",
       "     {'name': 'Haowen Fang'},\n",
       "     {'name': 'Ethan Rathbun'},\n",
       "     {'name': 'Caiwen Ding'},\n",
       "     {'name': 'Wujie Wen'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.03358v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.03358v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CR', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1308.2350v1',\n",
       "    'updated': '2013-08-10T22:56:26Z',\n",
       "    'published': '2013-08-10T22:56:26Z',\n",
       "    'title': 'Learning Features and their Transformations by Spatial and Temporal\\n  Spherical Clustering',\n",
       "    'summary': \"Learning features invariant to arbitrary transformations in the data is a\\nrequirement for any recognition system, biological or artificial. It is now\\nwidely accepted that simple cells in the primary visual cortex respond to\\nfeatures while the complex cells respond to features invariant to different\\ntransformations. We present a novel two-layered feedforward neural model that\\nlearns features in the first layer by spatial spherical clustering and\\ninvariance to transformations in the second layer by temporal spherical\\nclustering. Learning occurs in an online and unsupervised manner following the\\nHebbian rule. When exposed to natural videos acquired by a camera mounted on a\\ncat's head, the first and second layer neurons in our model develop simple and\\ncomplex cell-like receptive field properties. The model can predict by learning\\nlateral connections among the first layer neurons. A topographic map to their\\nspatial features emerges by exponentially decaying the flow of activation with\\ndistance from one neuron to another in the first layer that fire in close\\ntemporal proximity, thereby minimizing the pooling length in an online manner\\nsimultaneously with feature learning.\",\n",
       "    'author': [{'name': 'Jayanta K. Dutta'}, {'name': 'Bonny Banerjee'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1308.2350v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1308.2350v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2; I.4; I.5', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1409.6041v1',\n",
       "    'updated': '2014-09-21T20:42:00Z',\n",
       "    'published': '2014-09-21T20:42:00Z',\n",
       "    'title': 'Domain Adaptive Neural Networks for Object Recognition',\n",
       "    'summary': 'We propose a simple neural network model to deal with the domain adaptation\\nproblem in object recognition. Our model incorporates the Maximum Mean\\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\\nreduce the distribution mismatch between the source and target domains in the\\nlatent space. From experiments, we demonstrate that the MMD regularization is\\nan effective tool to provide good domain adaptation models on both SURF\\nfeatures and raw image pixels of a particular image data set. We also show that\\nour proposed model, preceded by the denoising auto-encoder pretraining,\\nachieves better performance than recent benchmark models on the same data sets.\\nThis work represents the first study of MMD measure in the context of neural\\nnetworks.',\n",
       "    'author': [{'name': 'Muhammad Ghifary'},\n",
       "     {'name': 'W. Bastiaan Kleijn'},\n",
       "     {'name': 'Mengjie Zhang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1409.6041v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1409.6041v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1506.01911v3',\n",
       "    'updated': '2016-02-10T16:50:29Z',\n",
       "    'published': '2015-06-05T13:43:01Z',\n",
       "    'title': 'Beyond Temporal Pooling: Recurrence and Temporal Convolutions for\\n  Gesture Recognition in Video',\n",
       "    'summary': 'Recent studies have demonstrated the power of recurrent neural networks for\\nmachine translation, image captioning and speech recognition. For the task of\\ncapturing temporal structure in video, however, there still remain numerous\\nopen research questions. Current research suggests using a simple temporal\\nfeature pooling strategy to take into account the temporal aspect of video. We\\ndemonstrate that this method is not sufficient for gesture recognition, where\\ntemporal information is more discriminative compared to general video\\nclassification tasks. We explore deep architectures for gesture recognition in\\nvideo and propose a new end-to-end trainable neural network architecture\\nincorporating temporal convolutions and bidirectional recurrence. Our main\\ncontributions are twofold; first, we show that recurrence is crucial for this\\ntask; second, we show that adding temporal convolutions leads to significant\\nimprovements. We evaluate the different approaches on the Montalbano gesture\\nrecognition dataset, where we achieve state-of-the-art results.',\n",
       "    'author': [{'name': 'Lionel Pigou'},\n",
       "     {'name': 'Aäron van den Oord'},\n",
       "     {'name': 'Sander Dieleman'},\n",
       "     {'name': 'Mieke Van Herreweghe'},\n",
       "     {'name': 'Joni Dambre'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1506.01911v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1506.01911v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1509.05962v2',\n",
       "    'updated': '2017-02-15T02:29:04Z',\n",
       "    'published': '2015-09-20T03:35:05Z',\n",
       "    'title': 'Telugu OCR Framework using Deep Learning',\n",
       "    'summary': 'In this paper, we address the task of Optical Character Recognition(OCR) for\\nthe Telugu script. We present an end-to-end framework that segments the text\\nimage, classifies the characters and extracts lines using a language model. The\\nsegmentation is based on mathematical morphology. The classification module,\\nwhich is the most challenging task of the three, is a deep convolutional neural\\nnetwork. The language is modelled as a third degree markov chain at the glyph\\nlevel. Telugu script is a complex alphasyllabary and the language is\\nagglutinative, making the problem hard. In this paper we apply the latest\\nadvances in neural networks to achieve state-of-the-art error rates. We also\\nreview convolutional neural networks in great detail and expound the\\nstatistical justification behind the many tricks needed to make Deep Learning\\nwork.',\n",
       "    'author': [{'name': 'Rakesh Achanta'}, {'name': 'Trevor Hastie'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1509.05962v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1509.05962v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'stat.ML',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'stat.ML',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1604.00289v3',\n",
       "    'updated': '2016-11-02T17:26:50Z',\n",
       "    'published': '2016-04-01T15:37:57Z',\n",
       "    'title': 'Building Machines That Learn and Think Like People',\n",
       "    'summary': 'Recent progress in artificial intelligence (AI) has renewed interest in\\nbuilding systems that learn and think like people. Many advances have come from\\nusing deep neural networks trained end-to-end in tasks such as object\\nrecognition, video games, and board games, achieving performance that equals or\\neven beats humans in some respects. Despite their biological inspiration and\\nperformance achievements, these systems differ from human intelligence in\\ncrucial ways. We review progress in cognitive science suggesting that truly\\nhuman-like learning and thinking machines will have to reach beyond current\\nengineering trends in both what they learn, and how they learn it.\\nSpecifically, we argue that these machines should (a) build causal models of\\nthe world that support explanation and understanding, rather than merely\\nsolving pattern recognition problems; (b) ground learning in intuitive theories\\nof physics and psychology, to support and enrich the knowledge that is learned;\\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\\ngeneralize knowledge to new tasks and situations. We suggest concrete\\nchallenges and promising routes towards these goals that can combine the\\nstrengths of recent neural network advances with more structured cognitive\\nmodels.',\n",
       "    'author': [{'name': 'Brenden M. Lake'},\n",
       "     {'name': 'Tomer D. Ullman'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Samuel J. Gershman'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'In press at Behavioral and Brain Sciences. Open call for commentary\\n  proposals (until Nov. 22, 2016).\\n  https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1604.00289v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1604.00289v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1604.01662v4',\n",
       "    'updated': '2021-01-06T03:14:13Z',\n",
       "    'published': '2016-04-06T15:35:08Z',\n",
       "    'title': 'A Survey on Bayesian Deep Learning',\n",
       "    'summary': \"A comprehensive artificial intelligence system needs to not only perceive the\\nenvironment with different `senses' (e.g., seeing and hearing) but also infer\\nthe world's conditional (or even causal) relations and corresponding\\nuncertainty. The past decade has seen major advances in many perception tasks\\nsuch as visual object recognition and speech recognition using deep learning\\nmodels. For higher-level inference, however, probabilistic graphical models\\nwith their Bayesian nature are still more powerful and flexible. In recent\\nyears, Bayesian deep learning has emerged as a unified probabilistic framework\\nto tightly integrate deep learning and Bayesian models. In this general\\nframework, the perception of text or images using deep learning can boost the\\nperformance of higher-level inference and in turn, the feedback from the\\ninference process is able to enhance the perception of text or images. This\\nsurvey provides a comprehensive introduction to Bayesian deep learning and\\nreviews its recent applications on recommender systems, topic models, control,\\netc. Besides, we also discuss the relationship and differences between Bayesian\\ndeep learning and other related topics such as Bayesian treatment of neural\\nnetworks. For a constantly updating project page, please refer to\\nhttps://github.com/js05212/BayesianDeepLearning-Survey.\",\n",
       "    'author': [{'name': 'Hao Wang'}, {'name': 'Dit-Yan Yeung'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published in ACM Computing Surveys (CSUR) 2020. Constantly updating\\n  project page at https://github.com/js05212/BayesianDeepLearning-Survey'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1604.01662v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1604.01662v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'stat.ML',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'stat.ML',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1604.06057v2',\n",
       "    'updated': '2016-05-31T14:45:58Z',\n",
       "    'published': '2016-04-20T18:47:48Z',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal\\n  Abstraction and Intrinsic Motivation',\n",
       "    'summary': \"Learning goal-directed behavior in environments with sparse feedback is a\\nmajor challenge for reinforcement learning algorithms. The primary difficulty\\narises due to insufficient exploration, resulting in an agent being unable to\\nlearn robust value functions. Intrinsically motivated agents can explore new\\nbehavior for its own sake rather than to directly solve problems. Such\\nintrinsic behaviors could eventually help the agent solve tasks posed by the\\nenvironment. We present hierarchical-DQN (h-DQN), a framework to integrate\\nhierarchical value functions, operating at different temporal scales, with\\nintrinsically motivated deep reinforcement learning. A top-level value function\\nlearns a policy over intrinsic goals, and a lower-level function learns a\\npolicy over atomic actions to satisfy the given goals. h-DQN allows for\\nflexible goal specifications, such as functions over entities and relations.\\nThis provides an efficient space for exploration in complicated environments.\\nWe demonstrate the strength of our approach on two problems with very sparse,\\ndelayed feedback: (1) a complex discrete stochastic decision process, and (2)\\nthe classic ATARI game `Montezuma's Revenge'.\",\n",
       "    'author': [{'name': 'Tejas D. Kulkarni'},\n",
       "     {'name': 'Karthik R. Narasimhan'},\n",
       "     {'name': 'Ardavan Saeedi'},\n",
       "     {'name': 'Joshua B. Tenenbaum'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages, 7 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1604.06057v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1604.06057v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1605.09782v7',\n",
       "    'updated': '2017-04-03T20:34:36Z',\n",
       "    'published': '2016-05-31T19:37:29Z',\n",
       "    'title': 'Adversarial Feature Learning',\n",
       "    'summary': 'The ability of the Generative Adversarial Networks (GANs) framework to learn\\ngenerative models mapping from simple latent distributions to arbitrarily\\ncomplex data distributions has been demonstrated empirically, with compelling\\nresults showing that the latent space of such generators captures semantic\\nvariation in the data distribution. Intuitively, models trained to predict\\nthese semantic latent representations given data may serve as useful feature\\nrepresentations for auxiliary problems where semantics are relevant. However,\\nin their existing form, GANs have no means of learning the inverse mapping --\\nprojecting data back into the latent space. We propose Bidirectional Generative\\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\\ndemonstrate that the resulting learned feature representation is useful for\\nauxiliary supervised discrimination tasks, competitive with contemporary\\napproaches to unsupervised and self-supervised feature learning.',\n",
       "    'author': [{'name': 'Jeff Donahue'},\n",
       "     {'name': 'Philipp Krähenbühl'},\n",
       "     {'name': 'Trevor Darrell'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2\\n  results improved 1-2% due to averaging predictions over 10 crops at test\\n  time, as done in Noroozi & Favaro; Table 3 VOC classification results\\n  slightly improved due to minor bugfix. (See v6 changelog for previous\\n  versions.)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1605.09782v7',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1605.09782v7',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1606.03490v3',\n",
       "    'updated': '2017-03-06T08:51:10Z',\n",
       "    'published': '2016-06-10T21:28:47Z',\n",
       "    'title': 'The Mythos of Model Interpretability',\n",
       "    'summary': 'Supervised machine learning models boast remarkable predictive capabilities.\\nBut can you trust your model? Will it work in deployment? What else can it tell\\nyou about the world? We want models to be not only good, but interpretable. And\\nyet the task of interpretation appears underspecified. Papers provide diverse\\nand sometimes non-overlapping motivations for interpretability, and offer\\nmyriad notions of what attributes render models interpretable. Despite this\\nambiguity, many papers proclaim interpretability axiomatically, absent further\\nexplanation. In this paper, we seek to refine the discourse on\\ninterpretability. First, we examine the motivations underlying interest in\\ninterpretability, finding them to be diverse and occasionally discordant. Then,\\nwe address model properties and techniques thought to confer interpretability,\\nidentifying transparency to humans and post-hoc explanations as competing\\nnotions. Throughout, we discuss the feasibility and desirability of different\\nnotions, and question the oft-made assertions that linear models are\\ninterpretable and that deep neural networks are not.',\n",
       "    'author': {'name': 'Zachary C. Lipton'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'presented at 2016 ICML Workshop on Human Interpretability in Machine\\n  Learning (WHI 2016), New York, NY'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1606.03490v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1606.03490v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1609.06492v1',\n",
       "    'updated': '2016-09-21T10:52:03Z',\n",
       "    'published': '2016-09-21T10:52:03Z',\n",
       "    'title': 'Document Image Coding and Clustering for Script Discrimination',\n",
       "    'summary': 'The paper introduces a new method for discrimination of documents given in\\ndifferent scripts. The document is mapped into a uniformly coded text of\\nnumerical values. It is derived from the position of the letters in the text\\nline, based on their typographical characteristics. Each code is considered as\\na gray level. Accordingly, the coded text determines a 1-D image, on which\\ntexture analysis by run-length statistics and local binary pattern is\\nperformed. It defines feature vectors representing the script content of the\\ndocument. A modified clustering approach employed on document feature vector\\ngroups documents written in the same script. Experimentation performed on two\\ncustom oriented databases of historical documents in old Cyrillic, angular and\\nround Glagolitic as well as Antiqua and Fraktur scripts demonstrates the\\nsuperiority of the proposed method with respect to well-known methods in the\\nstate-of-the-art.',\n",
       "    'author': [{'name': 'Darko Brodic'},\n",
       "     {'name': 'Alessia Amelio'},\n",
       "     {'name': 'Zoran N. Milivojevic'},\n",
       "     {'name': 'Milena Jevtic'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 4 figures, 2 tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICIC Express Letters Vol. 10 n. 7 July 2016 pp. 1561-1566'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1609.06492v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1609.06492v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '97R40, 62H35, 68U15, 68T50,',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1612.06370v2',\n",
       "    'updated': '2017-04-12T04:28:47Z',\n",
       "    'published': '2016-12-19T20:56:04Z',\n",
       "    'title': 'Learning Features by Watching Objects Move',\n",
       "    'summary': \"This paper presents a novel yet intuitive approach to unsupervised feature\\nlearning. Inspired by the human visual system, we explore whether low-level\\nmotion-based grouping cues can be used to learn an effective visual\\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\\nconvolutional network to segment objects from a single frame. Given the\\nextensive evidence that motion plays a key role in the development of the human\\nvisual system, we hope that this straightforward approach to unsupervised\\nlearning will be more effective than cleverly designed 'pretext' tasks studied\\nin the literature. Indeed, our extensive experiments show that this is the\\ncase. When used for transfer learning on object detection, our representation\\nsignificantly outperforms previous unsupervised approaches across multiple\\nsettings, especially when training data for the target task is scarce.\",\n",
       "    'author': [{'name': 'Deepak Pathak'},\n",
       "     {'name': 'Ross Girshick'},\n",
       "     {'name': 'Piotr Dollár'},\n",
       "     {'name': 'Trevor Darrell'},\n",
       "     {'name': 'Bharath Hariharan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'CVPR 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1612.06370v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1612.06370v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1701.06106v2',\n",
       "    'updated': '2017-02-19T08:15:55Z',\n",
       "    'published': '2017-01-22T00:35:24Z',\n",
       "    'title': 'Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a\\n  Changing World',\n",
       "    'summary': 'In this paper, we focus on online representation learning in non-stationary\\nenvironments which may require continuous adaptation of model architecture. We\\npropose a novel online dictionary-learning (sparse-coding) framework which\\nincorporates the addition and deletion of hidden units (dictionary elements),\\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\\nthe hippocampus, known to be associated with improved cognitive function and\\nadaptation to new environments. In the online learning setting, where new input\\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\\nadding new units with random initial weights (random dictionary elements); the\\nnumber of new units is determined by the current performance (representation\\nerror) of the dictionary, higher error causing an increase in the birth rate.\\nNeuronal-death is implemented by imposing l1/l2-regularization (group sparsity)\\non the dictionary within the block-coordinate descent optimization at each\\niteration of our online alternating minimization scheme, which iterates between\\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\\nis facilitated by introducing sparsity in dictionary elements. Our empirical\\nevaluation on several real-life datasets (images and language) as well as on\\nsynthetic data demonstrates that the proposed approach can considerably\\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\\nidentify certain properties of the data (e.g., sparse inputs with nearly\\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\\nassociated with such improvements.',\n",
       "    'author': [{'name': 'Sahil Garg'},\n",
       "     {'name': 'Irina Rish'},\n",
       "     {'name': 'Guillermo Cecchi'},\n",
       "     {'name': 'Aurelie Lozano'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1701.06106v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1701.06106v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1702.08690v2',\n",
       "    'updated': '2017-06-06T11:51:03Z',\n",
       "    'published': '2017-02-28T08:40:44Z',\n",
       "    'title': 'Borrowing Treasures from the Wealthy: Deep Transfer Learning through\\n  Selective Joint Fine-tuning',\n",
       "    'summary': 'Deep neural networks require a large amount of labeled training data during\\nsupervised learning. However, collecting and labeling so much data might be\\ninfeasible in many cases. In this paper, we introduce a source-target selective\\njoint fine-tuning scheme for improving the performance of deep learning tasks\\nwith insufficient training data. In this scheme, a target learning task with\\ninsufficient training data is carried out simultaneously with another source\\nlearning task with abundant training data. However, the source learning task\\ndoes not use all existing training data. Our core idea is to identify and use a\\nsubset of training images from the original source learning task whose\\nlow-level characteristics are similar to those from the target learning task,\\nand jointly fine-tune shared convolutional layers for both tasks. Specifically,\\nwe compute descriptors from linear or nonlinear filter bank responses on\\ntraining images from both tasks, and use such descriptors to search for a\\ndesired subset of training samples for the source learning task.\\n  Experiments demonstrate that our selective joint fine-tuning scheme achieves\\nstate-of-the-art performance on multiple visual classification tasks with\\ninsufficient training data for deep learning. Such tasks include Caltech 256,\\nMIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to\\nfine-tuning without a source domain, the proposed method can improve the\\nclassification accuracy by 2% - 10% using a single model.',\n",
       "    'author': [{'name': 'Weifeng Ge'}, {'name': 'Yizhou Yu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in 2017 IEEE Conference on Computer Vision and Pattern\\n  Recognition (CVPR 2017)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1702.08690v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1702.08690v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1704.00260v2',\n",
       "    'updated': '2017-10-16T05:34:24Z',\n",
       "    'published': '2017-04-02T08:01:30Z',\n",
       "    'title': 'Aligned Image-Word Representations Improve Inductive Transfer Across\\n  Vision-Language Tasks',\n",
       "    'summary': 'An important goal of computer vision is to build systems that learn visual\\nrepresentations over time that can be applied to many tasks. In this paper, we\\ninvestigate a vision-language embedding as a core representation and show that\\nit leads to better cross-task transfer than standard multi-task learning. In\\nparticular, the task of visual recognition is aligned to the task of visual\\nquestion answering by forcing each to use the same word-region embeddings. We\\nshow this leads to greater inductive transfer from recognition to VQA than\\nstandard multitask learning. Visual recognition also improves, especially for\\ncategories that have relatively few recognition training labels but appear\\noften in the VQA setting. Thus, our paper takes a small step towards creating\\nmore general vision systems by showing the benefit of interpretable, flexible,\\nand trainable core representations.',\n",
       "    'author': [{'name': 'Tanmay Gupta'},\n",
       "     {'name': 'Kevin Shih'},\n",
       "     {'name': 'Saurabh Singh'},\n",
       "     {'name': 'Derek Hoiem'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted in ICCV 2017. The arxiv version has an extra analysis on\\n  correlation with human attention'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1704.00260v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1704.00260v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1704.05712v3',\n",
       "    'updated': '2017-07-31T18:55:54Z',\n",
       "    'published': '2017-04-19T12:48:52Z',\n",
       "    'title': 'Universal Adversarial Perturbations Against Semantic Image Segmentation',\n",
       "    'summary': 'While deep learning is remarkably successful on perceptual tasks, it was also\\nshown to be vulnerable to adversarial perturbations of the input. These\\nperturbations denote noise added to the input that was generated specifically\\nto fool the system while being quasi-imperceptible for humans. More severely,\\nthere even exist universal perturbations that are input-agnostic but fool the\\nnetwork on the majority of inputs. While recent work has focused on image\\nclassification, this work proposes attacks against semantic image segmentation:\\nwe present an approach for generating (universal) adversarial perturbations\\nthat make the network yield a desired target segmentation as output. We show\\nempirically that there exist barely perceptible universal noise patterns which\\nresult in nearly the same predicted segmentation for arbitrary inputs.\\nFurthermore, we also show the existence of universal noise which removes a\\ntarget class (e.g., all pedestrians) from the segmentation while leaving the\\nsegmentation mostly unchanged otherwise.',\n",
       "    'author': [{'name': 'Jan Hendrik Metzen'},\n",
       "     {'name': 'Mummadi Chaithanya Kumar'},\n",
       "     {'name': 'Thomas Brox'},\n",
       "     {'name': 'Volker Fischer'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Final version for ICCV including supplementary material'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1704.05712v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1704.05712v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'stat.ML',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'stat.ML',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1704.08045v2',\n",
       "    'updated': '2017-06-12T19:43:39Z',\n",
       "    'published': '2017-04-26T10:24:54Z',\n",
       "    'title': 'The loss surface of deep and wide neural networks',\n",
       "    'summary': 'While the optimization problem behind deep neural networks is highly\\nnon-convex, it is frequently observed in practice that training deep networks\\nseems possible without getting stuck in suboptimal points. It has been argued\\nthat this is the case as all local minima are close to being globally optimal.\\nWe show that this is (almost) true, in fact almost all local minima are\\nglobally optimal, for a fully connected network with squared loss and analytic\\nactivation function given that the number of hidden units of one layer of the\\nnetwork is larger than the number of training points and the network structure\\nfrom this layer on is pyramidal.',\n",
       "    'author': [{'name': 'Quynh Nguyen'}, {'name': 'Matthias Hein'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2017. Main results now hold for larger classes of loss functions'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1704.08045v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1704.08045v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1705.07215v5',\n",
       "    'updated': '2017-12-10T15:24:13Z',\n",
       "    'published': '2017-05-19T22:41:56Z',\n",
       "    'title': 'On Convergence and Stability of GANs',\n",
       "    'summary': 'We propose studying GAN training dynamics as regret minimization, which is in\\ncontrast to the popular view that there is consistent minimization of a\\ndivergence between real and generated distributions. We analyze the convergence\\nof GAN training from this new point of view to understand why mode collapse\\nhappens. We hypothesize the existence of undesirable local equilibria in this\\nnon-convex game to be responsible for mode collapse. We observe that these\\nlocal equilibria often exhibit sharp gradients of the discriminator function\\naround some real data points. We demonstrate that these degenerate local\\nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show\\nthat DRAGAN enables faster training, achieves improved stability with fewer\\nmode collapses, and leads to generator networks with better modeling\\nperformance across a variety of architectures and objective functions.',\n",
       "    'author': [{'name': 'Naveen Kodali'},\n",
       "     {'name': 'Jacob Abernethy'},\n",
       "     {'name': 'James Hays'},\n",
       "     {'name': 'Zsolt Kira'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Analysis of convergence and mode collapse by studying GAN training\\n  process as regret minimization. Some new results'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1705.07215v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1705.07215v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1705.07904v3',\n",
       "    'updated': '2018-02-22T19:36:33Z',\n",
       "    'published': '2017-05-22T18:00:02Z',\n",
       "    'title': 'Semantically Decomposing the Latent Spaces of Generative Adversarial\\n  Networks',\n",
       "    'summary': \"We propose a new algorithm for training generative adversarial networks that\\njointly learns latent codes for both identities (e.g. individual humans) and\\nobservations (e.g. specific photographs). By fixing the identity portion of the\\nlatent codes, we can generate diverse images of the same subject, and by fixing\\nthe observation portion, we can traverse the manifold of subjects while\\nmaintaining contingent aspects such as lighting and pose. Our algorithm\\nfeatures a pairwise training scheme in which each sample from the generator\\nconsists of two images with a common identity code. Corresponding samples from\\nthe real dataset consist of two distinct photographs of the same subject. In\\norder to fool the discriminator, the generator must produce pairs that are\\nphotorealistic, distinct, and appear to depict the same individual. We augment\\nboth the DCGAN and BEGAN approaches with Siamese discriminators to facilitate\\npairwise training. Experiments with human judges and an off-the-shelf face\\nverification system demonstrate our algorithm's ability to generate convincing,\\nidentity-matched photographs.\",\n",
       "    'author': [{'name': 'Chris Donahue'},\n",
       "     {'name': 'Zachary C. Lipton'},\n",
       "     {'name': 'Akshay Balsubramani'},\n",
       "     {'name': 'Julian McAuley'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published as a conference paper at ICLR 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1705.07904v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1705.07904v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1705.07962v2',\n",
       "    'updated': '2017-09-19T11:27:47Z',\n",
       "    'published': '2017-05-22T19:32:20Z',\n",
       "    'title': 'pix2code: Generating Code from a Graphical User Interface Screenshot',\n",
       "    'summary': 'Transforming a graphical user interface screenshot created by a designer into\\ncomputer code is a typical task conducted by a developer in order to build\\ncustomized software, websites, and mobile applications. In this paper, we show\\nthat deep learning methods can be leveraged to train a model end-to-end to\\nautomatically generate code from a single input image with over 77% of accuracy\\nfor three different platforms (i.e. iOS, Android and web-based technologies).',\n",
       "    'author': {'name': 'Tony Beltramelli'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1705.07962v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1705.07962v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T45', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.1; I.2.10; I.2.2; I.2.6',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1706.02257v1',\n",
       "    'updated': '2017-06-07T17:00:08Z',\n",
       "    'published': '2017-06-07T17:00:08Z',\n",
       "    'title': 'Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural\\n  Network',\n",
       "    'summary': 'Advanced driver assistance systems (ADAS) can be significantly improved with\\neffective driver action prediction (DAP). Predicting driver actions early and\\naccurately can help mitigate the effects of potentially unsafe driving\\nbehaviors and avoid possible accidents. In this paper, we formulate driver\\naction prediction as a timeseries anomaly prediction problem. While the anomaly\\n(driver actions of interest) detection might be trivial in this context,\\nfinding patterns that consistently precede an anomaly requires searching for or\\nextracting features across multi-modal sensory inputs. We present such a driver\\naction prediction system, including a real-time data acquisition, processing\\nand learning framework for predicting future or impending driver action. The\\nproposed system incorporates camera-based knowledge of the driving environment\\nand the driver themselves, in addition to traditional vehicle dynamics. It then\\nuses a deep bidirectional recurrent neural network (DBRNN) to learn the\\ncorrelation between sensory inputs and impending driver behavior achieving\\naccurate and high horizon action prediction. The proposed system performs\\nbetter than other existing systems on driver action prediction tasks and can\\naccurately predict key driver actions including acceleration, braking, lane\\nchange and turning at durations of 5sec before the action is executed by the\\ndriver.',\n",
       "    'author': [{'name': 'Oluwatobi Olabiyi'},\n",
       "     {'name': 'Eric Martinson'},\n",
       "     {'name': 'Vijay Chintalapudi'},\n",
       "     {'name': 'Rui Guo'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"ITSC'17\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1706.02257v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1706.02257v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'stat.ML',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'stat.ML',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1710.05381v2',\n",
       "    'updated': '2018-10-13T02:02:17Z',\n",
       "    'published': '2017-10-15T19:01:43Z',\n",
       "    'title': 'A systematic study of the class imbalance problem in convolutional\\n  neural networks',\n",
       "    'summary': 'In this study, we systematically investigate the impact of class imbalance on\\nclassification performance of convolutional neural networks (CNNs) and compare\\nfrequently used methods to address the issue. Class imbalance is a common\\nproblem that has been comprehensively studied in classical machine learning,\\nyet very limited systematic research is available in the context of deep\\nlearning. In our study, we use three benchmark datasets of increasing\\ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of\\nimbalance on classification and perform an extensive comparison of several\\nmethods to address the issue: oversampling, undersampling, two-phase training,\\nand thresholding that compensates for prior class probabilities. Our main\\nevaluation metric is area under the receiver operating characteristic curve\\n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is\\nassociated with notable difficulties in the context of imbalanced data. Based\\non results from our experiments we conclude that (i) the effect of class\\nimbalance on classification performance is detrimental; (ii) the method of\\naddressing class imbalance that emerged as dominant in almost all analyzed\\nscenarios was oversampling; (iii) oversampling should be applied to the level\\nthat completely eliminates the imbalance, whereas the optimal undersampling\\nratio depends on the extent of imbalance; (iv) as opposed to some classical\\nmachine learning models, oversampling does not cause overfitting of CNNs; (v)\\nthresholding should be applied to compensate for prior class probabilities when\\noverall number of properly classified cases is of interest.',\n",
       "    'author': [{'name': 'Mateusz Buda'},\n",
       "     {'name': 'Atsuto Maki'},\n",
       "     {'name': 'Maciej A. Mazurowski'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.neunet.2018.07.011'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.neunet.2018.07.011',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1710.05381v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1710.05381v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1801.07648v2',\n",
       "    'updated': '2018-09-13T19:41:22Z',\n",
       "    'published': '2018-01-23T16:41:03Z',\n",
       "    'title': 'Clustering with Deep Learning: Taxonomy and New Methods',\n",
       "    'summary': 'Clustering methods based on deep neural networks have proven promising for\\nclustering real-world data because of their high representational power. In\\nthis paper, we propose a systematic taxonomy of clustering methods that utilize\\ndeep neural networks. We base our taxonomy on a comprehensive review of recent\\nwork and validate the taxonomy in a case study. In this case study, we show\\nthat the taxonomy enables researchers and practitioners to systematically\\ncreate new clustering methods by selectively recombining and replacing distinct\\naspects of previous methods with the goal of overcoming their individual\\nlimitations. The experimental evaluation confirms this and shows that the\\nmethod created for the case study achieves state-of-the-art clustering quality\\nand surpasses it in some cases.',\n",
       "    'author': [{'name': 'Elie Aljalbout'},\n",
       "     {'name': 'Vladimir Golkov'},\n",
       "     {'name': 'Yawar Siddiqui'},\n",
       "     {'name': 'Maximilian Strobel'},\n",
       "     {'name': 'Daniel Cremers'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1801.07648v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1801.07648v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '62H30, 62M45, 91C20',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'H.3.3; I.2.6; I.5; I.5.3; I.5.4',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1802.09816v1',\n",
       "    'updated': '2018-02-27T10:47:06Z',\n",
       "    'published': '2018-02-27T10:47:06Z',\n",
       "    'title': 'Coarse to fine non-rigid registration: a chain of scale-specific neural\\n  networks for multimodal image alignment with application to remote sensing',\n",
       "    'summary': 'We tackle here the problem of multimodal image non-rigid registration, which\\nis of prime importance in remote sensing and medical imaging. The difficulties\\nencountered by classical registration approaches include feature design and\\nslow optimization by gradient descent. By analyzing these methods, we note the\\nsignificance of the notion of scale. We design easy-to-train,\\nfully-convolutional neural networks able to learn scale-specific features. Once\\nchained appropriately, they perform global registration in linear time, getting\\nrid of gradient descent schemes by predicting directly the deformation.We show\\ntheir performance in terms of quality and speed through various tasks of remote\\nsensing multimodal image alignment. In particular, we are able to register\\ncorrectly cadastral maps of buildings as well as road polylines onto RGB\\nimages, and outperform current keypoint matching methods.',\n",
       "    'author': [{'name': 'Armand Zampieri',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'TITANE'}},\n",
       "     {'name': 'Guillaume Charpiat',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'TAU'}},\n",
       "     {'name': 'Yuliya Tarabalka',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'TITANE'}}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1802.09816v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1802.09816v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.01837v1',\n",
       "    'updated': '2018-05-04T16:13:36Z',\n",
       "    'published': '2018-05-04T16:13:36Z',\n",
       "    'title': 'Towards a Spectrum of Graph Convolutional Networks',\n",
       "    'summary': 'We present our ongoing work on understanding the limitations of graph\\nconvolutional networks (GCNs) as well as our work on generalizations of graph\\nconvolutions for representing more complex node attribute dependencies. Based\\non an analysis of GCNs with the help of the corresponding computation graphs,\\nwe propose a generalization of existing GCNs where the aggregation operations\\nare (a) determined by structural properties of the local neighborhood graphs\\nand (b) not restricted to weighted averages. We show that the proposed approach\\nis strictly more expressive while requiring only a modest increase in the\\nnumber of parameters and computations. We also show that the proposed\\ngeneralization is identical to standard convolutional layers when applied to\\nregular grid graphs.',\n",
       "    'author': [{'name': 'Mathias Niepert'}, {'name': 'Alberto Garcia-Duran'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1805.01837v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.01837v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.08249v3',\n",
       "    'updated': '2020-07-19T16:56:49Z',\n",
       "    'published': '2018-05-21T18:36:52Z',\n",
       "    'title': 'Classifier-agnostic saliency map extraction',\n",
       "    'summary': 'Currently available methods for extracting saliency maps identify parts of\\nthe input which are the most important to a specific fixed classifier. We show\\nthat this strong dependence on a given classifier hinders their performance. To\\naddress this problem, we propose classifier-agnostic saliency map extraction,\\nwhich finds all parts of the image that any classifier could use, not just one\\ngiven in advance. We observe that the proposed approach extracts higher quality\\nsaliency maps than prior work while being conceptually simple and easy to\\nimplement. The method sets the new state of the art result for localization\\ntask on the ImageNet data, outperforming all existing weakly-supervised\\nlocalization techniques, despite not using the ground truth labels at the\\ninference time. The code reproducing the results is available at\\nhttps://github.com/kondiz/casme .\\n  The final version of this manuscript is published in Computer Vision and\\nImage Understanding and is available online at\\nhttps://doi.org/10.1016/j.cviu.2020.102969 .',\n",
       "    'author': [{'name': 'Konrad Zolna'},\n",
       "     {'name': 'Krzysztof J. Geras'},\n",
       "     {'name': 'Kyunghyun Cho'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.cviu.2020.102969'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.cviu.2020.102969',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1805.08249v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.08249v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Computer Vision and Image Understanding, Volume 196, 2020, 102969,\\n  ISSN 1077-3142'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1805.08311v1',\n",
       "    'updated': '2018-05-21T22:36:11Z',\n",
       "    'published': '2018-05-21T22:36:11Z',\n",
       "    'title': 'AgileNet: Lightweight Dictionary-based Few-shot Learning',\n",
       "    'summary': 'The success of deep learning models is heavily tied to the use of massive\\namount of labeled data and excessively long training time. With the emergence\\nof intelligent edge applications that use these models, the critical challenge\\nis to obtain the same inference capability on a resource-constrained device\\nwhile providing adaptability to cope with the dynamic changes in the data. We\\npropose AgileNet, a novel lightweight dictionary-based few-shot learning\\nmethodology which provides reduced complexity deep neural network for efficient\\nexecution at the edge while enabling low-cost updates to capture the dynamics\\nof the new data. Evaluations of state-of-the-art few-shot learning benchmarks\\ndemonstrate the superior accuracy of AgileNet compared to prior arts.\\nAdditionally, AgileNet is the first few-shot learning approach that prevents\\nmodel updates by eliminating the knowledge obtained from the primary training.\\nThis property is ensured through the dictionaries learned by our novel\\nend-to-end structured decomposition, which also reduces the memory footprint\\nand computation complexity to match the edge device constraints.',\n",
       "    'author': [{'name': 'Mohammad Ghasemzadeh'},\n",
       "     {'name': 'Fang Lin'},\n",
       "     {'name': 'Bita Darvish Rouhani'},\n",
       "     {'name': 'Farinaz Koushanfar'},\n",
       "     {'name': 'Ke Huang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 Pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1805.08311v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1805.08311v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1806.02997v2',\n",
       "    'updated': '2018-10-25T17:34:51Z',\n",
       "    'published': '2018-06-08T07:28:36Z',\n",
       "    'title': 'q-Space Novelty Detection with Variational Autoencoders',\n",
       "    'summary': 'In machine learning, novelty detection is the task of identifying novel\\nunseen data. During training, only samples from the normal class are available.\\nTest samples are classified as normal or abnormal by assignment of a novelty\\nscore. Here we propose novelty detection methods based on training variational\\nautoencoders (VAEs) on normal data. Since abnormal samples are not used during\\ntraining, we define novelty metrics based on the (partially complementary)\\nassumptions that the VAE is less capable of reconstructing abnormal samples\\nwell; that abnormal samples more strongly violate the VAE regularizer; and that\\nabnormal samples differ from normal samples not only in input-feature space,\\nbut also in the VAE latent space and VAE output. These approaches, combined\\nwith various possibilities of using (e.g. sampling) the probabilistic VAE to\\nobtain scalar novelty scores, yield a large family of methods. We apply these\\nmethods to magnetic resonance imaging, namely to the detection of\\ndiffusion-space (q-space) abnormalities in diffusion MRI scans of multiple\\nsclerosis patients, i.e. to detect multiple sclerosis lesions without using any\\nlesion labels for training. Many of our methods outperform previously proposed\\nq-space novelty detection methods. We also evaluate the proposed methods on the\\nMNIST handwritten digits dataset and show that many of them are able to\\noutperform the state of the art.',\n",
       "    'author': [{'name': 'Aleksei Vasilev'},\n",
       "     {'name': 'Vladimir Golkov'},\n",
       "     {'name': 'Marc Meissner'},\n",
       "     {'name': 'Ilona Lipp'},\n",
       "     {'name': 'Eleonora Sgarlata'},\n",
       "     {'name': 'Valentina Tomassini'},\n",
       "     {'name': 'Derek K. Jones'},\n",
       "     {'name': 'Daniel Cremers'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '11 pages, 2 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1806.02997v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1806.02997v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'stat.ML',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'stat.ML',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '62F15, 62G07, 62M45, 68T30',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'G.3; H.3.3; I.2.4; I.2.6; I.4.6; I.5; I.5.4; J.3',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1806.04552v1',\n",
       "    'updated': '2018-06-12T14:24:02Z',\n",
       "    'published': '2018-06-12T14:24:02Z',\n",
       "    'title': 'Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed\\n  Exploration',\n",
       "    'summary': 'Q-Ensembles are a model-free approach where input images are fed into\\ndifferent Q-networks and exploration is driven by the assumption that\\nuncertainty is proportional to the variance of the output Q-values obtained.\\nThey have been shown to perform relatively well compared to other exploration\\nstrategies. Further, model-based approaches, such as encoder-decoder models\\nhave been used successfully for next frame prediction given previous frames.\\nThis paper proposes to integrate the model-free Q-ensembles and model-based\\napproaches with the hope of compounding the benefits of both and achieving\\nsuperior exploration as a result. Results show that a model-based trajectory\\nmemory approach when combined with Q-ensembles produces superior performance\\nwhen compared to only using Q-ensembles.',\n",
       "    'author': [{'name': 'Sreecharan Sankaranarayanan'},\n",
       "     {'name': 'Raghuram Mandyam Annasamy'},\n",
       "     {'name': 'Katia Sycara'},\n",
       "     {'name': 'Carolyn Penstein Rosé'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to the Thirty-Second Annual Conference on Neural\\n  Information Processing Systems (NIPS 2018)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1806.04552v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1806.04552v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1806.07550v2',\n",
       "    'updated': '2018-12-03T07:08:37Z',\n",
       "    'published': '2018-06-20T04:48:18Z',\n",
       "    'title': 'Binary Ensemble Neural Network: More Bits per Network or More Networks\\n  per Bit?',\n",
       "    'summary': 'Binary neural networks (BNN) have been studied extensively since they run\\ndramatically faster at lower memory and power consumption than floating-point\\nnetworks, thanks to the efficiency of bit operations. However, contemporary\\nBNNs whose weights and activations are both single bits suffer from severe\\naccuracy degradation. To understand why, we investigate the representation\\nability, speed and bias/variance of BNNs through extensive experiments. We\\nconclude that the error of BNNs is predominantly caused by the intrinsic\\ninstability (training time) and non-robustness (train & test time). Inspired by\\nthis investigation, we propose the Binary Ensemble Neural Network (BENN) which\\nleverages ensemble methods to improve the performance of BNNs with limited\\nefficiency cost. While ensemble techniques have been broadly believed to be\\nonly marginally helpful for strong classifiers such as deep neural networks,\\nour analyses and experiments show that they are naturally a perfect fit to\\nboost BNNs. We find that our BENN, which is faster and much more robust than\\nstate-of-the-art binary networks, can even surpass the accuracy of the\\nfull-precision floating number network with the same architecture.',\n",
       "    'author': [{'name': 'Shilin Zhu'},\n",
       "     {'name': 'Xin Dong'},\n",
       "     {'name': 'Hao Su'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1806.07550v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1806.07550v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1806.08568v3',\n",
       "    'updated': '2019-01-22T21:49:25Z',\n",
       "    'published': '2018-06-22T09:22:42Z',\n",
       "    'title': 'Continuous Learning in Single-Incremental-Task Scenarios',\n",
       "    'summary': 'It was recently shown that architectural, regularization and rehearsal\\nstrategies can be used to train deep models sequentially on a number of\\ndisjoint tasks without forgetting previously acquired knowledge. However, these\\nstrategies are still unsatisfactory if the tasks are not disjoint but\\nconstitute a single incremental task (e.g., class-incremental learning). In\\nthis paper we point out the differences between multi-task and\\nsingle-incremental-task scenarios and show that well-known approaches such as\\nLWF, EWC and SI are not ideal for incremental task scenarios. A new approach,\\ndenoted as AR1, combining architectural and regularization strategies is then\\nspecifically proposed. AR1 overhead (in term of memory and computation) is very\\nsmall thus making it suitable for online learning. When tested on CORe50 and\\niCIFAR-100, AR1 outperformed existing regularization strategies by a good\\nmargin.',\n",
       "    'author': [{'name': 'Davide Maltoni'}, {'name': 'Vincenzo Lomonaco'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4),\\n  several typos and minor mistakes corrected'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1806.08568v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1806.08568v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.00053v2',\n",
       "    'updated': '2018-10-27T03:49:01Z',\n",
       "    'published': '2018-06-20T20:27:23Z',\n",
       "    'title': 'Task-Driven Convolutional Recurrent Models of the Visual System',\n",
       "    'summary': \"Feed-forward convolutional neural networks (CNNs) are currently\\nstate-of-the-art for object classification tasks such as ImageNet. Further,\\nthey are quantitatively accurate models of temporally-averaged responses of\\nneurons in the primate brain's visual system. However, biological visual\\nsystems have two ubiquitous architectural features not shared with typical\\nCNNs: local recurrence within cortical areas, and long-range feedback from\\ndownstream areas to upstream areas. Here we explored the role of recurrence in\\nimproving classification performance. We found that standard forms of\\nrecurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the\\nImageNet task. In contrast, novel cells that incorporated two structural\\nfeatures, bypassing and gating, were able to boost task accuracy substantially.\\nWe extended these design principles in an automated search over thousands of\\nmodel architectures, which identified novel local recurrent cells and\\nlong-range feedback connections useful for object recognition. Moreover, these\\ntask-optimized ConvRNNs matched the dynamics of neural activity in the primate\\nvisual system better than feedforward networks, suggesting a role for the\\nbrain's recurrent connections in performing difficult visual behaviors.\",\n",
       "    'author': [{'name': 'Aran Nayebi'},\n",
       "     {'name': 'Daniel Bear'},\n",
       "     {'name': 'Jonas Kubilius'},\n",
       "     {'name': 'Kohitij Kar'},\n",
       "     {'name': 'Surya Ganguli'},\n",
       "     {'name': 'David Sussillo'},\n",
       "     {'name': 'James J. DiCarlo'},\n",
       "     {'name': 'Daniel L. K. Yamins'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NIPS 2018 Camera Ready Version, 16 pages including supplementary\\n  information, 6 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.00053v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.00053v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'q-bio.NC',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'q-bio.NC',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.01697v5',\n",
       "    'updated': '2019-04-27T18:19:39Z',\n",
       "    'published': '2018-07-04T17:57:11Z',\n",
       "    'title': 'Benchmarking Neural Network Robustness to Common Corruptions and Surface\\n  Variations',\n",
       "    'summary': 'In this paper we establish rigorous benchmarks for image classifier\\nrobustness. Our first benchmark, ImageNet-C, standardizes and expands the\\ncorruption robustness topic, while showing which classifiers are preferable in\\nsafety-critical applications. Unlike recent robustness research, this benchmark\\nevaluates performance on commonplace corruptions not worst-case adversarial\\ncorruptions. We find that there are negligible changes in relative corruption\\nrobustness from AlexNet to ResNet classifiers, and we discover ways to enhance\\ncorruption robustness. Then we propose a new dataset called Icons-50 which\\nopens research on a new kind of robustness, surface variation robustness. With\\nthis dataset we evaluate the frailty of classifiers on new styles of known\\nobjects and unexpected instances of known classes. We also demonstrate two\\nmethods that improve surface variation robustness. Together our benchmarks may\\naid future work toward networks that learn fundamental class structure and also\\nrobustly generalize.',\n",
       "    'author': [{'name': 'Dan Hendrycks'}, {'name': 'Thomas G. Dietterich'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Superseded by _Benchmarking Neural Network Robustness to Common\\n  Corruptions and Perturbations_ arXiv:1903.12261'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.01697v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.01697v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1807.06540v1',\n",
       "    'updated': '2018-07-17T16:35:51Z',\n",
       "    'published': '2018-07-17T16:35:51Z',\n",
       "    'title': 'Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try\\n  After Deep Learning',\n",
       "    'summary': 'We found an easy and quick post-learning method named \"Icing on the Cake\" to\\nenhance a classification performance in deep learning. The method is that we\\ntrain only the final classifier again after an ordinary training is done.',\n",
       "    'author': [{'name': 'Tomohiko Konno'}, {'name': 'Michiaki Iwazume'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '3 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1807.06540v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1807.06540v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1808.00300v1',\n",
       "    'updated': '2018-08-01T12:39:43Z',\n",
       "    'published': '2018-08-01T12:39:43Z',\n",
       "    'title': 'Learning Visual Question Answering by Bootstrapping Hard Attention',\n",
       "    'summary': \"Attention mechanisms in biological perception are thought to select subsets\\nof perceptual information for more sophisticated processing which would be\\nprohibitive to perform on all sensory inputs. In computer vision, however,\\nthere has been relatively little exploration of hard attention, where some\\ninformation is selectively ignored, in spite of the success of soft attention,\\nwhere information is re-weighted and aggregated, but never filtered out. Here,\\nwe introduce a new approach for hard attention and find it achieves very\\ncompetitive performance on a recently-released visual question answering\\ndatasets, equalling and in some cases surpassing similar soft attention\\narchitectures while entirely ignoring some features. Even though the hard\\nattention mechanism is thought to be non-differentiable, we found that the\\nfeature magnitudes correlate with semantic relevance, and provide a useful\\nsignal for our mechanism's attentional selection criterion. Because hard\\nattention selects important features of the input information, it can also be\\nmore efficient than analogous soft attention mechanisms. This is especially\\nimportant for recent approaches that use non-local pairwise operations, whereby\\ncomputational and memory costs are quadratic in the size of the set of\\nfeatures.\",\n",
       "    'author': [{'name': 'Mateusz Malinowski'},\n",
       "     {'name': 'Carl Doersch'},\n",
       "     {'name': 'Adam Santoro'},\n",
       "     {'name': 'Peter Battaglia'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ECCV 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1808.00300v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1808.00300v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1811.02657v2',\n",
       "    'updated': '2019-12-09T10:21:21Z',\n",
       "    'published': '2018-11-01T01:27:37Z',\n",
       "    'title': 'A Bayesian Perspective of Convolutional Neural Networks through a\\n  Deconvolutional Generative Model',\n",
       "    'summary': 'Inspired by the success of Convolutional Neural Networks (CNNs) for\\nsupervised prediction in images, we design the Deconvolutional Generative Model\\n(DGM), a new probabilistic generative model whose inference calculations\\ncorrespond to those in a given CNN architecture. The DGM uses a CNN to design\\nthe prior distribution in the probabilistic model. Furthermore, the DGM\\ngenerates images from coarse to finer scales. It introduces a small set of\\nlatent variables at each scale, and enforces dependencies among all the latent\\nvariables via a conjugate prior distribution. This conjugate prior yields a new\\nregularizer based on paths rendered in the generative model for training\\nCNNs-the Rendering Path Normalization (RPN). We demonstrate that this\\nregularizer improves generalization, both in theory and in practice. In\\naddition, likelihood estimation in the DGM yields training losses for CNNs, and\\ninspired by this, we design a new loss termed as the Max-Min cross entropy\\nwhich outperforms the traditional cross-entropy loss for object classification.\\nThe Max-Min cross entropy suggests a new deep network architecture, namely the\\nMax-Min network, which can learn from less labeled data while maintaining good\\nprediction performance. Our experiments demonstrate that the DGM with the RPN\\nand the Max-Min architecture exceeds or matches the-state-of-art on benchmarks\\nincluding SVHN, CIFAR10, and CIFAR100 for semi-supervised and supervised\\nlearning tasks.',\n",
       "    'author': [{'name': 'Tan Nguyen'},\n",
       "     {'name': 'Nhat Ho'},\n",
       "     {'name': 'Ankit Patel'},\n",
       "     {'name': 'Anima Anandkumar'},\n",
       "     {'name': 'Michael I. Jordan'},\n",
       "     {'name': 'Richard G. Baraniuk'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Keywords: neural nets, generative models, semi-supervised learning,\\n  cross-entropy, statistical guarantees 80 pages, 7 figures, 8 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1811.02657v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1811.02657v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1811.03567v3',\n",
       "    'updated': '2018-12-21T02:03:52Z',\n",
       "    'published': '2018-11-08T17:43:59Z',\n",
       "    'title': 'Biologically-plausible learning algorithms can scale to large datasets',\n",
       "    'summary': 'The backpropagation (BP) algorithm is often thought to be biologically\\nimplausible in the brain. One of the main reasons is that BP requires symmetric\\nweight matrices in the feedforward and feedback pathways. To address this\\n\"weight transport problem\" (Grossberg, 1987), two more biologically plausible\\nalgorithms, proposed by Liao et al. (2016) and Lillicrap et al. (2016), relax\\nBP\\'s weight symmetry requirements and demonstrate comparable learning\\ncapabilities to that of BP on small datasets. However, a recent study by\\nBartunov et al. (2018) evaluate variants of target-propagation (TP) and\\nfeedback alignment (FA) on MINIST, CIFAR, and ImageNet datasets, and find that\\nalthough many of the proposed algorithms perform well on MNIST and CIFAR, they\\nperform significantly worse than BP on ImageNet. Here, we additionally evaluate\\nthe sign-symmetry algorithm (Liao et al., 2016), which differs from both BP and\\nFA in that the feedback and feedforward weights share signs but not magnitudes.\\nWe examine the performance of sign-symmetry and feedback alignment on ImageNet\\nand MS COCO datasets using different network architectures (ResNet-18 and\\nAlexNet for ImageNet, RetinaNet for MS COCO). Surprisingly, networks trained\\nwith sign-symmetry can attain classification performance approaching that of\\nBP-trained networks. These results complement the study by Bartunov et al.\\n(2018), and establish a new benchmark for future biologically plausible\\nlearning algorithms on more difficult datasets and more complex architectures.',\n",
       "    'author': [{'name': 'Will Xiao'},\n",
       "     {'name': 'Honglin Chen'},\n",
       "     {'name': 'Qianli Liao'},\n",
       "     {'name': 'Tomaso Poggio'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1811.03567v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1811.03567v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1812.01717v2',\n",
       "    'updated': '2019-03-27T16:43:17Z',\n",
       "    'published': '2018-12-03T03:57:42Z',\n",
       "    'title': 'Towards Accurate Generative Models of Video: A New Metric & Challenges',\n",
       "    'summary': \"Recent advances in deep generative models have lead to remarkable progress in\\nsynthesizing high quality images. Following their successful application in\\nimage processing and representation learning, an important next step is to\\nconsider videos. Learning generative models of video is a much harder task,\\nrequiring a model to capture the temporal dynamics of a scene, in addition to\\nthe visual presentation of objects. While recent attempts at formulating\\ngenerative models of video have had some success, current progress is hampered\\nby (1) the lack of qualitative metrics that consider visual quality, temporal\\ncoherence, and diversity of samples, and (2) the wide gap between purely\\nsynthetic video data sets and challenging real-world data sets in terms of\\ncomplexity. To this extent we propose Fr\\\\'{e}chet Video Distance (FVD), a new\\nmetric for generative models of video, and StarCraft 2 Videos (SCV), a\\nbenchmark of game play from custom starcraft 2 scenarios that challenge the\\ncurrent capabilities of generative models of video. We contribute a large-scale\\nhuman study, which confirms that FVD correlates well with qualitative human\\njudgment of generated videos, and provide initial benchmark results on SCV.\",\n",
       "    'author': [{'name': 'Thomas Unterthiner'},\n",
       "     {'name': 'Sjoerd van Steenkiste'},\n",
       "     {'name': 'Karol Kurach'},\n",
       "     {'name': 'Raphael Marinier'},\n",
       "     {'name': 'Marcin Michalski'},\n",
       "     {'name': 'Sylvain Gelly'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1812.01717v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1812.01717v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1901.09972v1',\n",
       "    'updated': '2019-01-28T19:55:42Z',\n",
       "    'published': '2019-01-28T19:55:42Z',\n",
       "    'title': 'Heartbeat Anomaly Detection using Adversarial Oversampling',\n",
       "    'summary': 'Cardiovascular diseases are one of the most common causes of death in the\\nworld. Prevention, knowledge of previous cases in the family, and early\\ndetection is the best strategy to reduce this fact. Different machine learning\\napproaches to automatic diagnostic are being proposed to this task. As in most\\nhealth problems, the imbalance between examples and classes is predominant in\\nthis problem and affects the performance of the automated solution. In this\\npaper, we address the classification of heartbeats images in different\\ncardiovascular diseases. We propose a two-dimensional Convolutional Neural\\nNetwork for classification after using a InfoGAN architecture for generating\\nsynthetic images to unbalanced classes. We call this proposal Adversarial\\nOversampling and compare it with the classical oversampling methods as SMOTE,\\nADASYN, and RandomOversampling. The results show that the proposed approach\\nimproves the classifier performance for the minority classes without harming\\nthe performance in the balanced classes.',\n",
       "    'author': [{'name': 'Jefferson L. P. Lima'},\n",
       "     {'name': 'David Macêdo'},\n",
       "     {'name': 'Cleber Zanchettin'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/IJCNN.2019.8852242'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/IJCNN.2019.8852242',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1901.09972v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1901.09972v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2019 International Joint Conference on Neural Networks (IJCNN)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1902.10178v1',\n",
       "    'updated': '2019-02-26T19:25:11Z',\n",
       "    'published': '2019-02-26T19:25:11Z',\n",
       "    'title': 'Unmasking Clever Hans Predictors and Assessing What Machines Really\\n  Learn',\n",
       "    'summary': 'Current learning machines have successfully solved hard application problems,\\nreaching high accuracy and displaying seemingly \"intelligent\" behavior. Here we\\napply recent techniques for explaining decisions of state-of-the-art learning\\nmachines and analyze various tasks from computer vision and arcade games. This\\nshowcases a spectrum of problem-solving behaviors ranging from naive and\\nshort-sighted, to well-informed and strategic. We observe that standard\\nperformance evaluation metrics can be oblivious to distinguishing these diverse\\nproblem solving behaviors. Furthermore, we propose our semi-automated Spectral\\nRelevance Analysis that provides a practically effective way of characterizing\\nand validating the behavior of nonlinear learning machines. This helps to\\nassess whether a learned model indeed delivers reliably for the problem that it\\nwas conceived for. Furthermore, our work intends to add a voice of caution to\\nthe ongoing excitement about machine intelligence and pledges to evaluate and\\njudge some of these recent successes in a more nuanced manner.',\n",
       "    'author': [{'name': 'Sebastian Lapuschkin'},\n",
       "     {'name': 'Stephan Wäldchen'},\n",
       "     {'name': 'Alexander Binder'},\n",
       "     {'name': 'Grégoire Montavon'},\n",
       "     {'name': 'Wojciech Samek'},\n",
       "     {'name': 'Klaus-Robert Müller'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1038/s41467-019-08987-4'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1038/s41467-019-08987-4',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1902.10178v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1902.10178v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for publication in Nature Communications'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1903.11421v1',\n",
       "    'updated': '2019-03-27T13:41:17Z',\n",
       "    'published': '2019-03-27T13:41:17Z',\n",
       "    'title': 'Social Behavioral Phenotyping of Drosophila with a2D-3D Hybrid CNN\\n  Framework',\n",
       "    'summary': 'Behavioural phenotyping of Drosophila is an important means in biological and\\nmedical research to identify genetic, pathologic or psychologic impact on\\nanimal behaviour.',\n",
       "    'author': [{'name': 'Ziping Jiang'},\n",
       "     {'name': 'Paul L. Chazot'},\n",
       "     {'name': 'M. Emre Celebi'},\n",
       "     {'name': 'Danny Crookes'},\n",
       "     {'name': 'Richard Jiang'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Access 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1903.11421v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1903.11421v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.ET', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.05849v2',\n",
       "    'updated': '2019-09-11T05:32:07Z',\n",
       "    'published': '2019-05-14T21:26:56Z',\n",
       "    'title': 'Consensus-based Interpretable Deep Neural Networks with Application to\\n  Mortality Prediction',\n",
       "    'summary': 'Deep neural networks have achieved remarkable success in various challenging\\ntasks. However, the black-box nature of such networks is not acceptable to\\ncritical applications, such as healthcare. In particular, the existence of\\nadversarial examples and their overgeneralization to irrelevant,\\nout-of-distribution inputs with high confidence makes it difficult, if not\\nimpossible, to explain decisions by such networks. In this paper, we analyze\\nthe underlying mechanism of generalization of deep neural networks and propose\\nan ($n$, $k$) consensus algorithm which is insensitive to adversarial examples\\nand can reliably reject out-of-distribution samples. Furthermore, the consensus\\nalgorithm is able to improve classification accuracy by using multiple trained\\ndeep neural networks. To handle the complexity of deep neural networks, we\\ncluster linear approximations of individual models and identify highly\\ncorrelated clusters among different models to capture feature importance\\nrobustly, resulting in improved interpretability. Motivated by the importance\\nof building accurate and interpretable prediction models for healthcare, our\\nexperimental results on an ICU dataset show the effectiveness of our algorithm\\nin enhancing both the prediction accuracy and the interpretability of deep\\nneural network models on one-year patient mortality prediction. In particular,\\nwhile the proposed method maintains similar interpretability as conventional\\nshallow models such as logistic regression, it improves the prediction accuracy\\nsignificantly.',\n",
       "    'author': [{'name': 'Shaeke Salman'},\n",
       "     {'name': 'Seyedeh Neelufar Payrovnaziri'},\n",
       "     {'name': 'Xiuwen Liu'},\n",
       "     {'name': 'Pablo Rengifo-Moreno'},\n",
       "     {'name': 'Zhe He'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 6 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.05849v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.05849v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.13211v4',\n",
       "    'updated': '2020-02-15T06:56:25Z',\n",
       "    'published': '2019-05-30T17:53:30Z',\n",
       "    'title': 'What Can Neural Networks Reason About?',\n",
       "    'summary': 'Neural networks have succeeded in many reasoning tasks. Empirically, these\\ntasks require specialized network structures, e.g., Graph Neural Networks\\n(GNNs) perform well on many such tasks, but less structured networks fail.\\nTheoretically, there is limited understanding of why and when a network\\nstructure generalizes better than others, although they have equal expressive\\npower. In this paper, we develop a framework to characterize which reasoning\\ntasks a network can learn well, by studying how well its computation structure\\naligns with the algorithmic structure of the relevant reasoning process. We\\nformally define this algorithmic alignment and derive a sample complexity bound\\nthat decreases with better alignment. This framework offers an explanation for\\nthe empirical success of popular reasoning models, and suggests their\\nlimitations. As an example, we unify seemingly different reasoning tasks, such\\nas intuitive physics, visual question answering, and shortest paths, via the\\nlens of a powerful algorithmic paradigm, dynamic programming (DP). We show that\\nGNNs align with DP and thus are expected to solve these tasks. On several\\nreasoning tasks, our theory is supported by empirical results.',\n",
       "    'author': [{'name': 'Keyulu Xu'},\n",
       "     {'name': 'Jingling Li'},\n",
       "     {'name': 'Mozhi Zhang'},\n",
       "     {'name': 'Simon S. Du'},\n",
       "     {'name': 'Ken-ichi Kawarabayashi'},\n",
       "     {'name': 'Stefanie Jegelka'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.13211v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.13211v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1905.13607v1',\n",
       "    'updated': '2019-05-31T13:24:30Z',\n",
       "    'published': '2019-05-31T13:24:30Z',\n",
       "    'title': '3DPalsyNet: A Facial Palsy Grading and Motion Recognition Framework\\n  using Fully 3D Convolutional Neural Networks',\n",
       "    'summary': \"The capability to perform facial analysis from video sequences has\\nsignificant potential to positively impact in many areas of life. One such area\\nrelates to the medical domain to specifically aid in the diagnosis and\\nrehabilitation of patients with facial palsy. With this application in mind,\\nthis paper presents an end-to-end framework, named 3DPalsyNet, for the tasks of\\nmouth motion recognition and facial palsy grading. 3DPalsyNet utilizes a 3D CNN\\narchitecture with a ResNet backbone for the prediction of these dynamic tasks.\\nLeveraging transfer learning from a 3D CNNs pre-trained on the Kinetics data\\nset for general action recognition, the model is modified to apply joint\\nsupervised learning using center and softmax loss concepts. 3DPalsyNet is\\nevaluated on a test set consisting of individuals with varying ranges of facial\\npalsy and mouth motions and the results have shown an attractive level of\\nclassification accuracy in these task of 82% and 86% respectively. The frame\\nduration and the loss function affect was studied in terms of the predictive\\nqualities of the proposed 3DPalsyNet, where it was found shorter frame\\nduration's of 8 performed best for this specific task. Centre loss and softmax\\nhave shown improvements in spatio-temporal feature learning than softmax loss\\nalone, this is in agreement with earlier work involving the spatial domain.\",\n",
       "    'author': [{'name': 'Gary Storey'},\n",
       "     {'name': 'Richard Jiang'},\n",
       "     {'name': 'Shelagh Keogh'},\n",
       "     {'name': 'Ahmed Bouridane'},\n",
       "     {'name': 'Chang-Tsun Li'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Access 2019'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1905.13607v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1905.13607v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GR', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1906.11667v3',\n",
       "    'updated': '2020-07-16T13:34:50Z',\n",
       "    'published': '2019-06-27T14:12:52Z',\n",
       "    'title': 'Evolving Robust Neural Architectures to Defend from Adversarial Attacks',\n",
       "    'summary': 'Neural networks are prone to misclassify slightly modified input images.\\nRecently, many defences have been proposed, but none have improved the\\nrobustness of neural networks consistently. Here, we propose to use adversarial\\nattacks as a function evaluation to search for neural architectures that can\\nresist such attacks automatically. Experiments on neural architecture search\\nalgorithms from the literature show that although accurate, they are not able\\nto find robust architectures. A significant reason for this lies in their\\nlimited search space. By creating a novel neural architecture search with\\noptions for dense layers to connect with convolution layers and vice-versa as\\nwell as the addition of concatenation layers in the search, we were able to\\nevolve an architecture that is inherently accurate on adversarial samples.\\nInterestingly, this inherent robustness of the evolved architecture rivals\\nstate-of-the-art defences such as adversarial training while being trained only\\non the non-adversarial samples. Moreover, the evolved architecture makes use of\\nsome peculiar traits which might be useful for developing even more robust\\nones. Thus, the results here confirm that more robust architectures exist as\\nwell as opens up a new realm of feasibilities for the development and\\nexploration of neural networks.\\n  Code available at http://bit.ly/RobustArchitectureSearch.',\n",
       "    'author': [{'name': 'Shashank Kotyan'},\n",
       "     {'name': 'Danilo Vasconcellos Vargas'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Pre-print of the published article in Proceedings of the Workshop on\\n  Artificial Intelligence Safety 2020, co-located with the 29th International\\n  Joint Conference on Artificial Intelligence and the 17th Pacific Rim\\n  International Conference on Artificial Intelligence (IJCAI-PRICAI 2020)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1906.11667v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1906.11667v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CR', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1907.02124v2',\n",
       "    'updated': '2020-01-07T19:43:16Z',\n",
       "    'published': '2019-07-03T20:27:51Z',\n",
       "    'title': 'Non-Structured DNN Weight Pruning -- Is It Beneficial in Any Platform?',\n",
       "    'summary': 'Large deep neural network (DNN) models pose the key challenge to energy\\nefficiency due to the significantly higher energy consumption of off-chip DRAM\\naccesses than arithmetic or SRAM operations. It motivates the intensive\\nresearch on model compression with two main approaches. Weight pruning\\nleverages the redundancy in the number of weights and can be performed in a\\nnon-structured, which has higher flexibility and pruning rate but incurs index\\naccesses due to irregular weights, or structured manner, which preserves the\\nfull matrix structure with lower pruning rate. Weight quantization leverages\\nthe redundancy in the number of bits in weights. Compared to pruning,\\nquantization is much more hardware-friendly, and has become a \"must-do\" step\\nfor FPGA and ASIC implementations. This paper provides a definitive answer to\\nthe question for the first time. First, we build ADMM-NN-S by extending and\\nenhancing ADMM-NN, a recently proposed joint weight pruning and quantization\\nframework. Second, we develop a methodology for fair and fundamental comparison\\nof non-structured and structured pruning in terms of both storage and\\ncomputation efficiency. Our results show that ADMM-NN-S consistently\\noutperforms the prior art: (i) it achieves 348x, 36x, and 8x overall weight\\npruning on LeNet-5, AlexNet, and ResNet-50, respectively, with (almost) zero\\naccuracy loss; (ii) we demonstrate the first fully binarized (for all layers)\\nDNNs can be lossless in accuracy in many cases. These results provide a strong\\nbaseline and credibility of our study. Based on the proposed comparison\\nframework, with the same accuracy and quantization, the results show that\\nnon-structrued pruning is not competitive in terms of both storage and\\ncomputation efficiency. Thus, we conclude that non-structured pruning is\\nconsidered harmful. We urge the community not to continue the DNN inference\\nacceleration for non-structured sparsity.',\n",
       "    'author': [{'name': 'Xiaolong Ma'},\n",
       "     {'name': 'Sheng Lin'},\n",
       "     {'name': 'Shaokai Ye'},\n",
       "     {'name': 'Zhezhi He'},\n",
       "     {'name': 'Linfeng Zhang'},\n",
       "     {'name': 'Geng Yuan'},\n",
       "     {'name': 'Sia Huat Tan'},\n",
       "     {'name': 'Zhengang Li'},\n",
       "     {'name': 'Deliang Fan'},\n",
       "     {'name': 'Xuehai Qian'},\n",
       "     {'name': 'Xue Lin'},\n",
       "     {'name': 'Kaisheng Ma'},\n",
       "     {'name': 'Yanzhi Wang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1907.02124v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1907.02124v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1907.03141v2',\n",
       "    'updated': '2019-09-11T12:15:38Z',\n",
       "    'published': '2019-07-06T15:40:02Z',\n",
       "    'title': 'AutoCompress: An Automatic DNN Structured Pruning Framework for\\n  Ultra-High Compression Rates',\n",
       "    'summary': 'Structured weight pruning is a representative model compression technique of\\nDNNs to reduce the storage and computation requirements and accelerate\\ninference. An automatic hyperparameter determination process is necessary due\\nto the large number of flexible hyperparameters. This work proposes\\nAutoCompress, an automatic structured pruning framework with the following key\\nperformance improvements: (i) effectively incorporate the combination of\\nstructured pruning schemes in the automatic process; (ii) adopt the\\nstate-of-art ADMM-based structured weight pruning as the core algorithm, and\\npropose an innovative additional purification step for further weight reduction\\nwithout accuracy loss; and (iii) develop effective heuristic search method\\nenhanced by experience-based guided search, replacing the prior deep\\nreinforcement learning technique which has underlying incompatibility with the\\ntarget pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets\\ndemonstrate that AutoCompress is the key to achieve ultra-high pruning rates on\\nthe number of weights and FLOPs that cannot be achieved before. As an example,\\nAutoCompress outperforms the prior work on automatic model compression by up to\\n33x in pruning rate (120x reduction in the actual parameter count) under the\\nsame accuracy. Significant inference speedup has been observed from the\\nAutoCompress framework on actual measurements on smartphone. We release all\\nmodels of this work at anonymous link: http://bit.ly/2VZ63dS.',\n",
       "    'author': [{'name': 'Ning Liu'},\n",
       "     {'name': 'Xiaolong Ma'},\n",
       "     {'name': 'Zhiyuan Xu'},\n",
       "     {'name': 'Yanzhi Wang'},\n",
       "     {'name': 'Jian Tang'},\n",
       "     {'name': 'Jieping Ye'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1907.03141v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1907.03141v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1907.09050v2',\n",
       "    'updated': '2019-10-30T23:50:49Z',\n",
       "    'published': '2019-07-21T23:09:35Z',\n",
       "    'title': 'Shallow Unorganized Neural Networks using Smart Neuron Model for Visual\\n  Perception',\n",
       "    'summary': \"The recent success of Deep Neural Networks (DNNs) has revealed the\\nsignificant capability of neural computing in many challenging applications.\\nAlthough DNNs are derived from emulating biological neurons, there still exist\\ndoubts over whether or not DNNs are the final and best model to emulate the\\nmechanism of human intelligence. In particular, there are two discrepancies\\nbetween computational DNN models and the observed facts of biological neurons.\\nFirst, human neurons are interconnected randomly, while DNNs need\\ncarefully-designed architectures to work properly. Second, human neurons\\nusually have a long spiking latency (~100ms) which implies that not many layers\\ncan be involved in making a decision, while DNNs could have hundreds of layers\\nto guarantee high accuracy. In this paper, we propose a new computational\\nmodel, namely shallow unorganized neural networks (SUNNs), in contrast to\\nANNs/DNNs. The proposed SUNNs differ from standard ANNs or DNNs in three\\nfundamental aspects: 1) SUNNs are based on an adaptive neuron cell model, Smart\\nNeurons, that allows each artificial neuron cell to adaptively respond to its\\ninputs rather than carrying out a fixed weighted-sum operation like the classic\\nneuron model in ANNs/DNNs; 2) SUNNs can cope with computational tasks with very\\nshallow architectures; 3) SUNNs have a natural topology with random\\ninterconnections, as the human brain does, and as proposed by Turing's B-type\\nunorganized machines. We implemented the proposed SUNN architecture and tested\\nit on a number of unsupervised early stage visual perception tasks.\\nSurprisingly, such simple shallow architectures achieved very good results in\\nour experiments. The success of our new computational model makes it the first\\nworkable example of Turing's B-Type unorganized machine that can achieve\\ncomparable or better performance against the state-of-the-art algorithms.\",\n",
       "    'author': [{'name': 'Richard Jiang'}, {'name': 'Danny Crookes'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1907.09050v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1907.09050v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.IV', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.07490v1',\n",
       "    'updated': '2019-09-16T21:24:19Z',\n",
       "    'published': '2019-09-16T21:24:19Z',\n",
       "    'title': 'They Might NOT Be Giants: Crafting Black-Box Adversarial Examples with\\n  Fewer Queries Using Particle Swarm Optimization',\n",
       "    'summary': 'Machine learning models have been found to be susceptible to adversarial\\nexamples that are often indistinguishable from the original inputs. These\\nadversarial examples are created by applying adversarial perturbations to input\\nsamples, which would cause them to be misclassified by the target models.\\nAttacks that search and apply the perturbations to create adversarial examples\\nare performed in both white-box and black-box settings, depending on the\\ninformation available to the attacker about the target. For black-box attacks,\\nthe only capability available to the attacker is the ability to query the\\ntarget with specially crafted inputs and observing the labels returned by the\\nmodel. Current black-box attacks either have low success rates, requires a high\\nnumber of queries, or produce adversarial examples that are easily\\ndistinguishable from their sources. In this paper, we present AdversarialPSO, a\\nblack-box attack that uses fewer queries to create adversarial examples with\\nhigh success rates. AdversarialPSO is based on the evolutionary search\\nalgorithm Particle Swarm Optimization, a populationbased gradient-free\\noptimization algorithm. It is flexible in balancing the number of queries\\nsubmitted to the target vs the quality of imperceptible adversarial examples.\\nThe attack has been evaluated using the image classification benchmark datasets\\nCIFAR-10, MNIST, and Imagenet, achieving success rates of 99.6%, 96.3%, and\\n82.0%, respectively, while submitting substantially fewer queries than the\\nstate-of-the-art. We also present a black-box method for isolating salient\\nfeatures used by models when making classifications. This method, called Swarms\\nwith Individual Search Spaces or SWISS, creates adversarial examples by finding\\nand modifying the most important features in the input.',\n",
       "    'author': [{'name': 'Rayan Mosli'},\n",
       "     {'name': 'Matthew Wright'},\n",
       "     {'name': 'Bo Yuan'},\n",
       "     {'name': 'Yin Pan'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1909.07490v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.07490v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2001.08357v2',\n",
       "    'updated': '2020-02-22T03:00:10Z',\n",
       "    'published': '2020-01-23T03:30:56Z',\n",
       "    'title': 'BLK-REW: A Unified Block-based DNN Pruning Framework using Reweighted\\n  Regularization Method',\n",
       "    'summary': 'Accelerating DNN execution on various resource-limited computing platforms\\nhas been a long-standing problem. Prior works utilize l1-based group lasso or\\ndynamic regularization such as ADMM to perform structured pruning on DNN models\\nto leverage the parallel computing architectures. However, both of the pruning\\ndimensions and pruning methods lack universality, which leads to degraded\\nperformance and limited applicability. To solve the problem, we propose a new\\nblock-based pruning framework that comprises a general and flexible structured\\npruning dimension as well as a powerful and efficient reweighted regularization\\nmethod. Our framework is universal, which can be applied to both CNNs and RNNs,\\nimplying complete support for the two major kinds of computation-intensive\\nlayers (i.e., CONV and FC layers). To complete all aspects of the\\npruning-for-acceleration task, we also integrate compiler-based code\\noptimization into our framework that can perform DNN inference in a real-time\\nmanner. To the best of our knowledge, it is the first time that the weight\\npruning framework achieves universal coverage for both CNNs and RNNs with\\nreal-time mobile acceleration and no accuracy compromise.',\n",
       "    'author': [{'name': 'Xiaolong Ma'},\n",
       "     {'name': 'Zhengang Li'},\n",
       "     {'name': 'Yifan Gong'},\n",
       "     {'name': 'Tianyun Zhang'},\n",
       "     {'name': 'Wei Niu'},\n",
       "     {'name': 'Zheng Zhan'},\n",
       "     {'name': 'Pu Zhao'},\n",
       "     {'name': 'Jian Tang'},\n",
       "     {'name': 'Xue Lin'},\n",
       "     {'name': 'Bin Ren'},\n",
       "     {'name': 'Yanzhi Wang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2001.08357v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2001.08357v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.06513v2',\n",
       "    'updated': '2020-09-17T00:45:39Z',\n",
       "    'published': '2020-03-13T23:52:03Z',\n",
       "    'title': 'A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration\\n  Framework',\n",
       "    'summary': 'Weight pruning of deep neural networks (DNNs) has been proposed to satisfy\\nthe limited storage and computing capability of mobile edge devices. However,\\nprevious pruning methods mainly focus on reducing the model size and/or\\nimproving performance without considering the privacy of user data. To mitigate\\nthis concern, we propose a privacy-preserving-oriented pruning and mobile\\nacceleration framework that does not require the private training dataset. At\\nthe algorithm level of the proposed framework, a systematic weight pruning\\ntechnique based on the alternating direction method of multipliers (ADMM) is\\ndesigned to iteratively solve the pattern-based pruning problem for each layer\\nwith randomly generated synthetic data. In addition, corresponding\\noptimizations at the compiler level are leveraged for inference accelerations\\non devices. With the proposed framework, users could avoid the time-consuming\\npruning process for non-experts and directly benefit from compressed models.\\nExperimental results show that the proposed framework outperforms three\\nstate-of-art end-to-end DNN frameworks, i.e., TensorFlow-Lite, TVM, and MNN,\\nwith speedup up to 4.2X, 2.5X, and 2.0X, respectively, with almost no accuracy\\nloss, while preserving data privacy.',\n",
       "    'author': [{'name': 'Yifan Gong'},\n",
       "     {'name': 'Zheng Zhan'},\n",
       "     {'name': 'Zhengang Li'},\n",
       "     {'name': 'Wei Niu'},\n",
       "     {'name': 'Xiaolong Ma'},\n",
       "     {'name': 'Wenhao Wang'},\n",
       "     {'name': 'Bin Ren'},\n",
       "     {'name': 'Caiwen Ding'},\n",
       "     {'name': 'Xue Lin'},\n",
       "     {'name': 'Xiaolin Xu'},\n",
       "     {'name': 'Yanzhi Wang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.06513v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.06513v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.07631v2',\n",
       "    'updated': '2021-02-25T12:39:41Z',\n",
       "    'published': '2020-03-17T10:45:51Z',\n",
       "    'title': 'Explaining Deep Neural Networks and Beyond: A Review of Methods and\\n  Applications',\n",
       "    'summary': \"With the broader and highly successful usage of machine learning in industry\\nand the sciences, there has been a growing demand for Explainable AI.\\nInterpretability and explanation methods for gaining a better understanding\\nabout the problem solving abilities and strategies of nonlinear Machine\\nLearning, in particular, deep neural networks, are therefore receiving\\nincreased attention. In this work we aim to (1) provide a timely overview of\\nthis active emerging field, with a focus on 'post-hoc' explanations, and\\nexplain its theoretical foundations, (2) put interpretability algorithms to a\\ntest both from a theory and comparative evaluation perspective using extensive\\nsimulations, (3) outline best practice aspects i.e. how to best include\\ninterpretation methods into the standard usage of machine learning and (4)\\ndemonstrate successful usage of explainable AI in a representative selection of\\napplication scenarios. Finally, we discuss challenges and possible future\\ndirections of this exciting foundational field of machine learning.\",\n",
       "    'author': [{'name': 'Wojciech Samek'},\n",
       "     {'name': 'Grégoire Montavon'},\n",
       "     {'name': 'Sebastian Lapuschkin'},\n",
       "     {'name': 'Christopher J. Anders'},\n",
       "     {'name': 'Klaus-Robert Müller'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/JPROC.2021.3060483'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/JPROC.2021.3060483',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2003.07631v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.07631v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '30 pages, 20 figures'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.07693v2',\n",
       "    'updated': '2021-03-29T22:49:58Z',\n",
       "    'published': '2020-10-14T00:45:29Z',\n",
       "    'title': 'Linking average- and worst-case perturbation robustness via class\\n  selectivity and dimensionality',\n",
       "    'summary': \"Representational sparsity is known to affect robustness to input\\nperturbations in deep neural networks (DNNs), but less is known about how the\\nsemantic content of representations affects robustness. Class selectivity-the\\nvariability of a unit's responses across data classes or dimensions-is one way\\nof quantifying the sparsity of semantic representations. Given recent evidence\\nthat class selectivity may not be necessary for, and in some cases can impair\\ngeneralization, we investigate whether it also confers robustness (or\\nvulnerability) to perturbations of input data. We found that networks\\nregularized to have lower levels of class selectivity were more robust to\\naverage-case (naturalistic) perturbations, while networks with higher class\\nselectivity are more vulnerable. In contrast, class selectivity increases\\nrobustness to multiple types of worst-case (i.e. white box adversarial)\\nperturbations, suggesting that while decreasing class selectivity is helpful\\nfor average-case perturbations, it is harmful for worst-case perturbations. To\\nexplain this difference, we studied the dimensionality of the networks'\\nrepresentations: we found that the dimensionality of early-layer\\nrepresentations is inversely proportional to a network's class selectivity, and\\nthat adversarial samples cause a larger increase in early-layer dimensionality\\nthan corrupted samples. Furthermore, the input-unit gradient is more variable\\nacross samples and units in high-selectivity networks compared to\\nlow-selectivity networks. These results lead to the conclusion that units\\nparticipate more consistently in low-selectivity regimes compared to\\nhigh-selectivity regimes, effectively creating a larger attack surface and\\nhence vulnerability to worst-case perturbations.\",\n",
       "    'author': [{'name': 'Matthew L. Leavitt'}, {'name': 'Ari Morcos'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: text overlap with arXiv:2007.04440'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.07693v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.07693v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.08262v5',\n",
       "    'updated': '2021-10-25T10:23:56Z',\n",
       "    'published': '2020-10-16T09:32:35Z',\n",
       "    'title': 'Local plasticity rules can learn deep representations using\\n  self-supervised contrastive predictions',\n",
       "    'summary': 'Learning in the brain is poorly understood and learning rules that respect\\nbiological constraints, yet yield deep hierarchical representations, are still\\nunknown. Here, we propose a learning rule that takes inspiration from\\nneuroscience and recent advances in self-supervised deep learning. Learning\\nminimizes a simple layer-specific loss function and does not need to\\nback-propagate error signals within or between layers. Instead, weight updates\\nfollow a local, Hebbian, learning rule that only depends on pre- and\\npost-synaptic neuronal activity, predictive dendritic input and widely\\nbroadcasted modulation factors which are identical for large groups of neurons.\\nThe learning rule applies contrastive predictive learning to a causal,\\nbiological setting using saccades (i.e. rapid shifts in gaze direction). We\\nfind that networks trained with this self-supervised and local rule build deep\\nhierarchical representations of images, speech and video.',\n",
       "    'author': [{'name': 'Bernd Illing'},\n",
       "     {'name': 'Jean Ventura'},\n",
       "     {'name': 'Guillaume Bellec'},\n",
       "     {'name': 'Wulfram Gerstner'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.08262v5',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.08262v5',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AR', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.07661v1',\n",
       "    'updated': '2020-11-15T23:38:59Z',\n",
       "    'published': '2020-11-15T23:38:59Z',\n",
       "    'title': 'hyper-sinh: An Accurate and Reliable Function from Shallow to Deep\\n  Learning in TensorFlow and Keras',\n",
       "    'summary': \"This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation\\nfunction suitable for Deep Learning (DL)-based algorithms for supervised\\nlearning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in\\nthe open source Python libraries TensorFlow and Keras, is thus described and\\nvalidated as an accurate and reliable activation function for both shallow and\\ndeep neural networks. Improvements in accuracy and reliability in image and\\ntext classification tasks on five (N = 5) benchmark data sets available from\\nKeras are discussed. Experimental results demonstrate the overall competitive\\nclassification performance of both shallow and deep neural networks, obtained\\nvia this novel function. This function is evaluated with respect to gold\\nstandard activation functions, demonstrating its overall competitive accuracy\\nand reliability for both image and text classification.\",\n",
       "    'author': [{'name': 'Luca Parisi'},\n",
       "     {'name': 'Renfei Ma'},\n",
       "     {'name': 'Narrendar RaviChandran'},\n",
       "     {'name': 'Matteo Lanzillotta'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.mlwa.2021.100112'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.mlwa.2021.100112',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2011.07661v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.07661v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '19 pages, 6 listings/Python code snippets, 4 figures, 5 tables'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T07, 68T10, 68T45, 68T50, 68U35',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.1; I.2.7; I.2.10; I.4.9; I.5.1; I.5.4; I.5.5',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.08859v3',\n",
       "    'updated': '2021-08-27T13:02:16Z',\n",
       "    'published': '2020-12-16T11:00:19Z',\n",
       "    'title': 'Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces',\n",
       "    'summary': 'Current state-of-the-art Neural Architecture Search (NAS) methods neither\\nefficiently scale to multiple hardware platforms, nor handle diverse\\narchitectural search-spaces. To remedy this, we present DONNA (Distilling\\nOptimal Neural Network Architectures), a novel pipeline for rapid, scalable and\\ndiverse NAS, that scales to many user scenarios. DONNA consists of three\\nphases. First, an accuracy predictor is built using blockwise knowledge\\ndistillation from a reference model. This predictor enables searching across\\ndiverse networks with varying macro-architectural parameters such as layer\\ntypes and attention mechanisms, as well as across micro-architectural\\nparameters such as block repeats and expansion rates. Second, a rapid\\nevolutionary search finds a set of pareto-optimal architectures for any\\nscenario using the accuracy predictor and on-device measurements. Third,\\noptimal models are quickly finetuned to training-from-scratch accuracy. DONNA\\nis up to 100x faster than MNasNet in finding state-of-the-art architectures\\non-device. Classifying ImageNet, DONNA architectures are 20% faster than\\nEfficientNet-B0 and MobileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5%\\nhigher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition\\nto NAS, DONNA is used for search-space extension and exploration, as well as\\nhardware-aware model compression.',\n",
       "    'author': [{'name': 'Bert Moons'},\n",
       "     {'name': 'Parham Noorzad'},\n",
       "     {'name': 'Andrii Skliar'},\n",
       "     {'name': 'Giovanni Mariani'},\n",
       "     {'name': 'Dushyant Mehta'},\n",
       "     {'name': 'Chris Lott'},\n",
       "     {'name': 'Tijmen Blankevoort'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted at ICCV2021. Main text 9 pages, Full text 21 pages, 18\\n  figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2012.08859v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.08859v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2012.11905v3',\n",
       "    'updated': '2021-08-05T11:21:16Z',\n",
       "    'published': '2020-12-22T10:08:05Z',\n",
       "    'title': 'GANterfactual - Counterfactual Explanations for Medical Non-Experts\\n  using Generative Adversarial Learning',\n",
       "    'summary': 'With the ongoing rise of machine learning, the need for methods for\\nexplaining decisions made by artificial intelligence systems is becoming a more\\nand more important topic. Especially for image classification tasks, many\\nstate-of-the-art tools to explain such classifiers rely on visual highlighting\\nof important areas of the input data. Contrary, counterfactual explanation\\nsystems try to enable a counterfactual reasoning by modifying the input image\\nin a way such that the classifier would have made a different prediction. By\\ndoing so, the users of counterfactual explanation systems are equipped with a\\ncompletely different kind of explanatory information. However, methods for\\ngenerating realistic counterfactual explanations for image classifiers are\\nstill rare. Especially in medical contexts, where relevant information often\\nconsists of textural and structural information, high-quality counterfactual\\nimages have the potential to give meaningful insights into decision processes.\\nIn this work, we present GANterfactual, an approach to generate such\\ncounterfactual image explanations based on adversarial image-to-image\\ntranslation techniques. Additionally, we conduct a user study to evaluate our\\napproach in an exemplary medical use case. Our results show that, in the chosen\\nmedical use-case, counterfactual explanations lead to significantly better\\nresults regarding mental models, explanation satisfaction, trust, emotions, and\\nself-efficacy than two state-of-the-art systems that work with saliency maps,\\nnamely LIME and LRP.',\n",
       "    'author': [{'name': 'Silvan Mertes'},\n",
       "     {'name': 'Tobias Huber'},\n",
       "     {'name': 'Katharina Weitz'},\n",
       "     {'name': 'Alexander Heimerl'},\n",
       "     {'name': 'Elisabeth André'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.3389/frai.2022.825565'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.3389/frai.2022.825565',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2012.11905v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2012.11905v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.06358v1',\n",
       "    'updated': '2021-02-12T06:09:20Z',\n",
       "    'published': '2021-02-12T06:09:20Z',\n",
       "    'title': 'Min-Max-Plus Neural Networks',\n",
       "    'summary': 'We present a new model of neural networks called Min-Max-Plus Neural Networks\\n(MMP-NNs) based on operations in tropical arithmetic. In general, an MMP-NN is\\ncomposed of three types of alternately stacked layers, namely linear layers,\\nmin-plus layers and max-plus layers. Specifically, the latter two types of\\nlayers constitute the nonlinear part of the network which is trainable and more\\nsophisticated compared to the nonlinear part of conventional neural networks.\\nIn addition, we show that with higher capability of nonlinearity expression,\\nMMP-NNs are universal approximators of continuous functions, even when the\\nnumber of multiplication operations is tremendously reduced (possibly to none\\nin certain extreme cases). Furthermore, we formulate the backpropagation\\nalgorithm in the training process of MMP-NNs and introduce an algorithm of\\nnormalization to improve the rate of convergence in training.',\n",
       "    'author': [{'name': 'Ye Luo'}, {'name': 'Shiqing Fan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '21 pages, 5 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.06358v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.06358v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'math.CO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '14T90', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'G.2.1; I.2.6; I.5.1',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.11506v1',\n",
       "    'updated': '2021-02-23T05:43:54Z',\n",
       "    'published': '2021-02-23T05:43:54Z',\n",
       "    'title': 'Comparative evaluation of CNN architectures for Image Caption Generation',\n",
       "    'summary': 'Aided by recent advances in Deep Learning, Image Caption Generation has seen\\ntremendous progress over the last few years. Most methods use transfer learning\\nto extract visual information, in the form of image features, with the help of\\npre-trained Convolutional Neural Network models followed by transformation of\\nthe visual information using a Caption Generator module to generate the output\\nsentences. Different methods have used different Convolutional Neural Network\\nArchitectures and, to the best of our knowledge, there is no systematic study\\nwhich compares the relative efficacy of different Convolutional Neural Network\\narchitectures for extracting the visual information. In this work, we have\\nevaluated 17 different Convolutional Neural Networks on two popular Image\\nCaption Generation frameworks: the first based on Neural Image Caption (NIC)\\ngeneration model and the second based on Soft-Attention framework. We observe\\nthat model complexity of Convolutional Neural Network, as measured by number of\\nparameters, and the accuracy of the model on Object Recognition task does not\\nnecessarily co-relate with its efficacy on feature extraction for Image Caption\\nGeneration task.',\n",
       "    'author': [{'name': 'Sulabh Katiyar'}, {'name': 'Samir Kumar Borgohain'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.14569/IJACSA.2020.0111291'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.14569/IJACSA.2020.0111291',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2102.11506v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.11506v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Article Published in International Journal of Advanced Computer\\n  Science and Applications(IJACSA), Volume 11 Issue 12, 2020'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'in International Journal of Advanced Computer Science and\\n  Applications, 11(12), 2020'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.03905v3',\n",
       "    'updated': '2022-02-07T01:41:30Z',\n",
       "    'published': '2021-02-20T18:40:40Z',\n",
       "    'title': 'Kanerva++: extending The Kanerva Machine with differentiable, locally\\n  block allocated latent memory',\n",
       "    'summary': 'Episodic and semantic memory are critical components of the human memory\\nmodel. The theory of complementary learning systems (McClelland et al., 1995)\\nsuggests that the compressed representation produced by a serial event\\n(episodic memory) is later restructured to build a more generalized form of\\nreusable knowledge (semantic memory). In this work we develop a new principled\\nBayesian memory allocation scheme that bridges the gap between episodic and\\nsemantic memory via a hierarchical latent variable model. We take inspiration\\nfrom traditional heap allocation and extend the idea of locally contiguous\\nmemory to the Kanerva Machine, enabling a novel differentiable block allocated\\nlatent memory. In contrast to the Kanerva Machine, we simplify the process of\\nmemory writing by treating it as a fully feed forward deterministic process,\\nrelying on the stochasticity of the read key distribution to disperse\\ninformation within the memory. We demonstrate that this allocation scheme\\nimproves performance in memory conditional image generation, resulting in new\\nstate-of-the-art conditional likelihood values on binarized MNIST (<=41.58\\nnats/image) , binarized Omniglot (<=66.24 nats/image), as well as presenting\\ncompetitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32x32.',\n",
       "    'author': [{'name': 'Jason Ramapuram'},\n",
       "     {'name': 'Yan Wu'},\n",
       "     {'name': 'Alexandros Kalousis'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICLR 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2103.03905v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.03905v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.12423v4',\n",
       "    'updated': '2021-10-18T10:52:33Z',\n",
       "    'published': '2021-06-23T14:20:01Z',\n",
       "    'title': 'Alias-Free Generative Adversarial Networks',\n",
       "    'summary': 'We observe that despite their hierarchical convolutional nature, the\\nsynthesis process of typical generative adversarial networks depends on\\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\\nof depicted objects. We trace the root cause to careless signal processing that\\ncauses aliasing in the generator network. Interpreting all signals in the\\nnetwork as continuous, we derive generally applicable, small architectural\\nchanges that guarantee that unwanted information cannot leak into the\\nhierarchical synthesis process. The resulting networks match the FID of\\nStyleGAN2 but differ dramatically in their internal representations, and they\\nare fully equivariant to translation and rotation even at subpixel scales. Our\\nresults pave the way for generative models better suited for video and\\nanimation.',\n",
       "    'author': [{'name': 'Tero Karras'},\n",
       "     {'name': 'Miika Aittala'},\n",
       "     {'name': 'Samuli Laine'},\n",
       "     {'name': 'Erik Härkönen'},\n",
       "     {'name': 'Janne Hellsten'},\n",
       "     {'name': 'Jaakko Lehtinen'},\n",
       "     {'name': 'Timo Aila'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2106.12423v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.12423v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2107.05747v4',\n",
       "    'updated': '2022-11-12T21:37:57Z',\n",
       "    'published': '2021-07-12T21:34:45Z',\n",
       "    'title': 'SoftHebb: Bayesian Inference in Unsupervised Hebbian Soft\\n  Winner-Take-All Networks',\n",
       "    'summary': 'Hebbian plasticity in winner-take-all (WTA) networks is highly attractive for\\nneuromorphic on-chip learning, owing to its efficient, local, unsupervised, and\\non-line nature. Moreover, its biological plausibility may help overcome\\nimportant limitations of artificial algorithms, such as their susceptibility to\\nadversarial attacks, and their high demands for training-example quantity and\\nrepetition. However, Hebbian WTA learning has found little use in machine\\nlearning (ML), likely because it has been missing an optimization theory\\ncompatible with deep learning (DL). Here we show rigorously that WTA networks\\nconstructed by standard DL elements, combined with a Hebbian-like plasticity\\nthat we derive, maintain a Bayesian generative model of the data. Importantly,\\nwithout any supervision, our algorithm, SoftHebb, minimizes cross-entropy, i.e.\\na common loss function in supervised DL. We show this theoretically and in\\npractice. The key is a \"soft\" WTA where there is no absolute \"hard\" winner\\nneuron. Strikingly, in shallow-network comparisons with backpropagation (BP),\\nSoftHebb shows advantages beyond its Hebbian efficiency. Namely, it converges\\nin fewer iterations, and is significantly more robust to noise and adversarial\\nattacks. Notably, attacks that maximally confuse SoftHebb are also confusing to\\nthe human eye, potentially linking human perceptual robustness, with Hebbian\\nWTA circuits of cortex. Finally, SoftHebb can generate synthetic objects as\\ninterpolations of real object classes. All in all, Hebbian efficiency,\\ntheoretical underpinning, cross-entropy-minimization, and surprising empirical\\nadvantages, suggest that SoftHebb may inspire highly neuromorphic and radically\\ndifferent, but practical and advantageous learning algorithms and hardware\\naccelerators.',\n",
       "    'author': [{'name': 'Timoleon Moraitis'},\n",
       "     {'name': 'Dmitry Toichkin'},\n",
       "     {'name': 'Adrien Journé'},\n",
       "     {'name': 'Yansong Chua'},\n",
       "     {'name': 'Qinghai Guo'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2107.05747v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2107.05747v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.08910v1',\n",
       "    'updated': '2021-08-18T06:47:31Z',\n",
       "    'published': '2021-08-18T06:47:31Z',\n",
       "    'title': 'Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture\\n  and Pruning Search',\n",
       "    'summary': 'Though recent years have witnessed remarkable progress in single image\\nsuper-resolution (SISR) tasks with the prosperous development of deep neural\\nnetworks (DNNs), the deep learning methods are confronted with the computation\\nand memory consumption issues in practice, especially for resource-limited\\nplatforms such as mobile devices. To overcome the challenge and facilitate the\\nreal-time deployment of SISR tasks on mobile, we combine neural architecture\\nsearch with pruning search and propose an automatic search framework that\\nderives sparse super-resolution (SR) models with high image quality while\\nsatisfying the real-time inference requirement. To decrease the search cost, we\\nleverage the weight sharing strategy by introducing a supernet and decouple the\\nsearch problem into three stages, including supernet construction,\\ncompiler-aware architecture and pruning search, and compiler-aware pruning\\nratio search. With the proposed framework, we are the first to achieve\\nreal-time SR inference (with only tens of milliseconds per frame) for\\nimplementing 720p resolution with competitive image quality (in terms of PSNR\\nand SSIM) on mobile platforms (Samsung Galaxy S20).',\n",
       "    'author': [{'name': 'Zheng Zhan'},\n",
       "     {'name': 'Yifan Gong'},\n",
       "     {'name': 'Pu Zhao'},\n",
       "     {'name': 'Geng Yuan'},\n",
       "     {'name': 'Wei Niu'},\n",
       "     {'name': 'Yushu Wu'},\n",
       "     {'name': 'Tianyun Zhang'},\n",
       "     {'name': 'Malith Jayaweera'},\n",
       "     {'name': 'David Kaeli'},\n",
       "     {'name': 'Bin Ren'},\n",
       "     {'name': 'Xue Lin'},\n",
       "     {'name': 'Yanzhi Wang'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.08910v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.08910v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'eess.IV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'eess.IV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.01445v3',\n",
       "    'updated': '2021-12-08T14:35:29Z',\n",
       "    'published': '2021-10-01T12:00:43Z',\n",
       "    'title': 'Robust and Decomposable Average Precision for Image Retrieval',\n",
       "    'summary': 'In image retrieval, standard evaluation metrics rely on score ranking, e.g.\\naverage precision (AP). In this paper, we introduce a method for robust and\\ndecomposable average precision (ROADMAP) addressing two major challenges for\\nend-to-end training of deep neural networks with AP: non-differentiability and\\nnon-decomposability. Firstly, we propose a new differentiable approximation of\\nthe rank function, which provides an upper bound of the AP loss and ensures\\nrobust training. Secondly, we design a simple yet effective loss function to\\nreduce the decomposability gap between the AP in the whole training set and its\\naveraged batch approximation, for which we provide theoretical guarantees.\\nExtensive experiments conducted on three image retrieval datasets show that\\nROADMAP outperforms several recent AP approximation methods and highlight the\\nimportance of our two contributions. Finally, using ROADMAP for training deep\\nmodels yields very good performances, outperforming state-of-the-art results on\\nthe three datasets.',\n",
       "    'author': [{'name': 'Elias Ramzi',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CNAM, CEDRIC - VERTIGO'}},\n",
       "     {'name': 'Nicolas Thome',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CNAM, CEDRIC - VERTIGO'}},\n",
       "     {'name': 'Clément Rambour',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CNAM, CEDRIC - VERTIGO'}},\n",
       "     {'name': 'Nicolas Audebert',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'CNAM, CEDRIC - VERTIGO'}},\n",
       "     {'name': 'Xavier Bitot'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Thirty-fifth Conference on Neural Information Processing Systems\\n  (NeurIPS 2021), Dec 2021, Sydney, Australia'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.01445v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.01445v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.14934v2',\n",
       "    'updated': '2021-12-21T20:20:02Z',\n",
       "    'published': '2021-11-29T20:20:29Z',\n",
       "    'title': 'Generative Adversarial Networks with Conditional Neural Movement\\n  Primitives for An Interactive Generative Drawing Tool',\n",
       "    'summary': 'Sketches are abstract representations of visual perception and visuospatial\\nconstruction. In this work, we proposed a new framework, Generative Adversarial\\nNetworks with Conditional Neural Movement Primitives (GAN-CNMP), that\\nincorporates a novel adversarial loss on CNMP to increase sketch smoothness and\\nconsistency. Through the experiments, we show that our model can be trained\\nwith few unlabeled samples, can construct distributions automatically in the\\nlatent space, and produces better results than the base model in terms of shape\\nconsistency and smoothness.',\n",
       "    'author': [{'name': 'Suzan Ece Ada'}, {'name': 'M. Yunus Seker'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 10 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2111.14934v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.14934v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.GR',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.GR',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2202.05239v1',\n",
       "    'updated': '2022-02-10T18:48:56Z',\n",
       "    'published': '2022-02-10T18:48:56Z',\n",
       "    'title': 'F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization',\n",
       "    'summary': 'Neural network quantization is a promising compression technique to reduce\\nmemory footprint and save energy consumption, potentially leading to real-time\\ninference. However, there is a performance gap between quantized and\\nfull-precision models. To reduce it, existing quantization approaches require\\nhigh-precision INT32 or full-precision multiplication during inference for\\nscaling or dequantization. This introduces a noticeable cost in terms of\\nmemory, speed, and required energy. To tackle these issues, we present F8Net, a\\nnovel quantization framework consisting of only fixed-point 8-bit\\nmultiplication. To derive our method, we first discuss the advantages of\\nfixed-point multiplication with different formats of fixed-point numbers and\\nstudy the statistical behavior of the associated fixed-point numbers. Second,\\nbased on the statistical and algorithmic analysis, we apply different\\nfixed-point formats for weights and activations of different layers. We\\nintroduce a novel algorithm to automatically determine the right format for\\neach layer during training. Third, we analyze a previous quantization algorithm\\n-- parameterized clipping activation (PACT) -- and reformulate it using\\nfixed-point arithmetic. Finally, we unify the recently proposed method for\\nquantization fine-tuning and our fixed-point approach to show the potential of\\nour method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50.\\nOur approach achieves comparable and better performance, when compared not only\\nto existing quantization techniques with INT32 multiplication or floating-point\\narithmetic, but also to the full-precision counterparts, achieving\\nstate-of-the-art performance.',\n",
       "    'author': [{'name': 'Qing Jin'},\n",
       "     {'name': 'Jian Ren'},\n",
       "     {'name': 'Richard Zhuang'},\n",
       "     {'name': 'Sumant Hanumante'},\n",
       "     {'name': 'Zhengang Li'},\n",
       "     {'name': 'Zhiyu Chen'},\n",
       "     {'name': 'Yanzhi Wang'},\n",
       "     {'name': 'Kaiyuan Yang'},\n",
       "     {'name': 'Sergey Tulyakov'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICLR 2022 (Oral)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2202.05239v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2202.05239v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AR', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.06026v2',\n",
       "    'updated': '2022-09-07T07:29:27Z',\n",
       "    'published': '2022-03-11T15:50:06Z',\n",
       "    'title': 'The Role of ImageNet Classes in Fréchet Inception Distance',\n",
       "    'summary': 'Fr\\\\\\'echet Inception Distance (FID) is the primary metric for ranking models\\nin data-driven generative modeling. While remarkably successful, the metric is\\nknown to sometimes disagree with human judgement. We investigate a root cause\\nof these discrepancies, and visualize what FID \"looks at\" in generated images.\\nWe show that the feature space that FID is (typically) computed in is so close\\nto the ImageNet classifications that aligning the histograms of Top-$N$\\nclassifications between sets of generated and real images can reduce FID\\nsubstantially -- without actually improving the quality of results. Thus we\\nconclude that FID is prone to intentional or accidental distortions. As a\\npractical example of an accidental distortion, we discuss a case where an\\nImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while\\nbeing worse in terms of human evaluation',\n",
       "    'author': [{'name': 'Tuomas Kynkäänniemi'},\n",
       "     {'name': 'Tero Karras'},\n",
       "     {'name': 'Miika Aittala'},\n",
       "     {'name': 'Timo Aila'},\n",
       "     {'name': 'Jaakko Lehtinen'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Code: https://github.com/kynkaat/role-of-imagenet-classes-in-fid'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.06026v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.06026v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.CV',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.CV',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.06649v3',\n",
       "    'updated': '2022-10-17T20:49:10Z',\n",
       "    'published': '2022-03-08T23:08:35Z',\n",
       "    'title': 'Joint rotational invariance and adversarial training of a dual-stream\\n  Transformer yields state of the art Brain-Score for Area V4',\n",
       "    'summary': 'Modern high-scoring models of vision in the brain score competition do not\\nstem from Vision Transformers. However, in this paper, we provide evidence\\nagainst the unexpected trend of Vision Transformers (ViT) being not\\nperceptually aligned with human visual representations by showing how a\\ndual-stream Transformer, a CrossViT$~\\\\textit{a la}$ Chen et al. (2021), under a\\njoint rotationally-invariant and adversarial optimization procedure yields 2nd\\nplace in the aggregate Brain-Score 2022 competition(Schrimpf et al., 2020b)\\naveraged across all visual categories, and at the time of the competition held\\n1st place for the highest explainable variance of area V4. In addition, our\\ncurrent Transformer-based model also achieves greater explainable variance for\\nareas V4, IT and Behaviour than a biologically-inspired CNN (ResNet50) that\\nintegrates a frontal V1-like computation module (Dapello et al.,2020). To\\nassess the contribution of the optimization scheme with respect to the CrossViT\\narchitecture, we perform several additional experiments on differently\\noptimized CrossViT\\'s regarding adversarial robustness, common corruption\\nbenchmarks, mid-ventral stimuli interpretation and feature inversion. Against\\nour initial expectations, our family of results provides tentative support for\\nan $\\\\textit{\"All roads lead to Rome\"}$ argument enforced via a joint\\noptimization rule even for non biologically-motivated models of vision such as\\nVision Transformers. Code is available at\\nhttps://github.com/williamberrios/BrainScore-Transformers',\n",
       "    'author': [{'name': 'William Berrios'}, {'name': 'Arturo Deza'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Under review'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2203.06649v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.06649v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'q-bio.NC',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'q-bio.NC',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.01829v2',\n",
       "    'updated': '2022-06-27T11:21:52Z',\n",
       "    'published': '2022-06-03T21:40:22Z',\n",
       "    'title': 'Drawing out of Distribution with Neuro-Symbolic Generative Models',\n",
       "    'summary': 'Learning general-purpose representations from perceptual inputs is a hallmark\\nof human intelligence. For example, people can write out numbers or characters,\\nor even draw doodles, by characterizing these tasks as different instantiations\\nof the same generic underlying process -- compositional arrangements of\\ndifferent forms of pen strokes. Crucially, learning to do one task, say\\nwriting, implies reasonable competence at another, say drawing, on account of\\nthis shared process. We present Drawing out of Distribution (DooD), a\\nneuro-symbolic generative model of stroke-based drawing that can learn such\\ngeneral-purpose representations. In contrast to prior work, DooD operates\\ndirectly on images, requires no supervision or expensive test-time inference,\\nand performs unsupervised amortised inference with a symbolic stroke model that\\nbetter enables both interpretability and generalization. We evaluate DooD on\\nits ability to generalise across both data and tasks. We first perform\\nzero-shot transfer from one dataset (e.g. MNIST) to another (e.g. Quickdraw),\\nacross five different datasets, and show that DooD clearly outperforms\\ndifferent baselines. An analysis of the learnt representations further\\nhighlights the benefits of adopting a symbolic stroke model. We then adopt a\\nsubset of the Omniglot challenge tasks, and evaluate its ability to generate\\nnew exemplars (both unconditionally and conditionally), and perform one-shot\\nclassification, showing that DooD matches the state of the art. Taken together,\\nwe demonstrate that DooD does indeed capture general-purpose representations\\nacross both data and task, and takes a further step towards building general\\nand robust concept-learning systems.',\n",
       "    'author': [{'name': 'Yichao Liang'},\n",
       "     {'name': 'Joshua B. Tenenbaum'},\n",
       "     {'name': 'Tuan Anh Le'},\n",
       "     {'name': 'N. Siddharth'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Preprint. Under review. 25 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.01829v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.01829v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SC', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2207.12559v3',\n",
       "    'updated': '2022-10-03T01:01:26Z',\n",
       "    'published': '2022-07-25T22:28:04Z',\n",
       "    'title': 'Static Hand Gesture Recognition for American Sign Language using\\n  Neuromorphic Hardware',\n",
       "    'summary': \"In this paper, we develop four spiking neural network (SNN) models for two\\nstatic American Sign Language (ASL) hand gesture classification tasks, i.e.,\\nthe ASL Alphabet and ASL Digits. The SNN models are deployed on Intel's\\nneuromorphic platform, Loihi, and then compared against equivalent deep neural\\nnetwork (DNN) models deployed on an edge computing device, the Intel Neural\\nCompute Stick 2 (NCS2). We perform a comprehensive comparison between the two\\nsystems in terms of accuracy, latency, power consumption, and energy. The best\\nDNN model achieves an accuracy of 99.93% on the ASL Alphabet dataset, whereas\\nthe best performing SNN model has an accuracy of 99.30%. For the ASL-Digits\\ndataset, the best DNN model achieves an accuracy of 99.76% accuracy while the\\nSNN achieves 99.03%. Moreover, our obtained experimental results show that the\\nLoihi neuromorphic hardware implementations achieve up to 20.64x and 4.10x\\nreduction in power consumption and energy, respectively, when compared to NCS2.\",\n",
       "    'author': [{'name': 'MohammadReza Mohammadi'},\n",
       "     {'name': 'Peyton Chandarana'},\n",
       "     {'name': 'James Seekings'},\n",
       "     {'name': 'Sara Hendrix'},\n",
       "     {'name': 'Ramtin Zand'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1088/2634-4386/ac94f3'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1088/2634-4386/ac94f3',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2207.12559v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2207.12559v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Authors MohammedReza Mohammadi, and Peyton Chandarana contributed\\n  equally'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.12728v1',\n",
       "    'updated': '2022-09-26T14:32:50Z',\n",
       "    'published': '2022-09-26T14:32:50Z',\n",
       "    'title': 'Prayatul Matrix: A Direct Comparison Approach to Evaluate Performance of\\n  Supervised Machine Learning Models',\n",
       "    'summary': 'Performance comparison of supervised machine learning (ML) models are widely\\ndone in terms of different confusion matrix based scores obtained on test\\ndatasets. However, a dataset comprises several instances having different\\ndifficulty levels. Therefore, it is more logical to compare effectiveness of ML\\nmodels on individual instances instead of comparing scores obtained for the\\nentire dataset. In this paper, an alternative approach is proposed for direct\\ncomparison of supervised ML models in terms of individual instances within the\\ndataset. A direct comparison matrix called \\\\emph{Prayatul Matrix} is\\nintroduced, which accounts for comparative outcome of two ML algorithms on\\ndifferent instances of a dataset. Five different performance measures are\\ndesigned based on prayatul matrix. Efficacy of the proposed approach as well as\\ndesigned measures is analyzed with four classification techniques on three\\ndatasets. Also analyzed on four large-scale complex image datasets with four\\ndeep learning models namely ResNet50V2, MobileNetV2, EfficientNet, and\\nXceptionNet. Results are evident that the newly designed measure are capable of\\ngiving more insight about the comparing ML algorithms, which were impossible\\nwith existing confusion matrix based scores like accuracy, precision and\\nrecall.',\n",
       "    'author': {'name': 'Anupam Biswas'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to IEEE Journal'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.12728v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.12728v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.PF', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T05, 68T07', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.12951v1',\n",
       "    'updated': '2022-09-26T18:37:13Z',\n",
       "    'published': '2022-09-26T18:37:13Z',\n",
       "    'title': 'Liquid Structural State-Space Models',\n",
       "    'summary': \"A proper parametrization of state transition matrices of linear state-space\\nmodels (SSMs) followed by standard nonlinearities enables them to efficiently\\nlearn representations from sequential data, establishing the state-of-the-art\\non a large series of long-range sequence modeling benchmarks. In this paper, we\\nshow that we can improve further when the structural SSM such as S4 is given by\\na linear liquid time-constant (LTC) state-space model. LTC neural networks are\\ncausal continuous-time neural networks with an input-dependent state transition\\nmodule, which makes them learn to adapt to incoming inputs at inference. We\\nshow that by using a diagonal plus low-rank decomposition of the state\\ntransition matrix introduced in S4, and a few simplifications, the LTC-based\\nstructural state-space model, dubbed Liquid-S4, achieves the new\\nstate-of-the-art generalization across sequence modeling tasks with long-term\\ndependencies such as image, text, audio, and medical time-series, with an\\naverage performance of 87.32% on the Long-Range Arena benchmark. On the full\\nraw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with\\na 30% reduction in parameter counts compared to S4. The additional gain in\\nperformance is the direct result of the Liquid-S4's kernel structure that takes\\ninto account the similarities of the input sequence samples during training and\\ninference.\",\n",
       "    'author': [{'name': 'Ramin Hasani'},\n",
       "     {'name': 'Mathias Lechner'},\n",
       "     {'name': 'Tsun-Hsuan Wang'},\n",
       "     {'name': 'Makram Chahine'},\n",
       "     {'name': 'Alexander Amini'},\n",
       "     {'name': 'Daniela Rus'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.12951v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.12951v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.05006v2',\n",
       "    'updated': '2022-11-22T20:02:15Z',\n",
       "    'published': '2022-10-10T20:27:19Z',\n",
       "    'title': 'Energy-Efficient Deployment of Machine Learning Workloads on\\n  Neuromorphic Hardware',\n",
       "    'summary': \"As the technology industry is moving towards implementing tasks such as\\nnatural language processing, path planning, image classification, and more on\\nsmaller edge computing devices, the demand for more efficient implementations\\nof algorithms and hardware accelerators has become a significant area of\\nresearch. In recent years, several edge deep learning hardware accelerators\\nhave been released that specifically focus on reducing the power and area\\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\\nachieve substantial power reductions over even the aforementioned edge DNN\\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\\nhardware. While neuromorphic hardware has demonstrated great potential for\\naccelerating deep learning tasks at the edge, the current space of algorithms\\nand hardware is limited and still in rather early development. Thus, many\\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\\ninto SNNs while also presenting techniques to improve the deployment of\\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\\nenergy. Our experimental results show that when compared against the Intel\\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\\n27x less power and 5x less energy in the tested image classification tasks by\\nusing our SNN improvement techniques.\",\n",
       "    'author': [{'name': 'Peyton Chandarana'},\n",
       "     {'name': 'Mohammadreza Mohammadi'},\n",
       "     {'name': 'James Seekings'},\n",
       "     {'name': 'Ramtin Zand'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.05006v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.05006v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.ET', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.08031v2',\n",
       "    'updated': '2022-10-19T09:15:33Z',\n",
       "    'published': '2022-10-14T18:00:07Z',\n",
       "    'title': 'Neural Attentive Circuits',\n",
       "    'summary': 'Recent work has seen the development of general purpose neural architectures\\nthat can be trained to perform tasks across diverse data modalities. General\\npurpose models typically make few assumptions about the underlying\\ndata-structure and are known to perform well in the large-data regime. At the\\nsame time, there has been growing interest in modular neural architectures that\\nrepresent the data using sparsely interacting modules. These models can be more\\nrobust out-of-distribution, computationally efficient, and capable of\\nsample-efficient adaptation to new data. However, they tend to make\\ndomain-specific assumptions about the data, and present challenges in how\\nmodule behavior (i.e., parameterization) and connectivity (i.e., their layout)\\ncan be jointly learned. In this work, we introduce a general purpose, yet\\nmodular neural architecture called Neural Attentive Circuits (NACs) that\\njointly learns the parameterization and a sparse connectivity of neural modules\\nwithout using domain knowledge. NACs are best understood as the combination of\\ntwo systems that are jointly trained end-to-end: one that determines the module\\nconfiguration and the other that executes it on an input. We demonstrate\\nqualitatively that NACs learn diverse and meaningful module configurations on\\nthe NLVR2 dataset without additional supervision. Quantitatively, we show that\\nby incorporating modularity in this way, NACs improve upon a strong non-modular\\nbaseline in terms of low-shot adaptation on CIFAR and CUBs dataset by about\\n10%, and OOD robustness on Tiny ImageNet-R by about 2.5%. Further, we find that\\nNACs can achieve an 8x speedup at inference time while losing less than 3%\\nperformance. Finally, we find NACs to yield competitive results on diverse data\\nmodalities spanning point-cloud classification, symbolic processing and\\ntext-classification from ASCII bytes, thereby confirming its general purpose\\nnature.',\n",
       "    'author': [{'name': 'Nasim Rahaman'},\n",
       "     {'name': 'Martin Weiss'},\n",
       "     {'name': 'Francesco Locatello'},\n",
       "     {'name': 'Chris Pal'},\n",
       "     {'name': 'Yoshua Bengio'},\n",
       "     {'name': 'Bernhard Schölkopf'},\n",
       "     {'name': 'Li Erran Li'},\n",
       "     {'name': 'Nicolas Ballas'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear at NeurIPS 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.08031v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.08031v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.13461v1',\n",
       "    'updated': '2022-10-23T05:44:22Z',\n",
       "    'published': '2022-10-23T05:44:22Z',\n",
       "    'title': 'Active Predictive Coding: A Unified Neural Framework for Learning\\n  Hierarchical World Models for Perception and Planning',\n",
       "    'summary': 'Predictive coding has emerged as a prominent model of how the brain learns\\nthrough predictions, anticipating the importance accorded to predictive\\nlearning in recent AI architectures such as transformers. Here we propose a new\\nframework for predictive coding called active predictive coding which can learn\\nhierarchical world models and solve two radically different open problems in\\nAI: (1) how do we learn compositional representations, e.g., part-whole\\nhierarchies, for equivariant vision? and (2) how do we solve large-scale\\nplanning problems, which are hard for traditional reinforcement learning, by\\ncomposing complex action sequences from primitive policies? Our approach\\nexploits hypernetworks, self-supervised learning and reinforcement learning to\\nlearn hierarchical world models that combine task-invariant state transition\\nnetworks and task-dependent policy networks at multiple abstraction levels. We\\ndemonstrate the viability of our approach on a variety of vision datasets\\n(MNIST, FashionMNIST, Omniglot) as well as on a scalable hierarchical planning\\nproblem. Our results represent, to our knowledge, the first demonstration of a\\nunified solution to the part-whole learning problem posed by Hinton, the nested\\nreference frames problem posed by Hawkins, and the integrated state-action\\nhierarchy learning problem in reinforcement learning.',\n",
       "    'author': [{'name': 'Rajesh P. N. Rao'},\n",
       "     {'name': 'Dimitrios C. Gklezakos'},\n",
       "     {'name': 'Vishwas Sathish'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '15 pages, 10 figures, 2 supplementary figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.13461v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.13461v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.00286v1',\n",
       "    'updated': '2022-11-01T05:45:04Z',\n",
       "    'published': '2022-11-01T05:45:04Z',\n",
       "    'title': 'Strategies for Optimizing End-to-End Artificial Intelligence Pipelines\\n  on Intel Xeon Processors',\n",
       "    'summary': 'End-to-end (E2E) artificial intelligence (AI) pipelines are composed of\\nseveral stages including data preprocessing, data ingestion, defining and\\ntraining the model, hyperparameter optimization, deployment, inference,\\npostprocessing, followed by downstream analyses. To obtain efficient E2E\\nworkflow, it is required to optimize almost all the stages of pipeline. Intel\\nXeon processors come with large memory capacities, bundled with AI acceleration\\n(e.g., Intel Deep Learning Boost), well suited to run multiple instances of\\ntraining and inference pipelines in parallel and has low total cost of\\nownership (TCO). To showcase the performance on Xeon processors, we applied\\ncomprehensive optimization strategies coupled with software and hardware\\nacceleration on variety of E2E pipelines in the areas of Computer Vision, NLP,\\nRecommendation systems, etc. We were able to achieve a performance improvement,\\nranging from 1.8x to 81.7x across different E2E pipelines. In this paper, we\\nwill be highlighting the optimization strategies adopted by us to achieve this\\nperformance on Intel Xeon processors with a set of eight different E2E\\npipelines.',\n",
       "    'author': [{'name': 'Meena Arunachalam'},\n",
       "     {'name': 'Vrushabh Sanghavi'},\n",
       "     {'name': 'Yi A Yao'},\n",
       "     {'name': 'Yi A Zhou'},\n",
       "     {'name': 'Lifeng A Wang'},\n",
       "     {'name': 'Zongru Wen'},\n",
       "     {'name': 'Niroop Ammbashankar'},\n",
       "     {'name': 'Ning W Wang'},\n",
       "     {'name': 'Fahim Mohammad'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages, 11 figures, 3 tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The Parallel Universe Magazine, Issue 48, 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.00286v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.00286v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.DC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/cs/0210030v1',\n",
       "    'updated': '2002-10-30T11:59:57Z',\n",
       "    'published': '2002-10-30T11:59:57Z',\n",
       "    'title': 'Intelligence and Cooperative Search by Coupled Local Minimizers',\n",
       "    'summary': 'We show how coupling of local optimization processes can lead to better\\nsolutions than multi-start local optimization consisting of independent runs.\\nThis is achieved by minimizing the average energy cost of the ensemble, subject\\nto synchronization constraints between the state vectors of the individual\\nlocal minimizers. From an augmented Lagrangian which incorporates the\\nsynchronization constraints both as soft and hard constraints, a network is\\nderived wherein the local minimizers interact and exchange information through\\nthe synchronization constraints. From the viewpoint of neural networks, the\\narray can be considered as a Lagrange programming network for continuous\\noptimization and as a cellular neural network (CNN). The penalty weights\\nassociated with the soft state synchronization constraints follow from the\\nsolution to a linear program. This expresses that the energy cost of the\\nensemble should maximally decrease. In this way successful local minimizers can\\nimplicitly impose their state to the others through a mechanism of master-slave\\ndynamics resulting into a cooperative search mechanism. Improved information\\nspreading within the ensemble is obtained by applying the concept of\\nsmall-world networks. This work suggests, in an interdisciplinary context, the\\nimportance of information exchange and state synchronization within ensembles,\\ntowards issues as evolution, collective behaviour, optimality and intelligence.',\n",
       "    'author': [{'name': 'J. A. K. Suykens'},\n",
       "     {'name': 'J. Vandewalle'},\n",
       "     {'name': 'B. De Moor'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '25 pages, 10 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Int. J. Bifurcation and Chaos, Vol.11, No.8, pp.2133-2144, 2001'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/cs/0210030v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/cs/0210030v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'F.1.1', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/0803.3912v1',\n",
       "    'updated': '2008-03-27T12:55:59Z',\n",
       "    'published': '2008-03-27T12:55:59Z',\n",
       "    'title': 'Artificial Immune Systems Tutorial',\n",
       "    'summary': 'The biological immune system is a robust, complex, adaptive system that\\ndefends the body from foreign pathogens. It is able to categorize all cells (or\\nmolecules) within the body as self-cells or non-self cells. It does this with\\nthe help of a distributed task force that has the intelligence to take action\\nfrom a local and also a global perspective using its network of chemical\\nmessengers for communication. There are two major branches of the immune\\nsystem. The innate immune system is an unchanging mechanism that detects and\\ndestroys certain invading organisms, whilst the adaptive immune system responds\\nto previously unknown foreign cells and builds a response to them that can\\nremain in the body over a long period of time. This remarkable information\\nprocessing biological system has caught the attention of computer science in\\nrecent years. A novel computational intelligence technique, inspired by\\nimmunology, has emerged, called Artificial Immune Systems. Several concepts\\nfrom the immune have been extracted and applied for solution to real world\\nscience and engineering problems. In this tutorial, we briefly describe the\\nimmune system metaphors that are relevant to existing Artificial Immune Systems\\nmethods. We will then show illustrative real-world problems suitable for\\nArtificial Immune Systems and give a step-by-step algorithm walkthrough for one\\nsuch problem. A comparison of the Artificial Immune Systems to other well-known\\nalgorithms, areas for future work, tips & tricks and a list of resources will\\nround this tutorial off. It should be noted that as Artificial Immune Systems\\nis still a young and evolving field, there is not yet a fixed algorithm\\ntemplate and hence actual implementations might differ somewhat from time to\\ntime and from those examples given here.',\n",
       "    'author': [{'name': 'Uwe Aickelin'}, {'name': 'Dipankar Dasgupta'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/0803.3912v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/0803.3912v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1006.4949v1',\n",
       "    'updated': '2010-06-25T09:55:30Z',\n",
       "    'published': '2010-06-25T09:55:30Z',\n",
       "    'title': 'Artificial Immune Systems (2010)',\n",
       "    'summary': 'The human immune system has numerous properties that make it ripe for\\nexploitation in the computational domain, such as robustness and fault\\ntolerance, and many different algorithms, collectively termed Artificial Immune\\nSystems (AIS), have been inspired by it. Two generations of AIS are currently\\nin use, with the first generation relying on simplified immune models and the\\nsecond generation utilising interdisciplinary collaboration to develop a deeper\\nunderstanding of the immune system and hence produce more complex models. Both\\ngenerations of algorithms have been successfully applied to a variety of\\nproblems, including anomaly detection, pattern recognition, optimisation and\\nrobotics. In this chapter an overview of AIS is presented, its evolution is\\ndiscussed, and it is shown that the diversification of the field is linked to\\nthe diversity of the immune system itself, leading to a number of algorithms as\\nopposed to one archetypal system. Two case studies are also presented to help\\nprovide insight into the mechanisms of AIS; these are the idiotypic network\\napproach and the Dendritic Cell Algorithm.',\n",
       "    'author': [{'name': 'Julie Greensmith'},\n",
       "     {'name': 'Amanda Whitbrook'},\n",
       "     {'name': 'Uwe Aickelin'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '29 pages, 1 algorithm, 3 figures, Handbook of Metaheuristics, 2nd\\n  Edition, Springer'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1006.4949v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1006.4949v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1308.5032v4',\n",
       "    'updated': '2019-07-09T19:50:17Z',\n",
       "    'published': '2013-08-23T03:05:28Z',\n",
       "    'title': 'How Did Humans Become So Creative? A Computational Approach',\n",
       "    'summary': \"This paper summarizes efforts to computationally model two transitions in the\\nevolution of human creativity: its origins about two million years ago, and the\\n'big bang' of creativity about 50,000 years ago. Using a computational model of\\ncultural evolution in which neural network based agents evolve ideas for\\nactions through invention and imitation, we tested the hypothesis that human\\ncreativity began with onset of the capacity for recursive recall. We compared\\nruns in which agents were limited to single-step actions to runs in which they\\nused recursive recall to chain simple actions into complex ones. Chaining\\nresulted in higher diversity, open-ended novelty, no ceiling on the mean\\nfitness of actions, and greater ability to make use of learning. Using a\\ncomputational model of portrait painting, we tested the hypothesis that the\\nexplosion of creativity in the Middle/Upper Paleolithic was due to onset of\\ncon-textual focus: the capacity to shift between associative and analytic\\nthought. This resulted in faster convergence on portraits that resembled the\\nsitter, employed painterly techniques, and were rated as preferable. We\\nconclude that recursive recall and contextual focus provide a computationally\\nplausible explanation of how humans evolved the means to transform this planet.\",\n",
       "    'author': [{'name': 'Liane Gabora'}, {'name': 'Steve DiPaola'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the Third International Conference on Computational\\n  Creativity (pp. 203-210). May 31 - June 1, 2012, Dublin, Ireland'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1308.5032v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1308.5032v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1812.08252v3',\n",
       "    'updated': '2019-05-14T16:06:46Z',\n",
       "    'published': '2018-12-19T21:28:15Z',\n",
       "    'title': 'Towards an Evolvable Cancer Treatment Simulator',\n",
       "    'summary': 'The use of high-fidelity computational simulations promises to enable\\nhigh-throughput hypothesis testing and optimisation of cancer therapies.\\nHowever, increasing realism comes at the cost of increasing computational\\nrequirements. This article explores the use of surrogate-assisted evolutionary\\nalgorithms to optimise the targeted delivery of a therapeutic compound to\\ncancerous tumour cells with the multicellular simulator, PhysiCell. The use of\\nboth Gaussian process models and multi-layer perceptron neural network\\nsurrogate models are investigated. We find that evolutionary algorithms are\\nable to effectively explore the parameter space of biophysical properties\\nwithin the agent-based simulations, minimising the resulting number of\\ncancerous cells after a period of simulated treatment. Both model-assisted\\nalgorithms are found to outperform a standard evolutionary algorithm,\\ndemonstrating their ability to perform a more effective search within the very\\nsmall evaluation budget. This represents the first use of efficient\\nevolutionary algorithms within a high-throughput multicellular computing\\napproach to find therapeutic design optima that maximise tumour regression.',\n",
       "    'author': [{'name': 'Richard J. Preen'},\n",
       "     {'name': 'Larry Bull'},\n",
       "     {'name': 'Andrew Adamatzky'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.biosystems.2019.05.005'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.biosystems.2019.05.005',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1812.08252v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1812.08252v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'BioSystems (2019), 182:1-7'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1904.09681v2',\n",
       "    'updated': '2019-04-23T11:31:09Z',\n",
       "    'published': '2019-04-21T23:55:01Z',\n",
       "    'title': 'Structural Self-adaptation for Decentralized Pervasive Intelligence',\n",
       "    'summary': \"Communication structure plays a key role in the learning capability of\\ndecentralized systems. Structural self-adaptation, by means of\\nself-organization, changes the order as well as the input information of the\\nagents' collective decision-making. This paper studies the role of agents'\\nrepositioning on the same communication structure, i.e. a tree, as the means to\\nexpand the learning capacity in complex combinatorial optimization problems,\\nfor instance, load-balancing power demand to prevent blackouts or efficient\\nutilization of bike sharing stations. The optimality of structural\\nself-adaptations is rigorously studied by constructing a novel large-scale\\nbenchmark that consists of 4000 agents with synthetic and real-world data\\nperforming 4 million structural self-adaptations during which almost 320\\nbillion learning messages are exchanged. Based on this benchmark dataset, 124\\ndeterministic structural criteria, applied as learning meta-features, are\\nsystematically evaluated as well as two online structural self-adaptation\\nstrategies designed to expand learning capacity. Experimental evaluation\\nidentifies metrics that capture agents with influential information and their\\noptimal positioning. Significant gain in learning performance is observed for\\nthe two strategies especially under low-performing initialization. Strikingly,\\nthe strategy that triggers structural self-adaptation in a more exploratory\\nfashion is the most cost-effective.\",\n",
       "    'author': [{'name': 'Jovan Nikolic'}, {'name': 'Evangelos Pournaras'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1904.09681v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1904.09681v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.DC',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.DC',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2003.11637v1',\n",
       "    'updated': '2020-02-24T13:26:34Z',\n",
       "    'published': '2020-02-24T13:26:34Z',\n",
       "    'title': 'Bio-inspired Optimization: metaheuristic algorithms for optimization',\n",
       "    'summary': \"In today's day and time solving real-world complex problems has become\\nfundamentally vital and critical task. Many of these are combinatorial\\nproblems, where optimal solutions are sought rather than exact solutions.\\nTraditional optimization methods are found to be effective for small scale\\nproblems. However, for real-world large scale problems, traditional methods\\neither do not scale up or fail to obtain optimal solutions or they end-up\\ngiving solutions after a long running time. Even earlier artificial\\nintelligence based techniques used to solve these problems could not give\\nacceptable results. However, last two decades have seen many new methods in AI\\nbased on the characteristics and behaviors of the living organisms in the\\nnature which are categorized as bio-inspired or nature inspired optimization\\nalgorithms. These methods, are also termed meta-heuristic optimization methods,\\nhave been proved theoretically and implemented using simulation as well used to\\ncreate many useful applications. They have been used extensively to solve many\\nindustrial and engineering complex problems due to being easy to understand,\\nflexible, simple to adapt to the problem at hand and most importantly their\\nability to come out of local optima traps. This local optima avoidance property\\nhelps in finding global optimal solutions. This paper is aimed at understanding\\nhow nature has inspired many optimization algorithms, basic categorization of\\nthem, major bio-inspired optimization algorithms invented in recent time with\\ntheir applications.\",\n",
       "    'author': [{'name': 'Pravin S Game'},\n",
       "     {'name': 'Dr. Vinod Vaze'},\n",
       "     {'name': 'Dr. Emmanuel M'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'pp. 1-9, (17-18 January 2020)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2003.11637v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2003.11637v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2005.12553v1',\n",
       "    'updated': '2020-05-26T07:47:27Z',\n",
       "    'published': '2020-05-26T07:47:27Z',\n",
       "    'title': 'Efficient Use of heuristics for accelerating XCS-based Policy Learning\\n  in Markov Games',\n",
       "    'summary': 'In Markov games, playing against non-stationary opponents with learning\\nability is still challenging for reinforcement learning (RL) agents, because\\nthe opponents can evolve their policies concurrently. This increases the\\ncomplexity of the learning task and slows down the learning speed of the RL\\nagents. This paper proposes efficient use of rough heuristics to speed up\\npolicy learning when playing against concurrent learners. Specifically, we\\npropose an algorithm that can efficiently learn explainable and generalized\\naction selection rules by taking advantages of the representation of\\nquantitative heuristics and an opponent model with an eXtended classifier\\nsystem (XCS) in zero-sum Markov games. A neural network is used to model the\\nopponent from their behaviors and the corresponding policy is inferred for\\naction selection and rule evolution. In cases of multiple heuristic policies,\\nwe introduce the concept of Pareto optimality for action selection. Besides,\\ntaking advantages of the condition representation and matching mechanism of\\nXCS, the heuristic policies and the opponent model can provide guidance for\\nsituations with similar feature representation. Furthermore, we introduce an\\naccuracy-based eligibility trace mechanism to speed up rule evolution, i.e.,\\nclassifiers that can match the historical traces are reinforced according to\\ntheir accuracy. We demonstrate the advantages of the proposed algorithm over\\nseveral benchmark algorithms in a soccer and a thief-and-hunter scenarios.',\n",
       "    'author': [{'name': 'Hao Chen'},\n",
       "     {'name': 'Chang Wang'},\n",
       "     {'name': 'Jian Huang'},\n",
       "     {'name': 'Jianxing Gong'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2005.12553v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2005.12553v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2007.08656v1',\n",
       "    'updated': '2020-07-11T20:50:52Z',\n",
       "    'published': '2020-07-11T20:50:52Z',\n",
       "    'title': 'A Framework for Automatic Behavior Generation in Multi-Function Swarms',\n",
       "    'summary': 'Multi-function swarms are swarms that solve multiple tasks at once. For\\nexample, a quadcopter swarm could be tasked with exploring an area of interest\\nwhile simultaneously functioning as ad-hoc relays. With this type of\\nmulti-function comes the challenge of handling potentially conflicting\\nrequirements simultaneously. Using the Quality-Diversity algorithm MAP-elites\\nin combination with a suitable controller structure, a framework for automatic\\nbehavior generation in multi-function swarms is proposed. The framework is\\ntested on a scenario with three simultaneous tasks: exploration, communication\\nnetwork creation and geolocation of RF emitters. A repertoire is evolved,\\nconsisting of a wide range of controllers, or behavior primitives, with\\ndifferent characteristics and trade-offs in the different tasks. This\\nrepertoire would enable the swarm to transition between behavior trade-offs\\nonline, according to the situational requirements. Furthermore, the effect of\\nnoise on the behavior characteristics in MAP-elites is investigated. A moderate\\nnumber of re-evaluations is found to increase the robustness while keeping the\\ncomputational requirements relatively low. A few selected controllers are\\nexamined, and the dynamics of transitioning between these controllers are\\nexplored. Finally, the study develops a methodology for analyzing the makeup of\\nthe resulting controllers. This is done through a parameter variation study\\nwhere the importance of individual inputs to the swarm controllers is assessed\\nand analyzed.',\n",
       "    'author': [{'name': 'Sondre A. Engebraaten'},\n",
       "     {'name': 'Jonas Moen'},\n",
       "     {'name': 'Oleg A. Yakimenko'},\n",
       "     {'name': 'Kyrre Glette'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2007.08656v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2007.08656v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2103.15090v1',\n",
       "    'updated': '2021-03-28T09:22:10Z',\n",
       "    'published': '2021-03-28T09:22:10Z',\n",
       "    'title': 'Playing Against the Board: Rolling Horizon Evolutionary Algorithms\\n  Against Pandemic',\n",
       "    'summary': 'Competitive board games have provided a rich and diverse testbed for\\nartificial intelligence. This paper contends that collaborative board games\\npose a different challenge to artificial intelligence as it must balance\\nshort-term risk mitigation with long-term winning strategies. Collaborative\\nboard games task all players to coordinate their different powers or pool their\\nresources to overcome an escalating challenge posed by the board and a\\nstochastic ruleset. This paper focuses on the exemplary collaborative board\\ngame Pandemic and presents a rolling horizon evolutionary algorithm designed\\nspecifically for this game. The complex way in which the Pandemic game state\\nchanges in a stochastic but predictable way required a number of specially\\ndesigned forward models, macro-action representations for decision-making, and\\nrepair functions for the genetic operations of the evolutionary algorithm.\\nVariants of the algorithm which explore optimistic versus pessimistic game\\nstate evaluations, different mutation rates and event horizons are compared\\nagainst a baseline hierarchical policy agent. Results show that an evolutionary\\napproach via short-horizon rollouts can better account for the future dangers\\nthat the board may introduce, and guard against them. Results highlight the\\ntypes of challenges that collaborative board games pose to artificial\\nintelligence, especially for handling multi-player collaboration interactions.',\n",
       "    'author': [{'name': 'Konstantinos Sfikas'}, {'name': 'Antonios Liapis'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/TG.2021.3069766'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/TG.2021.3069766',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2103.15090v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2103.15090v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to IEEE Transactions on Games, 11 pages, 7 figures. arXiv\\n  admin note: text overlap with arXiv:2103.11388'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.03404v1',\n",
       "    'updated': '2021-04-07T21:31:05Z',\n",
       "    'published': '2021-04-07T21:31:05Z',\n",
       "    'title': 'Bootstrapping of memetic from genetic evolution via inter-agent\\n  selection pressures',\n",
       "    'summary': \"We create an artificial system of agents (attention-based neural networks)\\nwhich selectively exchange messages with each-other in order to study the\\nemergence of memetic evolution and how memetic evolutionary pressures interact\\nwith genetic evolution of the network weights. We observe that the ability of\\nagents to exert selection pressures on each-other is essential for memetic\\nevolution to bootstrap itself into a state which has both high-fidelity\\nreplication of memes, as well as continuing production of new memes over time.\\nHowever, in this system there is very little interaction between this memetic\\n'ecology' and underlying tasks driving individual fitness - the emergent meme\\nlayer appears to be neither helpful nor harmful to agents' ability to learn to\\nsolve tasks. Sourcecode for these experiments is available at\\nhttps://github.com/GoodAI/memes\",\n",
       "    'author': [{'name': 'Nicholas Guttenberg'}, {'name': 'Marek Rosa'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 3 figures, submitted to ALife 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.03404v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.03404v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2105.10907v1',\n",
       "    'updated': '2021-05-23T10:34:48Z',\n",
       "    'published': '2021-05-23T10:34:48Z',\n",
       "    'title': 'An Efficient Application of Neuroevolution for Competitive Multiagent\\n  Learning',\n",
       "    'summary': 'Multiagent systems provide an ideal environment for the evaluation and\\nanalysis of real-world problems using reinforcement learning algorithms. Most\\ntraditional approaches to multiagent learning are affected by long training\\nperiods as well as high computational complexity. NEAT (NeuroEvolution of\\nAugmenting Topologies) is a popular evolutionary strategy used to obtain the\\nbest performing neural network architecture often used to tackle optimization\\nproblems in the field of artificial intelligence. This paper utilizes the NEAT\\nalgorithm to achieve competitive multiagent learning on a modified pong game\\nenvironment in an efficient manner. The competing agents abide by different\\nrules while having similar observation space parameters. The proposed algorithm\\nutilizes this property of the environment to define a singular\\nneuroevolutionary procedure that obtains the optimal policy for all the agents.\\nThe compiled results indicate that the proposed implementation achieves ideal\\nbehaviour in a very short training period when compared to existing multiagent\\nreinforcement learning models.',\n",
       "    'author': [{'name': 'Unnikrishnan Rajendran Menon'},\n",
       "     {'name': 'Anirudh Rajiv Menon'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.14738/tmlai.93.10149'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.14738/tmlai.93.10149',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2105.10907v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2105.10907v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '13 pages, 7 figures, 2 tables'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Transactions on Machine Learning and Artificial Intelligence,\\n  9(3), 1-13 (2021)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.05521v1',\n",
       "    'updated': '2021-06-10T06:21:48Z',\n",
       "    'published': '2021-06-10T06:21:48Z',\n",
       "    'title': 'Swarm Intelligence for Self-Organized Clustering',\n",
       "    'summary': 'Algorithms implementing populations of agents which interact with one another\\nand sense their environment may exhibit emergent behavior such as\\nself-organization and swarm intelligence. Here a swarm system, called\\nDatabionic swarm (DBS), is introduced which is able to adapt itself to\\nstructures of high-dimensional data characterized by distance and/or\\ndensity-based structures in the data space. By exploiting the interrelations of\\nswarm intelligence, self-organization and emergence, DBS serves as an\\nalternative approach to the optimization of a global objective function in the\\ntask of clustering. The swarm omits the usage of a global objective function\\nand is parameter-free because it searches for the Nash equilibrium during its\\nannealing process. To our knowledge, DBS is the first swarm combining these\\napproaches. Its clustering can outperform common clustering methods such as\\nK-means, PAM, single linkage, spectral clustering, model-based clustering, and\\nWard, if no prior knowledge about the data is available. A central problem in\\nclustering is the correct estimation of the number of clusters. This is\\naddressed by a DBS visualization called topographic map which allows assessing\\nthe number of clusters. It is known that all clustering algorithms construct\\nclusters, irrespective of the data set contains clusters or not. In contrast to\\nmost other clustering algorithms, the topographic map identifies, that\\nclustering of the data is meaningless if the data contains no (natural)\\nclusters. The performance of DBS is demonstrated on a set of benchmark data,\\nwhich are constructed to pose difficult clustering problems and in two\\nreal-world applications.',\n",
       "    'author': [{'name': 'Michael C. Thrun'}, {'name': 'Alfred Ultsch'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.artint.2020.103237'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.artint.2020.103237',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2106.05521v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.05521v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '54 pages, 21 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Artificial intelligence, Vol. 290, pp. 103237. 2021'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T05 (Primary) 68T42, 91Axx (Secondary)',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2106.10015v2',\n",
       "    'updated': '2022-03-07T11:07:22Z',\n",
       "    'published': '2021-06-18T09:17:21Z',\n",
       "    'title': 'Meta-control of social learning strategies',\n",
       "    'summary': \"Social learning, copying other's behavior without actual experience, offers a\\ncost-effective means of knowledge acquisition. However, it raises the\\nfundamental question of which individuals have reliable information: successful\\nindividuals versus the majority. The former and the latter are known\\nrespectively as success-based and conformist social learning strategies. We\\nshow here that while the success-based strategy fully exploits the benign\\nenvironment of low uncertainly, it fails in uncertain environments. On the\\nother hand, the conformist strategy can effectively mitigate this adverse\\neffect. Based on these findings, we hypothesized that meta-control of\\nindividual and social learning strategies provides effective and\\nsample-efficient learning in volatile and uncertain environments. Simulations\\non a set of environments with various levels of volatility and uncertainty\\nconfirmed our hypothesis. The results imply that meta-control of social\\nlearning affords agents the leverage to resolve environmental uncertainty with\\nminimal exploration cost, by exploiting others' learning as an external\\nknowledge base.\",\n",
       "    'author': [{'name': 'Anil Yaman'},\n",
       "     {'name': 'Nicolas Bredeche'},\n",
       "     {'name': 'Onur Çaylak'},\n",
       "     {'name': 'Joel Z. Leibo'},\n",
       "     {'name': 'Sang Wan Lee'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1371/journal.pcbi.1009882'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1371/journal.pcbi.1009882',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2106.10015v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2106.10015v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'PLoS Comput Biol 18(2): e1009882 (2022)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.13844v1',\n",
       "    'updated': '2022-06-28T09:21:30Z',\n",
       "    'published': '2022-06-28T09:21:30Z',\n",
       "    'title': 'Cooperative Multi-Agent Search on Endogenously-Changing Fitness\\n  Landscapes',\n",
       "    'summary': 'We use a multi-agent system to model how agents (representing firms) may\\ncollaborate and adapt in a business \\'landscape\\' where some, more influential,\\nfirms are given the power to shape the landscape of other firms. The landscapes\\nwe study are based on the well-known NK model of Kauffman, with the addition of\\n\\'shapers\\', firms that can change the landscape\\'s features for themselves and\\nall other players. Our work investigates how firms that are additionally\\nendowed with cognitive and experiential search, and the ability to form\\ncollaborations with other firms, can use these capabilities to adapt more\\nquickly and adeptly. We find that, in a collaborative group, firms must still\\nhave a mind of their own and resist direct mimicry of stronger partners to\\nattain better heights collectively. Larger groups and groups with more\\ninfluential members generally do better, so targeted intelligent cooperation is\\nbeneficial. These conclusions are tentative, and our results show a sensitivity\\nto landscape ruggedness and \"malleability\" (i.e. the capacity of the landscape\\nto be changed by the shaper firms). Overall, our work demonstrates the\\npotential of computer science, evolution, and machine learning to contribute to\\nbusiness strategy in these complex environments.',\n",
       "    'author': [{'name': 'Chin Woei Lim'},\n",
       "     {'name': 'Richard Allmendinger'},\n",
       "     {'name': 'Joshua Knowles'},\n",
       "     {'name': 'Ayesha Alhosani'},\n",
       "     {'name': 'Mercedes Bleda'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2206.13844v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.13844v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.05568v3',\n",
       "    'updated': '2022-09-27T09:45:34Z',\n",
       "    'published': '2022-08-10T21:35:38Z',\n",
       "    'title': 'The emergence of division of labor through decentralized social\\n  sanctioning',\n",
       "    'summary': 'Human ecological success relies on our characteristic ability to flexibly\\nself-organize into cooperative social groups, the most successful of which\\nemploy substantial specialization and division of labor. Unlike most other\\nanimals, humans learn by trial and error during their lives what role to take\\non. However, when some critical roles are more attractive than others, and\\nindividuals are self-interested, then there is a social dilemma: each\\nindividual would prefer others take on the critical-but-unremunerative roles so\\nthey may remain free to take one that pays better. But disaster occurs if all\\nact thusly and a critical role goes unfilled. In such situations learning an\\noptimum role distribution may not be possible. Consequently, a fundamental\\nquestion is: how can division of labor emerge in groups of self-interested\\nlifetime-learning individuals? Here we show that by introducing a model of\\nsocial norms, which we regard as patterns of decentralized social sanctioning,\\nit becomes possible for groups of self-interested individuals to learn a\\nproductive division of labor involving all critical roles. Such social norms\\nwork by redistributing rewards within the population to disincentivize\\nantisocial roles while incentivizing prosocial roles that do not intrinsically\\npay as well as others.',\n",
       "    'author': [{'name': 'Anil Yaman'},\n",
       "     {'name': 'Joel Z. Leibo'},\n",
       "     {'name': 'Giovanni Iacca'},\n",
       "     {'name': 'Sang Wan Lee'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.05568v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.05568v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.08407v2',\n",
       "    'updated': '2022-11-16T20:47:12Z',\n",
       "    'published': '2022-10-27T13:37:50Z',\n",
       "    'title': 'Trust-Awareness to Secure Swarm Intelligence from Data Injection Attack',\n",
       "    'summary': 'Enabled by the emerging industrial agent (IA) technology, swarm intelligence\\n(SI) is envisaged to play an important role in future industrial Internet of\\nThings (IIoT) that is shaped by Sixth Generation (6G) mobile communications and\\ndigital twin (DT). However, its fragility against data injection attack may\\nhalt it from practical deployment. In this paper we propose an efficient trust\\napproach to address this security concern for SI.',\n",
       "    'author': [{'name': 'Bin Han'},\n",
       "     {'name': 'Dennis Krummacker'},\n",
       "     {'name': 'Qiuheng Zhou'},\n",
       "     {'name': 'Hans D. Schotten'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to ICC 2023'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.08407v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.08407v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2211.13746v3',\n",
       "    'updated': '2023-01-21T18:44:11Z',\n",
       "    'published': '2022-11-24T18:23:28Z',\n",
       "    'title': 'Melting Pot 2.0',\n",
       "    'summary': 'Multi-agent artificial intelligence research promises a path to develop\\nintelligent technologies that are more human-like and more human-compatible\\nthan those produced by \"solipsistic\" approaches, which do not consider\\ninteractions between agents. Melting Pot is a research tool developed to\\nfacilitate work on multi-agent artificial intelligence, and provides an\\nevaluation protocol that measures generalization to novel social partners in a\\nset of canonical test scenarios. Each scenario pairs a physical environment (a\\n\"substrate\") with a reference set of co-players (a \"background population\"), to\\ncreate a social situation with substantial interdependence between the\\nindividuals involved. For instance, some scenarios were inspired by\\ninstitutional-economics-based accounts of natural resource management and\\npublic-good-provision dilemmas. Others were inspired by considerations from\\nevolutionary biology, game theory, and artificial life. Melting Pot aims to\\ncover a maximally diverse set of interdependencies and incentives. It includes\\nthe commonly-studied extreme cases of perfectly-competitive (zero-sum)\\nmotivations and perfectly-cooperative (shared-reward) motivations, but does not\\nstop with them. As in real-life, a clear majority of scenarios in Melting Pot\\nhave mixed incentives. They are neither purely competitive nor purely\\ncooperative and thus demand successful agents be able to navigate the resulting\\nambiguity. Here we describe Melting Pot 2.0, which revises and expands on\\nMelting Pot. We also introduce support for scenarios with asymmetric roles, and\\nexplain how to integrate them into the evaluation protocol. This report also\\ncontains: (1) details of all substrates and scenarios; (2) a complete\\ndescription of all baseline algorithms and results. Our intention is for it to\\nserve as a reference for researchers using Melting Pot 2.0.',\n",
       "    'author': [{'name': 'John P. Agapiou'},\n",
       "     {'name': 'Alexander Sasha Vezhnevets'},\n",
       "     {'name': 'Edgar A. Duéñez-Guzmán'},\n",
       "     {'name': 'Jayd Matyas'},\n",
       "     {'name': 'Yiran Mao'},\n",
       "     {'name': 'Peter Sunehag'},\n",
       "     {'name': 'Raphael Köster'},\n",
       "     {'name': 'Udari Madhushani'},\n",
       "     {'name': 'Kavya Kopparapu'},\n",
       "     {'name': 'Ramona Comanescu'},\n",
       "     {'name': 'DJ Strouse'},\n",
       "     {'name': 'Michael B. Johanson'},\n",
       "     {'name': 'Sukhdeep Singh'},\n",
       "     {'name': 'Julia Haas'},\n",
       "     {'name': 'Igor Mordatch'},\n",
       "     {'name': 'Dean Mobbs'},\n",
       "     {'name': 'Joel Z. Leibo'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '59 pages, 54 figures. arXiv admin note: text overlap with\\n  arXiv:2107.06857'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2211.13746v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2211.13746v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/0907.0328v1',\n",
       "    'updated': '2009-07-02T09:59:39Z',\n",
       "    'published': '2009-07-02T09:59:39Z',\n",
       "    'title': 'Degenerate neutrality creates evolvable fitness landscapes',\n",
       "    'summary': 'Understanding how systems can be designed to be evolvable is fundamental to\\nresearch in optimization, evolution, and complex systems science. Many\\nresearchers have thus recognized the importance of evolvability, i.e. the\\nability to find new variants of higher fitness, in the fields of biological\\nevolution and evolutionary computation. Recent studies by Ciliberti et al\\n(Proc. Nat. Acad. Sci., 2007) and Wagner (Proc. R. Soc. B., 2008) propose a\\npotentially important link between the robustness and the evolvability of a\\nsystem. In particular, it has been suggested that robustness may actually lead\\nto the emergence of evolvability. Here we study two design principles,\\nredundancy and degeneracy, for achieving robustness and we show that they have\\na dramatically different impact on the evolvability of the system. In\\nparticular, purely redundant systems are found to have very little evolvability\\nwhile systems with degeneracy, i.e. distributed robustness, can be orders of\\nmagnitude more evolvable. These results offer insights into the general\\nprinciples for achieving evolvability and may prove to be an important step\\nforward in the pursuit of evolvable representations in evolutionary\\ncomputation.',\n",
       "    'author': [{'name': 'James M Whitacre'}, {'name': 'Axel Bender'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'WorldComp 2009, Las Vegas, NV, USA'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/0907.0328v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/0907.0328v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2008.03620v1',\n",
       "    'updated': '2020-08-09T00:25:06Z',\n",
       "    'published': '2020-08-09T00:25:06Z',\n",
       "    'title': 'Lights and Shadows in Evolutionary Deep Learning: Taxonomy, Critical\\n  Methodological Analysis, Cases of Study, Learned Lessons, Recommendations and\\n  Challenges',\n",
       "    'summary': \"Much has been said about the fusion of bio-inspired optimization algorithms\\nand Deep Learning models for several purposes: from the discovery of network\\ntopologies and hyper-parametric configurations with improved performance for a\\ngiven task, to the optimization of the model's parameters as a replacement for\\ngradient-based solvers. Indeed, the literature is rich in proposals showcasing\\nthe application of assorted nature-inspired approaches for these tasks. In this\\nwork we comprehensively review and critically examine contributions made so far\\nbased on three axes, each addressing a fundamental question in this research\\navenue: a) optimization and taxonomy (Why?), including a historical\\nperspective, definitions of optimization problems in Deep Learning, and a\\ntaxonomy associated with an in-depth analysis of the literature, b) critical\\nmethodological analysis (How?), which together with two case studies, allows us\\nto address learned lessons and recommendations for good practices following the\\nanalysis of the literature, and c) challenges and new directions of research\\n(What can be done, and what for?). In summary, three axes - optimization and\\ntaxonomy, critical analysis, and challenges - which outline a complete vision\\nof a merger of two technologies drawing up an exciting future for this area of\\nfusion research.\",\n",
       "    'author': [{'name': 'Aritz D. Martinez'},\n",
       "     {'name': 'Javier Del Ser'},\n",
       "     {'name': 'Esther Villar-Rodriguez'},\n",
       "     {'name': 'Eneko Osaba'},\n",
       "     {'name': 'Javier Poyatos'},\n",
       "     {'name': 'Siham Tabik'},\n",
       "     {'name': 'Daniel Molina'},\n",
       "     {'name': 'Francisco Herrera'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '64 pages, 18 figures, under review for its consideration in\\n  Information Fusion journal'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2008.03620v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2008.03620v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/cs/0502086v1',\n",
       "    'updated': '2005-02-22T09:51:16Z',\n",
       "    'published': '2005-02-22T09:51:16Z',\n",
       "    'title': 'The Self-Organization of Speech Sounds',\n",
       "    'summary': 'The speech code is a vehicle of language: it defines a set of forms used by a\\ncommunity to carry information. Such a code is necessary to support the\\nlinguistic interactions that allow humans to communicate. How then may a speech\\ncode be formed prior to the existence of linguistic interactions? Moreover, the\\nhuman speech code is discrete and compositional, shared by all the individuals\\nof a community but different across communities, and phoneme inventories are\\ncharacterized by statistical regularities. How can a speech code with these\\nproperties form? We try to approach these questions in the paper, using the\\n\"methodology of the artificial\". We build a society of artificial agents, and\\ndetail a mechanism that shows the formation of a discrete speech code without\\npre-supposing the existence of linguistic capacities or of coordinated\\ninteractions. The mechanism is based on a low-level model of sensory-motor\\ninteractions. We show that the integration of certain very simple and non\\nlanguage-specific neural devices leads to the formation of a speech code that\\nhas properties similar to the human speech code. This result relies on the\\nself-organizing properties of a generic coupling between perception and\\nproduction within agents, and on the interactions between agents. The\\nartificial system helps us to develop better intuitions on how speech might\\nhave appeared, by showing how self-organization might have helped natural\\nselection to find speech.',\n",
       "    'author': {'name': 'Pierre-Yves Oudeyer'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Journal of Theoretical Biology 233 (2005) Issue 3, Pages 435-449'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/cs/0502086v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/cs/0502086v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'math.DS', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/math/0605498v1',\n",
       "    'updated': '2006-05-18T07:47:58Z',\n",
       "    'published': '2006-05-18T07:47:58Z',\n",
       "    'title': 'Cross-Entropic Learning of a Machine for the Decision in a Partially\\n  Observable Universe',\n",
       "    'summary': 'Revision of the paper previously entitled \"Learning a Machine for the\\nDecision in a Partially Observable Markov Universe\" In this paper, we are\\ninterested in optimal decisions in a partially observable universe. Our\\napproach is to directly approximate an optimal strategic tree depending on the\\nobservation. This approximation is made by means of a parameterized\\nprobabilistic law. A particular family of hidden Markov models, with input\\n\\\\emph{and} output, is considered as a model of policy. A method for optimizing\\nthe parameters of these HMMs is proposed and applied. This optimization is\\nbased on the cross-entropic principle for rare events simulation developed by\\nRubinstein.',\n",
       "    'author': {'name': 'Frederic Dambreville',\n",
       "     'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "      '#text': 'DGA/CTA/DT/GIP'}},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Submitted to EJOR'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/math/0605498v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/math/0605498v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'math.OC',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'math.OC',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'math.ST', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.TH', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.01215v4',\n",
       "    'updated': '2020-07-07T15:49:16Z',\n",
       "    'published': '2019-09-25T19:28:33Z',\n",
       "    'title': 'ES-MAML: Simple Hessian-Free Meta Learning',\n",
       "    'summary': 'We introduce ES-MAML, a new framework for solving the model agnostic meta\\nlearning (MAML) problem based on Evolution Strategies (ES). Existing algorithms\\nfor MAML are based on policy gradients, and incur significant difficulties when\\nattempting to estimate second derivatives using backpropagation on stochastic\\npolicies. We show how ES can be applied to MAML to obtain an algorithm which\\navoids the problem of estimating second derivatives, and is also conceptually\\nsimple and easy to implement. Moreover, ES-MAML can handle new types of\\nnonsmooth adaptation operators, and other techniques for improving performance\\nand estimation of ES methods become applicable. We show empirically that\\nES-MAML is competitive with existing methods and often yields better adaptation\\nwith fewer queries.',\n",
       "    'author': [{'name': 'Xingyou Song'},\n",
       "     {'name': 'Wenbo Gao'},\n",
       "     {'name': 'Yuxiang Yang'},\n",
       "     {'name': 'Krzysztof Choromanski'},\n",
       "     {'name': 'Aldo Pacchiano'},\n",
       "     {'name': 'Yunhao Tang'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Published as a conference paper in ICLR 2020. Code can be found in\\n  http://github.com/google-research/google-research/tree/master/es_maml'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1910.01215v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.01215v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'math.OC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.09677v3',\n",
       "    'updated': '2022-01-25T06:39:24Z',\n",
       "    'published': '2021-02-18T23:50:20Z',\n",
       "    'title': 'Training a Resilient Q-Network against Observational Interference',\n",
       "    'summary': 'Deep reinforcement learning (DRL) has demonstrated impressive performance in\\nvarious gaming simulators and real-world applications. In practice, however, a\\nDRL agent may receive faulty observation by abrupt interferences such as\\nblack-out, frozen-screen, and adversarial perturbation. How to design a\\nresilient DRL algorithm against these rare but mission-critical and\\nsafety-crucial scenarios is an essential yet challenging task. In this paper,\\nwe consider a deep q-network (DQN) framework training with an auxiliary task of\\nobservational interferences such as artificial noises. Inspired by causal\\ninference for observational interference, we propose a causal inference based\\nDQN algorithm called causal inference Q-network (CIQ). We evaluate the\\nperformance of CIQ in several benchmark DQN environments with different types\\nof interferences as auxiliary labels. Our experimental results show that the\\nproposed CIQ method could achieve higher performance and more resilience\\nagainst observational interferences.',\n",
       "    'author': [{'name': 'Chao-Han Huck Yang'},\n",
       "     {'name': 'I-Te Danny Hung'},\n",
       "     {'name': 'Yi Ouyang'},\n",
       "     {'name': 'Pin-Yu Chen'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted to AAAI 2022. 9 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.09677v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.09677v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.14516v2',\n",
       "    'updated': '2021-10-07T14:32:44Z',\n",
       "    'published': '2021-09-29T16:04:39Z',\n",
       "    'title': 'On Assessing the Usefulness of Proxy Domains for Developing and\\n  Evaluating Embodied Agents',\n",
       "    'summary': 'In many situations it is either impossible or impractical to develop and\\nevaluate agents entirely on the target domain on which they will be deployed.\\nThis is particularly true in robotics, where doing experiments on hardware is\\nmuch more arduous than in simulation. This has become arguably more so in the\\ncase of learning-based agents. To this end, considerable recent effort has been\\ndevoted to developing increasingly realistic and higher fidelity simulators.\\nHowever, we lack any principled way to evaluate how good a \"proxy domain\" is,\\nspecifically in terms of how useful it is in helping us achieve our end\\nobjective of building an agent that performs well in the target domain. In this\\nwork, we investigate methods to address this need. We begin by clearly\\nseparating two uses of proxy domains that are often conflated: 1) their ability\\nto be a faithful predictor of agent performance and 2) their ability to be a\\nuseful tool for learning. In this paper, we attempt to clarify the role of\\nproxy domains and establish new proxy usefulness (PU) metrics to compare the\\nusefulness of different proxy domains. We propose the relative predictive PU to\\nassess the predictive ability of a proxy domain and the learning PU to quantify\\nthe usefulness of a proxy as a tool to generate learning data. Furthermore, we\\nargue that the value of a proxy is conditioned on the task that it is being\\nused to help solve. We demonstrate how these new metrics can be used to\\noptimize parameters of the proxy domain for which obtaining ground truth via\\nsystem identification is not trivial.',\n",
       "    'author': [{'name': 'Anthony Courchesne',\n",
       "      'arxiv:affiliation': [{'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'Mila'},\n",
       "       {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'Université de Montréal'}]},\n",
       "     {'name': 'Andrea Censi',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'ETH Zürich'}},\n",
       "     {'name': 'Liam Paull',\n",
       "      'arxiv:affiliation': [{'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'Mila'},\n",
       "       {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'Université de Montréal'}]}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 6 figures Accepted & Presented at IROS2021 For associated\\n  code, see https://github.com/duckietown'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.14516v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.14516v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68T40 (\"Primary\"), 68T07 (\"Secondary\")',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.6.4', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2110.00438v3',\n",
       "    'updated': '2021-11-09T15:41:04Z',\n",
       "    'published': '2021-10-01T14:20:00Z',\n",
       "    'title': 'Guiding Evolutionary Strategies by Differentiable Robot Simulators',\n",
       "    'summary': 'In recent years, Evolutionary Strategies were actively explored in robotic\\ntasks for policy search as they provide a simpler alternative to reinforcement\\nlearning algorithms. However, this class of algorithms is often claimed to be\\nextremely sample-inefficient. On the other hand, there is a growing interest in\\nDifferentiable Robot Simulators (DRS) as they potentially can find successful\\npolicies with only a handful of trajectories. But the resulting gradient is not\\nalways useful for the first-order optimization. In this work, we demonstrate\\nhow DRS gradient can be used in conjunction with Evolutionary Strategies.\\nPreliminary results suggest that this combination can reduce sample complexity\\nof Evolutionary Strategies by 3x-5x times in both simulation and the real\\nworld.',\n",
       "    'author': [{'name': 'Vladislav Kurenkov'}, {'name': 'Bulat Maksudov'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'NeurIPS 2021, 4th Robot Learning Workshop'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2110.00438v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2110.00438v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'eess.SY', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.07407v1',\n",
       "    'updated': '2022-08-02T06:04:32Z',\n",
       "    'published': '2022-08-02T06:04:32Z',\n",
       "    'title': 'Chemotaxis of sea urchin sperm cells through deep reinforcement learning',\n",
       "    'summary': 'By imitating biological microswimmers, microrobots can be designed to\\naccomplish targeted delivery of cargos and biomedical manipulations at\\nmicroscale. However, it is still a great challenge to enable microrobots to\\nmaneuver in a complex environment. Machine learning algorithms offer a tool to\\nboost mobility and flexibility of a synthetic microswimmer, hence could help us\\ndesign truly smart microrobots. In this work, we investigate how a model of sea\\nurchin sperm cell can self-learn chemotactic motion in a chemoattractant\\nconcentration field. We employ an artificial neural network to act as a\\ndecision-making agent and facilitate the sperm cell to discover efficient\\nmaneuver strategies through a deep reinforcement learning (DRL) algorithm. Our\\nresults show that chemotactic behaviours, very similar to the realistic ones,\\ncan be achieved by the DRL utilizing only limited environmental information. In\\nmost cases, the DRL algorithm discovers more efficient strategies than the\\nhuman-devised one. Furthermore, the DRL can even utilize an external\\ndisturbance to facilitate the chemotactic motion if the extra flow information\\nis also taken into account by the artificial neural network. Our results\\nprovide insights to the chemotactic process of sea urchin sperm cells and also\\nprepare guidance for the intelligent maneuver of microrobots.',\n",
       "    'author': [{'name': 'Chaojie Mo'}, {'name': 'Xin Bian'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.07407v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.07407v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'physics.bio-ph', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'physics.flu-dyn',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1909.06711v1',\n",
       "    'updated': '2019-09-15T02:02:22Z',\n",
       "    'published': '2019-09-15T02:02:22Z',\n",
       "    'title': 'Cognitive swarming in complex environments with attractor dynamics and\\n  oscillatory computing',\n",
       "    'summary': \"Neurobiological theories of spatial cognition developed with respect to\\nrecording data from relatively small and/or simplistic environments compared to\\nanimals' natural habitats. It has been unclear how to extend theoretical models\\nto large or complex spaces. Complementarily, in autonomous systems technology,\\napplications have been growing for distributed control methods that scale to\\nlarge numbers of low-footprint mobile platforms. Animals and many-robot groups\\nmust solve common problems of navigating complex and uncertain environments.\\nHere, we introduce the 'NeuroSwarms' control framework to investigate whether\\nadaptive, autonomous swarm control of minimal artificial agents can be achieved\\nby direct analogy to neural circuits of rodent spatial cognition. NeuroSwarms\\nanalogizes agents to neurons and swarming groups to recurrent networks. We\\nimplemented neuron-like agent interactions in which mutually visible agents\\noperate as if they were reciprocally-connected place cells in an attractor\\nnetwork. We attributed a phase state to agents to enable patterns of\\noscillatory synchronization similar to hippocampal models of theta-rhythmic\\n(5-12 Hz) sequence generation. We demonstrate that multi-agent swarming and\\nreward-approach dynamics can be expressed as a mobile form of Hebbian learning\\nand that NeuroSwarms supports a single-entity paradigm that directly informs\\ntheoretical models of animal cognition. We present emergent behaviors including\\nphase-organized rings and trajectory sequences that interact with environmental\\ncues and geometry in large, fragmented mazes. Thus, NeuroSwarms is a model\\nartificial spatial system that integrates autonomous control and theoretical\\nneuroscience to potentially uncover common principles to advance both domains.\",\n",
       "    'author': [{'name': 'Joseph D. Monaco'},\n",
       "     {'name': 'Grace M. Hwang'},\n",
       "     {'name': 'Kevin M. Schultz'},\n",
       "     {'name': 'Kechen Zhang'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/s00422-020-00823-z'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/s00422-020-00823-z',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1909.06711v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1909.06711v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '16 pages, 7 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Biol Cybern 114, 269-284 (2020)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'nlin.AO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/cs/0410042v1',\n",
       "    'updated': '2004-10-18T10:50:28Z',\n",
       "    'published': '2004-10-18T10:50:28Z',\n",
       "    'title': 'Neural Architectures for Robot Intelligence',\n",
       "    'summary': 'We argue that the direct experimental approaches to elucidate the\\narchitecture of higher brains may benefit from insights gained from exploring\\nthe possibilities and limits of artificial control architectures for robot\\nsystems. We present some of our recent work that has been motivated by that\\nview and that is centered around the study of various aspects of hand actions\\nsince these are intimately linked with many higher cognitive abilities. As\\nexamples, we report on the development of a modular system for the recognition\\nof continuous hand postures based on neural nets, the use of vision and tactile\\nsensing for guiding prehensile movements of a multifingered hand, and the\\nrecognition and use of hand gestures for robot teaching.\\n  Regarding the issue of learning, we propose to view real-world learning from\\nthe perspective of data mining and to focus more strongly on the imitation of\\nobserved actions instead of purely reinforcement-based exploration. As a\\nconcrete example of such an effort we report on the status of an ongoing\\nproject in our lab in which a robot equipped with an attention system with a\\nneurally inspired architecture is taught actions by using hand gestures in\\nconjunction with speech commands. We point out some of the lessons learnt from\\nthis system, and discuss how systems of this kind can contribute to the study\\nof issues at the junction between natural and artificial cognitive systems.',\n",
       "    'author': [{'name': 'H. Ritter'},\n",
       "     {'name': 'J. J. Steil'},\n",
       "     {'name': 'C. Noelker'},\n",
       "     {'name': 'F. Roethling'},\n",
       "     {'name': 'P. C. McGuire'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '37 pages, 17 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Reviews in the Neurosciences, vol. 14, no. 1-2, pp. 121-143 (2003)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/cs/0410042v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/cs/0410042v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.CV', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.10; I.2.6; H.1.2; H.2.8; I.5.4',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1611.01673v3',\n",
       "    'updated': '2017-03-02T21:20:59Z',\n",
       "    'published': '2016-11-05T16:56:44Z',\n",
       "    'title': 'Generative Multi-Adversarial Networks',\n",
       "    'summary': 'Generative adversarial networks (GANs) are a framework for producing a\\ngenerative model by way of a two-player minimax game. In this paper, we propose\\nthe \\\\emph{Generative Multi-Adversarial Network} (GMAN), a framework that\\nextends GANs to multiple discriminators. In previous work, the successful\\ntraining of GANs requires modifying the minimax objective to accelerate\\ntraining early on. In contrast, GMAN can be reliably trained with the original,\\nuntampered objective. We explore a number of design perspectives with the\\ndiscriminator role ranging from formidable adversary to forgiving teacher.\\nImage generation tasks comparing the proposed framework to standard GANs\\ndemonstrate GMAN produces higher quality samples in a fraction of the\\niterations when measured by a pairwise GAM-type metric.',\n",
       "    'author': [{'name': 'Ishan Durugkar'},\n",
       "     {'name': 'Ian Gemp'},\n",
       "     {'name': 'Sridhar Mahadevan'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted as a conference paper (poster) at ICLR 2017'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1611.01673v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1611.01673v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1802.05642v2',\n",
       "    'updated': '2018-06-06T13:26:15Z',\n",
       "    'published': '2018-02-15T16:32:48Z',\n",
       "    'title': 'The Mechanics of n-Player Differentiable Games',\n",
       "    'summary': 'The cornerstone underpinning deep learning is the guarantee that gradient\\ndescent on an objective converges to local minima. Unfortunately, this\\nguarantee fails in settings, such as generative adversarial nets, where there\\nare multiple interacting losses. The behavior of gradient-based methods in\\ngames is not well understood -- and is becoming increasingly important as\\nadversarial and multi-objective architectures proliferate. In this paper, we\\ndevelop new techniques to understand and control the dynamics in general games.\\nThe key result is to decompose the second-order dynamics into two components.\\nThe first is related to potential games, which reduce to gradient descent on an\\nimplicit function; the second relates to Hamiltonian games, a new class of\\ngames that obey a conservation law, akin to conservation laws in classical\\nmechanical systems. The decomposition motivates Symplectic Gradient Adjustment\\n(SGA), a new algorithm for finding stable fixed points in general games. Basic\\nexperiments show SGA is competitive with recently proposed algorithms for\\nfinding stable fixed points in GANs -- whilst at the same time being applicable\\nto -- and having guarantees in -- much more general games.',\n",
       "    'author': [{'name': 'David Balduzzi'},\n",
       "     {'name': 'Sebastien Racaniere'},\n",
       "     {'name': 'James Martens'},\n",
       "     {'name': 'Jakob Foerster'},\n",
       "     {'name': 'Karl Tuyls'},\n",
       "     {'name': 'Thore Graepel'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'ICML 2018, final version'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'PMLR volume 80, 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1802.05642v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1802.05642v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1808.00260v1',\n",
       "    'updated': '2018-08-01T10:53:01Z',\n",
       "    'published': '2018-08-01T10:53:01Z',\n",
       "    'title': 'A Review on the Application of Natural Computing in Environmental\\n  Informatics',\n",
       "    'summary': 'Natural computing offers new opportunities to understand, model and analyze\\nthe complexity of the physical and human-created environment. This paper\\nexamines the application of natural computing in environmental informatics, by\\ninvestigating related work in this research field. Various nature-inspired\\ntechniques are presented, which have been employed to solve different relevant\\nproblems. Advantages and disadvantages of these techniques are discussed,\\ntogether with analysis of how natural computing is generally used in\\nenvironmental research.',\n",
       "    'author': {'name': 'Andreas Kamilaris'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proc. of EnviroInfo 2018'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1808.00260v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1808.00260v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2006.11671v1',\n",
       "    'updated': '2020-06-20T22:53:32Z',\n",
       "    'published': '2020-06-20T22:53:32Z',\n",
       "    'title': 'Collective Learning by Ensembles of Altruistic Diversifying Neural\\n  Networks',\n",
       "    'summary': \"Combining the predictions of collections of neural networks often outperforms\\nthe best single network. Such ensembles are typically trained independently,\\nand their superior `wisdom of the crowd' originates from the differences\\nbetween networks. Collective foraging and decision making in socially\\ninteracting animal groups is often improved or even optimal thanks to local\\ninformation sharing between conspecifics. We therefore present a model for\\nco-learning by ensembles of interacting neural networks that aim to maximize\\ntheir own performance but also their functional relations to other networks. We\\nshow that ensembles of interacting networks outperform independent ones, and\\nthat optimal ensemble performance is reached when the coupling between networks\\nincreases diversity and degrades the performance of individual networks. Thus,\\neven without a global goal for the ensemble, optimal collective behavior\\nemerges from local interactions between networks. We show the scaling of\\noptimal coupling strength with ensemble size, and that networks in these\\nensembles specialize functionally and become more `confident' in their\\nassessments. Moreover, optimal co-learning networks differ structurally,\\nrelying on sparser activity, a wider range of synaptic weights, and higher\\nfiring rates - compared to independently trained networks. Finally, we explore\\ninteractions-based co-learning as a framework for expanding and boosting\\nensembles.\",\n",
       "    'author': [{'name': 'Benjamin Brazowski'}, {'name': 'Elad Schneidman'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2006.11671v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2006.11671v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.LG',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.LG',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'stat.ML', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2010.15045v1',\n",
       "    'updated': '2020-09-21T15:11:29Z',\n",
       "    'published': '2020-09-21T15:11:29Z',\n",
       "    'title': 'A multi-agent model for growing spiking neural networks',\n",
       "    'summary': 'Artificial Intelligence has looked into biological systems as a source of\\ninspiration. Although there are many aspects of the brain yet to be discovered,\\nneuroscience has found evidence that the connections between neurons\\ncontinuously grow and reshape as a part of the learning process. This differs\\nfrom the design of Artificial Neural Networks, that achieve learning by\\nevolving the weights in the synapses between them and their topology stays\\nunaltered through time.\\n  This project has explored rules for growing the connections between the\\nneurons in Spiking Neural Networks as a learning mechanism. These rules have\\nbeen implemented on a multi-agent system for creating simple logic functions,\\nthat establish a base for building up more complex systems and architectures.\\nResults in a simulation environment showed that for a given set of parameters\\nit is possible to reach topologies that reproduce the tested functions.\\n  This project also opens the door to the usage of techniques like genetic\\nalgorithms for obtaining the best suited values for the model parameters, and\\nhence creating neural networks that can adapt to different functions.',\n",
       "    'author': [{'name': 'Javier Lopez Randulfe'},\n",
       "     {'name': 'Leon Bonde Larsen'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': \"79 pages. Master's thesis\"},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2010.15045v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2010.15045v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2104.05610v2',\n",
       "    'updated': '2021-04-13T10:07:16Z',\n",
       "    'published': '2021-04-12T16:30:03Z',\n",
       "    'title': 'A coevolutionary approach to deep multi-agent reinforcement learning',\n",
       "    'summary': \"Traditionally, Deep Artificial Neural Networks (DNN's) are trained through\\ngradient descent. Recent research shows that Deep Neuroevolution (DNE) is also\\ncapable of evolving multi-million-parameter DNN's, which proved to be\\nparticularly useful in the field of Reinforcement Learning (RL). This is mainly\\ndue to its excellent scalability and simplicity compared to the traditional\\nMDP-based RL methods. So far, DNE has only been applied to complex single-agent\\nproblems. As evolutionary methods are a natural choice for multi-agent\\nproblems, the question arises whether DNE can also be applied in a complex\\nmulti-agent setting. In this paper, we describe and validate a new approach\\nbased on Coevolution. To validate our approach, we benchmark two Deep\\nCoevolutionary Algorithms on a range of multi-agent Atari games and compare our\\nresults against the results of Ape-X DQN. Our results show that these Deep\\nCoevolutionary algorithms (1) can be successfully trained to play various\\ngames, (2) outperform Ape-X DQN in some of them, and therefore (3) show that\\nCoevolution can be a viable approach to solving complex multi-agent\\ndecision-making problems.\",\n",
       "    'author': [{'name': 'Daan Klijn'}, {'name': 'A. E. Eiben'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2104.05610v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2104.05610v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.NE',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.NE',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1302.0785v1',\n",
       "    'updated': '2013-02-04T18:26:03Z',\n",
       "    'published': '2013-02-04T18:26:03Z',\n",
       "    'title': 'Beyond Markov Chains, Towards Adaptive Memristor Network-based Music\\n  Generation',\n",
       "    'summary': \"We undertook a study of the use of a memristor network for music generation,\\nmaking use of the memristor's memory to go beyond the Markov hypothesis. Seed\\ntransition matrices are created and populated using memristor equations, and\\nwhich are shown to generate musical melodies and change in style over time as a\\nresult of feedback into the transition matrix. The spiking properties of simple\\nmemristor networks are demonstrated and discussed with reference to\\napplications of music making. The limitations of simulating composing memristor\\nnetworks in von Neumann hardware is discussed and a hardware solution based on\\nphysical memristor properties is presented.\",\n",
       "    'author': [{'name': 'Ella Gale'},\n",
       "     {'name': 'Oliver Matthews'},\n",
       "     {'name': 'Ben de Lacy Costello'},\n",
       "     {'name': 'Andrew Adamatzky'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '22 pages, 13 pages, conference paper'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1302.0785v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1302.0785v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.ET',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.ET',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': '68Txx', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'H.5.5; J.5; C.2.1; F.1.1; G.2.2; I.6; C.1.3; I.2.6',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2004.04687v1',\n",
       "    'updated': '2020-04-07T22:45:42Z',\n",
       "    'published': '2020-04-07T22:45:42Z',\n",
       "    'title': 'GGA-MG: Generative Genetic Algorithm for Music Generation',\n",
       "    'summary': 'Music Generation (MG) is an interesting research topic that links the art of\\nmusic and Artificial Intelligence (AI). The goal is to train an artificial\\ncomposer to generate infinite, fresh, and pleasurable musical pieces. Music has\\ndifferent parts such as melody, harmony, and rhythm. In this paper, we propose\\na Generative Genetic Algorithm (GGA) to produce a melody automatically. The\\nmain GGA uses a Long Short-Term Memory (LSTM) recurrent neural network as the\\nobjective function, which should be trained by a spectrum of bad-to-good\\nmelodies. These melodies have to be provided by another GGA with a different\\nobjective function. Good melodies have been provided by CAMPINs collection. We\\nhave considered the rhythm in this work, too. The experimental results clearly\\nshow that the proposed GGA method is able to generate eligible melodies with\\nnatural transitions and without rhythm error.',\n",
       "    'author': [{'name': 'Majid Farzaneh'}, {'name': 'Rahil Mahdian Toroghi'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.13140/RG.2.2.16677.24805'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.13140/RG.2.2.16677.24805',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2004.04687v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2004.04687v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '14 pages, Submitted to Journal of Evolutionary Intelligence'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1207.5560v1',\n",
       "    'updated': '2012-07-23T23:25:36Z',\n",
       "    'published': '2012-07-23T23:25:36Z',\n",
       "    'title': 'Evolving Musical Counterpoint: The Chronopoint Musical Evolution System',\n",
       "    'summary': 'Musical counterpoint, a musical technique in which two or more independent\\nmelodies are played simultaneously with the goal of creating harmony, has been\\naround since the baroque era. However, to our knowledge computational\\ngeneration of aesthetically pleasing linear counterpoint based on subjective\\nfitness assessment has not been explored by the evolutionary computation\\ncommunity (although generation using objective fitness has been attempted in\\nquite a few cases). The independence of contrapuntal melodies and the\\nsubjective nature of musical aesthetics provide an excellent platform for the\\napplication of genetic algorithms. In this paper, a genetic algorithm approach\\nto generating contrapuntal melodies is explained, with a description of the\\nvarious musical heuristics used and of how variable-length chromosome strings\\nare used to avoid generating \"jerky\" rhythms and melodic phrases, as well as\\nhow subjectivity is incorporated into the algorithm\\'s fitness measures. Next,\\nresults from empirical testing of the algorithm are presented, with a focus on\\nhow a user\\'s musical sophistication influences their experience. Lastly,\\nfurther musical and compositional applications of the algorithm are discussed\\nalong with planned future work on the algorithm.',\n",
       "    'author': [{'name': 'Jeffrey Power Jacobs'}, {'name': 'James Reggia'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6 pages, 6 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Proceedings of the First International Workshop on Evolutionary\\n  Music, 2011 IEEE Congress on Evolutionary Computation, 6-11 (2011)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1207.5560v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1207.5560v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1302.6031v1',\n",
       "    'updated': '2013-02-25T10:13:09Z',\n",
       "    'published': '2013-02-25T10:13:09Z',\n",
       "    'title': 'Phoneme discrimination using KS algebra I',\n",
       "    'summary': 'In our work we define a new algebra of operators as a substitute for fuzzy\\nlogic. Its primary purpose is for construction of binary discriminators for\\nphonemes based on spectral content. It is optimized for design of\\nnon-parametric computational circuits, and makes uses of 4 operations: $\\\\min$,\\n$\\\\max$, the difference and generalized additively homogenuous means.',\n",
       "    'author': {'name': 'Ondrej Such'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1302.6031v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1302.6031v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.7; I.5.2; I.5.4',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1612.04742v4',\n",
       "    'updated': '2018-04-14T12:43:15Z',\n",
       "    'published': '2016-12-14T17:33:38Z',\n",
       "    'title': 'Imposing higher-level Structure in Polyphonic Music Generation using\\n  Convolutional Restricted Boltzmann Machines and Constraints',\n",
       "    'summary': 'We introduce a method for imposing higher-level structure on generated,\\npolyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a\\ngenerative model is combined with gradient descent constraint optimisation to\\nprovide further control over the generation process. Among other things, this\\nallows for the use of a \"template\" piece, from which some structural properties\\ncan be extracted, and transferred as constraints to the newly generated\\nmaterial. The sampling process is guided with Simulated Annealing to avoid\\nlocal optima, and to find solutions that both satisfy the constraints, and are\\nrelatively stable with respect to the C-RBM. Results show that with this\\napproach it is possible to control the higher-level self-similarity structure,\\nthe meter, and the tonal properties of the resulting musical piece, while\\npreserving its local musical coherence.',\n",
       "    'author': [{'name': 'Stefan Lattner'},\n",
       "     {'name': 'Maarten Grachten'},\n",
       "     {'name': 'Gerhard Widmer'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.5920/jcms.2018.01'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.5920/jcms.2018.01',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1612.04742v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1612.04742v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '31 pages, 11 figures'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Journal of Creative Music Systems, Volume 2, Issue 1, March 2018'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.SD',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.SD',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/0809.4784v1',\n",
       "    'updated': '2008-09-27T16:33:34Z',\n",
       "    'published': '2008-09-27T16:33:34Z',\n",
       "    'title': 'A Computational Study on Emotions and Temperament in Multi-Agent Systems',\n",
       "    'summary': 'Recent advances in neurosciences and psychology have provided evidence that\\naffective phenomena pervade intelligence at many levels, being inseparable from\\nthe cognitionaction loop. Perception, attention, memory, learning,\\ndecisionmaking, adaptation, communication and social interaction are some of\\nthe aspects influenced by them. This work draws its inspirations from\\nneurobiology, psychophysics and sociology to approach the problem of building\\nautonomous robots capable of interacting with each other and building\\nstrategies based on temperamental decision mechanism. Modelling emotions is a\\nrelatively recent focus in artificial intelligence and cognitive modelling.\\nSuch models can ideally inform our understanding of human behavior. We may see\\nthe development of computational models of emotion as a core research focus\\nthat will facilitate advances in the large array of computational systems that\\nmodel, interpret or influence human behavior. We propose a model based on a\\nscalable, flexible and modular approach to emotion which allows runtime\\nevaluation between emotional quality and performance. The results achieved\\nshowed that the strategies based on temperamental decision mechanism strongly\\ninfluence the system performance and there are evident dependency between\\nemotional state of the agents and their temperamental type, as well as the\\ndependency between the team performance and the temperamental configuration of\\nthe team members, and this enable us to conclude that the modular approach to\\nemotional programming based on temperamental theory is the good choice to\\ndevelop computational mind models for emotional behavioral Multi-Agent systems.',\n",
       "    'author': [{'name': 'Luis Paulo Reis'},\n",
       "     {'name': 'Daria Barteneva'},\n",
       "     {'name': 'Nuno Lau'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/0809.4784v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/0809.4784v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1211.3882v2',\n",
       "    'updated': '2012-11-21T04:03:21Z',\n",
       "    'published': '2012-11-16T13:20:59Z',\n",
       "    'title': 'Gliders2012: Development and Competition Results',\n",
       "    'summary': 'The RoboCup 2D Simulation League incorporates several challenging features,\\nsetting a benchmark for Artificial Intelligence (AI). In this paper we describe\\nsome of the ideas and tools around the development of our team, Gliders2012. In\\nour description, we focus on the evaluation function as one of our central\\nmechanisms for action selection. We also point to a new framework for watching\\nlog files in a web browser that we release for use and further development by\\nthe RoboCup community. Finally, we also summarize results of the group and\\nfinal matches we played during RoboCup 2012, with Gliders2012 finishing 4th out\\nof 19 teams.',\n",
       "    'author': [{'name': 'Edward Moore'},\n",
       "     {'name': 'Oliver Obst'},\n",
       "     {'name': 'Mikhail Prokopenko'},\n",
       "     {'name': 'Peter Wang'},\n",
       "     {'name': 'Jason Held'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1211.3882v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1211.3882v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1511.07209v2',\n",
       "    'updated': '2016-07-28T09:11:40Z',\n",
       "    'published': '2015-11-23T13:04:47Z',\n",
       "    'title': 'Multi-Agent Continuous Transportation with Online Balanced Partitioning',\n",
       "    'summary': 'We introduce the concept of continuous transportation task to the context of\\nmulti-agent systems. A continuous transportation task is one in which a\\nmulti-agent team visits a number of fixed locations, picks up objects, and\\ndelivers them to a final destination. The goal is to maximize the rate of\\ntransportation while the objects are replenished over time. Examples of\\nproblems that need continuous transportation are foraging, area sweeping, and\\nfirst/last mile problem. Previous approaches typically neglect the interference\\nand are highly dependent on communications among agents. Some also incorporate\\nan additional reconnaissance agent to gather information. In this paper, we\\npresent a hybrid of centralized and distributed approaches that minimize the\\ninterference and communications in the multi-agent team without the need for a\\nreconnaissance agent. We contribute two partitioning-transportation algorithms\\ninspired by existing algorithms, and contribute one novel online\\npartitioning-transportation algorithm with information gathering in the\\nmulti-agent team. Our algorithms have been implemented and tested extensively\\nin the simulation. The results presented in this paper demonstrate the\\neffectiveness of our algorithms that outperform the existing algorithms, even\\nwithout any communications between the agents and without the presence of a\\nreconnaissance agent.',\n",
       "    'author': [{'name': 'Chao Wang'},\n",
       "     {'name': 'Somchaya Liemhetcharat'},\n",
       "     {'name': 'Kian Hsiang Low'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2 pages, published in the proceedings of the 15th AAMAS conference'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1511.07209v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1511.07209v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.11', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1708.01930v1',\n",
       "    'updated': '2017-08-06T19:54:05Z',\n",
       "    'published': '2017-08-06T19:54:05Z',\n",
       "    'title': 'Enhanced Emotion Enabled Cognitive Agent Based Rear End Collision\\n  Avoidance Controller for Autonomous Vehicles',\n",
       "    'summary': 'Rear end collisions are deadliest in nature and cause most of traffic\\ncasualties and injuries. In the existing research, many rear end collision\\navoidance solutions have been proposed. However, the problem with these\\nproposed solutions is that they are highly dependent on precise mathematical\\nmodels. Whereas, the real road driving is influenced by non-linear factors such\\nas road surface situations, driver reaction time, pedestrian flow and vehicle\\ndynamics, hence obtaining the accurate mathematical model of the vehicle\\ncontrol system is challenging. This problem with precise control based rear end\\ncollision avoidance schemes has been addressed using fuzzy logic, but the\\nexcessive number of fuzzy rules straightforwardly prejudice their efficiency.\\nFurthermore, these fuzzy logic based controllers have been proposed without\\nusing proper agent based modeling that helps in mimicking the functions of an\\nartificial human driver executing these fuzzy rules. Keeping in view these\\nlimitations, we have proposed an Enhanced Emotion Enabled Cognitive Agent\\n(EEEC_Agent) based controller that helps the Autonomous Vehicles (AVs) to\\nperform rear end collision avoidance with less number of rules, designed after\\nfear emotion, and high efficiency. To introduce a fear emotion generation\\nmechanism in EEEC_Agent, Orton, Clore & Collins (OCC) model has been employed.\\nThe fear generation mechanism of EEEC_Agent has been verified using NetLogo\\nsimulation. Furthermore, practical validation of EEEC_Agent functions has been\\nperformed using specially built prototype AV platform. Eventually, the\\nqualitative comparative study with existing state of the art research works\\nreflect that proposed model outperforms recent research.',\n",
       "    'author': [{'name': 'Faisal Riaz'}, {'name': 'Muaz A. Niazi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '39 pages, 17 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1708.01930v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1708.01930v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.SY', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9, I.2.8, I.2.11, I.6, I.6.1, I.6.6',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1710.01447v1',\n",
       "    'updated': '2017-10-04T03:14:40Z',\n",
       "    'published': '2017-10-04T03:14:40Z',\n",
       "    'title': 'Feasibility Study: Moving Non-Homogeneous Teams in Congested Video Game\\n  Environments',\n",
       "    'summary': 'Multi-agent path finding (MAPF) is a well-studied problem in artificial\\nintelligence, where one needs to find collision-free paths for agents with\\ngiven start and goal locations. In video games, agents of different types often\\nform teams. In this paper, we demonstrate the usefulness of MAPF algorithms\\nfrom artificial intelligence for moving such non-homogeneous teams in congested\\nvideo game environments.',\n",
       "    'author': [{'name': 'Hang Ma'},\n",
       "     {'name': 'Jingxing Yang'},\n",
       "     {'name': 'Liron Cohen'},\n",
       "     {'name': 'T. K. Satish Kumar'},\n",
       "     {'name': 'Sven Koenig'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in AIIDE 17'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1710.01447v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1710.01447v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1907.04396v1',\n",
       "    'updated': '2019-07-09T20:29:48Z',\n",
       "    'published': '2019-07-09T20:29:48Z',\n",
       "    'title': 'Informative Path Planning with Local Penalization for Decentralized and\\n  Asynchronous Swarm Robotic Search',\n",
       "    'summary': \"Decentralized swarm robotic solutions to searching for targets that emit a\\nspatially varying signal promise task parallelism, time efficiency, and fault\\ntolerance. It is, however, challenging for swarm algorithms to offer\\nscalability and efficiency, while preserving mathematical insights into the\\nexhibited behavior. A new decentralized search method (called Bayes-Swarm),\\nfounded on batch Bayesian Optimization (BO) principles, is presented here to\\naddress these challenges. Unlike swarm heuristics approaches, Bayes-Swarm\\ndecouples the knowledge generation and task planning process, thus preserving\\ninsights into the emergent behavior. Key contributions lie in: 1) modeling\\nknowledge extraction over trajectories, unlike in BO; 2) time-adaptively\\nbalancing exploration/exploitation and using an efficient local penalization\\napproach to account for potential interactions among different robots' planned\\nsamples; and 3) presenting an asynchronous implementation of the algorithm.\\nThis algorithm is tested on case studies with bimodal and highly multimodal\\nsignal distributions. Up to 76 times better efficiency is demonstrated compared\\nto an exhaustive search baseline. The benefits of exploitation/exploration\\nbalancing, asynchronous planning, and local penalization, and scalability with\\nswarm size, are also demonstrated.\",\n",
       "    'author': [{'name': 'Payam Ghassemi'}, {'name': 'Souma Chowdhury'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Accepted for presentation in (and publication in the proceedings of)\\n  The IEEE 2019 International Symposium on Multi-Robot and Multi-Agent Systems\\n  (MRS)'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1907.04396v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1907.04396v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.12415v1',\n",
       "    'updated': '2019-10-28T03:11:22Z',\n",
       "    'published': '2019-10-28T03:11:22Z',\n",
       "    'title': 'Robotic Hierarchical Graph Neurons. A novel implementation of HGN for\\n  swarm robotic behaviour control',\n",
       "    'summary': 'This paper explores the use of a novel form of Hierarchical Graph Neurons\\n(HGN) for in-operation behaviour selection in a swarm of robotic agents. This\\nnew HGN is called Robotic-HGN (R-HGN), as it matches robot environment\\nobservations to environment labels via fusion of match probabilities from both\\ntemporal and intra-swarm collections. This approach is novel for HGN as it\\naddresses robotic observations being pseudo-continuous numbers, rather than\\ncategorical values. Additionally, the proposed approach is memory and\\ncomputation-power conservative and thus is acceptable for use in mobile devices\\nsuch as single-board computers, which are often used in mobile robotic agents.\\nThis R-HGN approach is validated against individual behaviour implementation\\nand random behaviour selection. This contrast is made in two sets of simulated\\nenvironments: environments designed to challenge the held behaviours of the\\nR-HGN, and randomly generated environments which are more challenging for the\\nrobotic swarm than R-HGN training conditions. R-HGN has been found to enable\\nappropriate behaviour selection in both these sets, allowing significant swarm\\nperformance in pre-trained and unexpected environment conditions.',\n",
       "    'author': [{'name': 'Phillip Smith'},\n",
       "     {'name': 'Aldeida Aleti'},\n",
       "     {'name': 'Vincent C. S. Lee'},\n",
       "     {'name': 'Robert Hunjet'},\n",
       "     {'name': 'Asad Khan'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1016/j.eswa.2021.115675'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1016/j.eswa.2021.115675',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/1910.12415v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.12415v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Expert Systems with Applications 2021'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1910.13880v1',\n",
       "    'updated': '2019-10-30T14:26:30Z',\n",
       "    'published': '2019-10-30T14:26:30Z',\n",
       "    'title': 'Path Planning Games',\n",
       "    'summary': \"Path planning is a fundamental and extensively explored problem in robotic\\ncontrol. We present a novel economic perspective on path planning.\\nSpecifically, we investigate strategic interactions among path planning agents\\nusing a game theoretic path planning framework. Our focus is on economic\\ntension between two important objectives: efficiency in the agents' achieving\\ntheir goals, and safety in navigating towards these. We begin by developing a\\nnovel mathematical formulation for path planning that trades off these\\nobjectives, when behavior of other agents is fixed. We then use this\\nformulation for approximating Nash equilibria in path planning games, as well\\nas to develop a multi-agent cooperative path planning formulation. Through\\nseveral case studies, we show that in a path planning game, safety is often\\nsignificantly compromised compared to a cooperative solution.\",\n",
       "    'author': [{'name': 'Yi Li'}, {'name': 'Yevgeniy Vorobeychik'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1910.13880v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1910.13880v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1911.07840v3',\n",
       "    'updated': '2021-02-21T09:39:03Z',\n",
       "    'published': '2019-11-16T13:02:59Z',\n",
       "    'title': 'Cooperative Pathfinding based on Multi-agent RRT* Fixed Node',\n",
       "    'summary': 'In cooperative pathfinding problems, non-conflict paths that bring several\\nagents from their start location to their destination need to be planned. This\\nproblem can be efficiently solved by Multi-agent RRT*(MA-RRT*) algorithm, which\\nis still state-of-the-art in the field of coupled methods. However, the\\nimplementation of this algorithm is hindered in systems with limited memory\\nbecause the number of nodes in the tree of RRT* grows indefinitely as the paths\\nget optimized. This paper proposes an improved version of MA-RRT*, called\\nMulti-agent RRT* Fixed Node(MA-RRT*FN), which limits the number of nodes stored\\nin the tree of RRT* by removing the weak nodes on the path which are not likely\\nto reach the goal. The results show that MA-RRT*FN performs close to MA-RRT* in\\nterms of scalability and solution quality while the memory required is much\\nlower and fixed.',\n",
       "    'author': [{'name': 'Jinmingwu Jiang'}, {'name': 'Kaigui Wu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'arXiv admin note: substantial text overlap with arXiv:1911.03927'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1911.07840v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1911.07840v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1912.00253v1',\n",
       "    'updated': '2019-11-30T19:16:18Z',\n",
       "    'published': '2019-11-30T19:16:18Z',\n",
       "    'title': 'Idle Time Optimization for Target Assignment and Path Finding in\\n  Sortation Centers',\n",
       "    'summary': 'In this paper, we study the one-shot and lifelong versions of the Target\\nAssignment and Path Finding problem in automated sortation centers, where each\\nagent needs to constantly assign itself a sorting station, move to its assigned\\nstation without colliding with obstacles or other agents, wait in the queue of\\nthat station to obtain a parcel for delivery, and then deliver the parcel to a\\nsorting bin. The throughput of such centers is largely determined by the total\\nidle time of all stations since their queues can frequently become empty. To\\naddress this problem, we first formalize and study the one-shot version that\\nassigns stations to a set of agents and finds collision-free paths for the\\nagents to their assigned stations. We present efficient algorithms for this\\ntask based on a novel min-cost max-flow formulation that minimizes the total\\nidle time of all stations in a fixed time window. We then demonstrate how our\\nalgorithms for solving the one-shot problem can be applied to solving the\\nlifelong problem as well. Experimentally, we believe to be the first\\nresearchers to consider real-world automated sortation centers using an\\nindustrial simulator with realistic data and a kinodynamic model of real\\nrobots. On this simulator, we showcase the benefits of our algorithms by\\ndemonstrating their efficiency and effectiveness for up to 350 agents.',\n",
       "    'author': [{'name': 'Ngai Meng Kou'},\n",
       "     {'name': 'Cheng Peng'},\n",
       "     {'name': 'Hang Ma'},\n",
       "     {'name': 'T. K. Satish Kumar'},\n",
       "     {'name': 'Sven Koenig'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'AAAI 2020, to appear'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1912.00253v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1912.00253v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2001.05994v2',\n",
       "    'updated': '2020-10-07T20:41:11Z',\n",
       "    'published': '2020-01-16T18:51:42Z',\n",
       "    'title': 'Adversarially Guided Self-Play for Adopting Social Conventions',\n",
       "    'summary': \"Robotic agents must adopt existing social conventions in order to be\\neffective teammates. These social conventions, such as driving on the right or\\nleft side of the road, are arbitrary choices among optimal policies, but all\\nagents on a successful team must use the same convention. Prior work has\\nidentified a method of combining self-play with paired input-output data\\ngathered from existing agents in order to learn their social convention without\\ninteracting with them. We build upon this work by introducing a technique\\ncalled Adversarial Self-Play (ASP) that uses adversarial training to shape the\\nspace of possible learned policies and substantially improves learning\\nefficiency. ASP only requires the addition of unpaired data: a dataset of\\noutputs produced by the social convention without associated inputs.\\nTheoretical analysis reveals how ASP shapes the policy space and the\\ncircumstances (when behaviors are clustered or exhibit some other structure)\\nunder which it offers the greatest benefits. Empirical results across three\\ndomains confirm ASP's advantages: it produces models that more closely match\\nthe desired social convention when given as few as two paired datapoints.\",\n",
       "    'author': [{'name': 'Mycal Tucker'},\n",
       "     {'name': 'Yilun Zhou'},\n",
       "     {'name': 'Julie Shah'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages, 8 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2001.05994v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2001.05994v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2009.10033v4',\n",
       "    'updated': '2021-01-21T17:49:20Z',\n",
       "    'published': '2020-09-21T17:13:50Z',\n",
       "    'title': 'Solution Concepts in Hierarchical Games under Bounded Rationality with\\n  Applications to Autonomous Driving',\n",
       "    'summary': 'With autonomous vehicles (AV) set to integrate further into regular human\\ntraffic, there is an increasing consensus of treating AV motion planning as a\\nmulti-agent problem. However, the traditional game theoretic assumption of\\ncomplete rationality is too strong for the purpose of human driving, and there\\nis a need for understanding human driving as a \\\\emph{bounded rational} activity\\nthrough a behavioral game theoretic lens. To that end, we adapt three\\nmetamodels of bounded rational behavior; two based on Quantal level-k and one\\nbased on Nash equilibrium with quantal errors. We formalize the different\\nsolution concepts that can be applied in the context of hierarchical games, a\\nframework used in multi-agent motion planning, for the purpose of creating game\\ntheoretic models of driving behavior. Furthermore, based on a contributed\\ndataset of human driving at a busy urban intersection with a total of ~4k\\nagents and ~44k decision points, we evaluate the behavior models on the basis\\nof model fit to naturalistic data, as well as their predictive capacity. Our\\nresults suggest that among the behavior models evaluated, modeling driving\\nbehavior as pure strategy NE with quantal errors at the level of maneuvers with\\nbounds sampling of actions at the level of trajectories provides the best fit\\nto naturalistic driving behavior, and there is a significant impact of\\nsituational factors on the performance of behavior models.',\n",
       "    'author': [{'name': 'Atrisha Sarkar'}, {'name': 'Krzysztof Czarnecki'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2009.10033v4',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2009.10033v4',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2011.00554v1',\n",
       "    'updated': '2020-11-01T16:43:10Z',\n",
       "    'published': '2020-11-01T16:43:10Z',\n",
       "    'title': 'Can a Robot Trust You? A DRL-Based Approach to Trust-Driven Human-Guided\\n  Navigation',\n",
       "    'summary': \"Humans are known to construct cognitive maps of their everyday surroundings\\nusing a variety of perceptual inputs. As such, when a human is asked for\\ndirections to a particular location, their wayfinding capability in converting\\nthis cognitive map into directional instructions is challenged. Owing to\\nspatial anxiety, the language used in the spoken instructions can be vague and\\noften unclear. To account for this unreliability in navigational guidance, we\\npropose a novel Deep Reinforcement Learning (DRL) based trust-driven robot\\nnavigation algorithm that learns humans' trustworthiness to perform a language\\nguided navigation task. Our approach seeks to answer the question as to whether\\na robot can trust a human's navigational guidance or not. To this end, we look\\nat training a policy that learns to navigate towards a goal location using only\\ntrustworthy human guidance, driven by its own robot trust metric. We look at\\nquantifying various affective features from language-based instructions and\\nincorporate them into our policy's observation space in the form of a human\\ntrust metric. We utilize both these trust metrics into an optimal cognitive\\nreasoning scheme that decides when and when not to trust the given guidance.\\nOur results show that the learned policy can navigate the environment in an\\noptimal, time-efficient manner as opposed to an explorative approach that\\nperforms the same task. We showcase the efficacy of our results both in\\nsimulation and a real-world environment.\",\n",
       "    'author': [{'name': 'Vishnu Sashank Dorbala'},\n",
       "     {'name': 'Arjun Srinivasan'},\n",
       "     {'name': 'Aniket Bera'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2011.00554v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2011.00554v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2102.07017v1',\n",
       "    'updated': '2021-02-13T22:15:00Z',\n",
       "    'published': '2021-02-13T22:15:00Z',\n",
       "    'title': 'Mitigating Negative Side Effects via Environment Shaping',\n",
       "    'summary': \"Agents operating in unstructured environments often produce negative side\\neffects (NSE), which are difficult to identify at design time. While the agent\\ncan learn to mitigate the side effects from human feedback, such feedback is\\noften expensive and the rate of learning is sensitive to the agent's state\\nrepresentation. We examine how humans can assist an agent, beyond providing\\nfeedback, and exploit their broader scope of knowledge to mitigate the impacts\\nof NSE. We formulate this problem as a human-agent team with decoupled\\nobjectives. The agent optimizes its assigned task, during which its actions may\\nproduce NSE. The human shapes the environment through minor reconfiguration\\nactions so as to mitigate the impacts of the agent's side effects, without\\naffecting the agent's ability to complete its assigned task. We present an\\nalgorithm to solve this problem and analyze its theoretical properties. Through\\nexperiments with human subjects, we assess the willingness of users to perform\\nminor environment modifications to mitigate the impacts of NSE. Empirical\\nevaluation of our approach shows that the proposed framework can successfully\\nmitigate NSE, without affecting the agent's ability to complete its assigned\\ntask.\",\n",
       "    'author': [{'name': 'Sandhya Saisubramanian'},\n",
       "     {'name': 'Shlomo Zilberstein'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '9 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2102.07017v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2102.07017v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.00366v1',\n",
       "    'updated': '2021-08-01T05:31:00Z',\n",
       "    'published': '2021-08-01T05:31:00Z',\n",
       "    'title': 'Agent-aware State Estimation in Autonomous Vehicles',\n",
       "    'summary': 'Autonomous systems often operate in environments where the behavior of\\nmultiple agents is coordinated by a shared global state. Reliable estimation of\\nthe global state is thus critical for successfully operating in a multi-agent\\nsetting. We introduce agent-aware state estimation -- a framework for\\ncalculating indirect estimations of state given observations of the behavior of\\nother agents in the environment. We also introduce transition-independent\\nagent-aware state estimation -- a tractable class of agent-aware state\\nestimation -- and show that it allows the speed of inference to scale linearly\\nwith the number of agents in the environment. As an example, we model traffic\\nlight classification in instances of complete loss of direct observation. By\\ntaking into account observations of vehicular behavior from multiple directions\\nof traffic, our approach exhibits accuracy higher than that of existing traffic\\nlight-only HMM methods on a real-world autonomous vehicle data set under a\\nvariety of simulated occlusion scenarios.',\n",
       "    'author': [{'name': 'Shane Parr'},\n",
       "     {'name': 'Ishan Khatri'},\n",
       "     {'name': 'Justin Svegliato'},\n",
       "     {'name': 'Shlomo Zilberstein'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear in IROS 2021'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.00366v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.00366v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.11885v1',\n",
       "    'updated': '2021-08-26T16:21:56Z',\n",
       "    'published': '2021-08-26T16:21:56Z',\n",
       "    'title': 'Human operator cognitive availability aware Mixed-Initiative control',\n",
       "    'summary': 'This paper presents a Cognitive Availability Aware Mixed-Initiative\\nController for remotely operated mobile robots. The controller enables dynamic\\nswitching between different levels of autonomy (LOA), initiated by either the\\nAI or the human operator. The controller leverages a state-of-the-art computer\\nvision method and an off-the-shelf web camera to infer the cognitive\\navailability of the operator and inform the AI-initiated LOA switching. This\\nconstitutes a qualitative advancement over previous Mixed-Initiative (MI)\\ncontrollers. The controller is evaluated in a disaster response experiment, in\\nwhich human operators have to conduct an exploration task with a remote robot.\\nMI systems are shown to effectively assist the operators, as demonstrated by\\nquantitative and qualitative results in performance and workload. Additionally,\\nsome insights into the experimental difficulties of evaluating complex MI\\ncontrollers are presented.',\n",
       "    'author': [{'name': 'Giannis Petousakis'},\n",
       "     {'name': 'Manolis Chiou'},\n",
       "     {'name': 'Grigoris Nikolaou'},\n",
       "     {'name': 'Rustam Stolkin'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/ICHMS49158.2020.9209582'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/ICHMS49158.2020.9209582',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2108.11885v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.11885v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '4 pages'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '2020 IEEE International Conference on Human-Machine Systems\\n  (ICHMS)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2108.12934v1',\n",
       "    'updated': '2021-08-29T23:12:38Z',\n",
       "    'published': '2021-08-29T23:12:38Z',\n",
       "    'title': 'Distributed Swarm Collision Avoidance Based on Angular Calculations',\n",
       "    'summary': 'Collision avoidance is one of the most important topics in the robotics\\nfield. The goal is to move the robots from initial locations to target\\nlocations such that they follow shortest non-colliding paths in the shortest\\ntime and with the least amount of energy. In this paper, a distributed and\\nreal-time algorithm for dense and complex 2D and 3D environments is proposed.\\nThis algorithm uses angular calculations to select the optimal direction for\\nthe movement of each robot and it has been shown that these separate\\ncalculations lead to a form of cooperative behavior among agents. We evaluated\\nthe proposed approach on various simulation and experimental scenarios and\\ncompared the results with FMP and ORCA, two important algorithms in this field.\\nThe results show that the proposed approach is at least 25% faster than ORCA\\nand at least 7% faster than FMP and also more reliable than both methods. The\\nproposed method is shown to enable fully autonomous navigation of a swarm of\\ncrazyflies.',\n",
       "    'author': [{'name': 'SeyedZahir Qazavi'},\n",
       "     {'name': 'Samaneh Hosseini Semnani'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2108.12934v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2108.12934v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2109.09807v1',\n",
       "    'updated': '2021-09-20T19:38:14Z',\n",
       "    'published': '2021-09-20T19:38:14Z',\n",
       "    'title': \"I Know You Can't See Me: Dynamic Occlusion-Aware Safety Validation of\\n  Strategic Planners for Autonomous Vehicles Using Hypergames\",\n",
       "    'summary': 'A particular challenge for both autonomous and human driving is dealing with\\nrisk associated with dynamic occlusion, i.e., occlusion caused by other\\nvehicles in traffic. Based on the theory of hypergames, we develop a novel\\nmulti-agent dynamic occlusion risk (DOR) measure for assessing situational risk\\nin dynamic occlusion scenarios. Furthermore, we present a white-box,\\nscenario-based, accelerated safety validation framework for assessing safety of\\nstrategic planners in AV. Based on evaluation over a large naturalistic\\ndatabase, our proposed validation method achieves a 4000% speedup compared to\\ndirect validation on naturalistic data, a more diverse coverage, and ability to\\ngeneralize beyond the dataset and generate commonly observed dynamic occlusion\\ncrashes in traffic in an automated manner.',\n",
       "    'author': [{'name': 'Maximilian Kahn'},\n",
       "     {'name': 'Atrisha Sarkar'},\n",
       "     {'name': 'Krzysztof Czarnecki'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This work is 6 pages long and contains 5 figures. For the\\n  supplementary video, see https://youtu.be/-crio3rA_IU'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2109.09807v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2109.09807v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.GT', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2111.07441v2',\n",
       "    'updated': '2021-12-25T11:27:19Z',\n",
       "    'published': '2021-11-14T20:40:00Z',\n",
       "    'title': 'A distributed, plug-n-play algorithm for multi-robot applications with a\\n  priori non-computable objective functions',\n",
       "    'summary': \"This paper presents a distributed algorithm applicable to a wide range of\\npractical multi-robot applications. In such multi-robot applications, the\\nuser-defined objectives of the mission can be cast as a general optimization\\nproblem, without explicit guidelines of the subtasks per different robot. Owing\\nto the unknown environment, unknown robot dynamics, sensor nonlinearities,\\netc., the analytic form of the optimization cost function is not available a\\npriori. Therefore, standard gradient-descent-like algorithms are not applicable\\nto these problems. To tackle this, we introduce a new algorithm that carefully\\ndesigns each robot's subcost function, the optimization of which can accomplish\\nthe overall team objective. Upon this transformation, we propose a distributed\\nmethodology based on the cognitive-based adaptive optimization (CAO) algorithm,\\nthat is able to approximate the evolution of each robot's cost function and to\\nadequately optimize its decision variables (robot actions). The latter can be\\nachieved by online learning only the problem-specific characteristics that\\naffect the accomplishment of mission objectives. The overall, low-complexity\\nalgorithm can straightforwardly incorporate any kind of operational constraint,\\nis fault-tolerant, and can appropriately tackle time-varying cost functions. A\\ncornerstone of this approach is that it shares the same convergence\\ncharacteristics as those of block coordinate descent algorithms. The proposed\\nalgorithm is evaluated in three heterogeneous simulation set-ups under multiple\\nscenarios, against both general-purpose and problem-specific algorithms. Source\\ncode is available at\\nhttps://github.com/athakapo/A-distributed-plug-n-play-algorithm-for-multi-robot-applications.\",\n",
       "    'author': [{'name': 'Athanasios Ch. Kapoutsis'},\n",
       "     {'name': 'Savvas A. Chatzichristofis'},\n",
       "     {'name': 'Elias B. Kosmatopoulos'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1177/0278364919845054'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1177/0278364919845054',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2111.07441v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2111.07441v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'The International Journal of Robotics Research, (2019), Volume: 38\\n  issue: 7, page(s): 813-832'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.06014v2',\n",
       "    'updated': '2022-01-19T02:44:05Z',\n",
       "    'published': '2022-01-16T10:28:52Z',\n",
       "    'title': 'Standby-Based Deadlock Avoidance Method for Multi-Agent Pickup and\\n  Delivery Tasks',\n",
       "    'summary': 'The multi-agent pickup and delivery (MAPD) problem, in which multiple agents\\niteratively carry materials without collisions, has received significant\\nattention. However, many conventional MAPD algorithms assume a specifically\\ndesigned grid-like environment, such as an automated warehouse. Therefore, they\\nhave many pickup and delivery locations where agents can stay for a lengthy\\nperiod, as well as plentiful detours to avoid collisions owing to the freedom\\nof movement in a grid. By contrast, because a maze-like environment such as a\\nsearch-and-rescue or construction site has fewer pickup/delivery locations and\\ntheir numbers may be unbalanced, many agents concentrate on such locations\\nresulting in inefficient operations, often becoming stuck or deadlocked. Thus,\\nto improve the transportation efficiency even in a maze-like restricted\\nenvironment, we propose a deadlock avoidance method, called standby-based\\ndeadlock avoidance (SBDA). SBDA uses standby nodes determined in real-time\\nusing the articulation-point-finding algorithm, and the agent is guaranteed to\\nstay there for a finite amount of time. We demonstrated that our proposed\\nmethod outperforms a conventional approach. We also analyzed how the parameters\\nused for selecting standby nodes affect the performance.',\n",
       "    'author': [{'name': 'Tomoki Yamauchi'},\n",
       "     {'name': 'Yuki Miyashita'},\n",
       "     {'name': 'Toshiharu Sugawara'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Extended version of paper accepted at AAMAS 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.06014v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.06014v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2201.10918v2',\n",
       "    'updated': '2022-05-30T04:58:56Z',\n",
       "    'published': '2022-01-26T13:16:02Z',\n",
       "    'title': 'Behavior Tree-Based Task Planning for Multiple Mobile Robots using a\\n  Data Distribution Service',\n",
       "    'summary': 'In this study, we propose task planning framework for multiple robots that\\nbuilds on a behavior tree (BT). BTs communicate with a data distribution\\nservice (DDS) to send and receive data. Since the standard BT derived from one\\nroot node with a single tick is unsuitable for multiple robots, a novel type of\\nBT action and improved nodes are proposed to control multiple robots through a\\nDDS asynchronously. To plan tasks for robots efficiently, a single task\\nplanning unit is implemented with the proposed task types. The task planning\\nunit assigns tasks to each robot simultaneously through a single coalesced BT.\\nIf any robot falls into a fault while performing its assigned task, another BT\\nembedded in the robot is executed; the robot enters the recovery mode in order\\nto overcome the fault. To perform this function, the action in the BT\\ncorresponding to the task is defined as a variable, which is shared with the\\nDDS so that any action can be exchanged between the task planning unit and\\nrobots. To show the feasibility of our framework in a real-world application,\\nthree mobile robots were experimentally coordinated for them to travel\\nalternately to four goal positions by the proposed single task planning unit\\nvia a DDS.',\n",
       "    'author': [{'name': 'Seungwoo Jeong'},\n",
       "     {'name': 'Taekwon Ga'},\n",
       "     {'name': 'Inhwan Jeong'},\n",
       "     {'name': 'Jongeun Choi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '8 pages, 12 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2201.10918v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2201.10918v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.NI', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2203.11618v3',\n",
       "    'updated': '2023-01-26T10:57:28Z',\n",
       "    'published': '2022-03-22T11:13:36Z',\n",
       "    'title': 'Distributing Collaborative Multi-Robot Planning with Gaussian Belief\\n  Propagation',\n",
       "    'summary': 'Precise coordinated planning over a forward time window enables safe and\\nhighly efficient motion when many robots must work together in tight spaces,\\nbut this would normally require centralised control of all devices which is\\ndifficult to scale. We demonstrate GBP Planning, a new purely distributed\\ntechnique based on Gaussian Belief Propagation for multi-robot planning\\nproblems, formulated by a generic factor graph defining dynamics and collision\\nconstraints over a forward time window. In simulations, we show that our method\\nallows high performance collaborative planning where robots are able to cross\\neach other in busy, intricate scenarios. They maintain shorter, quicker and\\nsmoother trajectories than alternative distributed planning techniques even in\\ncases of communication failure. We encourage the reader to view the\\naccompanying video demonstration at https://youtu.be/8VSrEUjH610.',\n",
       "    'author': [{'name': 'Aalok Patwardhan'},\n",
       "     {'name': 'Riku Murai'},\n",
       "     {'name': 'Andrew J. Davison'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1109/LRA.2022.3227858'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1109/LRA.2022.3227858',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2203.11618v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2203.11618v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IEEE Robotics and Automation Letters, vol. 8, no. 2, pp. 552-559,\\n  Feb. 2023'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2206.11319v1',\n",
       "    'updated': '2022-06-22T18:47:00Z',\n",
       "    'published': '2022-06-22T18:47:00Z',\n",
       "    'title': 'Graph-Based Multi-Robot Path Finding and Planning',\n",
       "    'summary': 'Purpose of Review\\n  Planning collision-free paths for multiple robots is important for real-world\\nmulti-robot systems and has been studied as an optimization problem on graphs,\\ncalled Multi-Agent Path Finding (MAPF). This review surveys different\\ncategories of classic and state-of-the-art MAPF algorithms and different\\nresearch attempts to tackle the challenges of generalizing MAPF techniques to\\nreal-world scenarios.\\n  Recent Findings\\n  Solving MAPF problems optimally is computationally challenging. Recent\\nadvances have resulted in MAPF algorithms that can compute collision-free paths\\nfor hundreds of robots and thousands of navigation tasks in seconds of runtime.\\nMany variants of MAPF have been formalized to adapt MAPF techniques to\\ndifferent real-world requirements, such as considerations of robot kinematics,\\nonline optimization for real-time systems, and the integration of task\\nassignment and path planning.\\n  Summary\\n  Algorithmic techniques for MAPF problems have addressed important aspects of\\nseveral multi-robot applications, including automated warehouse fulfillment and\\nsortation, automated train scheduling, and navigation of non-holonomic robots\\nand quadcopters. This showcases their potential for real-world applications of\\nlarge-scale multi-robot systems.',\n",
       "    'author': {'name': 'Hang Ma'},\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/s43154-022-00083-8'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/s43154-022-00083-8',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2206.11319v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2206.11319v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This preprint has not undergone peer review (when applicable) or any\\n  post-submission improvements or corrections. The Version of Record of this\\n  article is published in Current Robotics Reports, and is available online at\\n  https://doi.org/10.1007/s43154-022-00083-8'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2208.01223v1',\n",
       "    'updated': '2022-08-02T03:20:59Z',\n",
       "    'published': '2022-08-02T03:20:59Z',\n",
       "    'title': 'Multi-Goal Multi-Agent Pickup and Delivery',\n",
       "    'summary': 'In this work, we consider the Multi-Agent Pickup-and-Delivery (MAPD) problem,\\nwhere agents constantly engage with new tasks and need to plan collision-free\\npaths to execute them. To execute a task, an agent needs to visit a pair of\\ngoal locations, consisting of a pickup location and a delivery location. We\\npropose two variants of an algorithm that assigns a sequence of tasks to each\\nagent using the anytime algorithm Large Neighborhood Search (LNS) and plans\\npaths using the Multi-Agent Path Finding (MAPF) algorithm Priority-Based Search\\n(PBS). LNS-PBS is complete for well-formed MAPD instances, a realistic subclass\\nof MAPD instances, and empirically more effective than the existing complete\\nMAPD algorithm CENTRAL. LNS-wPBS provides no completeness guarantee but is\\nempirically more efficient and stable than LNS-PBS. It scales to thousands of\\nagents and thousands of tasks in a large warehouse and is empirically more\\neffective than the existing scalable MAPD algorithm HBH+MLA*. LNS-PBS and\\nLNS-wPBS also apply to a more general variant of MAPD, namely the Multi-Goal\\nMAPD (MG-MAPD) problem, where tasks can have different numbers of goal\\nlocations.',\n",
       "    'author': [{'name': 'Qinghong Xu'},\n",
       "     {'name': 'Jiaoyang Li'},\n",
       "     {'name': 'Sven Koenig'},\n",
       "     {'name': 'Hang Ma'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'IROS 2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2208.01223v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2208.01223v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2209.04696v2',\n",
       "    'updated': '2022-09-13T06:56:54Z',\n",
       "    'published': '2022-09-10T15:35:20Z',\n",
       "    'title': 'Cooperation and Competition: Flocking with Evolutionary Multi-Agent\\n  Reinforcement Learning',\n",
       "    'summary': \"Flocking is a very challenging problem in a multi-agent system; traditional\\nflocking methods also require complete knowledge of the environment and a\\nprecise model for control. In this paper, we propose Evolutionary Multi-Agent\\nReinforcement Learning (EMARL) in flocking tasks, a hybrid algorithm that\\ncombines cooperation and competition with little prior knowledge. As for\\ncooperation, we design the agents' reward for flocking tasks according to the\\nboids model. While for competition, agents with high fitness are designed as\\nsenior agents, and those with low fitness are designed as junior, letting\\njunior agents inherit the parameters of senior agents stochastically. To\\nintensify competition, we also design an evolutionary selection mechanism that\\nshows effectiveness on credit assignment in flocking tasks. Experimental\\nresults in a range of challenging and self-contrast benchmarks demonstrate that\\nEMARL significantly outperforms the full competition or cooperation methods.\",\n",
       "    'author': [{'name': 'Yunxiao Guo'},\n",
       "     {'name': 'Xinjia Xie'},\n",
       "     {'name': 'Runhao Zhao'},\n",
       "     {'name': 'Chenglan Zhu'},\n",
       "     {'name': 'Jiangting Yin'},\n",
       "     {'name': 'Han Long'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'We misplaced Fig.5 (b) on Page 11 ( This figure is from early\\n  experiments with poor results). We failed to resubmit, so we want to revise\\n  the whole paper by this chance'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2209.04696v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2209.04696v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.08672v1',\n",
       "    'updated': '2022-10-17T00:29:24Z',\n",
       "    'published': '2022-10-17T00:29:24Z',\n",
       "    'title': 'Decision-Making Among Bounded Rational Agents',\n",
       "    'summary': \"When robots share the same workspace with other intelligent agents (e.g.,\\nother robots or humans), they must be able to reason about the behaviors of\\ntheir neighboring agents while accomplishing the designated tasks. In practice,\\nfrequently, agents do not exhibit absolutely rational behavior due to their\\nlimited computational resources. Thus, predicting the optimal agent behaviors\\nis undesirable (because it demands prohibitive computational resources) and\\nundesirable (because the prediction may be wrong). Motivated by this\\nobservation, we remove the assumption of perfectly rational agents and propose\\nincorporating the concept of bounded rationality from an information-theoretic\\nview into the game-theoretic framework. This allows the robots to reason other\\nagents' sub-optimal behaviors and act accordingly under their computational\\nconstraints. Specifically, bounded rationality directly models the agent's\\ninformation processing ability, which is represented as the KL-divergence\\nbetween nominal and optimized stochastic policies, and the solution to the\\nbounded-optimal policy can be obtained by an efficient importance sampling\\napproach. Using both simulated and real-world experiments in multi-robot\\nnavigation tasks, we demonstrate that the resulting framework allows the robots\\nto reason about different levels of rational behaviors of other agents and\\ncompute a reasonable strategy under its computational constraint.\",\n",
       "    'author': [{'name': 'Junhong Xu'},\n",
       "     {'name': 'Durgakant Pushp'},\n",
       "     {'name': 'Kai Yin'},\n",
       "     {'name': 'Lantao Liu'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'accepted by DARS2022'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2210.08672v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.08672v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2210.14774v1',\n",
       "    'updated': '2022-10-26T15:15:19Z',\n",
       "    'published': '2022-10-26T15:15:19Z',\n",
       "    'title': 'Unknown area exploration for robots with energy constraints using a\\n  modified Butterfly Optimization Algorithm',\n",
       "    'summary': 'Butterfly Optimization Algorithm (BOA) is a recent metaheuristic that has\\nbeen used in several optimization problems. In this paper, we propose a new\\nversion of the algorithm (xBOA) based on the crossover operator and compare its\\nresults to the original BOA and 3 other variants recently introduced in the\\nliterature. We also proposed a framework for solving the unknown area\\nexploration problem with energy constraints using metaheuristics in both\\nsingle- and multi-robot scenarios. This framework allowed us to benchmark the\\nperformances of different metaheuristics for the robotics exploration problem.\\nWe conducted several experiments to validate this framework and used it to\\ncompare the effectiveness of xBOA with wellknown metaheuristics used in the\\nliterature through 5 evaluation criteria. Although BOA and xBOA are not optimal\\nin all these criteria, we found that BOA can be a good alternative to many\\nmetaheuristics in terms of the exploration time, while xBOA is more robust to\\nlocal optima; has better fitness convergence; and achieves better exploration\\nrates than the original BOA and its other variants.',\n",
       "    'author': [{'name': 'Amine Bendahmane'}, {'name': 'Redouane Tlemsani'}],\n",
       "    'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '10.1007/s00500-022-07530-w'},\n",
       "    'link': [{'@title': 'doi',\n",
       "      '@href': 'http://dx.doi.org/10.1007/s00500-022-07530-w',\n",
       "      '@rel': 'related'},\n",
       "     {'@href': 'http://arxiv.org/abs/2210.14774v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2210.14774v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'Soft Computing (2022)'},\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/2301.08038v1',\n",
       "    'updated': '2023-01-19T12:30:56Z',\n",
       "    'published': '2023-01-19T12:30:56Z',\n",
       "    'title': 'A Comprehensive Architecture for Dynamic Role Allocation and\\n  Collaborative Task Planning in Mixed Human-Robot Teams',\n",
       "    'summary': 'The growing deployment of human-robot collaborative processes in several\\nindustrial applications, such as handling, welding, and assembly, unfolds the\\npursuit of systems which are able to manage large heterogeneous teams and, at\\nthe same time, monitor the execution of complex tasks. In this paper, we\\npresent a novel architecture for dynamic role allocation and collaborative task\\nplanning in a mixed human-robot team of arbitrary size. The architecture\\ncapitalizes on a centralized reactive and modular task-agnostic planning method\\nbased on Behavior Trees (BTs), in charge of actions scheduling, while the\\nallocation problem is formulated through a Mixed-Integer Linear Program (MILP),\\nthat assigns dynamically individual roles or collaborations to the agents of\\nthe team. Different metrics used as MILP cost allow the architecture to favor\\nvarious aspects of the collaboration (e.g. makespan, ergonomics, human\\npreferences). Human preference are identified through a negotiation phase, in\\nwhich, an human agent can accept/refuse to execute the assigned task.In\\naddition, bilateral communication between humans and the system is achieved\\nthrough an Augmented Reality (AR) custom user interface that provides intuitive\\nfunctionalities to assist and coordinate workers in different action phases.\\nThe computational complexity of the proposed methodology outperforms literature\\napproaches in industrial sized jobs and teams (problems up to 50 actions and 20\\nagents in the team with collaborations are solved within 1\\\\;s). The different\\nallocated roles, as the cost functions change, highlights the flexibility of\\nthe architecture to several production requirements. Finally, the subjective\\nevaluation demonstrating the high usability level and the suitability for the\\ntargeted scenario.',\n",
       "    'author': [{'name': 'Edoardo Lamon',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Human-Robot Interfaces and Interaction, Istituto Italiano di Tecnologia, Genoa, Italy'}},\n",
       "     {'name': 'Fabio Fusaro',\n",
       "      'arxiv:affiliation': [{'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'Human-Robot Interfaces and Interaction, Istituto Italiano di Tecnologia, Genoa, Italy'},\n",
       "       {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy'}]},\n",
       "     {'name': 'Elena De Momi',\n",
       "      'arxiv:affiliation': [{'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'Human-Robot Interfaces and Interaction, Istituto Italiano di Tecnologia, Genoa, Italy'},\n",
       "       {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "        '#text': 'Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy'}]},\n",
       "     {'name': 'Arash Ajoudani',\n",
       "      'arxiv:affiliation': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "       '#text': 'Human-Robot Interfaces and Interaction, Istituto Italiano di Tecnologia, Genoa, Italy'}}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '16 pages, 20 fugures, submitted to Transaction on Robotics'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/2301.08038v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/2301.08038v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/cs/0501092v1',\n",
       "    'updated': '2005-01-31T01:03:54Z',\n",
       "    'published': '2005-01-31T01:03:54Z',\n",
       "    'title': 'Multi-Vehicle Cooperative Control Using Mixed Integer Linear Programming',\n",
       "    'summary': 'We present methods to synthesize cooperative strategies for multi-vehicle\\ncontrol problems using mixed integer linear programming. Complex multi-vehicle\\ncontrol problems are expressed as mixed logical dynamical systems. Optimal\\nstrategies for these systems are then solved for using mixed integer linear\\nprogramming. We motivate the methods on problems derived from an adversarial\\ngame between two teams of robots called RoboFlag. We assume the strategy for\\none team is fixed and governed by state machines. The strategy for the other\\nteam is generated using our methods. Finally, we perform an average case\\ncomputational complexity study on our approach.',\n",
       "    'author': [{'name': 'Matthew G. Earl'}, {'name': \"Raffaello D'Andrea\"}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 13 figures, submitted to IEEE Transactions on Robotics, for\\n  associated web page see http://control.mae.cornell.edu/earl/milp1'},\n",
       "    'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'M. G. Earl and R. D\\'Andrea, \"Multi-Vehicle Cooperative Control\\n  using Mixed Integer Linear Programming,\" In Cooperative Control of\\n  Distributed Multi-Agent Systems, J. S. Shamma ed., John Wiley & Sons, 2007'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/cs/0501092v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/cs/0501092v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.8; I.2.11',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1011.3397v1',\n",
       "    'updated': '2010-11-15T14:20:14Z',\n",
       "    'published': '2010-11-15T14:20:14Z',\n",
       "    'title': 'The Inverse Task of the Reflexive Game Theory: Theoretical Matters,\\n  Practical Applications and Relationship with Other Issues',\n",
       "    'summary': 'The Reflexive Game Theory (RGT) has been recently proposed by Vladimir\\nLefebvre to model behavior of individuals in groups. The goal of this study is\\nto introduce the Inverse task. We consider methods of solution together with\\npractical applications. We present a brief overview of the RGT for easy\\nunderstanding of the problem. We also develop the schematic representation of\\nthe RGT inference algorithms to create the basis for soft- and hardware\\nsolutions of the RGT tasks. We propose a unified hierarchy of schemas to\\nrepresent humans and robots. This hierarchy is considered as a unified\\nframework to solve the entire spectrum of the RGT tasks. We conclude by\\nillustrating how this framework can be applied for modeling of mixed groups of\\nhumans and robots. All together this provides the exhaustive solution of the\\nInverse task and clearly illustrates its role and relationships with other\\nissues considered in the RGT.',\n",
       "    'author': {'name': 'Sergey Tarasenko'},\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '27 pages, 6 figures, 3 tables'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1011.3397v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1011.3397v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1206.4185v1',\n",
       "    'updated': '2012-06-19T12:00:47Z',\n",
       "    'published': '2012-06-19T12:00:47Z',\n",
       "    'title': 'Ant Robotics: Covering Continuous Domains by Multi-A(ge)nt Systems',\n",
       "    'summary': 'In this work we present an algorithm for covering continuous connected\\ndomains by ant-like robots with very limited capabilities. The robots can mark\\nvisited places with pheromone marks and sense the level of the pheromone in\\ntheir local neighborhood. In case of multiple robots these pheromone marks can\\nbe sensed by all robots and provide the only way of (indirect) communication\\nbetween the robots. The robots are assumed to be memoryless, and to have no\\nglobal information such as the domain map, their own position (either absolute\\nor relative), total marked area percentage, maximal pheromone level, etc..\\nDespite the robots\\' simplicity, we show that they are able, by running a very\\nsimple rule of behavior, to ensure efficient covering of arbitrary connected\\ndomains, including non-planar and multidimensional ones. The novelty of our\\nalgorithm lies in the fact that, unlike previously proposed methods, our\\nalgorithm works on continuous domains without relying on some \"induced\"\\nunderlying graph, that effectively reduces the problem to a discrete case of\\ngraph covering. The algorithm guarantees complete coverage of any connected\\ndomain. We also prove that the algorithm is noise immune, i.e., it is able to\\ncope with any initial pheromone profile (noise). In addition the algorithm\\nprovides a bounded constant time between two successive visits of the robot,\\nand thus, is suitable for patrolling or surveillance applications.',\n",
       "    'author': {'name': 'Eliyahu Osherovich'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1206.4185v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1206.4185v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1209.4275v3',\n",
       "    'updated': '2012-10-02T08:44:59Z',\n",
       "    'published': '2012-09-19T15:15:08Z',\n",
       "    'title': 'Decision-Theoretic Coordination and Control for Active Multi-Camera\\n  Surveillance in Uncertain, Partially Observable Environments',\n",
       "    'summary': \"A central problem of surveillance is to monitor multiple targets moving in a\\nlarge-scale, obstacle-ridden environment with occlusions. This paper presents a\\nnovel principled Partially Observable Markov Decision Process-based approach to\\ncoordinating and controlling a network of active cameras for tracking and\\nobserving multiple mobile targets at high resolution in such surveillance\\nenvironments. Our proposed approach is capable of (a) maintaining a belief over\\nthe targets' states (i.e., locations, directions, and velocities) to track\\nthem, even when they may not be observed directly by the cameras at all times,\\n(b) coordinating the cameras' actions to simultaneously improve the belief over\\nthe targets' states and maximize the expected number of targets observed with a\\nguaranteed resolution, and (c) exploiting the inherent structure of our\\nsurveillance problem to improve its scalability (i.e., linear time) in the\\nnumber of targets to be observed. Quantitative comparisons with\\nstate-of-the-art multi-camera coordination and control techniques show that our\\napproach can achieve higher surveillance quality in real time. The practical\\nfeasibility of our approach is also demonstrated using real AXIS 214 PTZ\\ncameras\",\n",
       "    'author': [{'name': 'Prabhu Natarajan'},\n",
       "     {'name': 'Trong Nghia Hoang'},\n",
       "     {'name': 'Kian Hsiang Low'},\n",
       "     {'name': 'Mohan Kankanhalli'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '6th ACM/IEEE International Conference on Distributed Smart Cameras\\n  (ICDSC 2012), Extended version with proofs, 8 pages'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1209.4275v3',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1209.4275v3',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1302.2828v1',\n",
       "    'updated': '2013-02-12T15:47:43Z',\n",
       "    'published': '2013-02-12T15:47:43Z',\n",
       "    'title': 'Multi-agent RRT*: Sampling-based Cooperative Pathfinding (Extended\\n  Abstract)',\n",
       "    'summary': 'Cooperative pathfinding is a problem of finding a set of non-conflicting\\ntrajectories for a number of mobile agents. Its applications include planning\\nfor teams of mobile robots, such as autonomous aircrafts, cars, or underwater\\nvehicles. The state-of-the-art algorithms for cooperative pathfinding typically\\nrely on some heuristic forward-search pathfinding technique, where A* is often\\nthe algorithm of choice. Here, we propose MA-RRT*, a novel algorithm for\\nmulti-agent path planning that builds upon a recently proposed\\nasymptotically-optimal sampling-based algorithm for finding single-agent\\nshortest path called RRT*. We experimentally evaluate the performance of the\\nalgorithm and show that the sampling-based approach offers better scalability\\nthan the classical forward-search approach in relatively large, but sparse\\nenvironments, which are typical in real-world applications such as\\nmulti-aircraft collision avoidance.',\n",
       "    'author': [{'name': 'Michal Čáp'},\n",
       "     {'name': 'Peter Novák'},\n",
       "     {'name': 'Jiří Vokřínek'},\n",
       "     {'name': 'Michal Pěchouček'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'To appear at AAMAS 2013'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1302.2828v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1302.2828v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1305.2299v1',\n",
       "    'updated': '2013-05-10T10:06:17Z',\n",
       "    'published': '2013-05-10T10:06:17Z',\n",
       "    'title': 'Fast Collision Checking: From Single Robots to Multi-Robot Teams',\n",
       "    'summary': 'We examine three different algorithms that enable the collision certificate\\nmethod from [Bialkowski, et al.] to handle the case of a centralized\\nmulti-robot team. By taking advantage of symmetries in the configuration space\\nof multi-robot teams, our methods can significantly reduce the number of\\ncollision checks vs. both [Bialkowski, et al.] and standard collision checking\\nimplementations.',\n",
       "    'author': [{'name': 'Joshua Bialkowski'},\n",
       "     {'name': 'Michael Otte'},\n",
       "     {'name': 'Emilio Frazzoli'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1305.2299v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1305.2299v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1402.2871v1',\n",
       "    'updated': '2014-02-12T16:13:16Z',\n",
       "    'published': '2014-02-12T16:13:16Z',\n",
       "    'title': 'Planning for Decentralized Control of Multiple Robots Under Uncertainty',\n",
       "    'summary': 'We describe a probabilistic framework for synthesizing control policies for\\ngeneral multi-robot systems, given environment and sensor models and a cost\\nfunction. Decentralized, partially observable Markov decision processes\\n(Dec-POMDPs) are a general model of decision processes where a team of agents\\nmust cooperate to optimize some objective (specified by a shared reward or cost\\nfunction) in the presence of uncertainty, but where communication limitations\\nmean that the agents cannot share their state, so execution must proceed in a\\ndecentralized fashion. While Dec-POMDPs are typically intractable to solve for\\nreal-world problems, recent research on the use of macro-actions in Dec-POMDPs\\nhas significantly increased the size of problem that can be practically solved\\nas a Dec-POMDP. We describe this general model, and show how, in contrast to\\nmost existing methods that are specialized to a particular problem class, it\\ncan synthesize control policies that use whatever opportunities for\\ncoordination are present in the problem, while balancing off uncertainty in\\noutcomes, sensor information, and information about other agents. We use three\\nvariations on a warehouse task to show that a single planner of this type can\\ngenerate cooperative behavior using task allocation, direct communication, and\\nsignaling, as appropriate.',\n",
       "    'author': [{'name': 'Christopher Amato'},\n",
       "     {'name': 'George D. Konidaris'},\n",
       "     {'name': 'Gabriel Cruz'},\n",
       "     {'name': 'Christopher A. Maynor'},\n",
       "     {'name': 'Jonathan P. How'},\n",
       "     {'name': 'Leslie P. Kaelbling'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1402.2871v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1402.2871v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'I.2.9; I.2.11', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1402.4157v2',\n",
       "    'updated': '2014-05-12T15:38:42Z',\n",
       "    'published': '2014-02-17T21:46:26Z',\n",
       "    'title': 'Conservative collision prediction and avoidance for stochastic\\n  trajectories in continuous time and space',\n",
       "    'summary': 'Existing work in multi-agent collision prediction and avoidance typically\\nassumes discrete-time trajectories with Gaussian uncertainty or that are\\ncompletely deterministic. We propose an approach that allows detection of\\ncollisions even between continuous, stochastic trajectories with the only\\nrestriction that means and variances can be computed. To this end, we employ\\nprobabilistic bounds to derive criterion functions whose negative sign provably\\nis indicative of probable collisions. For criterion functions that are\\nLipschitz, an algorithm is provided to rapidly find negative values or prove\\ntheir absence. We propose an iterative policy-search approach that avoids prior\\ndiscretisations and yields collision-free trajectories with adjustably high\\ncertainty. We test our method with both fixed-priority and auction-based\\nprotocols for coordinating the iterative planning process. Results are provided\\nin collision-avoidance simulations of feedback controlled plants.',\n",
       "    'author': [{'name': 'Jan-Peter Calliess'},\n",
       "     {'name': 'Michael Osborne'},\n",
       "     {'name': 'Stephen Roberts'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'This preprint is an extended version of a conference paper that is to\\n  appear in \\\\textit{Proceedings of the 13th International Conference on\\n  Autonomous Agents and Multiagent Systems (AAMAS 2014)}'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1402.4157v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1402.4157v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1403.4023v2',\n",
       "    'updated': '2014-06-26T02:10:02Z',\n",
       "    'published': '2014-03-17T08:22:12Z',\n",
       "    'title': 'Simulation leagues: Analysis of competition formats',\n",
       "    'summary': 'The selection of an appropriate competition format is critical for both the\\nsuccess and credibility of any competition, both real and simulated. In this\\npaper, the automated parallelism offered by the RoboCupSoccer 2D simulation\\nleague is leveraged to conduct a 28,000 game round-robin between the top 8\\nteams from RoboCup 2012 and 2013. A proposed new competition format is found to\\nreduce variation from the resultant statistically significant team performance\\nrankings by 75% and 67%, when compared to the actual competition results from\\nRoboCup 2012 and 2013 respectively. These results are statistically validated\\nby generating 10,000 random tournaments for each of the three considered\\nformats and comparing the respective distributions of ranking discrepancy.',\n",
       "    'author': [{'name': 'David Budden'},\n",
       "     {'name': 'Peter Wang'},\n",
       "     {'name': 'Oliver Obst'},\n",
       "     {'name': 'Mikhail Prokopenko'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': '12 pages, 2 figures, presented at RoboCup 2014 symposium, Brazil'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1403.4023v2',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1403.4023v2',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1409.2399v1',\n",
       "    'updated': '2014-09-08T15:38:31Z',\n",
       "    'published': '2014-09-08T15:38:31Z',\n",
       "    'title': 'Prioritized Planning Algorithms for Trajectory Coordination of Multiple\\n  Mobile Robots',\n",
       "    'summary': 'An important capability of autonomous multi-robot systems is to prevent\\ncollision among the individual robots. One approach to this problem is to plan\\nconflict-free trajectories and let each of the robots follow its pre-planned\\ntrajectory. A widely used practical method for multi-robot trajectory planning\\nis prioritized planning, which has been shown to be effective in practice, but\\nis in general incomplete. Formal analysis of instances that are provably\\nsolvable by prioritized planning is still missing. Moreover, prioritized\\nplanning is a centralized algorithm, which may be in many situations\\nundesirable.\\n  In this paper we a) propose a revised version of prioritized planning and\\ncharacterize the class of instances that are provably solvable by the algorithm\\nand b) propose an asynchronous decentralized variant of prioritized planning,\\nwhich maintains the desirable properties of the centralized version and in the\\nsame time exploits the distributed computational power of the individual\\nrobots, which in most situations allows to find the joint trajectories faster.\\n  The experimental evaluation performed on real-world indoor maps shows that a)\\nthe revised version of prioritized planning reliably solves a wide class of\\ninstances on which both classical prioritized planning and popular reactive\\ntechnique ORCA fail and b) the asynchronous decentralized algorithm provides\\nsolution faster than the previously proposed synchronized decentralized\\nalgorithm.',\n",
       "    'author': [{'name': 'Michal Čáp'},\n",
       "     {'name': 'Peter Novák'},\n",
       "     {'name': 'Alexander Kleiner'},\n",
       "     {'name': 'Martin Selecký'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1409.2399v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1409.2399v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1502.06030v1',\n",
       "    'updated': '2015-02-20T22:56:00Z',\n",
       "    'published': '2015-02-20T22:56:00Z',\n",
       "    'title': 'Decentralized Control of Partially Observable Markov Decision Processes\\n  using Belief Space Macro-actions',\n",
       "    'summary': \"The focus of this paper is on solving multi-robot planning problems in\\ncontinuous spaces with partial observability. Decentralized partially\\nobservable Markov decision processes (Dec-POMDPs) are general models for\\nmulti-robot coordination problems, but representing and solving Dec-POMDPs is\\noften intractable for large problems. To allow for a high-level representation\\nthat is natural for multi-robot problems and scalable to large discrete and\\ncontinuous problems, this paper extends the Dec-POMDP model to the\\ndecentralized partially observable semi-Markov decision process (Dec-POSMDP).\\nThe Dec-POSMDP formulation allows asynchronous decision-making by the robots,\\nwhich is crucial in multi-robot domains. We also present an algorithm for\\nsolving this Dec-POSMDP which is much more scalable than previous methods since\\nit can incorporate closed-loop belief space macro-actions in planning. These\\nmacro-actions are automatically constructed to produce robust solutions. The\\nproposed method's performance is evaluated on a complex multi-robot package\\ndelivery problem under uncertainty, showing that our approach can naturally\\nrepresent multi-robot problems and provide high-quality solutions for\\nlarge-scale problems.\",\n",
       "    'author': [{'name': 'Shayegan Omidshafiei'},\n",
       "     {'name': 'Ali-akbar Agha-mohammadi'},\n",
       "     {'name': 'Christopher Amato'},\n",
       "     {'name': 'Jonathan P. How'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1502.06030v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1502.06030v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1503.00237v1',\n",
       "    'updated': '2015-03-01T08:33:28Z',\n",
       "    'published': '2015-03-01T08:33:28Z',\n",
       "    'title': 'Task Allocation in Robotic Swarms: Explicit Communication Based\\n  Approaches',\n",
       "    'summary': 'In this paper we study multi robot cooperative task allocation issue in a\\nsituation where a swarm of robots is deployed in a confined unknown environment\\nwhere the number of colored spots which represent tasks and the ratios of them\\nare unknown. The robots should cover this spots as far as possible to do\\ncleaning and sampling actions desirably. It means that they should discover the\\nspots cooperatively and spread proportional to the spots area and avoid from\\nremaining idle. We proposed 4 self-organized distributed methods which are\\ncalled hybrid methods for coping with this scenario. In two different\\nexperiments the performance of the methods is analyzed. We compared them with\\neach other and investigated their scalability and robustness in term of single\\npoint of failure.',\n",
       "    'author': [{'name': 'Aryo Jamshidpey'}, {'name': 'Mohsen Afsharchi'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'A short version of this paper is accepted by AI2015(conference). It\\n  has 13 pages and 4 figures'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1503.00237v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1503.00237v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.MA',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.MA',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1604.05942v1',\n",
       "    'updated': '2016-04-20T13:12:45Z',\n",
       "    'published': '2016-04-20T13:12:45Z',\n",
       "    'title': 'Multiplayer Games for Learning Multirobot Coordination Algorithms',\n",
       "    'summary': 'Humans have an impressive ability to solve complex coordination problems in a\\nfully distributed manner. This ability, if learned as a set of distributed\\nmultirobot coordination strategies, can enable programming large groups of\\nrobots to collaborate towards complex coordination objectives in a way similar\\nto humans. Such strategies would offer robustness, adaptability,\\nfault-tolerance, and, importantly, distributed decision-making. To that end, we\\nhave designed a networked gaming platform to investigate human group behavior,\\nspecifically in solving complex collaborative coordinated tasks. Through this\\nplatform, we are able to limit the communication, sensing, and actuation\\ncapabilities provided to the players. With the aim of learning coordination\\nalgorithms for robots in mind, we define these capabilities to mimic those of a\\nsimple ground robot.',\n",
       "    'author': [{'name': 'Arash Tavakoli'},\n",
       "     {'name': 'Haig Nalbandian'},\n",
       "     {'name': 'Nora Ayanian'}],\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1604.05942v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1604.05942v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.RO',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.RO',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.AI', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'}]},\n",
       "   {'id': 'http://arxiv.org/abs/1606.01380v1',\n",
       "    'updated': '2016-06-04T14:12:32Z',\n",
       "    'published': '2016-06-04T14:12:32Z',\n",
       "    'title': 'Effective Multi-Robot Spatial Task Allocation using Model Approximations',\n",
       "    'summary': 'Real-world multi-agent planning problems cannot be solved using\\ndecision-theoretic planning methods due to the exponential complexity. We\\napproximate firefighting in rescue simulation as a spatially distributed task\\nand model with multi-agent Markov decision process. We use recent approximation\\nmethods for spatial task problems to reduce the model complexity. Our\\napproximations are single-agent, static task, shortest path pruning, dynamic\\nplanning horizon, and task clustering. We create scenarios from RoboCup Rescue\\nSimulation maps and evaluate our methods on these graph worlds. The results\\nshow that our approach is faster and better than comparable methods and has\\nnegligible performance loss compared to the optimal policy. We also show that\\nour method has a similar performance as DCOP methods on example RCRS scenarios.',\n",
       "    'author': [{'name': 'Okan Aşık'}, {'name': 'H. Levent Akın'}],\n",
       "    'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '#text': 'RoboCup 2016 Symposium'},\n",
       "    'link': [{'@href': 'http://arxiv.org/abs/1606.01380v1',\n",
       "      '@rel': 'alternate',\n",
       "      '@type': 'text/html'},\n",
       "     {'@title': 'pdf',\n",
       "      '@href': 'http://arxiv.org/pdf/1606.01380v1',\n",
       "      '@rel': 'related',\n",
       "      '@type': 'application/pdf'}],\n",
       "    'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom',\n",
       "     '@term': 'cs.AI',\n",
       "     '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "    'category': [{'@term': 'cs.AI',\n",
       "      '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.MA', '@scheme': 'http://arxiv.org/schemas/atom'},\n",
       "     {'@term': 'cs.RO', '@scheme': 'http://arxiv.org/schemas/atom'}]}]}}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "11c8d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>updated_ts</th>\n",
       "      <th>published_ts</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/1709.06620v1</td>\n",
       "      <td>2017-09-19T19:26:20Z</td>\n",
       "      <td>2017-09-19T19:26:20Z</td>\n",
       "      <td>Learning of Coordination Policies for Robotic ...</td>\n",
       "      <td>Inspired by biological swarms, robotic swarms ...</td>\n",
       "      <td>Qiyang Li, Xintong Du, Yizhou Huang, Quinlan S...</td>\n",
       "      <td>cs.RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2011.05605v2</td>\n",
       "      <td>2020-11-20T18:19:32Z</td>\n",
       "      <td>2020-11-11T07:35:21Z</td>\n",
       "      <td>Decentralized Motion Planning for Multi-Robot ...</td>\n",
       "      <td>This work presents a decentralized motion plan...</td>\n",
       "      <td>Sivanathan Kandhasamy, Vinayagam Babu Kuppusam...</td>\n",
       "      <td>cs.RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2209.14745v2</td>\n",
       "      <td>2022-12-29T08:48:05Z</td>\n",
       "      <td>2022-09-29T13:02:58Z</td>\n",
       "      <td>A Multiagent Framework for the Asynchronous an...</td>\n",
       "      <td>The traditional ML development methodology doe...</td>\n",
       "      <td>Andrea Gesmundo</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2011.02608v1</td>\n",
       "      <td>2020-11-05T01:47:23Z</td>\n",
       "      <td>2020-11-05T01:47:23Z</td>\n",
       "      <td>Learning a Decentralized Multi-arm Motion Planner</td>\n",
       "      <td>We present a closed-loop multi-arm motion plan...</td>\n",
       "      <td>Huy Ha, Jingxi Xu, Shuran Song</td>\n",
       "      <td>cs.RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2012.05894v1</td>\n",
       "      <td>2020-12-10T18:55:51Z</td>\n",
       "      <td>2020-12-10T18:55:51Z</td>\n",
       "      <td>AutoSelect: Automatic and Dynamic Detection Se...</td>\n",
       "      <td>3D multi-object tracking is an important compo...</td>\n",
       "      <td>Xinshuo Weng, Kris Kitani</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>http://arxiv.org/abs/1409.2399v1</td>\n",
       "      <td>2014-09-08T15:38:31Z</td>\n",
       "      <td>2014-09-08T15:38:31Z</td>\n",
       "      <td>Prioritized Planning Algorithms for Trajectory...</td>\n",
       "      <td>An important capability of autonomous multi-ro...</td>\n",
       "      <td>Michal Čáp, Peter Novák, Alexander Kleiner, Ma...</td>\n",
       "      <td>cs.RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>http://arxiv.org/abs/1502.06030v1</td>\n",
       "      <td>2015-02-20T22:56:00Z</td>\n",
       "      <td>2015-02-20T22:56:00Z</td>\n",
       "      <td>Decentralized Control of Partially Observable ...</td>\n",
       "      <td>The focus of this paper is on solving multi-ro...</td>\n",
       "      <td>Shayegan Omidshafiei, Ali-akbar Agha-mohammadi...</td>\n",
       "      <td>cs.MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>http://arxiv.org/abs/1503.00237v1</td>\n",
       "      <td>2015-03-01T08:33:28Z</td>\n",
       "      <td>2015-03-01T08:33:28Z</td>\n",
       "      <td>Task Allocation in Robotic Swarms: Explicit Co...</td>\n",
       "      <td>In this paper we study multi robot cooperative...</td>\n",
       "      <td>Aryo Jamshidpey, Mohsen Afsharchi</td>\n",
       "      <td>cs.MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>http://arxiv.org/abs/1604.05942v1</td>\n",
       "      <td>2016-04-20T13:12:45Z</td>\n",
       "      <td>2016-04-20T13:12:45Z</td>\n",
       "      <td>Multiplayer Games for Learning Multirobot Coor...</td>\n",
       "      <td>Humans have an impressive ability to solve com...</td>\n",
       "      <td>Arash Tavakoli, Haig Nalbandian, Nora Ayanian</td>\n",
       "      <td>cs.RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>http://arxiv.org/abs/1606.01380v1</td>\n",
       "      <td>2016-06-04T14:12:32Z</td>\n",
       "      <td>2016-06-04T14:12:32Z</td>\n",
       "      <td>Effective Multi-Robot Spatial Task Allocation ...</td>\n",
       "      <td>Real-world multi-agent planning problems canno...</td>\n",
       "      <td>Okan Aşık, H. Levent Akın</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  link            updated_ts  \\\n",
       "0    http://arxiv.org/abs/1709.06620v1  2017-09-19T19:26:20Z   \n",
       "1    http://arxiv.org/abs/2011.05605v2  2020-11-20T18:19:32Z   \n",
       "2    http://arxiv.org/abs/2209.14745v2  2022-12-29T08:48:05Z   \n",
       "3    http://arxiv.org/abs/2011.02608v1  2020-11-05T01:47:23Z   \n",
       "4    http://arxiv.org/abs/2012.05894v1  2020-12-10T18:55:51Z   \n",
       "..                                 ...                   ...   \n",
       "995   http://arxiv.org/abs/1409.2399v1  2014-09-08T15:38:31Z   \n",
       "996  http://arxiv.org/abs/1502.06030v1  2015-02-20T22:56:00Z   \n",
       "997  http://arxiv.org/abs/1503.00237v1  2015-03-01T08:33:28Z   \n",
       "998  http://arxiv.org/abs/1604.05942v1  2016-04-20T13:12:45Z   \n",
       "999  http://arxiv.org/abs/1606.01380v1  2016-06-04T14:12:32Z   \n",
       "\n",
       "             published_ts                                              title  \\\n",
       "0    2017-09-19T19:26:20Z  Learning of Coordination Policies for Robotic ...   \n",
       "1    2020-11-11T07:35:21Z  Decentralized Motion Planning for Multi-Robot ...   \n",
       "2    2022-09-29T13:02:58Z  A Multiagent Framework for the Asynchronous an...   \n",
       "3    2020-11-05T01:47:23Z  Learning a Decentralized Multi-arm Motion Planner   \n",
       "4    2020-12-10T18:55:51Z  AutoSelect: Automatic and Dynamic Detection Se...   \n",
       "..                    ...                                                ...   \n",
       "995  2014-09-08T15:38:31Z  Prioritized Planning Algorithms for Trajectory...   \n",
       "996  2015-02-20T22:56:00Z  Decentralized Control of Partially Observable ...   \n",
       "997  2015-03-01T08:33:28Z  Task Allocation in Robotic Swarms: Explicit Co...   \n",
       "998  2016-04-20T13:12:45Z  Multiplayer Games for Learning Multirobot Coor...   \n",
       "999  2016-06-04T14:12:32Z  Effective Multi-Robot Spatial Task Allocation ...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    Inspired by biological swarms, robotic swarms ...   \n",
       "1    This work presents a decentralized motion plan...   \n",
       "2    The traditional ML development methodology doe...   \n",
       "3    We present a closed-loop multi-arm motion plan...   \n",
       "4    3D multi-object tracking is an important compo...   \n",
       "..                                                 ...   \n",
       "995  An important capability of autonomous multi-ro...   \n",
       "996  The focus of this paper is on solving multi-ro...   \n",
       "997  In this paper we study multi robot cooperative...   \n",
       "998  Humans have an impressive ability to solve com...   \n",
       "999  Real-world multi-agent planning problems canno...   \n",
       "\n",
       "                                                author category  \n",
       "0    Qiyang Li, Xintong Du, Yizhou Huang, Quinlan S...    cs.RO  \n",
       "1    Sivanathan Kandhasamy, Vinayagam Babu Kuppusam...    cs.RO  \n",
       "2                                      Andrea Gesmundo    cs.LG  \n",
       "3                       Huy Ha, Jingxi Xu, Shuran Song    cs.RO  \n",
       "4                            Xinshuo Weng, Kris Kitani    cs.CV  \n",
       "..                                                 ...      ...  \n",
       "995  Michal Čáp, Peter Novák, Alexander Kleiner, Ma...    cs.RO  \n",
       "996  Shayegan Omidshafiei, Ali-akbar Agha-mohammadi...    cs.MA  \n",
       "997                  Aryo Jamshidpey, Mohsen Afsharchi    cs.MA  \n",
       "998      Arash Tavakoli, Haig Nalbandian, Nora Ayanian    cs.RO  \n",
       "999                          Okan Aşık, H. Levent Akın    cs.AI  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219cd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:arxiv-scout]",
   "language": "python",
   "name": "conda-env-arxiv-scout-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
