{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a8bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fd5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure our src module can be found and imported\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import cohere\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data.arxiv_downloader import ArxivDownloader\n",
    "from src.models.cohere import CohereModel\n",
    "from src.utils import (\n",
    "    compute_top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e740eba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJ_DIR = Path.cwd().parent\n",
    "DOTENV_PATH = PROJ_DIR / '.env'\n",
    "dotenv.load_dotenv(DOTENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9da8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted Arxiv downloading logic into ArxivDownloader class that has simple caching functionality\n",
    "downloader = ArxivDownloader(download_refresh_interval_days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb8cd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 11.2 ms, total: 176 ms\n",
      "Wall time: 5.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles_df = downloader.retrieve_arxiv_articles_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a8783a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>updated_ts</th>\n",
       "      <th>published_ts</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/1709.06620v1</td>\n",
       "      <td>2017-09-19T19:26:20Z</td>\n",
       "      <td>2017-09-19T19:26:20Z</td>\n",
       "      <td>Learning of Coordination Policies for Robotic ...</td>\n",
       "      <td>Inspired by biological swarms, robotic swarms ...</td>\n",
       "      <td>Qiyang Li, Xintong Du, Yizhou Huang, Quinlan S...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>learning of coordination policies for robotic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2011.05605v2</td>\n",
       "      <td>2020-11-20T18:19:32Z</td>\n",
       "      <td>2020-11-11T07:35:21Z</td>\n",
       "      <td>Decentralized Motion Planning for Multi-Robot ...</td>\n",
       "      <td>This work presents a decentralized motion plan...</td>\n",
       "      <td>Sivanathan Kandhasamy, Vinayagam Babu Kuppusam...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>decentralized motion planning for multi-robot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2209.14745v2</td>\n",
       "      <td>2022-12-29T08:48:05Z</td>\n",
       "      <td>2022-09-29T13:02:58Z</td>\n",
       "      <td>A Multiagent Framework for the Asynchronous an...</td>\n",
       "      <td>The traditional ML development methodology doe...</td>\n",
       "      <td>Andrea Gesmundo</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>a multiagent framework for the asynchronous an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2003.08376v3</td>\n",
       "      <td>2020-11-07T02:48:22Z</td>\n",
       "      <td>2020-03-18T17:54:28Z</td>\n",
       "      <td>Inverting the Pose Forecasting Pipeline with S...</td>\n",
       "      <td>Many autonomous systems forecast aspects of th...</td>\n",
       "      <td>Xinshuo Weng, Jianren Wang, Sergey Levine, Kri...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>inverting the pose forecasting pipeline with s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2008.12760v1</td>\n",
       "      <td>2020-08-28T17:35:22Z</td>\n",
       "      <td>2020-08-28T17:35:22Z</td>\n",
       "      <td>AllenAct: A Framework for Embodied AI Research</td>\n",
       "      <td>The domain of Embodied AI, in which agents lea...</td>\n",
       "      <td>Luca Weihs, Jordi Salvador, Klemen Kotar, Unna...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>allenact: a framework for embodied ai research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>http://arxiv.org/abs/1912.07521v3</td>\n",
       "      <td>2022-03-31T21:56:45Z</td>\n",
       "      <td>2019-12-12T03:48:40Z</td>\n",
       "      <td>Exploration and Coordination of Complementary ...</td>\n",
       "      <td>The hunter and gatherer approach copes with th...</td>\n",
       "      <td>Mehdi Dadvar, Saeed Moazami, Harley R. Myler, ...</td>\n",
       "      <td>cs.MA</td>\n",
       "      <td>exploration and coordination of complementary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>http://arxiv.org/abs/2001.05994v2</td>\n",
       "      <td>2020-10-07T20:41:11Z</td>\n",
       "      <td>2020-01-16T18:51:42Z</td>\n",
       "      <td>Adversarially Guided Self-Play for Adopting So...</td>\n",
       "      <td>Robotic agents must adopt existing social conv...</td>\n",
       "      <td>Mycal Tucker, Yilun Zhou, Julie Shah</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>adversarially guided self-play for adopting so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>http://arxiv.org/abs/2002.06417v1</td>\n",
       "      <td>2020-02-15T17:36:00Z</td>\n",
       "      <td>2020-02-15T17:36:00Z</td>\n",
       "      <td>Designing Interaction for Multi-agent Cooperat...</td>\n",
       "      <td>Future intelligent system will involve very va...</td>\n",
       "      <td>Chao Wang, Stephan Hasler, Manuel Muehlig, Fra...</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>designing interaction for multi-agent cooperat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>http://arxiv.org/abs/2004.03053v3</td>\n",
       "      <td>2022-11-14T03:27:14Z</td>\n",
       "      <td>2020-04-07T00:34:36Z</td>\n",
       "      <td>Scenario-Transferable Semantic Graph Reasoning...</td>\n",
       "      <td>Accurately predicting the possible behaviors o...</td>\n",
       "      <td>Yeping Hu, Wei Zhan, Masayoshi Tomizuka</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>scenario-transferable semantic graph reasoning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>http://arxiv.org/abs/2004.12481v1</td>\n",
       "      <td>2020-04-26T21:06:20Z</td>\n",
       "      <td>2020-04-26T21:06:20Z</td>\n",
       "      <td>GymFG: A Framework with a Gym Interface for Fl...</td>\n",
       "      <td>Over the past decades, progress in deployable ...</td>\n",
       "      <td>Andrew Wood, Ali Sydney, Peter Chin, Bishal Th...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>gymfg: a framework with a gym interface for fl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  link            updated_ts  \\\n",
       "0    http://arxiv.org/abs/1709.06620v1  2017-09-19T19:26:20Z   \n",
       "1    http://arxiv.org/abs/2011.05605v2  2020-11-20T18:19:32Z   \n",
       "2    http://arxiv.org/abs/2209.14745v2  2022-12-29T08:48:05Z   \n",
       "3    http://arxiv.org/abs/2003.08376v3  2020-11-07T02:48:22Z   \n",
       "4    http://arxiv.org/abs/2008.12760v1  2020-08-28T17:35:22Z   \n",
       "..                                 ...                   ...   \n",
       "995  http://arxiv.org/abs/1912.07521v3  2022-03-31T21:56:45Z   \n",
       "996  http://arxiv.org/abs/2001.05994v2  2020-10-07T20:41:11Z   \n",
       "997  http://arxiv.org/abs/2002.06417v1  2020-02-15T17:36:00Z   \n",
       "998  http://arxiv.org/abs/2004.03053v3  2022-11-14T03:27:14Z   \n",
       "999  http://arxiv.org/abs/2004.12481v1  2020-04-26T21:06:20Z   \n",
       "\n",
       "             published_ts                                              title  \\\n",
       "0    2017-09-19T19:26:20Z  Learning of Coordination Policies for Robotic ...   \n",
       "1    2020-11-11T07:35:21Z  Decentralized Motion Planning for Multi-Robot ...   \n",
       "2    2022-09-29T13:02:58Z  A Multiagent Framework for the Asynchronous an...   \n",
       "3    2020-03-18T17:54:28Z  Inverting the Pose Forecasting Pipeline with S...   \n",
       "4    2020-08-28T17:35:22Z     AllenAct: A Framework for Embodied AI Research   \n",
       "..                    ...                                                ...   \n",
       "995  2019-12-12T03:48:40Z  Exploration and Coordination of Complementary ...   \n",
       "996  2020-01-16T18:51:42Z  Adversarially Guided Self-Play for Adopting So...   \n",
       "997  2020-02-15T17:36:00Z  Designing Interaction for Multi-agent Cooperat...   \n",
       "998  2020-04-07T00:34:36Z  Scenario-Transferable Semantic Graph Reasoning...   \n",
       "999  2020-04-26T21:06:20Z  GymFG: A Framework with a Gym Interface for Fl...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    Inspired by biological swarms, robotic swarms ...   \n",
       "1    This work presents a decentralized motion plan...   \n",
       "2    The traditional ML development methodology doe...   \n",
       "3    Many autonomous systems forecast aspects of th...   \n",
       "4    The domain of Embodied AI, in which agents lea...   \n",
       "..                                                 ...   \n",
       "995  The hunter and gatherer approach copes with th...   \n",
       "996  Robotic agents must adopt existing social conv...   \n",
       "997  Future intelligent system will involve very va...   \n",
       "998  Accurately predicting the possible behaviors o...   \n",
       "999  Over the past decades, progress in deployable ...   \n",
       "\n",
       "                                                author category  \\\n",
       "0    Qiyang Li, Xintong Du, Yizhou Huang, Quinlan S...    cs.RO   \n",
       "1    Sivanathan Kandhasamy, Vinayagam Babu Kuppusam...    cs.RO   \n",
       "2                                      Andrea Gesmundo    cs.LG   \n",
       "3    Xinshuo Weng, Jianren Wang, Sergey Levine, Kri...    cs.CV   \n",
       "4    Luca Weihs, Jordi Salvador, Klemen Kotar, Unna...    cs.CV   \n",
       "..                                                 ...      ...   \n",
       "995  Mehdi Dadvar, Saeed Moazami, Harley R. Myler, ...    cs.MA   \n",
       "996               Mycal Tucker, Yilun Zhou, Julie Shah    cs.AI   \n",
       "997  Chao Wang, Stephan Hasler, Manuel Muehlig, Fra...    cs.HC   \n",
       "998            Yeping Hu, Wei Zhan, Masayoshi Tomizuka    cs.RO   \n",
       "999  Andrew Wood, Ali Sydney, Peter Chin, Bishal Th...    cs.AI   \n",
       "\n",
       "                                         combined_text  \n",
       "0    learning of coordination policies for robotic ...  \n",
       "1    decentralized motion planning for multi-robot ...  \n",
       "2    a multiagent framework for the asynchronous an...  \n",
       "3    inverting the pose forecasting pipeline with s...  \n",
       "4    allenact: a framework for embodied ai research...  \n",
       "..                                                 ...  \n",
       "995  exploration and coordination of complementary ...  \n",
       "996  adversarially guided self-play for adopting so...  \n",
       "997  designing interaction for multi-agent cooperat...  \n",
       "998  scenario-transferable semantic graph reasoning...  \n",
       "999  gymfg: a framework with a gym interface for fl...  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d4184",
   "metadata": {},
   "source": [
    "## Get embeddings from Cohere API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "210231d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning of coordination policies for robotic swarms. inspired by biological swarms, robotic swarms are envisioned to solve real-world problems that are difficult for individual agents. biological swarms can achieve collective intelligence based on local interactions and simple rules; however, designing effective distributed policies for large-scale robotic swarms to achieve a global objective can be challenging. although it is often possible to design an optimal centralized strategy for smaller numbers of agents, those methods can fail as the number of agents increases. motivated by the growing success of machine learning, we develop a deep learning approach that learns distributed coordination policies from centralized policies. in contrast to traditional distributed control approaches, which are usually based on human-designed policies for relatively simple tasks, this learning-based approach can be adapted to more difficult tasks. we demonstrate the efficacy of our proposed approach on two different tasks, the well-known rendezvous problem and a more difficult particle assignment problem. for the latter, no known distributed policy exists. from extensive simulations, it is shown that the performance of the learned coordination policies is comparable to the centralized policies, surpassing state-of-the-art distributed policies. thereby, our proposed approach provides a promising alternative for real-world coordination problems that would be otherwise computationally expensive to solve or intangible to explore.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df['combined_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d6bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted Cohere API logic into CohereModel class\n",
    "cohere_model = CohereModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77bbb0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 532 ms, sys: 116 ms, total: 649 ms\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"speech to text whisper wav2vec\"\n",
    "\n",
    "res_embeddings = cohere_model.get_embeddings(texts=[query]+list(articles_df['combined_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "602136b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>updated_ts</th>\n",
       "      <th>published_ts</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>http://arxiv.org/abs/2106.04283v1</td>\n",
       "      <td>2021-06-08T12:22:29Z</td>\n",
       "      <td>2021-06-08T12:22:29Z</td>\n",
       "      <td>NWT: Towards natural audio-to-video generation...</td>\n",
       "      <td>In this work we introduce NWT, an expressive s...</td>\n",
       "      <td>Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, ...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>nwt: towards natural audio-to-video generation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>http://arxiv.org/abs/2106.09296v3</td>\n",
       "      <td>2022-01-14T16:43:19Z</td>\n",
       "      <td>2021-06-17T07:59:15Z</td>\n",
       "      <td>Voice2Series: Reprogramming Acoustic Models fo...</td>\n",
       "      <td>Learning to classify time series with limited ...</td>\n",
       "      <td>Chao-Han Huck Yang, Yun-Yun Tsai, Pin-Yu Chen</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>voice2series: reprogramming acoustic models fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>http://arxiv.org/abs/2202.08509v1</td>\n",
       "      <td>2022-02-17T08:26:25Z</td>\n",
       "      <td>2022-02-17T08:26:25Z</td>\n",
       "      <td>A Study of Designing Compact Audio-Visual Wake...</td>\n",
       "      <td>Audio-only-based wake word spotting (WWS) is c...</td>\n",
       "      <td>Hengshun Zhou, Jun Du, Chao-Han Huck Yang, Shi...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>a study of designing compact audio-visual wake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>http://arxiv.org/abs/1910.10942v2</td>\n",
       "      <td>2020-02-10T09:36:23Z</td>\n",
       "      <td>2019-10-24T06:54:36Z</td>\n",
       "      <td>A Recurrent Variational Autoencoder for Speech...</td>\n",
       "      <td>This paper presents a generative approach to s...</td>\n",
       "      <td>Simon Leglaive, Xavier Alameda-Pineda, Laurent...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>a recurrent variational autoencoder for speech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>http://arxiv.org/abs/2110.08791v1</td>\n",
       "      <td>2021-10-17T11:14:00Z</td>\n",
       "      <td>2021-10-17T11:14:00Z</td>\n",
       "      <td>Taming Visually Guided Sound Generation</td>\n",
       "      <td>Recent advances in visually-induced audio gene...</td>\n",
       "      <td>Vladimir Iashin, Esa Rahtu</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>taming visually guided sound generation. recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>http://arxiv.org/abs/2301.07851v1</td>\n",
       "      <td>2023-01-19T02:37:56Z</td>\n",
       "      <td>2023-01-19T02:37:56Z</td>\n",
       "      <td>From English to More Languages: Parameter-Effi...</td>\n",
       "      <td>In this work, we propose a new parameter-effic...</td>\n",
       "      <td>Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Ch...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>from english to more languages: parameter-effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>http://arxiv.org/abs/2202.08532v1</td>\n",
       "      <td>2022-02-17T09:17:58Z</td>\n",
       "      <td>2022-02-17T09:17:58Z</td>\n",
       "      <td>Mitigating Closed-model Adversarial Examples w...</td>\n",
       "      <td>In this work, we aim to enhance the system rob...</td>\n",
       "      <td>Chao-Han Huck Yang, Zeeshan Ahmed, Yile Gu, Jo...</td>\n",
       "      <td>eess.AS</td>\n",
       "      <td>mitigating closed-model adversarial examples w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>http://arxiv.org/abs/2211.01839v1</td>\n",
       "      <td>2022-11-03T14:20:32Z</td>\n",
       "      <td>2022-11-03T14:20:32Z</td>\n",
       "      <td>HyperSound: Generating Implicit Neural Represe...</td>\n",
       "      <td>Implicit neural representations (INRs) are a r...</td>\n",
       "      <td>Filip Szatkowski, Karol J. Piczak, Przemysław ...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>hypersound: generating implicit neural represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>http://arxiv.org/abs/2206.02211v3</td>\n",
       "      <td>2022-12-04T08:24:02Z</td>\n",
       "      <td>2022-06-05T16:18:27Z</td>\n",
       "      <td>Variable-rate hierarchical CPC leads to acoust...</td>\n",
       "      <td>The success of deep learning comes from its ab...</td>\n",
       "      <td>Santiago Cuervo, Adrian Łańcucki, Ricard Marxe...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>variable-rate hierarchical cpc leads to acoust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>http://arxiv.org/abs/2206.00393v1</td>\n",
       "      <td>2022-06-01T11:00:07Z</td>\n",
       "      <td>2022-06-01T11:00:07Z</td>\n",
       "      <td>Towards Generalisable Audio Representations fo...</td>\n",
       "      <td>In audio-visual navigation (AVN), an intellige...</td>\n",
       "      <td>Shunqi Mao, Chaoyi Zhang, Heng Wang, Weidong Cai</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>towards generalisable audio representations fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>http://arxiv.org/abs/2109.02096v2</td>\n",
       "      <td>2021-10-10T16:22:00Z</td>\n",
       "      <td>2021-09-05T15:06:53Z</td>\n",
       "      <td>Timbre Transfer with Variational Auto Encoding...</td>\n",
       "      <td>This research project investigates the applica...</td>\n",
       "      <td>Russell Sammut Bonnici, Charalampos Saitis, Ma...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>timbre transfer with variational auto encoding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>http://arxiv.org/abs/1902.03389v1</td>\n",
       "      <td>2019-02-09T07:49:42Z</td>\n",
       "      <td>2019-02-09T07:49:42Z</td>\n",
       "      <td>Generative Moment Matching Network-based Rando...</td>\n",
       "      <td>This paper proposes a generative moment matchi...</td>\n",
       "      <td>Hiroki Tamaru, Yuki Saito, Shinnosuke Takamich...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>generative moment matching network-based rando...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>http://arxiv.org/abs/1805.11797v2</td>\n",
       "      <td>2018-05-31T03:49:25Z</td>\n",
       "      <td>2018-05-30T04:15:58Z</td>\n",
       "      <td>Grow and Prune Compact, Fast, and Accurate LSTMs</td>\n",
       "      <td>Long short-term memory (LSTM) has been widely ...</td>\n",
       "      <td>Xiaoliang Dai, Hongxu Yin, Niraj K. Jha</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>grow and prune compact, fast, and accurate lst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>http://arxiv.org/abs/2003.00418v1</td>\n",
       "      <td>2020-03-01T06:42:43Z</td>\n",
       "      <td>2020-03-01T06:42:43Z</td>\n",
       "      <td>Towards Automatic Face-to-Face Translation</td>\n",
       "      <td>In light of the recent breakthroughs in automa...</td>\n",
       "      <td>Prajwal K R, Rudrabha Mukhopadhyay, Jerin Phil...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>towards automatic face-to-face translation. in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>http://arxiv.org/abs/1608.04363v2</td>\n",
       "      <td>2016-11-28T17:48:04Z</td>\n",
       "      <td>2016-08-15T18:57:10Z</td>\n",
       "      <td>Deep Convolutional Neural Networks and Data Au...</td>\n",
       "      <td>The ability of deep convolutional neural netwo...</td>\n",
       "      <td>Justin Salamon, Juan Pablo Bello</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>deep convolutional neural networks and data au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>http://arxiv.org/abs/2110.03894v3</td>\n",
       "      <td>2022-12-20T12:44:24Z</td>\n",
       "      <td>2021-10-08T05:07:35Z</td>\n",
       "      <td>Neural Model Reprogramming with Similarity Bas...</td>\n",
       "      <td>In this study, we propose a novel adversarial ...</td>\n",
       "      <td>Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu...</td>\n",
       "      <td>eess.AS</td>\n",
       "      <td>neural model reprogramming with similarity bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>http://arxiv.org/abs/2105.00173v2</td>\n",
       "      <td>2021-07-04T07:34:14Z</td>\n",
       "      <td>2021-05-01T05:47:15Z</td>\n",
       "      <td>Emotion Recognition of the Singing Voice: Towa...</td>\n",
       "      <td>Current computational-emotion research has foc...</td>\n",
       "      <td>Daniel Szelogowski</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>emotion recognition of the singing voice: towa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>http://arxiv.org/abs/2209.12573v3</td>\n",
       "      <td>2022-11-16T09:56:26Z</td>\n",
       "      <td>2022-09-26T10:38:39Z</td>\n",
       "      <td>Digital Audio Forensics: Blind Human Voice Mim...</td>\n",
       "      <td>Audio is one of the most used way of human com...</td>\n",
       "      <td>Sahar Al Ajmi, Khizar Hayat, Alaa M. Al Obaidi...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>digital audio forensics: blind human voice mim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>http://arxiv.org/abs/2104.00528v2</td>\n",
       "      <td>2021-04-19T03:25:50Z</td>\n",
       "      <td>2021-03-31T04:09:30Z</td>\n",
       "      <td>OutlierNets: Highly Compact Deep Autoencoder N...</td>\n",
       "      <td>Human operators often diagnose industrial mach...</td>\n",
       "      <td>Saad Abbasi, Mahmoud Famouri, Mohammad Javad S...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>outliernets: highly compact deep autoencoder n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>http://arxiv.org/abs/1906.04232v1</td>\n",
       "      <td>2019-06-10T19:04:09Z</td>\n",
       "      <td>2019-06-10T19:04:09Z</td>\n",
       "      <td>BowNet: Dilated Convolution Neural Network for...</td>\n",
       "      <td>Ultrasound imaging is safe, relatively afforda...</td>\n",
       "      <td>M. Hamed Mozaffari, Won-Sook Lee</td>\n",
       "      <td>eess.IV</td>\n",
       "      <td>bownet: dilated convolution neural network for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  link            updated_ts  \\\n",
       "814  http://arxiv.org/abs/2106.04283v1  2021-06-08T12:22:29Z   \n",
       "216  http://arxiv.org/abs/2106.09296v3  2022-01-14T16:43:19Z   \n",
       "810  http://arxiv.org/abs/2202.08509v1  2022-02-17T08:26:25Z   \n",
       "212  http://arxiv.org/abs/1910.10942v2  2020-02-10T09:36:23Z   \n",
       "817  http://arxiv.org/abs/2110.08791v1  2021-10-17T11:14:00Z   \n",
       "223  http://arxiv.org/abs/2301.07851v1  2023-01-19T02:37:56Z   \n",
       "219  http://arxiv.org/abs/2202.08532v1  2022-02-17T09:17:58Z   \n",
       "222  http://arxiv.org/abs/2211.01839v1  2022-11-03T14:20:32Z   \n",
       "429  http://arxiv.org/abs/2206.02211v3  2022-12-04T08:24:02Z   \n",
       "412  http://arxiv.org/abs/2206.00393v1  2022-06-01T11:00:07Z   \n",
       "815  http://arxiv.org/abs/2109.02096v2  2021-10-10T16:22:00Z   \n",
       "426  http://arxiv.org/abs/1902.03389v1  2019-02-09T07:49:42Z   \n",
       "236  http://arxiv.org/abs/1805.11797v2  2018-05-31T03:49:25Z   \n",
       "822  http://arxiv.org/abs/2003.00418v1  2020-03-01T06:42:43Z   \n",
       "122  http://arxiv.org/abs/1608.04363v2  2016-11-28T17:48:04Z   \n",
       "217  http://arxiv.org/abs/2110.03894v3  2022-12-20T12:44:24Z   \n",
       "428  http://arxiv.org/abs/2105.00173v2  2021-07-04T07:34:14Z   \n",
       "431  http://arxiv.org/abs/2209.12573v3  2022-11-16T09:56:26Z   \n",
       "120  http://arxiv.org/abs/2104.00528v2  2021-04-19T03:25:50Z   \n",
       "804  http://arxiv.org/abs/1906.04232v1  2019-06-10T19:04:09Z   \n",
       "\n",
       "             published_ts                                              title  \\\n",
       "814  2021-06-08T12:22:29Z  NWT: Towards natural audio-to-video generation...   \n",
       "216  2021-06-17T07:59:15Z  Voice2Series: Reprogramming Acoustic Models fo...   \n",
       "810  2022-02-17T08:26:25Z  A Study of Designing Compact Audio-Visual Wake...   \n",
       "212  2019-10-24T06:54:36Z  A Recurrent Variational Autoencoder for Speech...   \n",
       "817  2021-10-17T11:14:00Z            Taming Visually Guided Sound Generation   \n",
       "223  2023-01-19T02:37:56Z  From English to More Languages: Parameter-Effi...   \n",
       "219  2022-02-17T09:17:58Z  Mitigating Closed-model Adversarial Examples w...   \n",
       "222  2022-11-03T14:20:32Z  HyperSound: Generating Implicit Neural Represe...   \n",
       "429  2022-06-05T16:18:27Z  Variable-rate hierarchical CPC leads to acoust...   \n",
       "412  2022-06-01T11:00:07Z  Towards Generalisable Audio Representations fo...   \n",
       "815  2021-09-05T15:06:53Z  Timbre Transfer with Variational Auto Encoding...   \n",
       "426  2019-02-09T07:49:42Z  Generative Moment Matching Network-based Rando...   \n",
       "236  2018-05-30T04:15:58Z   Grow and Prune Compact, Fast, and Accurate LSTMs   \n",
       "822  2020-03-01T06:42:43Z         Towards Automatic Face-to-Face Translation   \n",
       "122  2016-08-15T18:57:10Z  Deep Convolutional Neural Networks and Data Au...   \n",
       "217  2021-10-08T05:07:35Z  Neural Model Reprogramming with Similarity Bas...   \n",
       "428  2021-05-01T05:47:15Z  Emotion Recognition of the Singing Voice: Towa...   \n",
       "431  2022-09-26T10:38:39Z  Digital Audio Forensics: Blind Human Voice Mim...   \n",
       "120  2021-03-31T04:09:30Z  OutlierNets: Highly Compact Deep Autoencoder N...   \n",
       "804  2019-06-10T19:04:09Z  BowNet: Dilated Convolution Neural Network for...   \n",
       "\n",
       "                                               summary  \\\n",
       "814  In this work we introduce NWT, an expressive s...   \n",
       "216  Learning to classify time series with limited ...   \n",
       "810  Audio-only-based wake word spotting (WWS) is c...   \n",
       "212  This paper presents a generative approach to s...   \n",
       "817  Recent advances in visually-induced audio gene...   \n",
       "223  In this work, we propose a new parameter-effic...   \n",
       "219  In this work, we aim to enhance the system rob...   \n",
       "222  Implicit neural representations (INRs) are a r...   \n",
       "429  The success of deep learning comes from its ab...   \n",
       "412  In audio-visual navigation (AVN), an intellige...   \n",
       "815  This research project investigates the applica...   \n",
       "426  This paper proposes a generative moment matchi...   \n",
       "236  Long short-term memory (LSTM) has been widely ...   \n",
       "822  In light of the recent breakthroughs in automa...   \n",
       "122  The ability of deep convolutional neural netwo...   \n",
       "217  In this study, we propose a novel adversarial ...   \n",
       "428  Current computational-emotion research has foc...   \n",
       "431  Audio is one of the most used way of human com...   \n",
       "120  Human operators often diagnose industrial mach...   \n",
       "804  Ultrasound imaging is safe, relatively afforda...   \n",
       "\n",
       "                                                author category  \\\n",
       "814  Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, ...    cs.SD   \n",
       "216      Chao-Han Huck Yang, Yun-Yun Tsai, Pin-Yu Chen    cs.LG   \n",
       "810  Hengshun Zhou, Jun Du, Chao-Han Huck Yang, Shi...    cs.SD   \n",
       "212  Simon Leglaive, Xavier Alameda-Pineda, Laurent...    cs.LG   \n",
       "817                         Vladimir Iashin, Esa Rahtu    cs.CV   \n",
       "223  Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Ch...    cs.SD   \n",
       "219  Chao-Han Huck Yang, Zeeshan Ahmed, Yile Gu, Jo...  eess.AS   \n",
       "222  Filip Szatkowski, Karol J. Piczak, Przemysław ...    cs.SD   \n",
       "429  Santiago Cuervo, Adrian Łańcucki, Ricard Marxe...    cs.SD   \n",
       "412   Shunqi Mao, Chaoyi Zhang, Heng Wang, Weidong Cai    cs.SD   \n",
       "815  Russell Sammut Bonnici, Charalampos Saitis, Ma...    cs.SD   \n",
       "426  Hiroki Tamaru, Yuki Saito, Shinnosuke Takamich...    cs.SD   \n",
       "236            Xiaoliang Dai, Hongxu Yin, Niraj K. Jha    cs.LG   \n",
       "822  Prajwal K R, Rudrabha Mukhopadhyay, Jerin Phil...    cs.CV   \n",
       "122                   Justin Salamon, Juan Pablo Bello    cs.SD   \n",
       "217  Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu...  eess.AS   \n",
       "428                                 Daniel Szelogowski    cs.SD   \n",
       "431  Sahar Al Ajmi, Khizar Hayat, Alaa M. Al Obaidi...    cs.SD   \n",
       "120  Saad Abbasi, Mahmoud Famouri, Mohammad Javad S...    cs.SD   \n",
       "804                   M. Hamed Mozaffari, Won-Sook Lee  eess.IV   \n",
       "\n",
       "                                         combined_text  \n",
       "814  nwt: towards natural audio-to-video generation...  \n",
       "216  voice2series: reprogramming acoustic models fo...  \n",
       "810  a study of designing compact audio-visual wake...  \n",
       "212  a recurrent variational autoencoder for speech...  \n",
       "817  taming visually guided sound generation. recen...  \n",
       "223  from english to more languages: parameter-effi...  \n",
       "219  mitigating closed-model adversarial examples w...  \n",
       "222  hypersound: generating implicit neural represe...  \n",
       "429  variable-rate hierarchical cpc leads to acoust...  \n",
       "412  towards generalisable audio representations fo...  \n",
       "815  timbre transfer with variational auto encoding...  \n",
       "426  generative moment matching network-based rando...  \n",
       "236  grow and prune compact, fast, and accurate lst...  \n",
       "822  towards automatic face-to-face translation. in...  \n",
       "122  deep convolutional neural networks and data au...  \n",
       "217  neural model reprogramming with similarity bas...  \n",
       "428  emotion recognition of the singing voice: towa...  \n",
       "431  digital audio forensics: blind human voice mim...  \n",
       "120  outliernets: highly compact deep autoencoder n...  \n",
       "804  bownet: dilated convolution neural network for...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = res_embeddings[0]\n",
    "article_embeddings = res_embeddings[1:]\n",
    "\n",
    "top_k_indices = compute_top_k(query_embedding, article_embeddings, k=20)\n",
    "articles_df.iloc[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddb34e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'link': 'http://arxiv.org/abs/2106.04283v1',\n",
       "  'updated_ts': '2021-06-08T12:22:29Z',\n",
       "  'published_ts': '2021-06-08T12:22:29Z',\n",
       "  'title': 'NWT: Towards natural audio-to-video generation with representation\\n  learning',\n",
       "  'summary': \"In this work we introduce NWT, an expressive speech-to-video model. Unlike\\napproaches that use domain-specific intermediate representations such as pose\\nkeypoints, NWT learns its own latent representations, with minimal assumptions\\nabout the audio and video content. To this end, we propose a novel discrete\\nvariational autoencoder with adversarial loss, dVAE-Adv, which learns a new\\ndiscrete latent representation we call Memcodes. Memcodes are straightforward\\nto implement, require no additional loss terms, are stable to train compared\\nwith other approaches, and show evidence of interpretability. To predict on the\\nMemcode space, we use an autoregressive encoder-decoder model conditioned on\\naudio. Additionally, our model can control latent attributes in the generated\\nvideo that are not annotated in the data. We train NWT on clips from HBO's Last\\nWeek Tonight with John Oliver. NWT consistently scores above other approaches\\nin Mean Opinion Score (MOS) on tests of overall video naturalness, facial\\nnaturalness and expressiveness, and lipsync quality. This work sets a strong\\nbaseline for generalized audio-to-video synthesis. Samples are available at\\nhttps://next-week-tonight.github.io/NWT/.\",\n",
       "  'author': 'Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, Cole Clifford, Ragavan Thurairatnam',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/2106.09296v3',\n",
       "  'updated_ts': '2022-01-14T16:43:19Z',\n",
       "  'published_ts': '2021-06-17T07:59:15Z',\n",
       "  'title': 'Voice2Series: Reprogramming Acoustic Models for Time Series\\n  Classification',\n",
       "  'summary': 'Learning to classify time series with limited data is a practical yet\\nchallenging problem. Current methods are primarily based on hand-designed\\nfeature extraction rules or domain-specific data augmentation. Motivated by the\\nadvances in deep speech processing models and the fact that voice data are\\nunivariate temporal signals, in this paper, we propose Voice2Series (V2S), a\\nnovel end-to-end approach that reprograms acoustic models for time series\\nclassification, through input transformation learning and output label mapping.\\nLeveraging the representation learning power of a large-scale pre-trained\\nspeech processing model, on 30 different time series tasks we show that V2S\\nperforms competitive results on 19 time series classification tasks. We further\\nprovide a theoretical justification of V2S by proving its population risk is\\nupper bounded by the source risk and a Wasserstein distance accounting for\\nfeature alignment via reprogramming. Our results offer new and effective means\\nto time series classification.',\n",
       "  'author': 'Chao-Han Huck Yang, Yun-Yun Tsai, Pin-Yu Chen',\n",
       "  'category': 'cs.LG'},\n",
       " {'link': 'http://arxiv.org/abs/2202.08509v1',\n",
       "  'updated_ts': '2022-02-17T08:26:25Z',\n",
       "  'published_ts': '2022-02-17T08:26:25Z',\n",
       "  'title': 'A Study of Designing Compact Audio-Visual Wake Word Spotting System\\n  Based on Iterative Fine-Tuning in Neural Network Pruning',\n",
       "  'summary': 'Audio-only-based wake word spotting (WWS) is challenging under noisy\\nconditions due to environmental interference in signal transmission. In this\\npaper, we investigate on designing a compact audio-visual WWS system by\\nutilizing visual information to alleviate the degradation. Specifically, in\\norder to use visual information, we first encode the detected lips to\\nfixed-size vectors with MobileNet and concatenate them with acoustic features\\nfollowed by the fusion network for WWS. However, the audio-visual model based\\non neural networks requires a large footprint and a high computational\\ncomplexity. To meet the application requirements, we introduce a neural network\\npruning strategy via the lottery ticket hypothesis in an iterative fine-tuning\\nmanner (LTH-IF), to the single-modal and multi-modal models, respectively.\\nTested on our in-house corpus for audio-visual WWS in a home TV scene, the\\nproposed audio-visual system achieves significant performance improvements over\\nthe single-modality (audio-only or video-only) system under different noisy\\nconditions. Moreover, LTH-IF pruning can largely reduce the network parameters\\nand computations with no degradation of WWS performance, leading to a potential\\nproduct solution for the TV wake-up scenario.',\n",
       "  'author': 'Hengshun Zhou, Jun Du, Chao-Han Huck Yang, Shifu Xiong, Chin-Hui Lee',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/1910.10942v2',\n",
       "  'updated_ts': '2020-02-10T09:36:23Z',\n",
       "  'published_ts': '2019-10-24T06:54:36Z',\n",
       "  'title': 'A Recurrent Variational Autoencoder for Speech Enhancement',\n",
       "  'summary': 'This paper presents a generative approach to speech enhancement based on a\\nrecurrent variational autoencoder (RVAE). The deep generative speech model is\\ntrained using clean speech signals only, and it is combined with a nonnegative\\nmatrix factorization noise model for speech enhancement. We propose a\\nvariational expectation-maximization algorithm where the encoder of the RVAE is\\nfine-tuned at test time, to approximate the distribution of the latent\\nvariables given the noisy speech observations. Compared with previous\\napproaches based on feed-forward fully-connected architectures, the proposed\\nrecurrent deep generative speech model induces a posterior temporal dynamic\\nover the latent variables, which is shown to improve the speech enhancement\\nresults.',\n",
       "  'author': 'Simon Leglaive, Xavier Alameda-Pineda, Laurent Girin, Radu Horaud',\n",
       "  'category': 'cs.LG'},\n",
       " {'link': 'http://arxiv.org/abs/2110.08791v1',\n",
       "  'updated_ts': '2021-10-17T11:14:00Z',\n",
       "  'published_ts': '2021-10-17T11:14:00Z',\n",
       "  'title': 'Taming Visually Guided Sound Generation',\n",
       "  'summary': 'Recent advances in visually-induced audio generation are based on sampling\\nshort, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio\\nfrom the state-of-the-art model takes minutes on a high-end GPU. In this work,\\nwe propose a single model capable of generating visually relevant,\\nhigh-fidelity sounds prompted with a set of frames from open-domain videos in\\nless time than it takes to play it on a single GPU.\\n  We train a transformer to sample a new spectrogram from the pre-trained\\nspectrogram codebook given the set of video features. The codebook is obtained\\nusing a variant of VQGAN trained to produce a compact sampling space with a\\nnovel spectrogram-based perceptual loss. The generated spectrogram is\\ntransformed into a waveform using a window-based GAN that significantly speeds\\nup generation. Considering the lack of metrics for automatic evaluation of\\ngenerated spectrograms, we also build a family of metrics called FID and MKL.\\nThese metrics are based on a novel sound classifier, called Melception, and\\ndesigned to evaluate the fidelity and relevance of open-domain samples.\\n  Both qualitative and quantitative studies are conducted on small- and\\nlarge-scale datasets to evaluate the fidelity and relevance of generated\\nsamples. We also compare our model to the state-of-the-art and observe a\\nsubstantial improvement in quality, size, and computation time. Code, demo, and\\nsamples: v-iashin.github.io/SpecVQGAN',\n",
       "  'author': 'Vladimir Iashin, Esa Rahtu',\n",
       "  'category': 'cs.CV'},\n",
       " {'link': 'http://arxiv.org/abs/2301.07851v1',\n",
       "  'updated_ts': '2023-01-19T02:37:56Z',\n",
       "  'published_ts': '2023-01-19T02:37:56Z',\n",
       "  'title': 'From English to More Languages: Parameter-Efficient Model Reprogramming\\n  for Cross-Lingual Speech Recognition',\n",
       "  'summary': 'In this work, we propose a new parameter-efficient learning framework based\\non neural model reprogramming for cross-lingual speech recognition, which can\\n\\\\textbf{re-purpose} well-trained English automatic speech recognition (ASR)\\nmodels to recognize the other languages. We design different auxiliary neural\\narchitectures focusing on learnable pre-trained feature enhancement that, for\\nthe first time, empowers model reprogramming on ASR. Specifically, we\\ninvestigate how to select trainable components (i.e., encoder) of a\\nconformer-based RNN-Transducer, as a frozen pre-trained backbone. Experiments\\non a seven-language multilingual LibriSpeech speech (MLS) task show that model\\nreprogramming only requires 4.2% (11M out of 270M) to 6.8% (45M out of 660M) of\\nits original trainable parameters from a full ASR model to perform competitive\\nresults in a range of 11.9% to 8.1% WER averaged across different languages. In\\naddition, we discover different setups to make large-scale pre-trained ASR\\nsucceed in both monolingual and multilingual speech recognition. Our methods\\noutperform existing ASR tuning architectures and their extension with\\nself-supervised losses (e.g., w2v-bert) in terms of lower WER and better\\ntraining efficiency.',\n",
       "  'author': 'Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Rohit Prabhavalkar, Tara N. Sainath, Trevor Strohman',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/2202.08532v1',\n",
       "  'updated_ts': '2022-02-17T09:17:58Z',\n",
       "  'published_ts': '2022-02-17T09:17:58Z',\n",
       "  'title': 'Mitigating Closed-model Adversarial Examples with Bayesian Neural\\n  Modeling for Enhanced End-to-End Speech Recognition',\n",
       "  'summary': 'In this work, we aim to enhance the system robustness of end-to-end automatic\\nspeech recognition (ASR) against adversarially-noisy speech examples. We focus\\non a rigorous and empirical \"closed-model adversarial robustness\" setting\\n(e.g., on-device or cloud applications). The adversarial noise is only\\ngenerated by closed-model optimization (e.g., evolutionary and zeroth-order\\nestimation) without accessing gradient information of a targeted ASR model\\ndirectly. We propose an advanced Bayesian neural network (BNN) based\\nadversarial detector, which could model latent distributions against adaptive\\nadversarial perturbation with divergence measurement. We further simulate\\ndeployment scenarios of RNN Transducer, Conformer, and wav2vec-2.0 based ASR\\nsystems with the proposed adversarial detection system. Leveraging the proposed\\nBNN based detection system, we improve detection rate by +2.77 to +5.42%\\n(relative +3.03 to +6.26%) and reduce the word error rate by 5.02 to 7.47% on\\nLibriSpeech datasets compared to the current model enhancement methods against\\nthe adversarial speech examples.',\n",
       "  'author': 'Chao-Han Huck Yang, Zeeshan Ahmed, Yile Gu, Joseph Szurley, Roger Ren, Linda Liu, Andreas Stolcke, Ivan Bulyko',\n",
       "  'category': 'eess.AS'},\n",
       " {'link': 'http://arxiv.org/abs/2211.01839v1',\n",
       "  'updated_ts': '2022-11-03T14:20:32Z',\n",
       "  'published_ts': '2022-11-03T14:20:32Z',\n",
       "  'title': 'HyperSound: Generating Implicit Neural Representations of Audio Signals\\n  with Hypernetworks',\n",
       "  'summary': 'Implicit neural representations (INRs) are a rapidly growing research field,\\nwhich provides alternative ways to represent multimedia signals. Recent\\napplications of INRs include image super-resolution, compression of\\nhigh-dimensional signals, or 3D rendering. However, these solutions usually\\nfocus on visual data, and adapting them to the audio domain is not trivial.\\nMoreover, it requires a separately trained model for every data sample. To\\naddress this limitation, we propose HyperSound, a meta-learning method\\nleveraging hypernetworks to produce INRs for audio signals unseen at training\\ntime. We show that our approach can reconstruct sound waves with quality\\ncomparable to other state-of-the-art models.',\n",
       "  'author': 'Filip Szatkowski, Karol J. Piczak, Przemysław Spurek, Jacek Tabor, Tomasz Trzciński',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/2206.02211v3',\n",
       "  'updated_ts': '2022-12-04T08:24:02Z',\n",
       "  'published_ts': '2022-06-05T16:18:27Z',\n",
       "  'title': 'Variable-rate hierarchical CPC leads to acoustic unit discovery in\\n  speech',\n",
       "  'summary': 'The success of deep learning comes from its ability to capture the\\nhierarchical structure of data by learning high-level representations defined\\nin terms of low-level ones. In this paper we explore self-supervised learning\\nof hierarchical representations of speech by applying multiple levels of\\nContrastive Predictive Coding (CPC). We observe that simply stacking two CPC\\nmodels does not yield significant improvements over single-level architectures.\\nInspired by the fact that speech is often described as a sequence of discrete\\nunits unevenly distributed in time, we propose a model in which the output of a\\nlow-level CPC module is non-uniformly downsampled to directly minimize the loss\\nof a high-level CPC module. The latter is designed to also enforce a prior of\\nseparability and discreteness in its representations by enforcing dissimilarity\\nof successive high-level representations through focused negative sampling, and\\nby quantization of the prediction targets. Accounting for the structure of the\\nspeech signal improves upon single-level CPC features and enhances the\\ndisentanglement of the learned representations, as measured by downstream\\nspeech recognition tasks, while resulting in a meaningful segmentation of the\\nsignal that closely resembles phone boundaries.',\n",
       "  'author': 'Santiago Cuervo, Adrian Łańcucki, Ricard Marxer, Paweł Rychlikowski, Jan Chorowski',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/2206.00393v1',\n",
       "  'updated_ts': '2022-06-01T11:00:07Z',\n",
       "  'published_ts': '2022-06-01T11:00:07Z',\n",
       "  'title': 'Towards Generalisable Audio Representations for Audio-Visual Navigation',\n",
       "  'summary': 'In audio-visual navigation (AVN), an intelligent agent needs to navigate to a\\nconstantly sound-making object in complex 3D environments based on its audio\\nand visual perceptions. While existing methods attempt to improve the\\nnavigation performance with preciously designed path planning or intricate task\\nsettings, none has improved the model generalisation on unheard sounds with\\ntask settings unchanged. We thus propose a contrastive learning-based method to\\ntackle this challenge by regularising the audio encoder, where the\\nsound-agnostic goal-driven latent representations can be learnt from various\\naudio signals of different classes. In addition, we consider two data\\naugmentation strategies to enrich the training sounds. We demonstrate that our\\ndesigns can be easily equipped to existing AVN frameworks to obtain an\\nimmediate performance gain (13.4%$\\\\uparrow$ in SPL on Replica and\\n12.2%$\\\\uparrow$ in SPL on MP3D). Our project is available at\\nhttps://AV-GeN.github.io/.',\n",
       "  'author': 'Shunqi Mao, Chaoyi Zhang, Heng Wang, Weidong Cai',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/2109.02096v2',\n",
       "  'updated_ts': '2021-10-10T16:22:00Z',\n",
       "  'published_ts': '2021-09-05T15:06:53Z',\n",
       "  'title': 'Timbre Transfer with Variational Auto Encoding and Cycle-Consistent\\n  Adversarial Networks',\n",
       "  'summary': \"This research project investigates the application of deep learning to timbre\\ntransfer, where the timbre of a source audio can be converted to the timbre of\\na target audio with minimal loss in quality. The adopted approach combines\\nVariational Autoencoders with Generative Adversarial Networks to construct\\nmeaningful representations of the source audio and produce realistic\\ngenerations of the target audio and is applied to the Flickr 8k Audio dataset\\nfor transferring the vocal timbre between speakers and the URMP dataset for\\ntransferring the musical timbre between instruments. Furthermore, variations of\\nthe adopted approach are trained, and generalised performance is compared using\\nthe metrics SSIM (Structural Similarity Index) and FAD (Frech\\\\'et Audio\\nDistance). It was found that a many-to-many approach supersedes a one-to-one\\napproach in terms of reconstructive capabilities, and that the adoption of a\\nbasic over a bottleneck residual block design is more suitable for enriching\\ncontent information about a latent space. It was also found that the decision\\non whether cyclic loss takes on a variational autoencoder or vanilla\\nautoencoder approach does not have a significant impact on reconstructive and\\nadversarial translation aspects of the model.\",\n",
       "  'author': 'Russell Sammut Bonnici, Charalampos Saitis, Martin Benning',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/1902.03389v1',\n",
       "  'updated_ts': '2019-02-09T07:49:42Z',\n",
       "  'published_ts': '2019-02-09T07:49:42Z',\n",
       "  'title': 'Generative Moment Matching Network-based Random Modulation Post-filter\\n  for DNN-based Singing Voice Synthesis and Neural Double-tracking',\n",
       "  'summary': 'This paper proposes a generative moment matching network (GMMN)-based\\npost-filter that provides inter-utterance pitch variation for deep neural\\nnetwork (DNN)-based singing voice synthesis. The natural pitch variation of a\\nhuman singing voice leads to a richer musical experience and is used in\\ndouble-tracking, a recording method in which two performances of the same\\nphrase are recorded and mixed to create a richer, layered sound. However,\\nsinging voices synthesized using conventional DNN-based methods never vary\\nbecause the synthesis process is deterministic and only one waveform is\\nsynthesized from one musical score. To address this problem, we use a GMMN to\\nmodel the variation of the modulation spectrum of the pitch contour of natural\\nsinging voices and add a randomized inter-utterance variation to the pitch\\ncontour generated by conventional DNN-based singing voice synthesis.\\nExperimental evaluations suggest that 1) our approach can provide perceptible\\ninter-utterance pitch variation while preserving speech quality. We extend our\\napproach to double-tracking, and the evaluation demonstrates that 2) GMMN-based\\nneural double-tracking is perceptually closer to natural double-tracking than\\nconventional signal processing-based artificial double-tracking is.',\n",
       "  'author': 'Hiroki Tamaru, Yuki Saito, Shinnosuke Takamichi, Tomoki Koriyama, Hiroshi Saruwatari',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/1805.11797v2',\n",
       "  'updated_ts': '2018-05-31T03:49:25Z',\n",
       "  'published_ts': '2018-05-30T04:15:58Z',\n",
       "  'title': 'Grow and Prune Compact, Fast, and Accurate LSTMs',\n",
       "  'summary': \"Long short-term memory (LSTM) has been widely used for sequential data\\nmodeling. Researchers have increased LSTM depth by stacking LSTM cells to\\nimprove performance. This incurs model redundancy, increases run-time delay,\\nand makes the LSTMs more prone to overfitting. To address these problems, we\\npropose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original\\none level non-linear control gates. H-LSTM increases accuracy while employing\\nfewer external stacked layers, thus reducing the number of parameters and\\nrun-time latency significantly. We employ grow-and-prune (GP) training to\\niteratively adjust the hidden layers through gradient-based growth and\\nmagnitude-based pruning of connections. This learns both the weights and the\\ncompact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for\\nimage captioning and speech recognition applications. For the NeuralTalk\\narchitecture on the MSCOCO dataset, our three models reduce the number of\\nparameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time\\nlatency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2\\narchitecture on the AN4 dataset, our two models reduce the number of parameters\\nby 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate\\nfrom 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast,\\nand accurate.\",\n",
       "  'author': 'Xiaoliang Dai, Hongxu Yin, Niraj K. Jha',\n",
       "  'category': 'cs.LG'},\n",
       " {'link': 'http://arxiv.org/abs/2003.00418v1',\n",
       "  'updated_ts': '2020-03-01T06:42:43Z',\n",
       "  'published_ts': '2020-03-01T06:42:43Z',\n",
       "  'title': 'Towards Automatic Face-to-Face Translation',\n",
       "  'summary': 'In light of the recent breakthroughs in automatic machine translation\\nsystems, we propose a novel approach that we term as \"Face-to-Face\\nTranslation\". As today\\'s digital communication becomes increasingly visual, we\\nargue that there is a need for systems that can automatically translate a video\\nof a person speaking in language A into a target language B with realistic lip\\nsynchronization. In this work, we create an automatic pipeline for this problem\\nand demonstrate its impact on multiple real-world applications. First, we build\\na working speech-to-speech translation system by bringing together multiple\\nexisting modules from speech and language. We then move towards \"Face-to-Face\\nTranslation\" by incorporating a novel visual module, LipGAN for generating\\nrealistic talking faces from the translated audio. Quantitative evaluation of\\nLipGAN on the standard LRW test set shows that it significantly outperforms\\nexisting approaches across all standard metrics. We also subject our\\nFace-to-Face Translation pipeline, to multiple human evaluations and show that\\nit can significantly improve the overall user experience for consuming and\\ninteracting with multimodal content across languages. Code, models and demo\\nvideo are made publicly available.\\n  Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0\\n  Code and models: https://github.com/Rudrabha/LipGAN',\n",
       "  'author': 'Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, C. V. Jawahar',\n",
       "  'category': 'cs.CV'},\n",
       " {'link': 'http://arxiv.org/abs/1608.04363v2',\n",
       "  'updated_ts': '2016-11-28T17:48:04Z',\n",
       "  'published_ts': '2016-08-15T18:57:10Z',\n",
       "  'title': 'Deep Convolutional Neural Networks and Data Augmentation for\\n  Environmental Sound Classification',\n",
       "  'summary': 'The ability of deep convolutional neural networks (CNN) to learn\\ndiscriminative spectro-temporal patterns makes them well suited to\\nenvironmental sound classification. However, the relative scarcity of labeled\\ndata has impeded the exploitation of this family of high-capacity models. This\\nstudy has two primary contributions: first, we propose a deep convolutional\\nneural network architecture for environmental sound classification. Second, we\\npropose the use of audio data augmentation for overcoming the problem of data\\nscarcity and explore the influence of different augmentations on the\\nperformance of the proposed CNN architecture. Combined with data augmentation,\\nthe proposed model produces state-of-the-art results for environmental sound\\nclassification. We show that the improved performance stems from the\\ncombination of a deep, high-capacity model and an augmented training set: this\\ncombination outperforms both the proposed CNN without augmentation and a\\n\"shallow\" dictionary learning model with augmentation. Finally, we examine the\\ninfluence of each augmentation on the model\\'s classification accuracy for each\\nclass, and observe that the accuracy for each class is influenced differently\\nby each augmentation, suggesting that the performance of the model could be\\nimproved further by applying class-conditional data augmentation.',\n",
       "  'author': 'Justin Salamon, Juan Pablo Bello',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/2110.03894v3',\n",
       "  'updated_ts': '2022-12-20T12:44:24Z',\n",
       "  'published_ts': '2021-10-08T05:07:35Z',\n",
       "  'title': 'Neural Model Reprogramming with Similarity Based Mapping for\\n  Low-Resource Spoken Command Classification',\n",
       "  'summary': 'In this study, we propose a novel adversarial reprogramming (AR) approach for\\nlow-resource spoken command recognition (SCR), and build an AR-SCR system. The\\nAR procedure aims to modify the acoustic signals (from the target domain) to\\nrepurpose a pretrained SCR model (from the source domain). To solve the label\\nmismatches between source and target domains, and further improve the stability\\nof AR, we propose a novel similarity-based label mapping technique to align\\nclasses. In addition, the transfer learning (TL) technique is combined with the\\noriginal AR process to improve the model adaptation capability. We evaluate the\\nproposed AR-SCR system on three low-resource SCR datasets, including Arabic,\\nLithuanian, and dysarthric Mandarin speech. Experimental results show that with\\na pretrained AM trained on a large-scale English dataset, the proposed AR-SCR\\nsystem outperforms the current state-of-the-art results on Arabic and\\nLithuanian speech commands datasets, with only a limited amount of training\\ndata.',\n",
       "  'author': 'Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Yu Tsao',\n",
       "  'category': 'eess.AS'},\n",
       " {'link': 'http://arxiv.org/abs/2105.00173v2',\n",
       "  'updated_ts': '2021-07-04T07:34:14Z',\n",
       "  'published_ts': '2021-05-01T05:47:15Z',\n",
       "  'title': 'Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis\\n  Tool for Singers',\n",
       "  'summary': 'Current computational-emotion research has focused on applying acoustic\\nproperties to analyze how emotions are perceived mathematically or used in\\nnatural language processing machine learning models. While recent interest has\\nfocused on analyzing emotions from the spoken voice, little experimentation has\\nbeen performed to discover how emotions are recognized in the singing voice --\\nboth in noiseless and noisy data (i.e., data that is either inaccurate,\\ndifficult to interpret, has corrupted/distorted/nonsense information like\\nactual noise sounds in this case, or has a low ratio of usable/unusable\\ninformation). Not only does this ignore the challenges of training machine\\nlearning models on more subjective data and testing them with much noisier\\ndata, but there is also a clear disconnect in progress between advancing the\\ndevelopment of convolutional neural networks and the goal of emotionally\\ncognizant artificial intelligence. By training a new model to include this type\\nof information with a rich comprehension of psycho-acoustic properties, not\\nonly can models be trained to recognize information within extremely noisy\\ndata, but advancement can be made toward more complex biofeedback applications\\n-- including creating a model which could recognize emotions given any human\\ninformation (language, breath, voice, body, posture) and be used in any\\nperformance medium (music, speech, acting) or psychological assistance for\\npatients with disorders such as BPD, alexithymia, autism, among others. This\\npaper seeks to reflect and expand upon the findings of related research and\\npresent a stepping-stone toward this end goal.',\n",
       "  'author': 'Daniel Szelogowski',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/2209.12573v3',\n",
       "  'updated_ts': '2022-11-16T09:56:26Z',\n",
       "  'published_ts': '2022-09-26T10:38:39Z',\n",
       "  'title': 'Digital Audio Forensics: Blind Human Voice Mimicry Detection',\n",
       "  'summary': 'Audio is one of the most used way of human communication, but at the same\\ntime it can be easily misused by to trick people. With the revolution of AI,\\nthe related technologies are now accessible to almost everyone thus making it\\nsimple for the criminals to commit crimes and forgeries. In this work, we\\nintroduce a deep learning method to develop a classifier that will blindly\\nclassify an input audio as real or mimicked. The proposed model was trained on\\na set of important features extracted from a large dataset of audios to get a\\nclassifier that was tested on the same set of features from different audios.\\nTwo datasets were created for this work; an all English data set and a mixed\\ndata set (Arabic and English). These datasets have been made available through\\nGitHub for the use of the research community at\\nhttps://github.com/SaSs7/Dataset. For the purpose of comparison, the audios\\nwere also classified through human inspection with the subjects being the\\nnative speakers. The ensued results were interesting and exhibited formidable\\naccuracy.',\n",
       "  'author': 'Sahar Al Ajmi, Khizar Hayat, Alaa M. Al Obaidi, Naresh Kumar, Munaf Najmuldeen, Baptiste Magnier',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/2104.00528v2',\n",
       "  'updated_ts': '2021-04-19T03:25:50Z',\n",
       "  'published_ts': '2021-03-31T04:09:30Z',\n",
       "  'title': 'OutlierNets: Highly Compact Deep Autoencoder Network Architectures for\\n  On-Device Acoustic Anomaly Detection',\n",
       "  'summary': 'Human operators often diagnose industrial machinery via anomalous sounds.\\nAutomated acoustic anomaly detection can lead to reliable maintenance of\\nmachinery. However, deep learning-driven anomaly detection methods often\\nrequire an extensive amount of computational resources which prohibits their\\ndeployment in factories. Here we explore a machine-driven design exploration\\nstrategy to create OutlierNets, a family of highly compact deep convolutional\\nautoencoder network architectures featuring as few as 686 parameters, model\\nsizes as small as 2.7 KB, and as low as 2.8 million FLOPs, with a detection\\naccuracy matching or exceeding published architectures with as many as 4\\nmillion parameters. Furthermore, CPU-accelerated latency experiments show that\\nthe OutlierNet architectures can achieve as much as 21x lower latency than\\npublished networks.',\n",
       "  'author': 'Saad Abbasi, Mahmoud Famouri, Mohammad Javad Shafiee, Alexander Wong',\n",
       "  'category': 'cs.SD'},\n",
       " {'link': 'http://arxiv.org/abs/1906.04232v1',\n",
       "  'updated_ts': '2019-06-10T19:04:09Z',\n",
       "  'published_ts': '2019-06-10T19:04:09Z',\n",
       "  'title': 'BowNet: Dilated Convolution Neural Network for Ultrasound Tongue Contour\\n  Extraction',\n",
       "  'summary': 'Ultrasound imaging is safe, relatively affordable, and capable of real-time\\nperformance. One application of this technology is to visualize and to\\ncharacterize human tongue shape and motion during a real-time speech to study\\nhealthy or impaired speech production. Due to the noisy nature of ultrasound\\nimages with low-contrast characteristic, it might require expertise for\\nnon-expert users to recognize organ shape such as tongue surface (dorsum). To\\nalleviate this difficulty for quantitative analysis of tongue shape and motion,\\ntongue surface can be extracted, tracked, and visualized instead of the whole\\ntongue region. Delineating the tongue surface from each frame is a cumbersome,\\nsubjective, and error-prone task. Furthermore, the rapidity and complexity of\\ntongue gestures have made it a challenging task, and manual segmentation is not\\na feasible solution for real-time applications. Employing the power of\\nstate-of-the-art deep neural network models and training techniques, it is\\nfeasible to implement new fully-automatic, accurate, and robust segmentation\\nmethods with the capability of real-time performance, applicable for tracking\\nof the tongue contours during the speech. This paper presents two novel deep\\nneural network models named BowNet and wBowNet benefits from the ability of\\nglobal prediction of decoding-encoding models, with integrated multi-scale\\ncontextual information, and capability of full-resolution (local) extraction of\\ndilated convolutions. Experimental results using several ultrasound tongue\\nimage datasets revealed that the combination of both localization and\\nglobalization searching could improve prediction result significantly.\\nAssessment of BowNet models using both qualitatively and quantitatively studies\\nshowed them outstanding achievements in terms of accuracy and robustness in\\ncomparison with similar techniques.',\n",
       "  'author': 'M. Hamed Mozaffari, Won-Sook Lee',\n",
       "  'category': 'eess.IV'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.iloc[top_k_indices][['link', 'updated_ts', 'published_ts', 'title', 'summary', 'author', 'category']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f58037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:arxiv-scout]",
   "language": "python",
   "name": "conda-env-arxiv-scout-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
