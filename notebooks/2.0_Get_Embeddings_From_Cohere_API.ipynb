{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2f75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4fd8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure our src module can be found and imported\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import cohere\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data.arxiv_downloader import ArxivDownloader\n",
    "from src.models.cohere import CohereModel\n",
    "from src.utils import (\n",
    "    compute_top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4c08c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJ_DIR = Path.cwd().parent\n",
    "DOTENV_PATH = PROJ_DIR / '.env'\n",
    "dotenv.load_dotenv(DOTENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2540f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted Arxiv downloading logic into ArxivDownloader class that has simple caching functionality\n",
    "downloader = ArxivDownloader(download_refresh_interval_days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4cb4e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 175 ms, sys: 16.6 ms, total: 191 ms\n",
      "Wall time: 7.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles_df = downloader.retrieve_arxiv_articles_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b56451c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>updated_ts</th>\n",
       "      <th>published_ts</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/1709.06620v1</td>\n",
       "      <td>2017-09-19T19:26:20Z</td>\n",
       "      <td>2017-09-19T19:26:20Z</td>\n",
       "      <td>Learning of Coordination Policies for Robotic ...</td>\n",
       "      <td>Inspired by biological swarms, robotic swarms ...</td>\n",
       "      <td>Qiyang Li, Xintong Du, Yizhou Huang, Quinlan S...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>learning of coordination policies for robotic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2011.05605v2</td>\n",
       "      <td>2020-11-20T18:19:32Z</td>\n",
       "      <td>2020-11-11T07:35:21Z</td>\n",
       "      <td>Decentralized Motion Planning for Multi-Robot ...</td>\n",
       "      <td>This work presents a decentralized motion plan...</td>\n",
       "      <td>Sivanathan Kandhasamy, Vinayagam Babu Kuppusam...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>decentralized motion planning for multi-robot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2209.14745v2</td>\n",
       "      <td>2022-12-29T08:48:05Z</td>\n",
       "      <td>2022-09-29T13:02:58Z</td>\n",
       "      <td>A Multiagent Framework for the Asynchronous an...</td>\n",
       "      <td>The traditional ML development methodology doe...</td>\n",
       "      <td>Andrea Gesmundo</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>a multiagent framework for the asynchronous an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/1910.07882v1</td>\n",
       "      <td>2019-10-15T01:27:09Z</td>\n",
       "      <td>2019-10-15T01:27:09Z</td>\n",
       "      <td>Visual Hide and Seek</td>\n",
       "      <td>We train embodied agents to play Visual Hide a...</td>\n",
       "      <td>Boyuan Chen, Shuran Song, Hod Lipson, Carl Von...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>visual hide and seek. we train embodied agents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2003.08376v3</td>\n",
       "      <td>2020-11-07T02:48:22Z</td>\n",
       "      <td>2020-03-18T17:54:28Z</td>\n",
       "      <td>Inverting the Pose Forecasting Pipeline with S...</td>\n",
       "      <td>Many autonomous systems forecast aspects of th...</td>\n",
       "      <td>Xinshuo Weng, Jianren Wang, Sergey Levine, Kri...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>inverting the pose forecasting pipeline with s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>http://arxiv.org/abs/1808.02550v1</td>\n",
       "      <td>2018-08-07T20:50:56Z</td>\n",
       "      <td>2018-08-07T20:50:56Z</td>\n",
       "      <td>Collaborative Planning for Mixed-Autonomy Lane...</td>\n",
       "      <td>Driving is a social activity: drivers often in...</td>\n",
       "      <td>Shray Bansal, Akansel Cosgun, Alireza Nakhaei,...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>collaborative planning for mixed-autonomy lane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>http://arxiv.org/abs/1811.09914v1</td>\n",
       "      <td>2018-11-25T00:43:32Z</td>\n",
       "      <td>2018-11-25T00:43:32Z</td>\n",
       "      <td>RADMPC: A Fast Decentralized Approach for Chan...</td>\n",
       "      <td>Robust multi-vehicle path-planning is importan...</td>\n",
       "      <td>Aaron Huang, Benjamin J. Ayton, Brian C. Williams</td>\n",
       "      <td>cs.SY</td>\n",
       "      <td>radmpc: a fast decentralized approach for chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>http://arxiv.org/abs/1902.02393v1</td>\n",
       "      <td>2019-02-06T20:41:51Z</td>\n",
       "      <td>2019-02-06T20:41:51Z</td>\n",
       "      <td>Distributed Synthesis of Surveillance Strategi...</td>\n",
       "      <td>We study the problem of synthesizing strategie...</td>\n",
       "      <td>Suda Bharadwaj, Rayna Dimitrova, Ufuk Topcu</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>distributed synthesis of surveillance strategi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>http://arxiv.org/abs/1903.06847v1</td>\n",
       "      <td>2019-03-16T00:14:34Z</td>\n",
       "      <td>2019-03-16T00:14:34Z</td>\n",
       "      <td>Safe Coordination of Human-Robot Firefighting ...</td>\n",
       "      <td>Wildfires are destructive and inflict massive,...</td>\n",
       "      <td>Esmaeil Seraj, Andrew Silva, Matthew Gombolay</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>safe coordination of human-robot firefighting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>http://arxiv.org/abs/1905.00988v1</td>\n",
       "      <td>2019-05-02T22:45:26Z</td>\n",
       "      <td>2019-05-02T22:45:26Z</td>\n",
       "      <td>Behavior Planning of Autonomous Cars with Soci...</td>\n",
       "      <td>Autonomous cars have to navigate in dynamic en...</td>\n",
       "      <td>Liting Sun, Wei Zhan, Ching-Yao Chan, Masayosh...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>behavior planning of autonomous cars with soci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  link            updated_ts  \\\n",
       "0    http://arxiv.org/abs/1709.06620v1  2017-09-19T19:26:20Z   \n",
       "1    http://arxiv.org/abs/2011.05605v2  2020-11-20T18:19:32Z   \n",
       "2    http://arxiv.org/abs/2209.14745v2  2022-12-29T08:48:05Z   \n",
       "3    http://arxiv.org/abs/1910.07882v1  2019-10-15T01:27:09Z   \n",
       "4    http://arxiv.org/abs/2003.08376v3  2020-11-07T02:48:22Z   \n",
       "..                                 ...                   ...   \n",
       "995  http://arxiv.org/abs/1808.02550v1  2018-08-07T20:50:56Z   \n",
       "996  http://arxiv.org/abs/1811.09914v1  2018-11-25T00:43:32Z   \n",
       "997  http://arxiv.org/abs/1902.02393v1  2019-02-06T20:41:51Z   \n",
       "998  http://arxiv.org/abs/1903.06847v1  2019-03-16T00:14:34Z   \n",
       "999  http://arxiv.org/abs/1905.00988v1  2019-05-02T22:45:26Z   \n",
       "\n",
       "             published_ts                                              title  \\\n",
       "0    2017-09-19T19:26:20Z  Learning of Coordination Policies for Robotic ...   \n",
       "1    2020-11-11T07:35:21Z  Decentralized Motion Planning for Multi-Robot ...   \n",
       "2    2022-09-29T13:02:58Z  A Multiagent Framework for the Asynchronous an...   \n",
       "3    2019-10-15T01:27:09Z                               Visual Hide and Seek   \n",
       "4    2020-03-18T17:54:28Z  Inverting the Pose Forecasting Pipeline with S...   \n",
       "..                    ...                                                ...   \n",
       "995  2018-08-07T20:50:56Z  Collaborative Planning for Mixed-Autonomy Lane...   \n",
       "996  2018-11-25T00:43:32Z  RADMPC: A Fast Decentralized Approach for Chan...   \n",
       "997  2019-02-06T20:41:51Z  Distributed Synthesis of Surveillance Strategi...   \n",
       "998  2019-03-16T00:14:34Z  Safe Coordination of Human-Robot Firefighting ...   \n",
       "999  2019-05-02T22:45:26Z  Behavior Planning of Autonomous Cars with Soci...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    Inspired by biological swarms, robotic swarms ...   \n",
       "1    This work presents a decentralized motion plan...   \n",
       "2    The traditional ML development methodology doe...   \n",
       "3    We train embodied agents to play Visual Hide a...   \n",
       "4    Many autonomous systems forecast aspects of th...   \n",
       "..                                                 ...   \n",
       "995  Driving is a social activity: drivers often in...   \n",
       "996  Robust multi-vehicle path-planning is importan...   \n",
       "997  We study the problem of synthesizing strategie...   \n",
       "998  Wildfires are destructive and inflict massive,...   \n",
       "999  Autonomous cars have to navigate in dynamic en...   \n",
       "\n",
       "                                                author category  \\\n",
       "0    Qiyang Li, Xintong Du, Yizhou Huang, Quinlan S...    cs.RO   \n",
       "1    Sivanathan Kandhasamy, Vinayagam Babu Kuppusam...    cs.RO   \n",
       "2                                      Andrea Gesmundo    cs.LG   \n",
       "3    Boyuan Chen, Shuran Song, Hod Lipson, Carl Von...    cs.AI   \n",
       "4    Xinshuo Weng, Jianren Wang, Sergey Levine, Kri...    cs.CV   \n",
       "..                                                 ...      ...   \n",
       "995  Shray Bansal, Akansel Cosgun, Alireza Nakhaei,...    cs.AI   \n",
       "996  Aaron Huang, Benjamin J. Ayton, Brian C. Williams    cs.SY   \n",
       "997        Suda Bharadwaj, Rayna Dimitrova, Ufuk Topcu    cs.AI   \n",
       "998      Esmaeil Seraj, Andrew Silva, Matthew Gombolay    cs.RO   \n",
       "999  Liting Sun, Wei Zhan, Ching-Yao Chan, Masayosh...    cs.RO   \n",
       "\n",
       "                                         combined_text  \n",
       "0    learning of coordination policies for robotic ...  \n",
       "1    decentralized motion planning for multi-robot ...  \n",
       "2    a multiagent framework for the asynchronous an...  \n",
       "3    visual hide and seek. we train embodied agents...  \n",
       "4    inverting the pose forecasting pipeline with s...  \n",
       "..                                                 ...  \n",
       "995  collaborative planning for mixed-autonomy lane...  \n",
       "996  radmpc: a fast decentralized approach for chan...  \n",
       "997  distributed synthesis of surveillance strategi...  \n",
       "998  safe coordination of human-robot firefighting ...  \n",
       "999  behavior planning of autonomous cars with soci...  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fe0b6",
   "metadata": {},
   "source": [
    "## Get embeddings from Cohere API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d43f96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning of coordination policies for robotic swarms. inspired by biological swarms, robotic swarms are envisioned to solve real-world problems that are difficult for individual agents. biological swarms can achieve collective intelligence based on local interactions and simple rules; however, designing effective distributed policies for large-scale robotic swarms to achieve a global objective can be challenging. although it is often possible to design an optimal centralized strategy for smaller numbers of agents, those methods can fail as the number of agents increases. motivated by the growing success of machine learning, we develop a deep learning approach that learns distributed coordination policies from centralized policies. in contrast to traditional distributed control approaches, which are usually based on human-designed policies for relatively simple tasks, this learning-based approach can be adapted to more difficult tasks. we demonstrate the efficacy of our proposed approach on two different tasks, the well-known rendezvous problem and a more difficult particle assignment problem. for the latter, no known distributed policy exists. from extensive simulations, it is shown that the performance of the learned coordination policies is comparable to the centralized policies, surpassing state-of-the-art distributed policies. thereby, our proposed approach provides a promising alternative for real-world coordination problems that would be otherwise computationally expensive to solve or intangible to explore.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df['combined_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665fe8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted Cohere API logic into CohereModel class\n",
    "cohere_model = CohereModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7e4223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 473 ms, sys: 102 ms, total: 575 ms\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"speech to text whisper wav2vec\"\n",
    "\n",
    "res_embeddings = cohere_model.get_embeddings(texts=[query]+list(articles_df['combined_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2087e014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/b79vxxyn1mx4cwpgx_m0p48r0000gn/T/ipykernel_27444/1774032411.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df['similarity'] = similarity_scores\n"
     ]
    }
   ],
   "source": [
    "query_embedding = res_embeddings[0]\n",
    "article_embeddings = res_embeddings[1:]\n",
    "\n",
    "top_k_indices, similarity_scores = compute_top_k(query_embedding, article_embeddings, k=20)\n",
    "results_df = articles_df.iloc[top_k_indices]\n",
    "results_df['similarity'] = similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc6b9580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>updated_ts</th>\n",
       "      <th>published_ts</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>http://arxiv.org/abs/2106.04283v1</td>\n",
       "      <td>2021-06-08T12:22:29Z</td>\n",
       "      <td>2021-06-08T12:22:29Z</td>\n",
       "      <td>NWT: Towards natural audio-to-video generation...</td>\n",
       "      <td>In this work we introduce NWT, an expressive s...</td>\n",
       "      <td>Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, ...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>nwt: towards natural audio-to-video generation...</td>\n",
       "      <td>0.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>http://arxiv.org/abs/2106.09296v3</td>\n",
       "      <td>2022-01-14T16:43:19Z</td>\n",
       "      <td>2021-06-17T07:59:15Z</td>\n",
       "      <td>Voice2Series: Reprogramming Acoustic Models fo...</td>\n",
       "      <td>Learning to classify time series with limited ...</td>\n",
       "      <td>Chao-Han Huck Yang, Yun-Yun Tsai, Pin-Yu Chen</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>voice2series: reprogramming acoustic models fo...</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>http://arxiv.org/abs/2202.08509v1</td>\n",
       "      <td>2022-02-17T08:26:25Z</td>\n",
       "      <td>2022-02-17T08:26:25Z</td>\n",
       "      <td>A Study of Designing Compact Audio-Visual Wake...</td>\n",
       "      <td>Audio-only-based wake word spotting (WWS) is c...</td>\n",
       "      <td>Hengshun Zhou, Jun Du, Chao-Han Huck Yang, Shi...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>a study of designing compact audio-visual wake...</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>http://arxiv.org/abs/1910.10942v2</td>\n",
       "      <td>2020-02-10T09:36:23Z</td>\n",
       "      <td>2019-10-24T06:54:36Z</td>\n",
       "      <td>A Recurrent Variational Autoencoder for Speech...</td>\n",
       "      <td>This paper presents a generative approach to s...</td>\n",
       "      <td>Simon Leglaive, Xavier Alameda-Pineda, Laurent...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>a recurrent variational autoencoder for speech...</td>\n",
       "      <td>0.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>http://arxiv.org/abs/2110.08791v1</td>\n",
       "      <td>2021-10-17T11:14:00Z</td>\n",
       "      <td>2021-10-17T11:14:00Z</td>\n",
       "      <td>Taming Visually Guided Sound Generation</td>\n",
       "      <td>Recent advances in visually-induced audio gene...</td>\n",
       "      <td>Vladimir Iashin, Esa Rahtu</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>taming visually guided sound generation. recen...</td>\n",
       "      <td>0.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>http://arxiv.org/abs/2206.00393v1</td>\n",
       "      <td>2022-06-01T11:00:07Z</td>\n",
       "      <td>2022-06-01T11:00:07Z</td>\n",
       "      <td>Towards Generalisable Audio Representations fo...</td>\n",
       "      <td>In audio-visual navigation (AVN), an intellige...</td>\n",
       "      <td>Shunqi Mao, Chaoyi Zhang, Heng Wang, Weidong Cai</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>towards generalisable audio representations fo...</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>http://arxiv.org/abs/2301.07851v1</td>\n",
       "      <td>2023-01-19T02:37:56Z</td>\n",
       "      <td>2023-01-19T02:37:56Z</td>\n",
       "      <td>From English to More Languages: Parameter-Effi...</td>\n",
       "      <td>In this work, we propose a new parameter-effic...</td>\n",
       "      <td>Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Ch...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>from english to more languages: parameter-effi...</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>http://arxiv.org/abs/1805.11797v2</td>\n",
       "      <td>2018-05-31T03:49:25Z</td>\n",
       "      <td>2018-05-30T04:15:58Z</td>\n",
       "      <td>Grow and Prune Compact, Fast, and Accurate LSTMs</td>\n",
       "      <td>Long short-term memory (LSTM) has been widely ...</td>\n",
       "      <td>Xiaoliang Dai, Hongxu Yin, Niraj K. Jha</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>grow and prune compact, fast, and accurate lst...</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>http://arxiv.org/abs/2206.02211v3</td>\n",
       "      <td>2022-12-04T08:24:02Z</td>\n",
       "      <td>2022-06-05T16:18:27Z</td>\n",
       "      <td>Variable-rate hierarchical CPC leads to acoust...</td>\n",
       "      <td>The success of deep learning comes from its ab...</td>\n",
       "      <td>Santiago Cuervo, Adrian Łańcucki, Ricard Marxe...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>variable-rate hierarchical cpc leads to acoust...</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>http://arxiv.org/abs/2202.08532v1</td>\n",
       "      <td>2022-02-17T09:17:58Z</td>\n",
       "      <td>2022-02-17T09:17:58Z</td>\n",
       "      <td>Mitigating Closed-model Adversarial Examples w...</td>\n",
       "      <td>In this work, we aim to enhance the system rob...</td>\n",
       "      <td>Chao-Han Huck Yang, Zeeshan Ahmed, Yile Gu, Jo...</td>\n",
       "      <td>eess.AS</td>\n",
       "      <td>mitigating closed-model adversarial examples w...</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>http://arxiv.org/abs/2109.02096v2</td>\n",
       "      <td>2021-10-10T16:22:00Z</td>\n",
       "      <td>2021-09-05T15:06:53Z</td>\n",
       "      <td>Timbre Transfer with Variational Auto Encoding...</td>\n",
       "      <td>This research project investigates the applica...</td>\n",
       "      <td>Russell Sammut Bonnici, Charalampos Saitis, Ma...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>timbre transfer with variational auto encoding...</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>http://arxiv.org/abs/2003.00418v1</td>\n",
       "      <td>2020-03-01T06:42:43Z</td>\n",
       "      <td>2020-03-01T06:42:43Z</td>\n",
       "      <td>Towards Automatic Face-to-Face Translation</td>\n",
       "      <td>In light of the recent breakthroughs in automa...</td>\n",
       "      <td>Prajwal K R, Rudrabha Mukhopadhyay, Jerin Phil...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>towards automatic face-to-face translation. in...</td>\n",
       "      <td>0.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>http://arxiv.org/abs/1902.03389v1</td>\n",
       "      <td>2019-02-09T07:49:42Z</td>\n",
       "      <td>2019-02-09T07:49:42Z</td>\n",
       "      <td>Generative Moment Matching Network-based Rando...</td>\n",
       "      <td>This paper proposes a generative moment matchi...</td>\n",
       "      <td>Hiroki Tamaru, Yuki Saito, Shinnosuke Takamich...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>generative moment matching network-based rando...</td>\n",
       "      <td>0.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>http://arxiv.org/abs/2209.12573v3</td>\n",
       "      <td>2022-11-16T09:56:26Z</td>\n",
       "      <td>2022-09-26T10:38:39Z</td>\n",
       "      <td>Digital Audio Forensics: Blind Human Voice Mim...</td>\n",
       "      <td>Audio is one of the most used way of human com...</td>\n",
       "      <td>Sahar Al Ajmi, Khizar Hayat, Alaa M. Al Obaidi...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>digital audio forensics: blind human voice mim...</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>http://arxiv.org/abs/2211.01839v1</td>\n",
       "      <td>2022-11-03T14:20:32Z</td>\n",
       "      <td>2022-11-03T14:20:32Z</td>\n",
       "      <td>HyperSound: Generating Implicit Neural Represe...</td>\n",
       "      <td>Implicit neural representations (INRs) are a r...</td>\n",
       "      <td>Filip Szatkowski, Karol J. Piczak, Przemysław ...</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>hypersound: generating implicit neural represe...</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>http://arxiv.org/abs/1608.04363v2</td>\n",
       "      <td>2016-11-28T17:48:04Z</td>\n",
       "      <td>2016-08-15T18:57:10Z</td>\n",
       "      <td>Deep Convolutional Neural Networks and Data Au...</td>\n",
       "      <td>The ability of deep convolutional neural netwo...</td>\n",
       "      <td>Justin Salamon, Juan Pablo Bello</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>deep convolutional neural networks and data au...</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>http://arxiv.org/abs/2105.00173v2</td>\n",
       "      <td>2021-07-04T07:34:14Z</td>\n",
       "      <td>2021-05-01T05:47:15Z</td>\n",
       "      <td>Emotion Recognition of the Singing Voice: Towa...</td>\n",
       "      <td>Current computational-emotion research has foc...</td>\n",
       "      <td>Daniel Szelogowski</td>\n",
       "      <td>cs.SD</td>\n",
       "      <td>emotion recognition of the singing voice: towa...</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>http://arxiv.org/abs/2110.03894v3</td>\n",
       "      <td>2022-12-20T12:44:24Z</td>\n",
       "      <td>2021-10-08T05:07:35Z</td>\n",
       "      <td>Neural Model Reprogramming with Similarity Bas...</td>\n",
       "      <td>In this study, we propose a novel adversarial ...</td>\n",
       "      <td>Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu...</td>\n",
       "      <td>eess.AS</td>\n",
       "      <td>neural model reprogramming with similarity bas...</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>http://arxiv.org/abs/1908.07656v2</td>\n",
       "      <td>2019-12-01T03:30:37Z</td>\n",
       "      <td>2019-08-16T16:40:49Z</td>\n",
       "      <td>Survey on Deep Neural Networks in Speech and V...</td>\n",
       "      <td>This survey presents a review of state-of-the-...</td>\n",
       "      <td>Mahbubul Alam, Manar D. Samad, Lasitha Vidyara...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>survey on deep neural networks in speech and v...</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>http://arxiv.org/abs/2105.07142v2</td>\n",
       "      <td>2021-08-26T00:47:33Z</td>\n",
       "      <td>2021-05-15T04:58:08Z</td>\n",
       "      <td>Move2Hear: Active Audio-Visual Source Separation</td>\n",
       "      <td>We introduce the active audio-visual source se...</td>\n",
       "      <td>Sagnik Majumder, Ziad Al-Halah, Kristen Grauman</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>move2hear: active audio-visual source separati...</td>\n",
       "      <td>0.432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  link            updated_ts  \\\n",
       "820  http://arxiv.org/abs/2106.04283v1  2021-06-08T12:22:29Z   \n",
       "216  http://arxiv.org/abs/2106.09296v3  2022-01-14T16:43:19Z   \n",
       "824  http://arxiv.org/abs/2202.08509v1  2022-02-17T08:26:25Z   \n",
       "212  http://arxiv.org/abs/1910.10942v2  2020-02-10T09:36:23Z   \n",
       "814  http://arxiv.org/abs/2110.08791v1  2021-10-17T11:14:00Z   \n",
       "408  http://arxiv.org/abs/2206.00393v1  2022-06-01T11:00:07Z   \n",
       "223  http://arxiv.org/abs/2301.07851v1  2023-01-19T02:37:56Z   \n",
       "237  http://arxiv.org/abs/1805.11797v2  2018-05-31T03:49:25Z   \n",
       "426  http://arxiv.org/abs/2206.02211v3  2022-12-04T08:24:02Z   \n",
       "219  http://arxiv.org/abs/2202.08532v1  2022-02-17T09:17:58Z   \n",
       "822  http://arxiv.org/abs/2109.02096v2  2021-10-10T16:22:00Z   \n",
       "818  http://arxiv.org/abs/2003.00418v1  2020-03-01T06:42:43Z   \n",
       "427  http://arxiv.org/abs/1902.03389v1  2019-02-09T07:49:42Z   \n",
       "431  http://arxiv.org/abs/2209.12573v3  2022-11-16T09:56:26Z   \n",
       "222  http://arxiv.org/abs/2211.01839v1  2022-11-03T14:20:32Z   \n",
       "121  http://arxiv.org/abs/1608.04363v2  2016-11-28T17:48:04Z   \n",
       "429  http://arxiv.org/abs/2105.00173v2  2021-07-04T07:34:14Z   \n",
       "217  http://arxiv.org/abs/2110.03894v3  2022-12-20T12:44:24Z   \n",
       "807  http://arxiv.org/abs/1908.07656v2  2019-12-01T03:30:37Z   \n",
       "405  http://arxiv.org/abs/2105.07142v2  2021-08-26T00:47:33Z   \n",
       "\n",
       "             published_ts                                              title  \\\n",
       "820  2021-06-08T12:22:29Z  NWT: Towards natural audio-to-video generation...   \n",
       "216  2021-06-17T07:59:15Z  Voice2Series: Reprogramming Acoustic Models fo...   \n",
       "824  2022-02-17T08:26:25Z  A Study of Designing Compact Audio-Visual Wake...   \n",
       "212  2019-10-24T06:54:36Z  A Recurrent Variational Autoencoder for Speech...   \n",
       "814  2021-10-17T11:14:00Z            Taming Visually Guided Sound Generation   \n",
       "408  2022-06-01T11:00:07Z  Towards Generalisable Audio Representations fo...   \n",
       "223  2023-01-19T02:37:56Z  From English to More Languages: Parameter-Effi...   \n",
       "237  2018-05-30T04:15:58Z   Grow and Prune Compact, Fast, and Accurate LSTMs   \n",
       "426  2022-06-05T16:18:27Z  Variable-rate hierarchical CPC leads to acoust...   \n",
       "219  2022-02-17T09:17:58Z  Mitigating Closed-model Adversarial Examples w...   \n",
       "822  2021-09-05T15:06:53Z  Timbre Transfer with Variational Auto Encoding...   \n",
       "818  2020-03-01T06:42:43Z         Towards Automatic Face-to-Face Translation   \n",
       "427  2019-02-09T07:49:42Z  Generative Moment Matching Network-based Rando...   \n",
       "431  2022-09-26T10:38:39Z  Digital Audio Forensics: Blind Human Voice Mim...   \n",
       "222  2022-11-03T14:20:32Z  HyperSound: Generating Implicit Neural Represe...   \n",
       "121  2016-08-15T18:57:10Z  Deep Convolutional Neural Networks and Data Au...   \n",
       "429  2021-05-01T05:47:15Z  Emotion Recognition of the Singing Voice: Towa...   \n",
       "217  2021-10-08T05:07:35Z  Neural Model Reprogramming with Similarity Bas...   \n",
       "807  2019-08-16T16:40:49Z  Survey on Deep Neural Networks in Speech and V...   \n",
       "405  2021-05-15T04:58:08Z   Move2Hear: Active Audio-Visual Source Separation   \n",
       "\n",
       "                                               summary  \\\n",
       "820  In this work we introduce NWT, an expressive s...   \n",
       "216  Learning to classify time series with limited ...   \n",
       "824  Audio-only-based wake word spotting (WWS) is c...   \n",
       "212  This paper presents a generative approach to s...   \n",
       "814  Recent advances in visually-induced audio gene...   \n",
       "408  In audio-visual navigation (AVN), an intellige...   \n",
       "223  In this work, we propose a new parameter-effic...   \n",
       "237  Long short-term memory (LSTM) has been widely ...   \n",
       "426  The success of deep learning comes from its ab...   \n",
       "219  In this work, we aim to enhance the system rob...   \n",
       "822  This research project investigates the applica...   \n",
       "818  In light of the recent breakthroughs in automa...   \n",
       "427  This paper proposes a generative moment matchi...   \n",
       "431  Audio is one of the most used way of human com...   \n",
       "222  Implicit neural representations (INRs) are a r...   \n",
       "121  The ability of deep convolutional neural netwo...   \n",
       "429  Current computational-emotion research has foc...   \n",
       "217  In this study, we propose a novel adversarial ...   \n",
       "807  This survey presents a review of state-of-the-...   \n",
       "405  We introduce the active audio-visual source se...   \n",
       "\n",
       "                                                author category  \\\n",
       "820  Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, ...    cs.SD   \n",
       "216      Chao-Han Huck Yang, Yun-Yun Tsai, Pin-Yu Chen    cs.LG   \n",
       "824  Hengshun Zhou, Jun Du, Chao-Han Huck Yang, Shi...    cs.SD   \n",
       "212  Simon Leglaive, Xavier Alameda-Pineda, Laurent...    cs.LG   \n",
       "814                         Vladimir Iashin, Esa Rahtu    cs.CV   \n",
       "408   Shunqi Mao, Chaoyi Zhang, Heng Wang, Weidong Cai    cs.SD   \n",
       "223  Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Ch...    cs.SD   \n",
       "237            Xiaoliang Dai, Hongxu Yin, Niraj K. Jha    cs.LG   \n",
       "426  Santiago Cuervo, Adrian Łańcucki, Ricard Marxe...    cs.SD   \n",
       "219  Chao-Han Huck Yang, Zeeshan Ahmed, Yile Gu, Jo...  eess.AS   \n",
       "822  Russell Sammut Bonnici, Charalampos Saitis, Ma...    cs.SD   \n",
       "818  Prajwal K R, Rudrabha Mukhopadhyay, Jerin Phil...    cs.CV   \n",
       "427  Hiroki Tamaru, Yuki Saito, Shinnosuke Takamich...    cs.SD   \n",
       "431  Sahar Al Ajmi, Khizar Hayat, Alaa M. Al Obaidi...    cs.SD   \n",
       "222  Filip Szatkowski, Karol J. Piczak, Przemysław ...    cs.SD   \n",
       "121                   Justin Salamon, Juan Pablo Bello    cs.SD   \n",
       "429                                 Daniel Szelogowski    cs.SD   \n",
       "217  Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu...  eess.AS   \n",
       "807  Mahbubul Alam, Manar D. Samad, Lasitha Vidyara...    cs.CV   \n",
       "405    Sagnik Majumder, Ziad Al-Halah, Kristen Grauman    cs.CV   \n",
       "\n",
       "                                         combined_text  similarity  \n",
       "820  nwt: towards natural audio-to-video generation...       0.598  \n",
       "216  voice2series: reprogramming acoustic models fo...       0.591  \n",
       "824  a study of designing compact audio-visual wake...       0.565  \n",
       "212  a recurrent variational autoencoder for speech...       0.555  \n",
       "814  taming visually guided sound generation. recen...       0.521  \n",
       "408  towards generalisable audio representations fo...       0.493  \n",
       "223  from english to more languages: parameter-effi...       0.480  \n",
       "237  grow and prune compact, fast, and accurate lst...       0.478  \n",
       "426  variable-rate hierarchical cpc leads to acoust...       0.478  \n",
       "219  mitigating closed-model adversarial examples w...       0.471  \n",
       "822  timbre transfer with variational auto encoding...       0.470  \n",
       "818  towards automatic face-to-face translation. in...       0.468  \n",
       "427  generative moment matching network-based rando...       0.456  \n",
       "431  digital audio forensics: blind human voice mim...       0.453  \n",
       "222  hypersound: generating implicit neural represe...       0.450  \n",
       "121  deep convolutional neural networks and data au...       0.450  \n",
       "429  emotion recognition of the singing voice: towa...       0.446  \n",
       "217  neural model reprogramming with similarity bas...       0.444  \n",
       "807  survey on deep neural networks in speech and v...       0.437  \n",
       "405  move2hear: active audio-visual source separati...       0.432  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d09bd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'link': 'http://arxiv.org/abs/2106.04283v1',\n",
       "  'updated_ts': '2021-06-08T12:22:29Z',\n",
       "  'published_ts': '2021-06-08T12:22:29Z',\n",
       "  'title': 'NWT: Towards natural audio-to-video generation with representation\\n  learning',\n",
       "  'summary': \"In this work we introduce NWT, an expressive speech-to-video model. Unlike approaches that use domain-specific intermediate representations such as pose keypoints, NWT learns its own latent representations, with minimal assumptions about the audio and video content. To this end, we propose a novel discrete variational autoencoder with adversarial loss, dVAE-Adv, which learns a new discrete latent representation we call Memcodes. Memcodes are straightforward to implement, require no additional loss terms, are stable to train compared with other approaches, and show evidence of interpretability. To predict on the Memcode space, we use an autoregressive encoder-decoder model conditioned on audio. Additionally, our model can control latent attributes in the generated video that are not annotated in the data. We train NWT on clips from HBO's Last Week Tonight with John Oliver. NWT consistently scores above other approaches in Mean Opinion Score (MOS) on tests of overall video naturalness, facial naturalness and expressiveness, and lipsync quality. This work sets a strong baseline for generalized audio-to-video synthesis. Samples are available at https://next-week-tonight.github.io/NWT/.\",\n",
       "  'author': 'Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, Cole Clifford, Ragavan Thurairatnam',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.598},\n",
       " {'link': 'http://arxiv.org/abs/2106.09296v3',\n",
       "  'updated_ts': '2022-01-14T16:43:19Z',\n",
       "  'published_ts': '2021-06-17T07:59:15Z',\n",
       "  'title': 'Voice2Series: Reprogramming Acoustic Models for Time Series\\n  Classification',\n",
       "  'summary': 'Learning to classify time series with limited data is a practical yet challenging problem. Current methods are primarily based on hand-designed feature extraction rules or domain-specific data augmentation. Motivated by the advances in deep speech processing models and the fact that voice data are univariate temporal signals, in this paper, we propose Voice2Series (V2S), a novel end-to-end approach that reprograms acoustic models for time series classification, through input transformation learning and output label mapping. Leveraging the representation learning power of a large-scale pre-trained speech processing model, on 30 different time series tasks we show that V2S performs competitive results on 19 time series classification tasks. We further provide a theoretical justification of V2S by proving its population risk is upper bounded by the source risk and a Wasserstein distance accounting for feature alignment via reprogramming. Our results offer new and effective means to time series classification.',\n",
       "  'author': 'Chao-Han Huck Yang, Yun-Yun Tsai, Pin-Yu Chen',\n",
       "  'category': 'cs.LG',\n",
       "  'similarity': 0.591},\n",
       " {'link': 'http://arxiv.org/abs/2202.08509v1',\n",
       "  'updated_ts': '2022-02-17T08:26:25Z',\n",
       "  'published_ts': '2022-02-17T08:26:25Z',\n",
       "  'title': 'A Study of Designing Compact Audio-Visual Wake Word Spotting System\\n  Based on Iterative Fine-Tuning in Neural Network Pruning',\n",
       "  'summary': 'Audio-only-based wake word spotting (WWS) is challenging under noisy conditions due to environmental interference in signal transmission. In this paper, we investigate on designing a compact audio-visual WWS system by utilizing visual information to alleviate the degradation. Specifically, in order to use visual information, we first encode the detected lips to fixed-size vectors with MobileNet and concatenate them with acoustic features followed by the fusion network for WWS. However, the audio-visual model based on neural networks requires a large footprint and a high computational complexity. To meet the application requirements, we introduce a neural network pruning strategy via the lottery ticket hypothesis in an iterative fine-tuning manner (LTH-IF), to the single-modal and multi-modal models, respectively. Tested on our in-house corpus for audio-visual WWS in a home TV scene, the proposed audio-visual system achieves significant performance improvements over the single-modality (audio-only or video-only) system under different noisy conditions. Moreover, LTH-IF pruning can largely reduce the network parameters and computations with no degradation of WWS performance, leading to a potential product solution for the TV wake-up scenario.',\n",
       "  'author': 'Hengshun Zhou, Jun Du, Chao-Han Huck Yang, Shifu Xiong, Chin-Hui Lee',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.565},\n",
       " {'link': 'http://arxiv.org/abs/1910.10942v2',\n",
       "  'updated_ts': '2020-02-10T09:36:23Z',\n",
       "  'published_ts': '2019-10-24T06:54:36Z',\n",
       "  'title': 'A Recurrent Variational Autoencoder for Speech Enhancement',\n",
       "  'summary': 'This paper presents a generative approach to speech enhancement based on a recurrent variational autoencoder (RVAE). The deep generative speech model is trained using clean speech signals only, and it is combined with a nonnegative matrix factorization noise model for speech enhancement. We propose a variational expectation-maximization algorithm where the encoder of the RVAE is fine-tuned at test time, to approximate the distribution of the latent variables given the noisy speech observations. Compared with previous approaches based on feed-forward fully-connected architectures, the proposed recurrent deep generative speech model induces a posterior temporal dynamic over the latent variables, which is shown to improve the speech enhancement results.',\n",
       "  'author': 'Simon Leglaive, Xavier Alameda-Pineda, Laurent Girin, Radu Horaud',\n",
       "  'category': 'cs.LG',\n",
       "  'similarity': 0.555},\n",
       " {'link': 'http://arxiv.org/abs/2110.08791v1',\n",
       "  'updated_ts': '2021-10-17T11:14:00Z',\n",
       "  'published_ts': '2021-10-17T11:14:00Z',\n",
       "  'title': 'Taming Visually Guided Sound Generation',\n",
       "  'summary': 'Recent advances in visually-induced audio generation are based on sampling short, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio from the state-of-the-art model takes minutes on a high-end GPU. In this work, we propose a single model capable of generating visually relevant, high-fidelity sounds prompted with a set of frames from open-domain videos in less time than it takes to play it on a single GPU.   We train a transformer to sample a new spectrogram from the pre-trained spectrogram codebook given the set of video features. The codebook is obtained using a variant of VQGAN trained to produce a compact sampling space with a novel spectrogram-based perceptual loss. The generated spectrogram is transformed into a waveform using a window-based GAN that significantly speeds up generation. Considering the lack of metrics for automatic evaluation of generated spectrograms, we also build a family of metrics called FID and MKL. These metrics are based on a novel sound classifier, called Melception, and designed to evaluate the fidelity and relevance of open-domain samples.   Both qualitative and quantitative studies are conducted on small- and large-scale datasets to evaluate the fidelity and relevance of generated samples. We also compare our model to the state-of-the-art and observe a substantial improvement in quality, size, and computation time. Code, demo, and samples: v-iashin.github.io/SpecVQGAN',\n",
       "  'author': 'Vladimir Iashin, Esa Rahtu',\n",
       "  'category': 'cs.CV',\n",
       "  'similarity': 0.521},\n",
       " {'link': 'http://arxiv.org/abs/2206.00393v1',\n",
       "  'updated_ts': '2022-06-01T11:00:07Z',\n",
       "  'published_ts': '2022-06-01T11:00:07Z',\n",
       "  'title': 'Towards Generalisable Audio Representations for Audio-Visual Navigation',\n",
       "  'summary': 'In audio-visual navigation (AVN), an intelligent agent needs to navigate to a constantly sound-making object in complex 3D environments based on its audio and visual perceptions. While existing methods attempt to improve the navigation performance with preciously designed path planning or intricate task settings, none has improved the model generalisation on unheard sounds with task settings unchanged. We thus propose a contrastive learning-based method to tackle this challenge by regularising the audio encoder, where the sound-agnostic goal-driven latent representations can be learnt from various audio signals of different classes. In addition, we consider two data augmentation strategies to enrich the training sounds. We demonstrate that our designs can be easily equipped to existing AVN frameworks to obtain an immediate performance gain (13.4%$\\\\uparrow$ in SPL on Replica and 12.2%$\\\\uparrow$ in SPL on MP3D). Our project is available at https://AV-GeN.github.io/.',\n",
       "  'author': 'Shunqi Mao, Chaoyi Zhang, Heng Wang, Weidong Cai',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.493},\n",
       " {'link': 'http://arxiv.org/abs/2301.07851v1',\n",
       "  'updated_ts': '2023-01-19T02:37:56Z',\n",
       "  'published_ts': '2023-01-19T02:37:56Z',\n",
       "  'title': 'From English to More Languages: Parameter-Efficient Model Reprogramming\\n  for Cross-Lingual Speech Recognition',\n",
       "  'summary': 'In this work, we propose a new parameter-efficient learning framework based on neural model reprogramming for cross-lingual speech recognition, which can \\\\textbf{re-purpose} well-trained English automatic speech recognition (ASR) models to recognize the other languages. We design different auxiliary neural architectures focusing on learnable pre-trained feature enhancement that, for the first time, empowers model reprogramming on ASR. Specifically, we investigate how to select trainable components (i.e., encoder) of a conformer-based RNN-Transducer, as a frozen pre-trained backbone. Experiments on a seven-language multilingual LibriSpeech speech (MLS) task show that model reprogramming only requires 4.2% (11M out of 270M) to 6.8% (45M out of 660M) of its original trainable parameters from a full ASR model to perform competitive results in a range of 11.9% to 8.1% WER averaged across different languages. In addition, we discover different setups to make large-scale pre-trained ASR succeed in both monolingual and multilingual speech recognition. Our methods outperform existing ASR tuning architectures and their extension with self-supervised losses (e.g., w2v-bert) in terms of lower WER and better training efficiency.',\n",
       "  'author': 'Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Rohit Prabhavalkar, Tara N. Sainath, Trevor Strohman',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.48},\n",
       " {'link': 'http://arxiv.org/abs/1805.11797v2',\n",
       "  'updated_ts': '2018-05-31T03:49:25Z',\n",
       "  'published_ts': '2018-05-30T04:15:58Z',\n",
       "  'title': 'Grow and Prune Compact, Fast, and Accurate LSTMs',\n",
       "  'summary': \"Long short-term memory (LSTM) has been widely used for sequential data modeling. Researchers have increased LSTM depth by stacking LSTM cells to improve performance. This incurs model redundancy, increases run-time delay, and makes the LSTMs more prone to overfitting. To address these problems, we propose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original one level non-linear control gates. H-LSTM increases accuracy while employing fewer external stacked layers, thus reducing the number of parameters and run-time latency significantly. We employ grow-and-prune (GP) training to iteratively adjust the hidden layers through gradient-based growth and magnitude-based pruning of connections. This learns both the weights and the compact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for image captioning and speech recognition applications. For the NeuralTalk architecture on the MSCOCO dataset, our three models reduce the number of parameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time latency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2 architecture on the AN4 dataset, our two models reduce the number of parameters by 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate from 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast, and accurate.\",\n",
       "  'author': 'Xiaoliang Dai, Hongxu Yin, Niraj K. Jha',\n",
       "  'category': 'cs.LG',\n",
       "  'similarity': 0.478},\n",
       " {'link': 'http://arxiv.org/abs/2206.02211v3',\n",
       "  'updated_ts': '2022-12-04T08:24:02Z',\n",
       "  'published_ts': '2022-06-05T16:18:27Z',\n",
       "  'title': 'Variable-rate hierarchical CPC leads to acoustic unit discovery in\\n  speech',\n",
       "  'summary': 'The success of deep learning comes from its ability to capture the hierarchical structure of data by learning high-level representations defined in terms of low-level ones. In this paper we explore self-supervised learning of hierarchical representations of speech by applying multiple levels of Contrastive Predictive Coding (CPC). We observe that simply stacking two CPC models does not yield significant improvements over single-level architectures. Inspired by the fact that speech is often described as a sequence of discrete units unevenly distributed in time, we propose a model in which the output of a low-level CPC module is non-uniformly downsampled to directly minimize the loss of a high-level CPC module. The latter is designed to also enforce a prior of separability and discreteness in its representations by enforcing dissimilarity of successive high-level representations through focused negative sampling, and by quantization of the prediction targets. Accounting for the structure of the speech signal improves upon single-level CPC features and enhances the disentanglement of the learned representations, as measured by downstream speech recognition tasks, while resulting in a meaningful segmentation of the signal that closely resembles phone boundaries.',\n",
       "  'author': 'Santiago Cuervo, Adrian Łańcucki, Ricard Marxer, Paweł Rychlikowski, Jan Chorowski',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.478},\n",
       " {'link': 'http://arxiv.org/abs/2202.08532v1',\n",
       "  'updated_ts': '2022-02-17T09:17:58Z',\n",
       "  'published_ts': '2022-02-17T09:17:58Z',\n",
       "  'title': 'Mitigating Closed-model Adversarial Examples with Bayesian Neural\\n  Modeling for Enhanced End-to-End Speech Recognition',\n",
       "  'summary': 'In this work, we aim to enhance the system robustness of end-to-end automatic speech recognition (ASR) against adversarially-noisy speech examples. We focus on a rigorous and empirical \"closed-model adversarial robustness\" setting (e.g., on-device or cloud applications). The adversarial noise is only generated by closed-model optimization (e.g., evolutionary and zeroth-order estimation) without accessing gradient information of a targeted ASR model directly. We propose an advanced Bayesian neural network (BNN) based adversarial detector, which could model latent distributions against adaptive adversarial perturbation with divergence measurement. We further simulate deployment scenarios of RNN Transducer, Conformer, and wav2vec-2.0 based ASR systems with the proposed adversarial detection system. Leveraging the proposed BNN based detection system, we improve detection rate by +2.77 to +5.42% (relative +3.03 to +6.26%) and reduce the word error rate by 5.02 to 7.47% on LibriSpeech datasets compared to the current model enhancement methods against the adversarial speech examples.',\n",
       "  'author': 'Chao-Han Huck Yang, Zeeshan Ahmed, Yile Gu, Joseph Szurley, Roger Ren, Linda Liu, Andreas Stolcke, Ivan Bulyko',\n",
       "  'category': 'eess.AS',\n",
       "  'similarity': 0.471},\n",
       " {'link': 'http://arxiv.org/abs/2109.02096v2',\n",
       "  'updated_ts': '2021-10-10T16:22:00Z',\n",
       "  'published_ts': '2021-09-05T15:06:53Z',\n",
       "  'title': 'Timbre Transfer with Variational Auto Encoding and Cycle-Consistent\\n  Adversarial Networks',\n",
       "  'summary': \"This research project investigates the application of deep learning to timbre transfer, where the timbre of a source audio can be converted to the timbre of a target audio with minimal loss in quality. The adopted approach combines Variational Autoencoders with Generative Adversarial Networks to construct meaningful representations of the source audio and produce realistic generations of the target audio and is applied to the Flickr 8k Audio dataset for transferring the vocal timbre between speakers and the URMP dataset for transferring the musical timbre between instruments. Furthermore, variations of the adopted approach are trained, and generalised performance is compared using the metrics SSIM (Structural Similarity Index) and FAD (Frech\\\\'et Audio Distance). It was found that a many-to-many approach supersedes a one-to-one approach in terms of reconstructive capabilities, and that the adoption of a basic over a bottleneck residual block design is more suitable for enriching content information about a latent space. It was also found that the decision on whether cyclic loss takes on a variational autoencoder or vanilla autoencoder approach does not have a significant impact on reconstructive and adversarial translation aspects of the model.\",\n",
       "  'author': 'Russell Sammut Bonnici, Charalampos Saitis, Martin Benning',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.47},\n",
       " {'link': 'http://arxiv.org/abs/2003.00418v1',\n",
       "  'updated_ts': '2020-03-01T06:42:43Z',\n",
       "  'published_ts': '2020-03-01T06:42:43Z',\n",
       "  'title': 'Towards Automatic Face-to-Face Translation',\n",
       "  'summary': 'In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as \"Face-to-Face Translation\". As today\\'s digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact on multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards \"Face-to-Face Translation\" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available.   Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0   Code and models: https://github.com/Rudrabha/LipGAN',\n",
       "  'author': 'Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, C. V. Jawahar',\n",
       "  'category': 'cs.CV',\n",
       "  'similarity': 0.468},\n",
       " {'link': 'http://arxiv.org/abs/1902.03389v1',\n",
       "  'updated_ts': '2019-02-09T07:49:42Z',\n",
       "  'published_ts': '2019-02-09T07:49:42Z',\n",
       "  'title': 'Generative Moment Matching Network-based Random Modulation Post-filter\\n  for DNN-based Singing Voice Synthesis and Neural Double-tracking',\n",
       "  'summary': 'This paper proposes a generative moment matching network (GMMN)-based post-filter that provides inter-utterance pitch variation for deep neural network (DNN)-based singing voice synthesis. The natural pitch variation of a human singing voice leads to a richer musical experience and is used in double-tracking, a recording method in which two performances of the same phrase are recorded and mixed to create a richer, layered sound. However, singing voices synthesized using conventional DNN-based methods never vary because the synthesis process is deterministic and only one waveform is synthesized from one musical score. To address this problem, we use a GMMN to model the variation of the modulation spectrum of the pitch contour of natural singing voices and add a randomized inter-utterance variation to the pitch contour generated by conventional DNN-based singing voice synthesis. Experimental evaluations suggest that 1) our approach can provide perceptible inter-utterance pitch variation while preserving speech quality. We extend our approach to double-tracking, and the evaluation demonstrates that 2) GMMN-based neural double-tracking is perceptually closer to natural double-tracking than conventional signal processing-based artificial double-tracking is.',\n",
       "  'author': 'Hiroki Tamaru, Yuki Saito, Shinnosuke Takamichi, Tomoki Koriyama, Hiroshi Saruwatari',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.456},\n",
       " {'link': 'http://arxiv.org/abs/2209.12573v3',\n",
       "  'updated_ts': '2022-11-16T09:56:26Z',\n",
       "  'published_ts': '2022-09-26T10:38:39Z',\n",
       "  'title': 'Digital Audio Forensics: Blind Human Voice Mimicry Detection',\n",
       "  'summary': 'Audio is one of the most used way of human communication, but at the same time it can be easily misused by to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. Two datasets were created for this work; an all English data set and a mixed data set (Arabic and English). These datasets have been made available through GitHub for the use of the research community at https://github.com/SaSs7/Dataset. For the purpose of comparison, the audios were also classified through human inspection with the subjects being the native speakers. The ensued results were interesting and exhibited formidable accuracy.',\n",
       "  'author': 'Sahar Al Ajmi, Khizar Hayat, Alaa M. Al Obaidi, Naresh Kumar, Munaf Najmuldeen, Baptiste Magnier',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.453},\n",
       " {'link': 'http://arxiv.org/abs/2211.01839v1',\n",
       "  'updated_ts': '2022-11-03T14:20:32Z',\n",
       "  'published_ts': '2022-11-03T14:20:32Z',\n",
       "  'title': 'HyperSound: Generating Implicit Neural Representations of Audio Signals\\n  with Hypernetworks',\n",
       "  'summary': 'Implicit neural representations (INRs) are a rapidly growing research field, which provides alternative ways to represent multimedia signals. Recent applications of INRs include image super-resolution, compression of high-dimensional signals, or 3D rendering. However, these solutions usually focus on visual data, and adapting them to the audio domain is not trivial. Moreover, it requires a separately trained model for every data sample. To address this limitation, we propose HyperSound, a meta-learning method leveraging hypernetworks to produce INRs for audio signals unseen at training time. We show that our approach can reconstruct sound waves with quality comparable to other state-of-the-art models.',\n",
       "  'author': 'Filip Szatkowski, Karol J. Piczak, Przemysław Spurek, Jacek Tabor, Tomasz Trzciński',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.45},\n",
       " {'link': 'http://arxiv.org/abs/1608.04363v2',\n",
       "  'updated_ts': '2016-11-28T17:48:04Z',\n",
       "  'published_ts': '2016-08-15T18:57:10Z',\n",
       "  'title': 'Deep Convolutional Neural Networks and Data Augmentation for\\n  Environmental Sound Classification',\n",
       "  'summary': 'The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a \"shallow\" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model\\'s classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.',\n",
       "  'author': 'Justin Salamon, Juan Pablo Bello',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.45},\n",
       " {'link': 'http://arxiv.org/abs/2105.00173v2',\n",
       "  'updated_ts': '2021-07-04T07:34:14Z',\n",
       "  'published_ts': '2021-05-01T05:47:15Z',\n",
       "  'title': 'Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis\\n  Tool for Singers',\n",
       "  'summary': 'Current computational-emotion research has focused on applying acoustic properties to analyze how emotions are perceived mathematically or used in natural language processing machine learning models. While recent interest has focused on analyzing emotions from the spoken voice, little experimentation has been performed to discover how emotions are recognized in the singing voice -- both in noiseless and noisy data (i.e., data that is either inaccurate, difficult to interpret, has corrupted/distorted/nonsense information like actual noise sounds in this case, or has a low ratio of usable/unusable information). Not only does this ignore the challenges of training machine learning models on more subjective data and testing them with much noisier data, but there is also a clear disconnect in progress between advancing the development of convolutional neural networks and the goal of emotionally cognizant artificial intelligence. By training a new model to include this type of information with a rich comprehension of psycho-acoustic properties, not only can models be trained to recognize information within extremely noisy data, but advancement can be made toward more complex biofeedback applications -- including creating a model which could recognize emotions given any human information (language, breath, voice, body, posture) and be used in any performance medium (music, speech, acting) or psychological assistance for patients with disorders such as BPD, alexithymia, autism, among others. This paper seeks to reflect and expand upon the findings of related research and present a stepping-stone toward this end goal.',\n",
       "  'author': 'Daniel Szelogowski',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.446},\n",
       " {'link': 'http://arxiv.org/abs/2110.03894v3',\n",
       "  'updated_ts': '2022-12-20T12:44:24Z',\n",
       "  'published_ts': '2021-10-08T05:07:35Z',\n",
       "  'title': 'Neural Model Reprogramming with Similarity Based Mapping for\\n  Low-Resource Spoken Command Classification',\n",
       "  'summary': 'In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of training data.',\n",
       "  'author': 'Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Yu Tsao',\n",
       "  'category': 'eess.AS',\n",
       "  'similarity': 0.444},\n",
       " {'link': 'http://arxiv.org/abs/1908.07656v2',\n",
       "  'updated_ts': '2019-12-01T03:30:37Z',\n",
       "  'published_ts': '2019-08-16T16:40:49Z',\n",
       "  'title': 'Survey on Deep Neural Networks in Speech and Vision Systems',\n",
       "  'summary': 'This survey presents a review of state-of-the-art deep neural network architectures, algorithms, and systems in vision and speech applications. Recent advances in deep artificial neural network algorithms and architectures have spurred rapid innovation and development of intelligent vision and speech systems. With availability of vast amounts of sensor data and cloud computing for processing and training of deep neural networks, and with increased sophistication in mobile and embedded technology, the next-generation intelligent systems are poised to revolutionize personal and commercial computing. This survey begins by providing background and evolution of some of the most successful deep learning models for intelligent vision and speech systems to date. An overview of large-scale industrial research and development efforts is provided to emphasize future trends and prospects of intelligent vision and speech systems. Robust and efficient intelligent systems demand low-latency and high fidelity in resource-constrained hardware platforms such as mobile devices, robots, and automobiles. Therefore, this survey also provides a summary of key challenges and recent successes in running deep neural networks on hardware-restricted platforms, i.e. within limited memory, battery life, and processing capabilities. Finally, emerging applications of vision and speech across disciplines such as affective computing, intelligent transportation, and precision medicine are discussed. To our knowledge, this paper provides one of the most comprehensive surveys on the latest developments in intelligent vision and speech applications from the perspectives of both software and hardware systems. Many of these emerging technologies using deep neural networks show tremendous promise to revolutionize research and development for future vision and speech systems.',\n",
       "  'author': 'Mahbubul Alam, Manar D. Samad, Lasitha Vidyaratne, Alexander Glandon, Khan M. Iftekharuddin',\n",
       "  'category': 'cs.CV',\n",
       "  'similarity': 0.437},\n",
       " {'link': 'http://arxiv.org/abs/2105.07142v2',\n",
       "  'updated_ts': '2021-08-26T00:47:33Z',\n",
       "  'published_ts': '2021-05-15T04:58:08Z',\n",
       "  'title': 'Move2Hear: Active Audio-Visual Source Separation',\n",
       "  'summary': \"We introduce the active audio-visual source separation problem, where an agent must move intelligently in order to better isolate the sounds coming from an object of interest in its environment. The agent hears multiple audio sources simultaneously (e.g., a person speaking down the hall in a noisy household) and it must use its eyes and ears to automatically separate out the sounds originating from a target object within a limited time budget. Towards this goal, we introduce a reinforcement learning approach that trains movement policies controlling the agent's camera and microphone placement over time, guided by the improvement in predicted audio separation quality. We demonstrate our approach in scenarios motivated by both augmented reality (system is already co-located with the target object) and mobile robotics (agent begins arbitrarily far from the target object). Using state-of-the-art realistic audio-visual simulations in 3D environments, we demonstrate our model's ability to find minimal movement sequences with maximal payoff for audio source separation. Project: http://vision.cs.utexas.edu/projects/move2hear.\",\n",
       "  'author': 'Sagnik Majumder, Ziad Al-Halah, Kristen Grauman',\n",
       "  'category': 'cs.CV',\n",
       "  'similarity': 0.432}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[['link', 'updated_ts', 'published_ts', 'title', 'summary', 'author', 'category', 'similarity']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb711d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:arxiv-scout]",
   "language": "python",
   "name": "conda-env-arxiv-scout-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
