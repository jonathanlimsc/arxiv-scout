{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db58bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6b578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure our src module can be found and imported\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import cohere\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data.arxiv_downloader import ArxivDownloader\n",
    "from src.models.cohere import CohereModel\n",
    "from src.utils import (\n",
    "    compute_top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df9268b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJ_DIR = Path.cwd().parent\n",
    "DOTENV_PATH = PROJ_DIR / '.env'\n",
    "dotenv.load_dotenv(DOTENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c523c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted Arxiv downloading logic into ArxivDownloader class that has simple caching functionality\n",
    "downloader = ArxivDownloader(download_refresh_interval_days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08c1bfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 1e+03 ns, total: 18 µs\n",
      "Wall time: 20 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles_df, is_from_cache = downloader.retrieve_arxiv_articles_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c2bfdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True if the articles_df was retrieved from cache, False otherwise. \n",
    "# This can be passed to CohereModel to return a cached version of document embeddings for fast response times\n",
    "is_from_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98282389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>updated_ts</th>\n",
       "      <th>published_ts</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/1709.06620v1</td>\n",
       "      <td>2017-09-19T19:26:20Z</td>\n",
       "      <td>2017-09-19T19:26:20Z</td>\n",
       "      <td>Learning of Coordination Policies for Robotic ...</td>\n",
       "      <td>Inspired by biological swarms, robotic swarms ...</td>\n",
       "      <td>Qiyang Li, Xintong Du, Yizhou Huang, Quinlan S...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>learning of coordination policies for robotic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2011.05605v2</td>\n",
       "      <td>2020-11-20T18:19:32Z</td>\n",
       "      <td>2020-11-11T07:35:21Z</td>\n",
       "      <td>Decentralized Motion Planning for Multi-Robot ...</td>\n",
       "      <td>This work presents a decentralized motion plan...</td>\n",
       "      <td>Sivanathan Kandhasamy, Vinayagam Babu Kuppusam...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>decentralized motion planning for multi-robot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2209.14745v2</td>\n",
       "      <td>2022-12-29T08:48:05Z</td>\n",
       "      <td>2022-09-29T13:02:58Z</td>\n",
       "      <td>A Multiagent Framework for the Asynchronous an...</td>\n",
       "      <td>The traditional ML development methodology doe...</td>\n",
       "      <td>Andrea Gesmundo</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>a multiagent framework for the asynchronous an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2003.08376v3</td>\n",
       "      <td>2020-11-07T02:48:22Z</td>\n",
       "      <td>2020-03-18T17:54:28Z</td>\n",
       "      <td>Inverting the Pose Forecasting Pipeline with S...</td>\n",
       "      <td>Many autonomous systems forecast aspects of th...</td>\n",
       "      <td>Xinshuo Weng, Jianren Wang, Sergey Levine, Kri...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>inverting the pose forecasting pipeline with s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2008.12760v1</td>\n",
       "      <td>2020-08-28T17:35:22Z</td>\n",
       "      <td>2020-08-28T17:35:22Z</td>\n",
       "      <td>AllenAct: A Framework for Embodied AI Research</td>\n",
       "      <td>The domain of Embodied AI, in which agents lea...</td>\n",
       "      <td>Luca Weihs, Jordi Salvador, Klemen Kotar, Unna...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>allenact: a framework for embodied ai research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>http://arxiv.org/abs/2109.15266v3</td>\n",
       "      <td>2022-05-31T12:56:48Z</td>\n",
       "      <td>2021-09-30T17:06:39Z</td>\n",
       "      <td>Modeling Interactions of Autonomous Vehicles a...</td>\n",
       "      <td>Reliable pedestrian crash avoidance mitigation...</td>\n",
       "      <td>Raphael Trumpp, Harald Bayerlein, David Gesbert</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>modeling interactions of autonomous vehicles a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>http://arxiv.org/abs/2110.08229v1</td>\n",
       "      <td>2021-10-05T16:46:04Z</td>\n",
       "      <td>2021-10-05T16:46:04Z</td>\n",
       "      <td>Influencing Towards Stable Multi-Agent Interac...</td>\n",
       "      <td>Learning in multi-agent environments is diffic...</td>\n",
       "      <td>Woodrow Z. Wang, Andy Shih, Annie Xie, Dorsa S...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>influencing towards stable multi-agent interac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>http://arxiv.org/abs/2112.09012v1</td>\n",
       "      <td>2021-12-16T16:47:00Z</td>\n",
       "      <td>2021-12-16T16:47:00Z</td>\n",
       "      <td>Centralizing State-Values in Dueling Networks ...</td>\n",
       "      <td>We study the problem of multi-robot mapless na...</td>\n",
       "      <td>Enrico Marchesini, Alessandro Farinelli</td>\n",
       "      <td>cs.MA</td>\n",
       "      <td>centralizing state-values in dueling networks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>http://arxiv.org/abs/2201.08484v4</td>\n",
       "      <td>2022-06-24T20:24:35Z</td>\n",
       "      <td>2022-01-20T22:54:32Z</td>\n",
       "      <td>Iterated Reasoning with Mutual Information in ...</td>\n",
       "      <td>Information sharing is key in building team co...</td>\n",
       "      <td>Sachin Konan, Esmaeil Seraj, Matthew Gombolay</td>\n",
       "      <td>cs.MA</td>\n",
       "      <td>iterated reasoning with mutual information in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>http://arxiv.org/abs/2201.11994v2</td>\n",
       "      <td>2022-01-31T08:01:14Z</td>\n",
       "      <td>2022-01-28T09:12:01Z</td>\n",
       "      <td>FCMNet: Full Communication Memory Net for Team...</td>\n",
       "      <td>Decentralized cooperation in partially-observa...</td>\n",
       "      <td>Yutong Wang, Guillaume Sartoretti</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>fcmnet: full communication memory net for team...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 link            updated_ts  \\\n",
       "0   http://arxiv.org/abs/1709.06620v1  2017-09-19T19:26:20Z   \n",
       "1   http://arxiv.org/abs/2011.05605v2  2020-11-20T18:19:32Z   \n",
       "2   http://arxiv.org/abs/2209.14745v2  2022-12-29T08:48:05Z   \n",
       "3   http://arxiv.org/abs/2003.08376v3  2020-11-07T02:48:22Z   \n",
       "4   http://arxiv.org/abs/2008.12760v1  2020-08-28T17:35:22Z   \n",
       "..                                ...                   ...   \n",
       "95  http://arxiv.org/abs/2109.15266v3  2022-05-31T12:56:48Z   \n",
       "96  http://arxiv.org/abs/2110.08229v1  2021-10-05T16:46:04Z   \n",
       "97  http://arxiv.org/abs/2112.09012v1  2021-12-16T16:47:00Z   \n",
       "98  http://arxiv.org/abs/2201.08484v4  2022-06-24T20:24:35Z   \n",
       "99  http://arxiv.org/abs/2201.11994v2  2022-01-31T08:01:14Z   \n",
       "\n",
       "            published_ts                                              title  \\\n",
       "0   2017-09-19T19:26:20Z  Learning of Coordination Policies for Robotic ...   \n",
       "1   2020-11-11T07:35:21Z  Decentralized Motion Planning for Multi-Robot ...   \n",
       "2   2022-09-29T13:02:58Z  A Multiagent Framework for the Asynchronous an...   \n",
       "3   2020-03-18T17:54:28Z  Inverting the Pose Forecasting Pipeline with S...   \n",
       "4   2020-08-28T17:35:22Z     AllenAct: A Framework for Embodied AI Research   \n",
       "..                   ...                                                ...   \n",
       "95  2021-09-30T17:06:39Z  Modeling Interactions of Autonomous Vehicles a...   \n",
       "96  2021-10-05T16:46:04Z  Influencing Towards Stable Multi-Agent Interac...   \n",
       "97  2021-12-16T16:47:00Z  Centralizing State-Values in Dueling Networks ...   \n",
       "98  2022-01-20T22:54:32Z  Iterated Reasoning with Mutual Information in ...   \n",
       "99  2022-01-28T09:12:01Z  FCMNet: Full Communication Memory Net for Team...   \n",
       "\n",
       "                                              summary  \\\n",
       "0   Inspired by biological swarms, robotic swarms ...   \n",
       "1   This work presents a decentralized motion plan...   \n",
       "2   The traditional ML development methodology doe...   \n",
       "3   Many autonomous systems forecast aspects of th...   \n",
       "4   The domain of Embodied AI, in which agents lea...   \n",
       "..                                                ...   \n",
       "95  Reliable pedestrian crash avoidance mitigation...   \n",
       "96  Learning in multi-agent environments is diffic...   \n",
       "97  We study the problem of multi-robot mapless na...   \n",
       "98  Information sharing is key in building team co...   \n",
       "99  Decentralized cooperation in partially-observa...   \n",
       "\n",
       "                                               author category  \\\n",
       "0   Qiyang Li, Xintong Du, Yizhou Huang, Quinlan S...    cs.RO   \n",
       "1   Sivanathan Kandhasamy, Vinayagam Babu Kuppusam...    cs.RO   \n",
       "2                                     Andrea Gesmundo    cs.LG   \n",
       "3   Xinshuo Weng, Jianren Wang, Sergey Levine, Kri...    cs.CV   \n",
       "4   Luca Weihs, Jordi Salvador, Klemen Kotar, Unna...    cs.CV   \n",
       "..                                                ...      ...   \n",
       "95    Raphael Trumpp, Harald Bayerlein, David Gesbert    cs.RO   \n",
       "96  Woodrow Z. Wang, Andy Shih, Annie Xie, Dorsa S...    cs.RO   \n",
       "97            Enrico Marchesini, Alessandro Farinelli    cs.MA   \n",
       "98      Sachin Konan, Esmaeil Seraj, Matthew Gombolay    cs.MA   \n",
       "99                  Yutong Wang, Guillaume Sartoretti    cs.RO   \n",
       "\n",
       "                                        combined_text  \n",
       "0   learning of coordination policies for robotic ...  \n",
       "1   decentralized motion planning for multi-robot ...  \n",
       "2   a multiagent framework for the asynchronous an...  \n",
       "3   inverting the pose forecasting pipeline with s...  \n",
       "4   allenact: a framework for embodied ai research...  \n",
       "..                                                ...  \n",
       "95  modeling interactions of autonomous vehicles a...  \n",
       "96  influencing towards stable multi-agent interac...  \n",
       "97  centralizing state-values in dueling networks ...  \n",
       "98  iterated reasoning with mutual information in ...  \n",
       "99  fcmnet: full communication memory net for team...  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08774571",
   "metadata": {},
   "source": [
    "## Get embeddings from Cohere API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a544fd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning of coordination policies for robotic swarms. inspired by biological swarms, robotic swarms are envisioned to solve real-world problems that are difficult for individual agents. biological swarms can achieve collective intelligence based on local interactions and simple rules; however, designing effective distributed policies for large-scale robotic swarms to achieve a global objective can be challenging. although it is often possible to design an optimal centralized strategy for smaller numbers of agents, those methods can fail as the number of agents increases. motivated by the growing success of machine learning, we develop a deep learning approach that learns distributed coordination policies from centralized policies. in contrast to traditional distributed control approaches, which are usually based on human-designed policies for relatively simple tasks, this learning-based approach can be adapted to more difficult tasks. we demonstrate the efficacy of our proposed approach on two different tasks, the well-known rendezvous problem and a more difficult particle assignment problem. for the latter, no known distributed policy exists. from extensive simulations, it is shown that the performance of the learned coordination policies is comparable to the centralized policies, surpassing state-of-the-art distributed policies. thereby, our proposed approach provides a promising alternative for real-world coordination problems that would be otherwise computationally expensive to solve or intangible to explore.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df['combined_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a165c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracted Cohere API logic into CohereModel class\n",
    "cohere_model = CohereModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1ef3833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 ms, sys: 1.91 ms, total: 19.2 ms\n",
      "Wall time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"speech to text whisper wav2vec\"\n",
    "\n",
    "res_embeddings = cohere_model.get_embeddings(texts=[query]+list(articles_df['combined_text']), from_cache=is_from_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75dde57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/b79vxxyn1mx4cwpgx_m0p48r0000gn/T/ipykernel_34639/1774032411.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df['similarity'] = similarity_scores\n"
     ]
    }
   ],
   "source": [
    "query_embedding = res_embeddings[0]\n",
    "article_embeddings = res_embeddings[1:]\n",
    "\n",
    "top_k_indices, similarity_scores = compute_top_k(query_embedding, article_embeddings, k=20)\n",
    "results_df = articles_df.iloc[top_k_indices]\n",
    "results_df['similarity'] = similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b6906ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>updated_ts</th>\n",
       "      <th>published_ts</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>http://arxiv.org/abs/2008.09622v3</td>\n",
       "      <td>2021-02-11T18:36:45Z</td>\n",
       "      <td>2020-08-21T18:00:33Z</td>\n",
       "      <td>Learning to Set Waypoints for Audio-Visual Nav...</td>\n",
       "      <td>In audio-visual navigation, an agent intellige...</td>\n",
       "      <td>Changan Chen, Sagnik Majumder, Ziad Al-Halah, ...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>learning to set waypoints for audio-visual nav...</td>\n",
       "      <td>0.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>http://arxiv.org/abs/2209.15007v2</td>\n",
       "      <td>2022-11-02T17:59:47Z</td>\n",
       "      <td>2022-09-29T17:59:55Z</td>\n",
       "      <td>Understanding Collapse in Non-Contrastive Siam...</td>\n",
       "      <td>Contrastive methods have led a recent surge in...</td>\n",
       "      <td>Alexander C. Li, Alexei A. Efros, Deepak Pathak</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>understanding collapse in non-contrastive siam...</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://arxiv.org/abs/2103.14023v3</td>\n",
       "      <td>2021-10-07T05:04:39Z</td>\n",
       "      <td>2021-03-25T17:59:01Z</td>\n",
       "      <td>AgentFormer: Agent-Aware Transformers for Soci...</td>\n",
       "      <td>Predicting accurate future trajectories of mul...</td>\n",
       "      <td>Ye Yuan, Xinshuo Weng, Yanglan Ou, Kris Kitani</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>agentformer: agent-aware transformers for soci...</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>http://arxiv.org/abs/2102.02886v3</td>\n",
       "      <td>2021-04-05T17:59:16Z</td>\n",
       "      <td>2021-02-04T20:58:37Z</td>\n",
       "      <td>Ivy: Templated Deep Learning for Inter-Framewo...</td>\n",
       "      <td>We introduce Ivy, a templated Deep Learning (D...</td>\n",
       "      <td>Daniel Lenton, Fabio Pardo, Fabian Falck, Step...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>ivy: templated deep learning for inter-framewo...</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>http://arxiv.org/abs/1602.00991v2</td>\n",
       "      <td>2016-03-08T22:09:05Z</td>\n",
       "      <td>2016-02-02T16:10:16Z</td>\n",
       "      <td>Deep Tracking: Seeing Beyond Seeing Using Recu...</td>\n",
       "      <td>This paper presents to the best of our knowled...</td>\n",
       "      <td>Peter Ondruska, Ingmar Posner</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>deep tracking: seeing beyond seeing using recu...</td>\n",
       "      <td>0.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>http://arxiv.org/abs/1609.06666v2</td>\n",
       "      <td>2017-03-05T15:29:45Z</td>\n",
       "      <td>2016-09-21T18:32:11Z</td>\n",
       "      <td>Vote3Deep: Fast Object Detection in 3D Point C...</td>\n",
       "      <td>This paper proposes a computationally efficien...</td>\n",
       "      <td>Martin Engelcke, Dushyant Rao, Dominic Zeng Wa...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>vote3deep: fast object detection in 3d point c...</td>\n",
       "      <td>0.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>http://arxiv.org/abs/2112.03257v1</td>\n",
       "      <td>2021-12-06T18:59:52Z</td>\n",
       "      <td>2021-12-06T18:59:52Z</td>\n",
       "      <td>Functional Regularization for Reinforcement Le...</td>\n",
       "      <td>We propose a simple architecture for deep rein...</td>\n",
       "      <td>Alexander C. Li, Deepak Pathak</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>functional regularization for reinforcement le...</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>http://arxiv.org/abs/2109.06514v1</td>\n",
       "      <td>2021-09-14T08:18:47Z</td>\n",
       "      <td>2021-09-14T08:18:47Z</td>\n",
       "      <td>Vision Transformer for Learning Driving Polici...</td>\n",
       "      <td>Driving in a complex urban environment is a di...</td>\n",
       "      <td>Eshagh Kargar, Ville Kyrki</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>vision transformer for learning driving polici...</td>\n",
       "      <td>0.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://arxiv.org/abs/2104.00563v3</td>\n",
       "      <td>2022-02-11T04:59:43Z</td>\n",
       "      <td>2021-02-19T18:53:26Z</td>\n",
       "      <td>Latent Variable Sequential Set Transformers Fo...</td>\n",
       "      <td>Robust multi-agent trajectory prediction is es...</td>\n",
       "      <td>Roger Girgis, Florian Golemo, Felipe Codevilla...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>latent variable sequential set transformers fo...</td>\n",
       "      <td>0.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>http://arxiv.org/abs/2007.13729v1</td>\n",
       "      <td>2020-07-27T17:59:08Z</td>\n",
       "      <td>2020-07-27T17:59:08Z</td>\n",
       "      <td>Noisy Agents: Self-supervised Exploration by P...</td>\n",
       "      <td>Humans integrate multiple sensory modalities (...</td>\n",
       "      <td>Chuang Gan, Xiaoyu Chen, Phillip Isola, Antoni...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>noisy agents: self-supervised exploration by p...</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2003.08376v3</td>\n",
       "      <td>2020-11-07T02:48:22Z</td>\n",
       "      <td>2020-03-18T17:54:28Z</td>\n",
       "      <td>Inverting the Pose Forecasting Pipeline with S...</td>\n",
       "      <td>Many autonomous systems forecast aspects of th...</td>\n",
       "      <td>Xinshuo Weng, Jianren Wang, Sergey Levine, Kri...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>inverting the pose forecasting pipeline with s...</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>http://arxiv.org/abs/1604.05091v2</td>\n",
       "      <td>2016-04-19T14:09:26Z</td>\n",
       "      <td>2016-04-18T11:15:56Z</td>\n",
       "      <td>End-to-End Tracking and Semantic Segmentation ...</td>\n",
       "      <td>In this work we present a novel end-to-end fra...</td>\n",
       "      <td>Peter Ondruska, Julie Dequaire, Dominic Zeng W...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>end-to-end tracking and semantic segmentation ...</td>\n",
       "      <td>0.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://arxiv.org/abs/2108.01316v1</td>\n",
       "      <td>2021-08-03T06:30:30Z</td>\n",
       "      <td>2021-08-03T06:30:30Z</td>\n",
       "      <td>RAIN: Reinforced Hybrid Attention Inference Ne...</td>\n",
       "      <td>Motion forecasting plays a significant role in...</td>\n",
       "      <td>Jiachen Li, Fan Yang, Hengbo Ma, Srikanth Mall...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>rain: reinforced hybrid attention inference ne...</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>http://arxiv.org/abs/2211.02545v2</td>\n",
       "      <td>2022-11-08T21:31:55Z</td>\n",
       "      <td>2022-11-04T16:10:50Z</td>\n",
       "      <td>GoRela: Go Relative for Viewpoint-Invariant Mo...</td>\n",
       "      <td>The task of motion forecasting is critical for...</td>\n",
       "      <td>Alexander Cui, Sergio Casas, Kelvin Wong, Simo...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>gorela: go relative for viewpoint-invariant mo...</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>http://arxiv.org/abs/2103.01636v1</td>\n",
       "      <td>2021-03-02T10:48:29Z</td>\n",
       "      <td>2021-03-02T10:48:29Z</td>\n",
       "      <td>Sparse Training Theory for Scalable and Effici...</td>\n",
       "      <td>A fundamental task for artificial intelligence...</td>\n",
       "      <td>Decebal Constantin Mocanu, Elena Mocanu, Tiago...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>sparse training theory for scalable and effici...</td>\n",
       "      <td>0.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>http://arxiv.org/abs/2109.02173v3</td>\n",
       "      <td>2022-03-02T20:37:01Z</td>\n",
       "      <td>2021-09-05T21:56:54Z</td>\n",
       "      <td>Multi-Agent Variational Occlusion Inference Us...</td>\n",
       "      <td>Autonomous vehicles must reason about spatial ...</td>\n",
       "      <td>Masha Itkina, Ye-Ji Mun, Katherine Driggs-Camp...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>multi-agent variational occlusion inference us...</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>http://arxiv.org/abs/1807.09341v1</td>\n",
       "      <td>2018-07-24T20:46:05Z</td>\n",
       "      <td>2018-07-24T20:46:05Z</td>\n",
       "      <td>Learning Plannable Representations with Causal...</td>\n",
       "      <td>In recent years, deep generative models have b...</td>\n",
       "      <td>Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart ...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>learning plannable representations with causal...</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>http://arxiv.org/abs/2108.04867v1</td>\n",
       "      <td>2021-08-10T18:37:54Z</td>\n",
       "      <td>2021-08-10T18:37:54Z</td>\n",
       "      <td>AuraSense: Robot Collision Avoidance by Full S...</td>\n",
       "      <td>Perceiving obstacles and avoiding collisions i...</td>\n",
       "      <td>Xiaoran Fan, Riley Simmons-Edler, Daewon Lee, ...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>aurasense: robot collision avoidance by full s...</td>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>http://arxiv.org/abs/2201.11994v2</td>\n",
       "      <td>2022-01-31T08:01:14Z</td>\n",
       "      <td>2022-01-28T09:12:01Z</td>\n",
       "      <td>FCMNet: Full Communication Memory Net for Team...</td>\n",
       "      <td>Decentralized cooperation in partially-observa...</td>\n",
       "      <td>Yutong Wang, Guillaume Sartoretti</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>fcmnet: full communication memory net for team...</td>\n",
       "      <td>0.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://arxiv.org/abs/2108.08236v3</td>\n",
       "      <td>2021-09-17T16:38:31Z</td>\n",
       "      <td>2021-08-18T16:57:03Z</td>\n",
       "      <td>LOKI: Long Term and Key Intentions for Traject...</td>\n",
       "      <td>Recent advances in trajectory prediction have ...</td>\n",
       "      <td>Harshayu Girase, Haiming Gang, Srikanth Malla,...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>loki: long term and key intentions for traject...</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 link            updated_ts  \\\n",
       "26  http://arxiv.org/abs/2008.09622v3  2021-02-11T18:36:45Z   \n",
       "42  http://arxiv.org/abs/2209.15007v2  2022-11-02T17:59:47Z   \n",
       "10  http://arxiv.org/abs/2103.14023v3  2021-10-07T05:04:39Z   \n",
       "33  http://arxiv.org/abs/2102.02886v3  2021-04-05T17:59:16Z   \n",
       "30  http://arxiv.org/abs/1602.00991v2  2016-03-08T22:09:05Z   \n",
       "29  http://arxiv.org/abs/1609.06666v2  2017-03-05T15:29:45Z   \n",
       "34  http://arxiv.org/abs/2112.03257v1  2021-12-06T18:59:52Z   \n",
       "94  http://arxiv.org/abs/2109.06514v1  2021-09-14T08:18:47Z   \n",
       "11  http://arxiv.org/abs/2104.00563v3  2022-02-11T04:59:43Z   \n",
       "63  http://arxiv.org/abs/2007.13729v1  2020-07-27T17:59:08Z   \n",
       "3   http://arxiv.org/abs/2003.08376v3  2020-11-07T02:48:22Z   \n",
       "35  http://arxiv.org/abs/1604.05091v2  2016-04-19T14:09:26Z   \n",
       "13  http://arxiv.org/abs/2108.01316v1  2021-08-03T06:30:30Z   \n",
       "19  http://arxiv.org/abs/2211.02545v2  2022-11-08T21:31:55Z   \n",
       "51  http://arxiv.org/abs/2103.01636v1  2021-03-02T10:48:29Z   \n",
       "15  http://arxiv.org/abs/2109.02173v3  2022-03-02T20:37:01Z   \n",
       "64  http://arxiv.org/abs/1807.09341v1  2018-07-24T20:46:05Z   \n",
       "27  http://arxiv.org/abs/2108.04867v1  2021-08-10T18:37:54Z   \n",
       "99  http://arxiv.org/abs/2201.11994v2  2022-01-31T08:01:14Z   \n",
       "14  http://arxiv.org/abs/2108.08236v3  2021-09-17T16:38:31Z   \n",
       "\n",
       "            published_ts                                              title  \\\n",
       "26  2020-08-21T18:00:33Z  Learning to Set Waypoints for Audio-Visual Nav...   \n",
       "42  2022-09-29T17:59:55Z  Understanding Collapse in Non-Contrastive Siam...   \n",
       "10  2021-03-25T17:59:01Z  AgentFormer: Agent-Aware Transformers for Soci...   \n",
       "33  2021-02-04T20:58:37Z  Ivy: Templated Deep Learning for Inter-Framewo...   \n",
       "30  2016-02-02T16:10:16Z  Deep Tracking: Seeing Beyond Seeing Using Recu...   \n",
       "29  2016-09-21T18:32:11Z  Vote3Deep: Fast Object Detection in 3D Point C...   \n",
       "34  2021-12-06T18:59:52Z  Functional Regularization for Reinforcement Le...   \n",
       "94  2021-09-14T08:18:47Z  Vision Transformer for Learning Driving Polici...   \n",
       "11  2021-02-19T18:53:26Z  Latent Variable Sequential Set Transformers Fo...   \n",
       "63  2020-07-27T17:59:08Z  Noisy Agents: Self-supervised Exploration by P...   \n",
       "3   2020-03-18T17:54:28Z  Inverting the Pose Forecasting Pipeline with S...   \n",
       "35  2016-04-18T11:15:56Z  End-to-End Tracking and Semantic Segmentation ...   \n",
       "13  2021-08-03T06:30:30Z  RAIN: Reinforced Hybrid Attention Inference Ne...   \n",
       "19  2022-11-04T16:10:50Z  GoRela: Go Relative for Viewpoint-Invariant Mo...   \n",
       "51  2021-03-02T10:48:29Z  Sparse Training Theory for Scalable and Effici...   \n",
       "15  2021-09-05T21:56:54Z  Multi-Agent Variational Occlusion Inference Us...   \n",
       "64  2018-07-24T20:46:05Z  Learning Plannable Representations with Causal...   \n",
       "27  2021-08-10T18:37:54Z  AuraSense: Robot Collision Avoidance by Full S...   \n",
       "99  2022-01-28T09:12:01Z  FCMNet: Full Communication Memory Net for Team...   \n",
       "14  2021-08-18T16:57:03Z  LOKI: Long Term and Key Intentions for Traject...   \n",
       "\n",
       "                                              summary  \\\n",
       "26  In audio-visual navigation, an agent intellige...   \n",
       "42  Contrastive methods have led a recent surge in...   \n",
       "10  Predicting accurate future trajectories of mul...   \n",
       "33  We introduce Ivy, a templated Deep Learning (D...   \n",
       "30  This paper presents to the best of our knowled...   \n",
       "29  This paper proposes a computationally efficien...   \n",
       "34  We propose a simple architecture for deep rein...   \n",
       "94  Driving in a complex urban environment is a di...   \n",
       "11  Robust multi-agent trajectory prediction is es...   \n",
       "63  Humans integrate multiple sensory modalities (...   \n",
       "3   Many autonomous systems forecast aspects of th...   \n",
       "35  In this work we present a novel end-to-end fra...   \n",
       "13  Motion forecasting plays a significant role in...   \n",
       "19  The task of motion forecasting is critical for...   \n",
       "51  A fundamental task for artificial intelligence...   \n",
       "15  Autonomous vehicles must reason about spatial ...   \n",
       "64  In recent years, deep generative models have b...   \n",
       "27  Perceiving obstacles and avoiding collisions i...   \n",
       "99  Decentralized cooperation in partially-observa...   \n",
       "14  Recent advances in trajectory prediction have ...   \n",
       "\n",
       "                                               author category  \\\n",
       "26  Changan Chen, Sagnik Majumder, Ziad Al-Halah, ...    cs.CV   \n",
       "42    Alexander C. Li, Alexei A. Efros, Deepak Pathak    cs.LG   \n",
       "10     Ye Yuan, Xinshuo Weng, Yanglan Ou, Kris Kitani    cs.AI   \n",
       "33  Daniel Lenton, Fabio Pardo, Fabian Falck, Step...    cs.LG   \n",
       "30                      Peter Ondruska, Ingmar Posner    cs.LG   \n",
       "29  Martin Engelcke, Dushyant Rao, Dominic Zeng Wa...    cs.RO   \n",
       "34                     Alexander C. Li, Deepak Pathak    cs.LG   \n",
       "94                         Eshagh Kargar, Ville Kyrki    cs.LG   \n",
       "11  Roger Girgis, Florian Golemo, Felipe Codevilla...    cs.RO   \n",
       "63  Chuang Gan, Xiaoyu Chen, Phillip Isola, Antoni...    cs.CV   \n",
       "3   Xinshuo Weng, Jianren Wang, Sergey Levine, Kri...    cs.CV   \n",
       "35  Peter Ondruska, Julie Dequaire, Dominic Zeng W...    cs.LG   \n",
       "13  Jiachen Li, Fan Yang, Hengbo Ma, Srikanth Mall...    cs.CV   \n",
       "19  Alexander Cui, Sergio Casas, Kelvin Wong, Simo...    cs.RO   \n",
       "51  Decebal Constantin Mocanu, Elena Mocanu, Tiago...    cs.AI   \n",
       "15  Masha Itkina, Ye-Ji Mun, Katherine Driggs-Camp...    cs.RO   \n",
       "64  Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart ...    cs.LG   \n",
       "27  Xiaoran Fan, Riley Simmons-Edler, Daewon Lee, ...    cs.RO   \n",
       "99                  Yutong Wang, Guillaume Sartoretti    cs.RO   \n",
       "14  Harshayu Girase, Haiming Gang, Srikanth Malla,...    cs.CV   \n",
       "\n",
       "                                        combined_text  similarity  \n",
       "26  learning to set waypoints for audio-visual nav...       0.364  \n",
       "42  understanding collapse in non-contrastive siam...       0.303  \n",
       "10  agentformer: agent-aware transformers for soci...       0.292  \n",
       "33  ivy: templated deep learning for inter-framewo...       0.289  \n",
       "30  deep tracking: seeing beyond seeing using recu...       0.273  \n",
       "29  vote3deep: fast object detection in 3d point c...       0.273  \n",
       "34  functional regularization for reinforcement le...       0.271  \n",
       "94  vision transformer for learning driving polici...       0.266  \n",
       "11  latent variable sequential set transformers fo...       0.265  \n",
       "63  noisy agents: self-supervised exploration by p...       0.264  \n",
       "3   inverting the pose forecasting pipeline with s...       0.252  \n",
       "35  end-to-end tracking and semantic segmentation ...       0.249  \n",
       "13  rain: reinforced hybrid attention inference ne...       0.241  \n",
       "19  gorela: go relative for viewpoint-invariant mo...       0.240  \n",
       "51  sparse training theory for scalable and effici...       0.233  \n",
       "15  multi-agent variational occlusion inference us...       0.210  \n",
       "64  learning plannable representations with causal...       0.200  \n",
       "27  aurasense: robot collision avoidance by full s...       0.198  \n",
       "99  fcmnet: full communication memory net for team...       0.195  \n",
       "14  loki: long term and key intentions for traject...       0.194  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39d0f5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'link': 'http://arxiv.org/abs/2106.04283v1',\n",
       "  'updated_ts': '2021-06-08T12:22:29Z',\n",
       "  'published_ts': '2021-06-08T12:22:29Z',\n",
       "  'title': 'NWT: Towards natural audio-to-video generation with representation\\n  learning',\n",
       "  'summary': \"In this work we introduce NWT, an expressive speech-to-video model. Unlike approaches that use domain-specific intermediate representations such as pose keypoints, NWT learns its own latent representations, with minimal assumptions about the audio and video content. To this end, we propose a novel discrete variational autoencoder with adversarial loss, dVAE-Adv, which learns a new discrete latent representation we call Memcodes. Memcodes are straightforward to implement, require no additional loss terms, are stable to train compared with other approaches, and show evidence of interpretability. To predict on the Memcode space, we use an autoregressive encoder-decoder model conditioned on audio. Additionally, our model can control latent attributes in the generated video that are not annotated in the data. We train NWT on clips from HBO's Last Week Tonight with John Oliver. NWT consistently scores above other approaches in Mean Opinion Score (MOS) on tests of overall video naturalness, facial naturalness and expressiveness, and lipsync quality. This work sets a strong baseline for generalized audio-to-video synthesis. Samples are available at https://next-week-tonight.github.io/NWT/.\",\n",
       "  'author': 'Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, Cole Clifford, Ragavan Thurairatnam',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.598},\n",
       " {'link': 'http://arxiv.org/abs/2106.09296v3',\n",
       "  'updated_ts': '2022-01-14T16:43:19Z',\n",
       "  'published_ts': '2021-06-17T07:59:15Z',\n",
       "  'title': 'Voice2Series: Reprogramming Acoustic Models for Time Series\\n  Classification',\n",
       "  'summary': 'Learning to classify time series with limited data is a practical yet challenging problem. Current methods are primarily based on hand-designed feature extraction rules or domain-specific data augmentation. Motivated by the advances in deep speech processing models and the fact that voice data are univariate temporal signals, in this paper, we propose Voice2Series (V2S), a novel end-to-end approach that reprograms acoustic models for time series classification, through input transformation learning and output label mapping. Leveraging the representation learning power of a large-scale pre-trained speech processing model, on 30 different time series tasks we show that V2S performs competitive results on 19 time series classification tasks. We further provide a theoretical justification of V2S by proving its population risk is upper bounded by the source risk and a Wasserstein distance accounting for feature alignment via reprogramming. Our results offer new and effective means to time series classification.',\n",
       "  'author': 'Chao-Han Huck Yang, Yun-Yun Tsai, Pin-Yu Chen',\n",
       "  'category': 'cs.LG',\n",
       "  'similarity': 0.591},\n",
       " {'link': 'http://arxiv.org/abs/2202.08509v1',\n",
       "  'updated_ts': '2022-02-17T08:26:25Z',\n",
       "  'published_ts': '2022-02-17T08:26:25Z',\n",
       "  'title': 'A Study of Designing Compact Audio-Visual Wake Word Spotting System\\n  Based on Iterative Fine-Tuning in Neural Network Pruning',\n",
       "  'summary': 'Audio-only-based wake word spotting (WWS) is challenging under noisy conditions due to environmental interference in signal transmission. In this paper, we investigate on designing a compact audio-visual WWS system by utilizing visual information to alleviate the degradation. Specifically, in order to use visual information, we first encode the detected lips to fixed-size vectors with MobileNet and concatenate them with acoustic features followed by the fusion network for WWS. However, the audio-visual model based on neural networks requires a large footprint and a high computational complexity. To meet the application requirements, we introduce a neural network pruning strategy via the lottery ticket hypothesis in an iterative fine-tuning manner (LTH-IF), to the single-modal and multi-modal models, respectively. Tested on our in-house corpus for audio-visual WWS in a home TV scene, the proposed audio-visual system achieves significant performance improvements over the single-modality (audio-only or video-only) system under different noisy conditions. Moreover, LTH-IF pruning can largely reduce the network parameters and computations with no degradation of WWS performance, leading to a potential product solution for the TV wake-up scenario.',\n",
       "  'author': 'Hengshun Zhou, Jun Du, Chao-Han Huck Yang, Shifu Xiong, Chin-Hui Lee',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.565},\n",
       " {'link': 'http://arxiv.org/abs/1910.10942v2',\n",
       "  'updated_ts': '2020-02-10T09:36:23Z',\n",
       "  'published_ts': '2019-10-24T06:54:36Z',\n",
       "  'title': 'A Recurrent Variational Autoencoder for Speech Enhancement',\n",
       "  'summary': 'This paper presents a generative approach to speech enhancement based on a recurrent variational autoencoder (RVAE). The deep generative speech model is trained using clean speech signals only, and it is combined with a nonnegative matrix factorization noise model for speech enhancement. We propose a variational expectation-maximization algorithm where the encoder of the RVAE is fine-tuned at test time, to approximate the distribution of the latent variables given the noisy speech observations. Compared with previous approaches based on feed-forward fully-connected architectures, the proposed recurrent deep generative speech model induces a posterior temporal dynamic over the latent variables, which is shown to improve the speech enhancement results.',\n",
       "  'author': 'Simon Leglaive, Xavier Alameda-Pineda, Laurent Girin, Radu Horaud',\n",
       "  'category': 'cs.LG',\n",
       "  'similarity': 0.555},\n",
       " {'link': 'http://arxiv.org/abs/2110.08791v1',\n",
       "  'updated_ts': '2021-10-17T11:14:00Z',\n",
       "  'published_ts': '2021-10-17T11:14:00Z',\n",
       "  'title': 'Taming Visually Guided Sound Generation',\n",
       "  'summary': 'Recent advances in visually-induced audio generation are based on sampling short, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio from the state-of-the-art model takes minutes on a high-end GPU. In this work, we propose a single model capable of generating visually relevant, high-fidelity sounds prompted with a set of frames from open-domain videos in less time than it takes to play it on a single GPU.   We train a transformer to sample a new spectrogram from the pre-trained spectrogram codebook given the set of video features. The codebook is obtained using a variant of VQGAN trained to produce a compact sampling space with a novel spectrogram-based perceptual loss. The generated spectrogram is transformed into a waveform using a window-based GAN that significantly speeds up generation. Considering the lack of metrics for automatic evaluation of generated spectrograms, we also build a family of metrics called FID and MKL. These metrics are based on a novel sound classifier, called Melception, and designed to evaluate the fidelity and relevance of open-domain samples.   Both qualitative and quantitative studies are conducted on small- and large-scale datasets to evaluate the fidelity and relevance of generated samples. We also compare our model to the state-of-the-art and observe a substantial improvement in quality, size, and computation time. Code, demo, and samples: v-iashin.github.io/SpecVQGAN',\n",
       "  'author': 'Vladimir Iashin, Esa Rahtu',\n",
       "  'category': 'cs.CV',\n",
       "  'similarity': 0.521},\n",
       " {'link': 'http://arxiv.org/abs/2206.00393v1',\n",
       "  'updated_ts': '2022-06-01T11:00:07Z',\n",
       "  'published_ts': '2022-06-01T11:00:07Z',\n",
       "  'title': 'Towards Generalisable Audio Representations for Audio-Visual Navigation',\n",
       "  'summary': 'In audio-visual navigation (AVN), an intelligent agent needs to navigate to a constantly sound-making object in complex 3D environments based on its audio and visual perceptions. While existing methods attempt to improve the navigation performance with preciously designed path planning or intricate task settings, none has improved the model generalisation on unheard sounds with task settings unchanged. We thus propose a contrastive learning-based method to tackle this challenge by regularising the audio encoder, where the sound-agnostic goal-driven latent representations can be learnt from various audio signals of different classes. In addition, we consider two data augmentation strategies to enrich the training sounds. We demonstrate that our designs can be easily equipped to existing AVN frameworks to obtain an immediate performance gain (13.4%$\\\\uparrow$ in SPL on Replica and 12.2%$\\\\uparrow$ in SPL on MP3D). Our project is available at https://AV-GeN.github.io/.',\n",
       "  'author': 'Shunqi Mao, Chaoyi Zhang, Heng Wang, Weidong Cai',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.493},\n",
       " {'link': 'http://arxiv.org/abs/2301.07851v1',\n",
       "  'updated_ts': '2023-01-19T02:37:56Z',\n",
       "  'published_ts': '2023-01-19T02:37:56Z',\n",
       "  'title': 'From English to More Languages: Parameter-Efficient Model Reprogramming\\n  for Cross-Lingual Speech Recognition',\n",
       "  'summary': 'In this work, we propose a new parameter-efficient learning framework based on neural model reprogramming for cross-lingual speech recognition, which can \\\\textbf{re-purpose} well-trained English automatic speech recognition (ASR) models to recognize the other languages. We design different auxiliary neural architectures focusing on learnable pre-trained feature enhancement that, for the first time, empowers model reprogramming on ASR. Specifically, we investigate how to select trainable components (i.e., encoder) of a conformer-based RNN-Transducer, as a frozen pre-trained backbone. Experiments on a seven-language multilingual LibriSpeech speech (MLS) task show that model reprogramming only requires 4.2% (11M out of 270M) to 6.8% (45M out of 660M) of its original trainable parameters from a full ASR model to perform competitive results in a range of 11.9% to 8.1% WER averaged across different languages. In addition, we discover different setups to make large-scale pre-trained ASR succeed in both monolingual and multilingual speech recognition. Our methods outperform existing ASR tuning architectures and their extension with self-supervised losses (e.g., w2v-bert) in terms of lower WER and better training efficiency.',\n",
       "  'author': 'Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Rohit Prabhavalkar, Tara N. Sainath, Trevor Strohman',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.48},\n",
       " {'link': 'http://arxiv.org/abs/1805.11797v2',\n",
       "  'updated_ts': '2018-05-31T03:49:25Z',\n",
       "  'published_ts': '2018-05-30T04:15:58Z',\n",
       "  'title': 'Grow and Prune Compact, Fast, and Accurate LSTMs',\n",
       "  'summary': \"Long short-term memory (LSTM) has been widely used for sequential data modeling. Researchers have increased LSTM depth by stacking LSTM cells to improve performance. This incurs model redundancy, increases run-time delay, and makes the LSTMs more prone to overfitting. To address these problems, we propose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original one level non-linear control gates. H-LSTM increases accuracy while employing fewer external stacked layers, thus reducing the number of parameters and run-time latency significantly. We employ grow-and-prune (GP) training to iteratively adjust the hidden layers through gradient-based growth and magnitude-based pruning of connections. This learns both the weights and the compact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for image captioning and speech recognition applications. For the NeuralTalk architecture on the MSCOCO dataset, our three models reduce the number of parameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time latency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2 architecture on the AN4 dataset, our two models reduce the number of parameters by 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate from 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast, and accurate.\",\n",
       "  'author': 'Xiaoliang Dai, Hongxu Yin, Niraj K. Jha',\n",
       "  'category': 'cs.LG',\n",
       "  'similarity': 0.478},\n",
       " {'link': 'http://arxiv.org/abs/2206.02211v3',\n",
       "  'updated_ts': '2022-12-04T08:24:02Z',\n",
       "  'published_ts': '2022-06-05T16:18:27Z',\n",
       "  'title': 'Variable-rate hierarchical CPC leads to acoustic unit discovery in\\n  speech',\n",
       "  'summary': 'The success of deep learning comes from its ability to capture the hierarchical structure of data by learning high-level representations defined in terms of low-level ones. In this paper we explore self-supervised learning of hierarchical representations of speech by applying multiple levels of Contrastive Predictive Coding (CPC). We observe that simply stacking two CPC models does not yield significant improvements over single-level architectures. Inspired by the fact that speech is often described as a sequence of discrete units unevenly distributed in time, we propose a model in which the output of a low-level CPC module is non-uniformly downsampled to directly minimize the loss of a high-level CPC module. The latter is designed to also enforce a prior of separability and discreteness in its representations by enforcing dissimilarity of successive high-level representations through focused negative sampling, and by quantization of the prediction targets. Accounting for the structure of the speech signal improves upon single-level CPC features and enhances the disentanglement of the learned representations, as measured by downstream speech recognition tasks, while resulting in a meaningful segmentation of the signal that closely resembles phone boundaries.',\n",
       "  'author': 'Santiago Cuervo, Adrian Łańcucki, Ricard Marxer, Paweł Rychlikowski, Jan Chorowski',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.478},\n",
       " {'link': 'http://arxiv.org/abs/2202.08532v1',\n",
       "  'updated_ts': '2022-02-17T09:17:58Z',\n",
       "  'published_ts': '2022-02-17T09:17:58Z',\n",
       "  'title': 'Mitigating Closed-model Adversarial Examples with Bayesian Neural\\n  Modeling for Enhanced End-to-End Speech Recognition',\n",
       "  'summary': 'In this work, we aim to enhance the system robustness of end-to-end automatic speech recognition (ASR) against adversarially-noisy speech examples. We focus on a rigorous and empirical \"closed-model adversarial robustness\" setting (e.g., on-device or cloud applications). The adversarial noise is only generated by closed-model optimization (e.g., evolutionary and zeroth-order estimation) without accessing gradient information of a targeted ASR model directly. We propose an advanced Bayesian neural network (BNN) based adversarial detector, which could model latent distributions against adaptive adversarial perturbation with divergence measurement. We further simulate deployment scenarios of RNN Transducer, Conformer, and wav2vec-2.0 based ASR systems with the proposed adversarial detection system. Leveraging the proposed BNN based detection system, we improve detection rate by +2.77 to +5.42% (relative +3.03 to +6.26%) and reduce the word error rate by 5.02 to 7.47% on LibriSpeech datasets compared to the current model enhancement methods against the adversarial speech examples.',\n",
       "  'author': 'Chao-Han Huck Yang, Zeeshan Ahmed, Yile Gu, Joseph Szurley, Roger Ren, Linda Liu, Andreas Stolcke, Ivan Bulyko',\n",
       "  'category': 'eess.AS',\n",
       "  'similarity': 0.471},\n",
       " {'link': 'http://arxiv.org/abs/2109.02096v2',\n",
       "  'updated_ts': '2021-10-10T16:22:00Z',\n",
       "  'published_ts': '2021-09-05T15:06:53Z',\n",
       "  'title': 'Timbre Transfer with Variational Auto Encoding and Cycle-Consistent\\n  Adversarial Networks',\n",
       "  'summary': \"This research project investigates the application of deep learning to timbre transfer, where the timbre of a source audio can be converted to the timbre of a target audio with minimal loss in quality. The adopted approach combines Variational Autoencoders with Generative Adversarial Networks to construct meaningful representations of the source audio and produce realistic generations of the target audio and is applied to the Flickr 8k Audio dataset for transferring the vocal timbre between speakers and the URMP dataset for transferring the musical timbre between instruments. Furthermore, variations of the adopted approach are trained, and generalised performance is compared using the metrics SSIM (Structural Similarity Index) and FAD (Frech\\\\'et Audio Distance). It was found that a many-to-many approach supersedes a one-to-one approach in terms of reconstructive capabilities, and that the adoption of a basic over a bottleneck residual block design is more suitable for enriching content information about a latent space. It was also found that the decision on whether cyclic loss takes on a variational autoencoder or vanilla autoencoder approach does not have a significant impact on reconstructive and adversarial translation aspects of the model.\",\n",
       "  'author': 'Russell Sammut Bonnici, Charalampos Saitis, Martin Benning',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.47},\n",
       " {'link': 'http://arxiv.org/abs/2003.00418v1',\n",
       "  'updated_ts': '2020-03-01T06:42:43Z',\n",
       "  'published_ts': '2020-03-01T06:42:43Z',\n",
       "  'title': 'Towards Automatic Face-to-Face Translation',\n",
       "  'summary': 'In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as \"Face-to-Face Translation\". As today\\'s digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact on multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards \"Face-to-Face Translation\" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available.   Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0   Code and models: https://github.com/Rudrabha/LipGAN',\n",
       "  'author': 'Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, C. V. Jawahar',\n",
       "  'category': 'cs.CV',\n",
       "  'similarity': 0.468},\n",
       " {'link': 'http://arxiv.org/abs/1902.03389v1',\n",
       "  'updated_ts': '2019-02-09T07:49:42Z',\n",
       "  'published_ts': '2019-02-09T07:49:42Z',\n",
       "  'title': 'Generative Moment Matching Network-based Random Modulation Post-filter\\n  for DNN-based Singing Voice Synthesis and Neural Double-tracking',\n",
       "  'summary': 'This paper proposes a generative moment matching network (GMMN)-based post-filter that provides inter-utterance pitch variation for deep neural network (DNN)-based singing voice synthesis. The natural pitch variation of a human singing voice leads to a richer musical experience and is used in double-tracking, a recording method in which two performances of the same phrase are recorded and mixed to create a richer, layered sound. However, singing voices synthesized using conventional DNN-based methods never vary because the synthesis process is deterministic and only one waveform is synthesized from one musical score. To address this problem, we use a GMMN to model the variation of the modulation spectrum of the pitch contour of natural singing voices and add a randomized inter-utterance variation to the pitch contour generated by conventional DNN-based singing voice synthesis. Experimental evaluations suggest that 1) our approach can provide perceptible inter-utterance pitch variation while preserving speech quality. We extend our approach to double-tracking, and the evaluation demonstrates that 2) GMMN-based neural double-tracking is perceptually closer to natural double-tracking than conventional signal processing-based artificial double-tracking is.',\n",
       "  'author': 'Hiroki Tamaru, Yuki Saito, Shinnosuke Takamichi, Tomoki Koriyama, Hiroshi Saruwatari',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.456},\n",
       " {'link': 'http://arxiv.org/abs/2209.12573v3',\n",
       "  'updated_ts': '2022-11-16T09:56:26Z',\n",
       "  'published_ts': '2022-09-26T10:38:39Z',\n",
       "  'title': 'Digital Audio Forensics: Blind Human Voice Mimicry Detection',\n",
       "  'summary': 'Audio is one of the most used way of human communication, but at the same time it can be easily misused by to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. Two datasets were created for this work; an all English data set and a mixed data set (Arabic and English). These datasets have been made available through GitHub for the use of the research community at https://github.com/SaSs7/Dataset. For the purpose of comparison, the audios were also classified through human inspection with the subjects being the native speakers. The ensued results were interesting and exhibited formidable accuracy.',\n",
       "  'author': 'Sahar Al Ajmi, Khizar Hayat, Alaa M. Al Obaidi, Naresh Kumar, Munaf Najmuldeen, Baptiste Magnier',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.453},\n",
       " {'link': 'http://arxiv.org/abs/2211.01839v1',\n",
       "  'updated_ts': '2022-11-03T14:20:32Z',\n",
       "  'published_ts': '2022-11-03T14:20:32Z',\n",
       "  'title': 'HyperSound: Generating Implicit Neural Representations of Audio Signals\\n  with Hypernetworks',\n",
       "  'summary': 'Implicit neural representations (INRs) are a rapidly growing research field, which provides alternative ways to represent multimedia signals. Recent applications of INRs include image super-resolution, compression of high-dimensional signals, or 3D rendering. However, these solutions usually focus on visual data, and adapting them to the audio domain is not trivial. Moreover, it requires a separately trained model for every data sample. To address this limitation, we propose HyperSound, a meta-learning method leveraging hypernetworks to produce INRs for audio signals unseen at training time. We show that our approach can reconstruct sound waves with quality comparable to other state-of-the-art models.',\n",
       "  'author': 'Filip Szatkowski, Karol J. Piczak, Przemysław Spurek, Jacek Tabor, Tomasz Trzciński',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.45},\n",
       " {'link': 'http://arxiv.org/abs/1608.04363v2',\n",
       "  'updated_ts': '2016-11-28T17:48:04Z',\n",
       "  'published_ts': '2016-08-15T18:57:10Z',\n",
       "  'title': 'Deep Convolutional Neural Networks and Data Augmentation for\\n  Environmental Sound Classification',\n",
       "  'summary': 'The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a \"shallow\" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model\\'s classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.',\n",
       "  'author': 'Justin Salamon, Juan Pablo Bello',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.45},\n",
       " {'link': 'http://arxiv.org/abs/2105.00173v2',\n",
       "  'updated_ts': '2021-07-04T07:34:14Z',\n",
       "  'published_ts': '2021-05-01T05:47:15Z',\n",
       "  'title': 'Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis\\n  Tool for Singers',\n",
       "  'summary': 'Current computational-emotion research has focused on applying acoustic properties to analyze how emotions are perceived mathematically or used in natural language processing machine learning models. While recent interest has focused on analyzing emotions from the spoken voice, little experimentation has been performed to discover how emotions are recognized in the singing voice -- both in noiseless and noisy data (i.e., data that is either inaccurate, difficult to interpret, has corrupted/distorted/nonsense information like actual noise sounds in this case, or has a low ratio of usable/unusable information). Not only does this ignore the challenges of training machine learning models on more subjective data and testing them with much noisier data, but there is also a clear disconnect in progress between advancing the development of convolutional neural networks and the goal of emotionally cognizant artificial intelligence. By training a new model to include this type of information with a rich comprehension of psycho-acoustic properties, not only can models be trained to recognize information within extremely noisy data, but advancement can be made toward more complex biofeedback applications -- including creating a model which could recognize emotions given any human information (language, breath, voice, body, posture) and be used in any performance medium (music, speech, acting) or psychological assistance for patients with disorders such as BPD, alexithymia, autism, among others. This paper seeks to reflect and expand upon the findings of related research and present a stepping-stone toward this end goal.',\n",
       "  'author': 'Daniel Szelogowski',\n",
       "  'category': 'cs.SD',\n",
       "  'similarity': 0.446},\n",
       " {'link': 'http://arxiv.org/abs/2110.03894v3',\n",
       "  'updated_ts': '2022-12-20T12:44:24Z',\n",
       "  'published_ts': '2021-10-08T05:07:35Z',\n",
       "  'title': 'Neural Model Reprogramming with Similarity Based Mapping for\\n  Low-Resource Spoken Command Classification',\n",
       "  'summary': 'In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of training data.',\n",
       "  'author': 'Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Yu Tsao',\n",
       "  'category': 'eess.AS',\n",
       "  'similarity': 0.444},\n",
       " {'link': 'http://arxiv.org/abs/1908.07656v2',\n",
       "  'updated_ts': '2019-12-01T03:30:37Z',\n",
       "  'published_ts': '2019-08-16T16:40:49Z',\n",
       "  'title': 'Survey on Deep Neural Networks in Speech and Vision Systems',\n",
       "  'summary': 'This survey presents a review of state-of-the-art deep neural network architectures, algorithms, and systems in vision and speech applications. Recent advances in deep artificial neural network algorithms and architectures have spurred rapid innovation and development of intelligent vision and speech systems. With availability of vast amounts of sensor data and cloud computing for processing and training of deep neural networks, and with increased sophistication in mobile and embedded technology, the next-generation intelligent systems are poised to revolutionize personal and commercial computing. This survey begins by providing background and evolution of some of the most successful deep learning models for intelligent vision and speech systems to date. An overview of large-scale industrial research and development efforts is provided to emphasize future trends and prospects of intelligent vision and speech systems. Robust and efficient intelligent systems demand low-latency and high fidelity in resource-constrained hardware platforms such as mobile devices, robots, and automobiles. Therefore, this survey also provides a summary of key challenges and recent successes in running deep neural networks on hardware-restricted platforms, i.e. within limited memory, battery life, and processing capabilities. Finally, emerging applications of vision and speech across disciplines such as affective computing, intelligent transportation, and precision medicine are discussed. To our knowledge, this paper provides one of the most comprehensive surveys on the latest developments in intelligent vision and speech applications from the perspectives of both software and hardware systems. Many of these emerging technologies using deep neural networks show tremendous promise to revolutionize research and development for future vision and speech systems.',\n",
       "  'author': 'Mahbubul Alam, Manar D. Samad, Lasitha Vidyaratne, Alexander Glandon, Khan M. Iftekharuddin',\n",
       "  'category': 'cs.CV',\n",
       "  'similarity': 0.437},\n",
       " {'link': 'http://arxiv.org/abs/2105.07142v2',\n",
       "  'updated_ts': '2021-08-26T00:47:33Z',\n",
       "  'published_ts': '2021-05-15T04:58:08Z',\n",
       "  'title': 'Move2Hear: Active Audio-Visual Source Separation',\n",
       "  'summary': \"We introduce the active audio-visual source separation problem, where an agent must move intelligently in order to better isolate the sounds coming from an object of interest in its environment. The agent hears multiple audio sources simultaneously (e.g., a person speaking down the hall in a noisy household) and it must use its eyes and ears to automatically separate out the sounds originating from a target object within a limited time budget. Towards this goal, we introduce a reinforcement learning approach that trains movement policies controlling the agent's camera and microphone placement over time, guided by the improvement in predicted audio separation quality. We demonstrate our approach in scenarios motivated by both augmented reality (system is already co-located with the target object) and mobile robotics (agent begins arbitrarily far from the target object). Using state-of-the-art realistic audio-visual simulations in 3D environments, we demonstrate our model's ability to find minimal movement sequences with maximal payoff for audio source separation. Project: http://vision.cs.utexas.edu/projects/move2hear.\",\n",
       "  'author': 'Sagnik Majumder, Ziad Al-Halah, Kristen Grauman',\n",
       "  'category': 'cs.CV',\n",
       "  'similarity': 0.432}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[['link', 'updated_ts', 'published_ts', 'title', 'summary', 'author', 'category', 'similarity']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488d869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:arxiv-scout]",
   "language": "python",
   "name": "conda-env-arxiv-scout-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
